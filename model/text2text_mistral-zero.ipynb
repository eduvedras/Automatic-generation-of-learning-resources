{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduvedras/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = load_dataset('eduvedras/Desc_Questions',split='test',trust_remote_code=True)\n",
    "#dataset[0]['Questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.82s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "#import huggingface_hub\n",
    "\n",
    "#huggingface_hub.login('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',True,False,False)\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "filetag = \"mistral-zeroshot\"\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "\n",
    "#model_aux = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\", torch_dtype = torch.float16, offload_folder=\"offload/\")\n",
    "#model_aux.config.use_cache = False\n",
    "#path = \"eduvedras/zephyr-7b-sft-lora\"  #  might be \"model_output\" for you\n",
    "#model = PeftModel.from_pretrained(model_aux, path, offload_folder = \"offload/\")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype = torch.float16)#.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|▋         | 1/15 [01:23<19:35, 83.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"An image showing a decision tree with depth = 2 where the first decision is made with the condition Pclass <= 2.5 and the second with the condition Parch <= 0.5.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"It is clear that variable Pclass is one of the five most relevant features.\".'}]\n",
      "---------\n",
      " The decision tree has a depth of 2 and the first split is based on the condition Pclass <= 2.5, followed by a second split with condition Parch <= 0.5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [04:04<27:59, 129.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A multi-line chart showing the overfitting of a mlp where the y-axis represents the accuracy and the x-axis represents the number of iterations ranging from 100 to 1000.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.\".'}]\n",
      "---------\n",
      " The chart indicates that as the number of iterations increases, the accuracy of the MLP model also increases but at a decreasing rate, eventually plateauing and then dropping. This behavior is indicative of overfitting. Therefore, the sentence would be:\n",
      "\n",
      "\"The MLP model exhibits overfitting when trained for more than approximately 700 iterations.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [06:27<27:05, 135.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A multi-line chart showing the overfitting of gradient boosting where the y-axis represents the accuracy and the x-axis represents the number of estimators ranging from 2 to 2002.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.\".'}]\n",
      "---------\n",
      " The accuracy of gradient boosting models increases as the number of estimators grows, but at some point, it starts to decrease due to overfitting, which can be observed when the number of estimators is greater than a certain value. (However, the chart does not provide enough information to determine the exact number of estimators at which overfitting begins.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [08:57<25:53, 141.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A multi-line chart showing the overfitting of random forest where the y-axis represents the accuracy and the x-axis represents the number of estimators ranging from 2 to 2002.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.\".'}]\n",
      "---------\n",
      " The chart shows that as the number of estimators in the Random Forest model increases, the accuracy also increases but at a decreasing rate, eventually reaching a plateau and then starting to decrease, indicating overfitting.\n",
      "\n",
      "False: The results for Random Forests with 10 estimators cannot be explained by underfitting based on the information provided in the chart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [10:40<21:11, 127.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A multi-line chart showing the overfitting of k-nearest neighbors where the y-axis represents the accuracy and the x-axis represents the number of neighbors ranging from 1 to 23.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"KNN is in overfitting for k less than 17.\".'}]\n",
      "---------\n",
      " The accuracy of KNN increases as the number of neighbors rises up to a certain point, after which it begins to overfit and the accuracy decreases. Specifically, overfitting occurs when the number of neighbors is greater than approximately 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [11:55<16:26, 109.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A multi-line chart showing the overfitting of a decision tree where the y-axis represents the accuracy and the x-axis represents the max depth ranging from 2 to 25.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.\".'}]\n",
      "---------\n",
      " According to the given chart, the decision tree exhibits increasing accuracy with increasing max depth up to a certain point, after which the accuracy starts to decrease, indicative of overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [13:26<13:47, 103.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A multi-line chart showing the overfitting of decision tree where the y-axis represents the performance of both accuracy and recall and the x-axis represents the max depth ranging from 2 to 25.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.\".'}]\n",
      "---------\n",
      " The performance of both accuracy and recall increases as the max depth of the decision tree grows, but the trend for accuracy is generally higher and more consistent, while recall shows greater variability and eventual decrease for deeper trees, indicating overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [18:10<18:47, 161.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A bar chart showing the explained variance ratio of 5 principal components.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"The first 4 principal components are enough for explaining half the data variance.\".'}]\n",
      "---------\n",
      " Based on the given data chart, the five principal components collectively explain a certain percentage of the total data variance. The exact percentage explained by each principal component can be determined by reading the chart.\n",
      "\n",
      "Here's a possible true or false sentence: \"The five principal components in this bar chart explain more than 90% of the data variance.\" (Answer: This depends on the specific data in the chart, so it cannot be definitively answered as true or false without examining the chart itself.)\n",
      "\n",
      "Note that generating a sentence without knowing the specific content of the chart can be a challenge, so the example given above is based on the general context of a bar chart showing the explained variance ratio of principal components.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [19:25<13:23, 133.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A heatmap showing the correlation between the variables of the dataset. The variables are [\\'Pclass\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Fare\\'].\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"The intrinsic dimensionality of this dataset is 2.\".'}]\n",
      "---------\n",
      " The heatmap shows the correlation degrees among the variables 'Pclass', 'Age', 'SibSp', 'Parch', and 'Fare' in the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [23:05<13:22, 160.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A set of boxplots of the variables [\\'Pclass\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Fare\\'].\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"Variable Fare is balanced.\".'}]\n",
      "---------\n",
      " It is not possible to generate a true or false sentence based on the given description as there is no specific information provided about the data Chart itself, such as what the boxplots show in terms of the distribution, outliers, or comparisons between variables. Boxplots are used to visually analyze and compare the distribution of continuous data, so sentences about balance, spread, or quartiles may be appropriate depending on the specific characteristics of the data. Without further context, any generated sentence would be speculative or misleading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [23:49<08:20, 125.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A set of bar charts of the variables [\\'Embarked\\', \\'Sex\\'].\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"All variables, but the class, should be dealt with as date.\".'}]\n",
      "---------\n",
      " The bar charts provide information on the distributions of the Embarked and Sex variables in the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [24:34<05:01, 100.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A bar chart showing the number of missing values per variable of the dataset. The variables that have missing values are: [\\'Age\\', \\'Embarked\\'].\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"Discarding variable Age would be better than discarding all the records with missing values for that variable.\".'}]\n",
      "---------\n",
      " The bar chart indicates that both variables 'Age' and 'Embarked' have missing values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [25:50<03:06, 93.09s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A bar chart showing the distribution of the target variable Survived.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"Balancing this dataset would be mandatory to improve the results.\".'}]\n",
      "---------\n",
      " The chart indicates the number of passengers who survived the Titanic sinking by class. (Assuming the chart is titled appropriately and the classes are clear from the bars.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [29:08<02:04, 124.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A bar chart showing the number of records and variables of the dataset.\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.\".'}]\n",
      "---------\n",
      " The bar chart provides information on the number of records and variables in the dataset, but it does not provide enough details to make the sentence \"Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.\" true or false. Without additional context or information about the nature of the variables, it is not possible to determine if the curse of dimensionality is a concern. Therefore, the sentence is neither true nor false based on the given data chart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [32:28<00:00, 129.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Consider the following data chart \"A set of histograms of the variables [\\'Pclass\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Fare\\'].\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\"All variables, but the class, should be dealt with as date.\".'}]\n",
      "---------\n",
      " I cannot generate a sentence based on the given data chart alone, as I cannot see the actual data or the specific information contained in the histograms. Histograms display the frequency distribution of continuous data, and in this case, the variables are 'Pclass', 'Age', 'SibSp', 'Parch', and 'Fare'. Without additional context or specific information, it is impossible to make a true or false statement about these variables based on the histograms alone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "predictions = []\n",
    "references = []\n",
    "for i in tqdm(range(len(dataset))):\n",
    "#i=14\n",
    "    inputs = []\n",
    "    questions = dataset[i][\"Questions\"][2:-2].split(\"\\', \\'\")\n",
    "    inputs.append({\"role\": \"user\", \"content\": \"Consider the following data chart \\\"\" +dataset[i][\"Description\"] + \"\\\", generate a true or false sentence based on this data chart. In your answer only generate the sentence. As an example consider the following sentence:\\\"\" + questions[0]+ \"\\\".\"})\n",
    "    model_inputs = tokenizer.apply_chat_template(inputs, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=200, do_sample=True)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    result = decoded\n",
    "    result = decoded[0].split(\"[/INST]\")[1] \n",
    "    result = result.split(\"</s>\")[0]\n",
    "    #result = result[0].split(\"</s>\")[1]\n",
    "    \n",
    "    predictions.append(result)\n",
    "    references.append(dataset[i][\"Questions\"][2:-2].split('\", \"'))\n",
    "    print(inputs)\n",
    "    print(\"---------\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "predictions_final = []\n",
    "\n",
    "for el in predictions:\n",
    "    preds = el.split(\"\\\"\")\n",
    "    if len(preds) == 1:\n",
    "        preds = el.split(\"(\")[0]\n",
    "        predictions_final.append(preds)\n",
    "    else:\n",
    "        if \"(\" in preds[0]:\n",
    "            predictions_final.append(preds[0].split(\" (\")[0])\n",
    "        else:\n",
    "            if len(preds[1].split(\" \")) == 1:\n",
    "                predictions_final.append(el)\n",
    "            else:\n",
    "                predictions_final.append(preds[1])\n",
    "        \n",
    "predictions_final =[]\n",
    "predictions_final = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "file = open(f\"./resultados-ger-perguntas/{filetag}.txt\", \"a\")\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions_final, references=references)\n",
    "print(results)\n",
    "file.write(f\"BLEU: {results}\\n\")\n",
    "\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "results = meteor.compute(predictions=predictions_final, references=references)\n",
    "print(results)\n",
    "file.write(f\"METEOR: {results}\\n\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(predictions=predictions_final, references=references)\n",
    "print(results)\n",
    "file.write(f\"ROUGE: {results}\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChart_name\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m&\u001b[39m (new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[43mpredictions_final\u001b[49m[i]))\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m      8\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_final' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_df = pd.DataFrame(columns=['Image','Prediction','References'])\n",
    "\n",
    "i=0\n",
    "while i < len(dataset):\n",
    "    if ((new_df['Image'] == dataset[i]['Chart_name']) & (new_df['Prediction'] == predictions_final[i])).any():\n",
    "        i += 1\n",
    "        continue\n",
    "    else:\n",
    "        new_df.loc[len(new_df)] = {'Image': dataset[i]['Chart_name'], 'Prediction': predictions_final[i], 'References': references[i]}\n",
    "        i += 1\n",
    "    \n",
    "new_df.to_csv(f'./resultados-ger-perguntas/{filetag}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
