Template;Space1;Space2;Space3;Space4;Category;Sub-Category;Charts
Given the number of records and that some variables are [1], we might be facing the curse of dimensionality.;[numeric,binary,date,symbolic];;;;Dimensionality;Curse of dimensionality;[Nr records x nr variables]
We face the curse of dimensionality when training a classifier with this dataset.;;;;;Dimensionality;Curse of dimensionality;[Nr records x nr variables]
The intrinsic dimensionality of this dataset is [1].;[10..100];;;;Dimensionality;Intrinsic dimensionality;[Correlation heatmap]
The figure doesn’t show any missing values for [1], but these may be hidden as some non-pre-identified value.;[<all_variables>];;;;Dimensionality;Missing values;[Histograms + MV]
All variables, but the class, should be dealt with as [1].;[numeric,binary,date,symbolic];;;;Dimensionality;Variable type;[Scatter-plots,All_Histograms]
The variable [1] can be seen as ordinal.;[<all_variables>];;;;Dimensionality;Variable type;[Scatter-plots,All_Histograms]
The variable [1] can be seen as ordinal without losing information.;[<all_variables>];;;;Dimensionality;Variable type;[Scatter-plots,All_Histograms]
Variable [1] is balanced.;[<variables>];;;;Distribution;Balancing;[Histograms,Boxplots]
Those boxplots show that the data is not normalized.;;;;;Distribution;Data scale;[Boxplot global,Single boxplots]
It is clear that variable [1] shows some outliers, but we can’t be sure of the same for variable [2].;[<variables>];[<variables>];;;Distribution;Outliers;[Boxplots,Histograms]
Outliers seem to be a problem in the dataset.;;;;;Distribution;Outliers;[Boxplots,Histograms]
Variable [1] shows [2] outlier values.;[<variables>];[some,a high number of];;;Distribution;Outliers;[Boxplots,Histograms]
Variable [1] doesn’t have any outliers.;[<variables>];;;;Distribution;Outliers;[Boxplots,Histograms]
Variable [1] presents some outliers.;[<variables>];;;;Distribution;Outliers;[Boxplots,Histograms]
At least [1] of the variables present outliers.;[50,60,75,85];;;;Distribution;Outliers;[Boxplots,Histograms]
The [1] presented show a large number of outliers for most of the numeric variables.;[boxplots,histograms];;;;Distribution;Outliers;[Boxplots,Histograms]
The existence of outliers is one of the problems to tackle in this dataset.;;;;;Distribution;Outliers;[Boxplots,Histograms]
Variable [1] is a false predictor.;[<all_variables>,<class>];;;;Feature importance;Data leakage;[Description]
One of the variables [1] or [2] can be discarded without losing information.;[<variables>];[<variables>];;;Feature importance;Redundancy;[Correlation heatmap]
The variable [1] can be discarded without risking losing information.;[<variables>];;;;Feature importance;Redundancy;[Correlation heatmap]
Variables [1] and [2] are redundant, but we can’t say the same for the pair [3] and [4].;[<variables>];[<variables>];[<variables>];[<variables>];Feature importance;Redundancy;[Correlation heatmap]
Variables [1] and [2] are redundant.;[<variables>];[<variables>];;;Feature importance;Redundancy;[Correlation heatmap]
From the correlation analysis alone, it is clear that there are relevant variables.;;;;;Feature importance;Relevancy;[Correlation heatmap]
It is clear that variable [1] is one of the [2] most relevant features.;[<variables>];[two,three,four,five];;;Feature importance;Relevancy;[Decision tree]
The variable [1] seems to be one of the [2] most relevant features.;[<variables>];[two,three,four,five];;;Feature importance;Relevancy;[Decision tree]
The variable [1] discriminates between the target values, as shown in the decision tree.;[<variables>];;;;Feature importance;Relevancy;[Decision tree]
It is possible to state that [1] is the [2] most discriminative variable regarding the class.;[<variables>];[first,second];;;Feature importance;Relevancy;[Decision tree]
Variable [1] is one of the most relevant variables.;[<variables>];;;;Feature importance;Relevancy;[Decision tree]
Variable [1] seems to be relevant for the majority of mining tasks.;[<variables>];;;;Feature importance;Relevancy;[Decision tree,Correlation heatmap]
Variables [1] and [2] seem to be useful for classification tasks.;[<variables>];[<variables>];;;Feature importance;Relevancy;[Decision tree,Correlation heatmap]
A scaling transformation is mandatory, in order to improve the [1] performance in this dataset.;[Naive Bayes, KNN];;;;Scaling;;[Boxplot global,Single boxplots]
Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;;;;;Feature selection;All classifiers;[Correlation heatmap]
Balancing this dataset by SMOTE would be riskier than oversampling by replication.;;;;;Balancing;;[Scatter-plots]
Balancing this dataset by SMOTE would most probably be preferable over undersampling.;;;;;Balancing;;[Nr records x nr variables]
Balancing this dataset would be mandatory to improve the results.;;;;;;;[Class histogram]
Both [1] and [2] variables could be used to derive a new variable using a concept hierarchy.;[<all_variables>];[<all_variables>];;;Feature generation;;[Description]
The generation of a new feature through the conjunction of variables, would require some domain knowledge.;;;;;Feature generation;;[Description]
Considering the common semantics for [1] and [2] variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[<all_variables>];[<all_variables>];;;Variable encoding;Dummification;[Description,All_Histograms]
Considering the common semantics for [1] variable, dummification would be the most adequate encoding.;[<all_variables>];;;;Variable encoding;Dummification;[Description,All_Histograms]
The variable [1] can be coded as ordinal without losing information.;[<all_variables>];;;;Variable encoding;;[All_Histograms]
Discarding variables [1] and [2] would be better than discarding all the records with missing values for those variables.;[<all_variables>];[<all_variables>];;;MVI;;[Missing values]
Dropping all records with missing values would be better than to drop the variables with missing values.;;;;;MVI;;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than [1]% of the original data.;[25,30,40];;;;MVI;;[Missing values]
There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;;;;;MVI;;[Missing values]
Dummification is mandatory in this dataset.;;;;;Variable encoding;Dummification;[]
Dummifying the variables will improve the mining results.;;;;;Variable encoding;Dummification;[]
Feature generation based on both variables [1] and [2] seems to be promising.;[<all_variables>];[<all_variables>];;;Feature generation;;[Missing values,All_Histograms]
Feature generation based on the use of variable [1] wouldn’t be useful, but the use of [2] seems to be promising.;[<all_variables>];[<all_variables>];;;Feature generation;;[Missing values,All_Histograms]
Given the usual semantics of [1] variable, dummification would have been a better codification.;[<all_variables>];;;;Variable encoding;Dummification;[All_Histograms]
If [1] and [2] were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[<variables>];[<variables>];;;Feature selection;KNN;[]
It is better to drop the variable [1] than removing all records with missing values.;[<all_variables>];;;;MVI;;[Missing values,All_Histograms]
Knowing that C and F are strongly correlated (correlation=1), we can say that removing one of those variables, would not have any impact on the performance of a KNN classifier.;;;;;Feature selection;KNN;[]
Missing value imputation using the mean value per class improves the quality of discovered patterns.;;;;;MVI;;[]
Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;;;;;Scaling;;[Boxplots]
Normalization of this dataset could not have impact on a KNN classifier.;;;;;Scaling;KNN;[Boxplot global,Single boxplots]
Normalization of this dataset should have a high impact on naïve Bayes classifier.;;;;;Scaling;NB;[]
Scaling this dataset should lead to a faster training with multi-layer perceptron.;;;;;Scaling;MLP;[]
Not knowing the semantics of [1] variable, dummification could have been a more adequate codification.;[<all_variables>];;;;Variable encoding;Dummification;[All_Histograms]
Removing the [1] variable from the training will improve model performance over any non-observed records.;[<all_variables>,<class>];;;;Feature selection;Data leakage;[Description]
Removing variable [1] might improve the training of decision trees .;[<variables>];;;;Feature selection;Redundancy;[Correlation heatmap]
Scaling this dataset would be mandatory to improve the results with distance-based methods.;;;;;Scaling;KNN;[Boxplots]
The first [1] principal components are enough for explaining half the data variance.;[2,3,4];;;;Feature extraction;;[PCA]
There is evidence in favour for sequential backward selection to select variable [1] previously than variable [2].;[<variables>];[<variables>];;;Feature selection;Algorithms;[Correlation heatmap]
Using the first [1] principal components would imply an error between [2] and [3]%.;[2,3,4];[5,10,15];[20,25,30];;Feature extraction;;[PCA]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of [1]%.;[5..10];;;;Classification;Decision tree;[Decision tree]
According to the decision tree overfitting chart, the tree with [1] nodes of depth is in overfitting.;[3..8];;;;Overfitting;Decision tree;[Overfitting Decision Tree]
According to the charts, KNN and Decision Trees present a similar behaviour.;;;;;Overfitting;Decision tree + KNN;[Overfitting Decision Tree + Overfitting KNN]
As reported in the tree, the number of False Positive is [1] than the number of False Negatives.;[smaller,bigger];;;;Evaluation;;[Decision tree]
Decision trees and KNN show similar behaviours.;;;;;Overfitting;;[Overfitting Decision Tree + Overfitting KNN]
KNN and Decision Trees show a different trend in the majority of hyperparameters tested.;;;;;Overfitting;;[Overfitting Decision Tree + Overfitting KNN]
KNN is in overfitting for k [1] than [2].;[less,larger];[2..8];;;Overfitting;;[Overfitting KNN]
KNN with [1] neighbour is in overfitting.;[1..10];;;;Overfitting;;[Overfitting KNN]
KNN with [1] than [2] neighbours is in overfitting.;[more,less];[2..8];;;Overfitting;;[Overfitting KNN]
Results for Random Forests identified as [1], may be explained by its estimators being in [2].;[2,3,10];[underfitting,overfitting];;;Overfitting;;[Overfitting RF]
The [1] for the presented tree is [2] than [3]%.;[accuracy,recall,precision,specificity];[higher,lower];[60..90];;Evaluation;;[Decision tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than [1].;[3,4,5,6,7,8,9,10];;;;Overfitting;;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above [1].;[3,4,5,6,7,8,9,10];;;;Overfitting;;[Overfitting Decision Tree]
The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;;;;;Overfitting;;[Overfitting Accuracy + Recall]
The number of [1] reported in the same tree is [2].;[True Positives,False Positives,True Negatives,False Negatives];[10..50];;;Evaluation;;[Decision tree]
The number of [1] is [2] than the number of [3] for the presented tree.;[True Positives,False Positives,True Negatives,False Negatives];[higher,lower];[True Positives,False Positives,True Negatives,False Negatives];;Evaluation;;[Decision tree]
The [1] for the presented tree is [2] than its [3].;[accuracy,recall,precision,specificity];[higher,lower];[accuracy,recall,precision,specificity];;Evaluation;;[Decision tree]
The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;;;;;Overfitting;;[Overfitting RF]
We are able to identify the existence of overfitting for decision tree models with more than [1] nodes of depth.;[2,3,4,5,6];;;;Overfitting;;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for KNN models with less than [1] neighbors.;[2,3,4,5,6];;;;Overfitting;;[Overfitting KNN]
We are able to identify the existence of overfitting for random forest models with more than [1] estimators.;[50..150];;;;Overfitting;;[Overfitting RF]
We are able to identify the existence of overfitting for gradient boosting models with more than [1] estimators.;[50..150];;;;Overfitting;;[Overfitting GB]
We are able to identify the existence of overfitting for MLP models trained longer than [1] episodes.;[500..1500];;;;Overfitting;;[Overfitting MLP]
Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], the Decision Tree presented classifies [1] as [2].;[(A,B)/(not A, B)/(A, not B)/(not A, not B)];[<target-values>];;;Evaluation;;[Decision tree]
Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], it is possible to state that KNN algorithm classifies [1] as [2] for any k ≤ [3].;[(A,B)/(not A, B)/(A, not B)/(not A, not B)];[<target-values>];[<neighbors>];;Evaluation;;[Decision tree]
Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], it is possible to state that Naive Bayes algorithm classifies [1], as [2].;[(A,B)/(not A, B)/(A, not B)/(not A, not B)];[<target-values>];;;Evaluation;;[Decision tree]