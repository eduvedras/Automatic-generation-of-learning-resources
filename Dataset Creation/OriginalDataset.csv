Id;Chart;Question
0;ObesityDataSet_decision_tree.png;The variable FAF discriminates between the target values, as shown in the decision tree.
1;ObesityDataSet_decision_tree.png;Variable Height is one of the most relevant variables.
2;ObesityDataSet_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
3;ObesityDataSet_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
4;ObesityDataSet_decision_tree.png;The specificity for the presented tree is higher than 75%.
5;ObesityDataSet_decision_tree.png;The number of True Negatives reported in the same tree is 50.
6;ObesityDataSet_decision_tree.png;The number of False Positives is lower than the number of False Negatives for the presented tree.
7;ObesityDataSet_decision_tree.png;The variable FAF seems to be one of the two most relevant features.
8;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that Naive Bayes algorithm classifies (not A, B), as Overweight_Level_I.
9;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], the Decision Tree presented classifies (A, not B) as Obesity_Type_III.
10;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that KNN algorithm classifies (A, not B) as Insufficient_Weight for any k ≤ 160.
11;ObesityDataSet_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
12;ObesityDataSet_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
13;ObesityDataSet_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.
14;ObesityDataSet_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
15;ObesityDataSet_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
16;ObesityDataSet_overfitting_knn.png;KNN is in overfitting for k larger than 17.
17;ObesityDataSet_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
18;ObesityDataSet_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.
19;ObesityDataSet_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
20;ObesityDataSet_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.
21;ObesityDataSet_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.
22;ObesityDataSet_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.
23;ObesityDataSet_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.
24;ObesityDataSet_pca.png;The first 7 principal components are enough for explaining half the data variance.
25;ObesityDataSet_pca.png;Using the first 7 principal components would imply an error between 15 and 20%.
26;ObesityDataSet_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.
27;ObesityDataSet_correlation_heatmap.png;One of the variables Age or Height can be discarded without losing information.
28;ObesityDataSet_correlation_heatmap.png;The variable Weight can be discarded without risking losing information.
29;ObesityDataSet_correlation_heatmap.png;Variables NCP and TUE are redundant, but we can’t say the same for the pair Weight and Height.
30;ObesityDataSet_correlation_heatmap.png;Variables FAF and TUE are redundant.
31;ObesityDataSet_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
32;ObesityDataSet_correlation_heatmap.png;Variable Height seems to be relevant for the majority of mining tasks.
33;ObesityDataSet_correlation_heatmap.png;Variables FAF and Height seem to be useful for classification tasks.
34;ObesityDataSet_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
35;ObesityDataSet_correlation_heatmap.png;Removing variable CH2O might improve the training of decision trees .
36;ObesityDataSet_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable Height.
37;ObesityDataSet_boxplots.png;Variable CH2O is balanced.
38;ObesityDataSet_boxplots.png;Those boxplots show that the data is not normalized.
39;ObesityDataSet_boxplots.png;It is clear that variable FCVC shows some outliers, but we can’t be sure of the same for variable TUE.
40;ObesityDataSet_boxplots.png;Outliers seem to be a problem in the dataset.
41;ObesityDataSet_boxplots.png;Variable FAF shows some outlier values.
42;ObesityDataSet_boxplots.png;Variable NCP doesn’t have any outliers.
43;ObesityDataSet_boxplots.png;Variable Height presents some outliers.
44;ObesityDataSet_boxplots.png;At least 75 of the variables present outliers.
45;ObesityDataSet_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
46;ObesityDataSet_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
47;ObesityDataSet_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
48;ObesityDataSet_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
49;ObesityDataSet_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
50;ObesityDataSet_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
51;ObesityDataSet_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.
52;ObesityDataSet_histograms_symbolic.png;The variable SMOKE can be seen as ordinal.
53;ObesityDataSet_histograms_symbolic.png;The variable FAVC can be seen as ordinal without losing information.
54;ObesityDataSet_histograms_symbolic.png;Considering the common semantics for FAVC and CAEC variables, dummification if applied would increase the risk of facing the curse of dimensionality.
55;ObesityDataSet_histograms_symbolic.png;Considering the common semantics for family_history_with_overweight variable, dummification would be the most adequate encoding.
56;ObesityDataSet_histograms_symbolic.png;The variable MTRANS can be coded as ordinal without losing information.
57;ObesityDataSet_histograms_symbolic.png;Feature generation based on variable family_history_with_overweight seems to be promising.
58;ObesityDataSet_histograms_symbolic.png;Feature generation based on the use of variable SCC wouldn’t be useful, but the use of CAEC seems to be promising.
59;ObesityDataSet_histograms_symbolic.png;Given the usual semantics of family_history_with_overweight variable, dummification would have been a better codification.
60;ObesityDataSet_histograms_symbolic.png;It is better to drop the variable CALC than removing all records with missing values.
61;ObesityDataSet_histograms_symbolic.png;Not knowing the semantics of family_history_with_overweight variable, dummification could have been a more adequate codification.
62;ObesityDataSet_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
63;ObesityDataSet_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.
64;ObesityDataSet_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
65;ObesityDataSet_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
66;ObesityDataSet_histograms_numeric.png;All variables, but the class, should be dealt with as symbolic.
67;ObesityDataSet_histograms_numeric.png;The variable Height can be seen as ordinal.
68;ObesityDataSet_histograms_numeric.png;The variable NCP can be seen as ordinal without losing information.
69;ObesityDataSet_histograms_numeric.png;Variable FAF is balanced.
70;ObesityDataSet_histograms_numeric.png;It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable CH2O.
71;ObesityDataSet_histograms_numeric.png;Outliers seem to be a problem in the dataset.
72;ObesityDataSet_histograms_numeric.png;Variable Height shows a high number of outlier values.
73;ObesityDataSet_histograms_numeric.png;Variable TUE doesn’t have any outliers.
74;ObesityDataSet_histograms_numeric.png;Variable FCVC presents some outliers.
75;ObesityDataSet_histograms_numeric.png;At least 60 of the variables present outliers.
76;ObesityDataSet_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
77;ObesityDataSet_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
78;ObesityDataSet_histograms_numeric.png;Considering the common semantics for Weight and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.
79;ObesityDataSet_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.
80;ObesityDataSet_histograms_numeric.png;The variable Weight can be coded as ordinal without losing information.
81;ObesityDataSet_histograms_numeric.png;Feature generation based on variable TUE seems to be promising.
82;ObesityDataSet_histograms_numeric.png;Feature generation based on the use of variable Weight wouldn’t be useful, but the use of Age seems to be promising.
83;ObesityDataSet_histograms_numeric.png;Given the usual semantics of FAF variable, dummification would have been a better codification.
84;ObesityDataSet_histograms_numeric.png;It is better to drop the variable Age than removing all records with missing values.
85;ObesityDataSet_histograms_numeric.png;Not knowing the semantics of CH2O variable, dummification could have been a more adequate codification.
86;customer_segmentation_decision_tree.png;The variable Family_Size discriminates between the target values, as shown in the decision tree.
87;customer_segmentation_decision_tree.png;Variable Work_Experience is one of the most relevant variables.
88;customer_segmentation_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
89;customer_segmentation_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
90;customer_segmentation_decision_tree.png;The precision for the presented tree is lower than 60%.
91;customer_segmentation_decision_tree.png;The number of True Negatives is higher than the number of False Positives for the presented tree.
92;customer_segmentation_decision_tree.png;The number of True Negatives is higher than the number of True Positives for the presented tree.
93;customer_segmentation_decision_tree.png;The recall for the presented tree is higher than its accuracy.
94;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A,B) as B for any k ≤ 11.
95;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A, not B) as C for any k ≤ 723.
96;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (not A, B) as B for any k ≤ 524.
97;customer_segmentation_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
98;customer_segmentation_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
99;customer_segmentation_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.
100;customer_segmentation_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
101;customer_segmentation_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
102;customer_segmentation_overfitting_knn.png;KNN is in overfitting for k larger than 13.
103;customer_segmentation_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
104;customer_segmentation_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.
105;customer_segmentation_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.
106;customer_segmentation_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
107;customer_segmentation_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.
108;customer_segmentation_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.
109;customer_segmentation_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
110;customer_segmentation_pca.png;The first 2 principal components are enough for explaining half the data variance.
111;customer_segmentation_pca.png;Using the first 2 principal components would imply an error between 10 and 20%.
112;customer_segmentation_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
113;customer_segmentation_correlation_heatmap.png;One of the variables Age or Family_Size can be discarded without losing information.
114;customer_segmentation_correlation_heatmap.png;The variable Age can be discarded without risking losing information.
115;customer_segmentation_correlation_heatmap.png;Variables Age and Work_Experience seem to be useful for classification tasks.
116;customer_segmentation_correlation_heatmap.png;Variables Age and Work_Experience are redundant.
117;customer_segmentation_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
118;customer_segmentation_correlation_heatmap.png;Variable Family_Size seems to be relevant for the majority of mining tasks.
119;customer_segmentation_correlation_heatmap.png;Variables Family_Size and Work_Experience seem to be useful for classification tasks.
120;customer_segmentation_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
121;customer_segmentation_correlation_heatmap.png;Removing variable Work_Experience might improve the training of decision trees .
122;customer_segmentation_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable Family_Size.
123;customer_segmentation_boxplots.png;Variable Family_Size is balanced.
124;customer_segmentation_boxplots.png;Those boxplots show that the data is not normalized.
125;customer_segmentation_boxplots.png;It is clear that variable Work_Experience shows some outliers, but we can’t be sure of the same for variable Family_Size.
126;customer_segmentation_boxplots.png;Outliers seem to be a problem in the dataset.
127;customer_segmentation_boxplots.png;Variable Work_Experience shows a high number of outlier values.
128;customer_segmentation_boxplots.png;Variable Work_Experience doesn’t have any outliers.
129;customer_segmentation_boxplots.png;Variable Age presents some outliers.
130;customer_segmentation_boxplots.png;At least 50 of the variables present outliers.
131;customer_segmentation_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
132;customer_segmentation_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
133;customer_segmentation_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
134;customer_segmentation_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
135;customer_segmentation_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
136;customer_segmentation_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
137;customer_segmentation_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.
138;customer_segmentation_histograms_symbolic.png;The variable Gender can be seen as ordinal.
139;customer_segmentation_histograms_symbolic.png;The variable Ever_Married can be seen as ordinal without losing information.
140;customer_segmentation_histograms_symbolic.png;Considering the common semantics for Var_1 and Profession variables, dummification if applied would increase the risk of facing the curse of dimensionality.
141;customer_segmentation_histograms_symbolic.png;Considering the common semantics for Profession variable, dummification would be the most adequate encoding.
142;customer_segmentation_histograms_symbolic.png;The variable Graduated can be coded as ordinal without losing information.
143;customer_segmentation_histograms_symbolic.png;Feature generation based on variable Gender seems to be promising.
144;customer_segmentation_histograms_symbolic.png;Feature generation based on the use of variable Graduated wouldn’t be useful, but the use of Profession seems to be promising.
145;customer_segmentation_histograms_symbolic.png;Given the usual semantics of Profession variable, dummification would have been a better codification.
146;customer_segmentation_histograms_symbolic.png;It is better to drop the variable Graduated than removing all records with missing values.
147;customer_segmentation_histograms_symbolic.png;Not knowing the semantics of Spending_Score variable, dummification could have been a more adequate codification.
148;customer_segmentation_mv.png;Discarding variable Var_1 would be better than discarding all the records with missing values for that variable.
149;customer_segmentation_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.
150;customer_segmentation_mv.png;Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.
151;customer_segmentation_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.
152;customer_segmentation_mv.png;Feature generation based on variable Var_1 seems to be promising.
153;customer_segmentation_mv.png;It is better to drop the variable Family_Size than removing all records with missing values.
154;customer_segmentation_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
155;customer_segmentation_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.
156;customer_segmentation_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
157;customer_segmentation_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
158;customer_segmentation_histograms_numeric.png;All variables, but the class, should be dealt with as symbolic.
159;customer_segmentation_histograms_numeric.png;The variable Family_Size can be seen as ordinal.
160;customer_segmentation_histograms_numeric.png;The variable Age can be seen as ordinal without losing information.
161;customer_segmentation_histograms_numeric.png;Variable Family_Size is balanced.
162;customer_segmentation_histograms_numeric.png;It is clear that variable Work_Experience shows some outliers, but we can’t be sure of the same for variable Age.
163;customer_segmentation_histograms_numeric.png;Outliers seem to be a problem in the dataset.
164;customer_segmentation_histograms_numeric.png;Variable Age shows some outlier values.
165;customer_segmentation_histograms_numeric.png;Variable Family_Size doesn’t have any outliers.
166;customer_segmentation_histograms_numeric.png;Variable Work_Experience presents some outliers.
167;customer_segmentation_histograms_numeric.png;At least 75 of the variables present outliers.
168;customer_segmentation_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
169;customer_segmentation_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
170;customer_segmentation_histograms_numeric.png;Considering the common semantics for Family_Size and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.
171;customer_segmentation_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.
172;customer_segmentation_histograms_numeric.png;The variable Age can be coded as ordinal without losing information.
173;customer_segmentation_histograms_numeric.png;Feature generation based on variable Work_Experience seems to be promising.
174;customer_segmentation_histograms_numeric.png;Feature generation based on the use of variable Work_Experience wouldn’t be useful, but the use of Age seems to be promising.
175;customer_segmentation_histograms_numeric.png;Given the usual semantics of Age variable, dummification would have been a better codification.
176;customer_segmentation_histograms_numeric.png;It is better to drop the variable Age than removing all records with missing values.
177;customer_segmentation_histograms_numeric.png;Not knowing the semantics of Family_Size variable, dummification could have been a more adequate codification.
178;urinalysis_tests_decision_tree.png;The variable Age discriminates between the target values, as shown in the decision tree.
179;urinalysis_tests_decision_tree.png;Variable Age is one of the most relevant variables.
180;urinalysis_tests_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.
181;urinalysis_tests_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
182;urinalysis_tests_decision_tree.png;The specificity for the presented tree is higher than 60%.
183;urinalysis_tests_decision_tree.png;The number of True Positives reported in the same tree is 10.
184;urinalysis_tests_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree.
185;urinalysis_tests_decision_tree.png;The recall for the presented tree is lower than its specificity.
186;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as NEGATIVE.
187;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as POSITIVE.
188;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], it is possible to state that KNN algorithm classifies (not A, B) as NEGATIVE for any k ≤ 763.
189;urinalysis_tests_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
190;urinalysis_tests_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
191;urinalysis_tests_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.
192;urinalysis_tests_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
193;urinalysis_tests_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
194;urinalysis_tests_overfitting_knn.png;KNN is in overfitting for k larger than 5.
195;urinalysis_tests_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
196;urinalysis_tests_overfitting_knn.png;KNN with less than 17 neighbours is in overfitting.
197;urinalysis_tests_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.
198;urinalysis_tests_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.
199;urinalysis_tests_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.
200;urinalysis_tests_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.
201;urinalysis_tests_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
202;urinalysis_tests_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
203;urinalysis_tests_pca.png;The first 2 principal components are enough for explaining half the data variance.
204;urinalysis_tests_pca.png;Using the first 2 principal components would imply an error between 5 and 20%.
205;urinalysis_tests_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
206;urinalysis_tests_correlation_heatmap.png;One of the variables pH or Age can be discarded without losing information.
207;urinalysis_tests_correlation_heatmap.png;The variable Age can be discarded without risking losing information.
208;urinalysis_tests_correlation_heatmap.png;Variables Specific Gravity and Age seem to be useful for classification tasks.
209;urinalysis_tests_correlation_heatmap.png;Variables Age and pH are redundant.
210;urinalysis_tests_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
211;urinalysis_tests_correlation_heatmap.png;Variable Specific Gravity seems to be relevant for the majority of mining tasks.
212;urinalysis_tests_correlation_heatmap.png;Variables Specific Gravity and pH seem to be useful for classification tasks.
213;urinalysis_tests_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
214;urinalysis_tests_correlation_heatmap.png;Removing variable pH might improve the training of decision trees .
215;urinalysis_tests_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable pH.
216;urinalysis_tests_boxplots.png;Variable pH is balanced.
217;urinalysis_tests_boxplots.png;Those boxplots show that the data is not normalized.
218;urinalysis_tests_boxplots.png;It is clear that variable Specific Gravity shows some outliers, but we can’t be sure of the same for variable Age.
219;urinalysis_tests_boxplots.png;Outliers seem to be a problem in the dataset.
220;urinalysis_tests_boxplots.png;Variable Specific Gravity shows a high number of outlier values.
221;urinalysis_tests_boxplots.png;Variable Age doesn’t have any outliers.
222;urinalysis_tests_boxplots.png;Variable Age presents some outliers.
223;urinalysis_tests_boxplots.png;At least 60 of the variables present outliers.
224;urinalysis_tests_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
225;urinalysis_tests_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
226;urinalysis_tests_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
227;urinalysis_tests_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
228;urinalysis_tests_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
229;urinalysis_tests_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
230;urinalysis_tests_histograms_symbolic.png;All variables, but the class, should be dealt with as symbolic.
231;urinalysis_tests_histograms_symbolic.png;The variable Gender can be seen as ordinal.
232;urinalysis_tests_histograms_symbolic.png;The variable Mucous Threads can be seen as ordinal without losing information.
233;urinalysis_tests_histograms_symbolic.png;Considering the common semantics for Epithelial Cells and Color variables, dummification if applied would increase the risk of facing the curse of dimensionality.
234;urinalysis_tests_histograms_symbolic.png;Considering the common semantics for Amorphous Urates variable, dummification would be the most adequate encoding.
235;urinalysis_tests_histograms_symbolic.png;The variable Color can be coded as ordinal without losing information.
236;urinalysis_tests_histograms_symbolic.png;Feature generation based on variable Amorphous Urates seems to be promising.
237;urinalysis_tests_histograms_symbolic.png;Feature generation based on the use of variable Protein wouldn’t be useful, but the use of Color seems to be promising.
238;urinalysis_tests_histograms_symbolic.png;Given the usual semantics of Bacteria variable, dummification would have been a better codification.
239;urinalysis_tests_histograms_symbolic.png;It is better to drop the variable Bacteria than removing all records with missing values.
240;urinalysis_tests_histograms_symbolic.png;Not knowing the semantics of Epithelial Cells variable, dummification could have been a more adequate codification.
241;urinalysis_tests_mv.png;Discarding variable Color would be better than discarding all the records with missing values for that variable.
242;urinalysis_tests_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.
243;urinalysis_tests_mv.png;Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.
244;urinalysis_tests_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.
245;urinalysis_tests_mv.png;Feature generation based on variable Color seems to be promising.
246;urinalysis_tests_mv.png;It is better to drop the variable Color than removing all records with missing values.
247;urinalysis_tests_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
248;urinalysis_tests_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
249;urinalysis_tests_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
250;urinalysis_tests_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
251;urinalysis_tests_histograms_numeric.png;All variables, but the class, should be dealt with as binary.
252;urinalysis_tests_histograms_numeric.png;The variable Specific Gravity can be seen as ordinal.
253;urinalysis_tests_histograms_numeric.png;The variable Specific Gravity can be seen as ordinal without losing information.
254;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity is balanced.
255;urinalysis_tests_histograms_numeric.png;It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable pH.
256;urinalysis_tests_histograms_numeric.png;Outliers seem to be a problem in the dataset.
257;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity shows a high number of outlier values.
258;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity doesn’t have any outliers.
259;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity presents some outliers.
260;urinalysis_tests_histograms_numeric.png;At least 50 of the variables present outliers.
261;urinalysis_tests_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
262;urinalysis_tests_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
263;urinalysis_tests_histograms_numeric.png;Considering the common semantics for Age and pH variables, dummification if applied would increase the risk of facing the curse of dimensionality.
264;urinalysis_tests_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.
265;urinalysis_tests_histograms_numeric.png;The variable pH can be coded as ordinal without losing information.
266;urinalysis_tests_histograms_numeric.png;Feature generation based on variable Age seems to be promising.
267;urinalysis_tests_histograms_numeric.png;Feature generation based on the use of variable Age wouldn’t be useful, but the use of pH seems to be promising.
268;urinalysis_tests_histograms_numeric.png;Given the usual semantics of Specific Gravity variable, dummification would have been a better codification.
269;urinalysis_tests_histograms_numeric.png;It is better to drop the variable pH than removing all records with missing values.
270;urinalysis_tests_histograms_numeric.png;Not knowing the semantics of Age variable, dummification could have been a more adequate codification.
271;detect_dataset_decision_tree.png;The variable Ic discriminates between the target values, as shown in the decision tree.
272;detect_dataset_decision_tree.png;Variable Vb is one of the most relevant variables.
273;detect_dataset_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
274;detect_dataset_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
275;detect_dataset_decision_tree.png;The precision for the presented tree is higher than 75%.
276;detect_dataset_decision_tree.png;The number of False Negatives is lower than the number of True Negatives for the presented tree.
277;detect_dataset_decision_tree.png;The number of True Positives is lower than the number of False Positives for the presented tree.
278;detect_dataset_decision_tree.png;The number of False Negatives reported in the same tree is 50.
279;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 3.
280;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], the Decision Tree presented classifies (A, not B) as 0.
281;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], the Decision Tree presented classifies (A,B) as 0.
282;detect_dataset_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
283;detect_dataset_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
284;detect_dataset_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.
285;detect_dataset_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
286;detect_dataset_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
287;detect_dataset_overfitting_knn.png;KNN is in overfitting for k less than 17.
288;detect_dataset_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
289;detect_dataset_overfitting_knn.png;KNN with less than 17 neighbours is in overfitting.
290;detect_dataset_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.
291;detect_dataset_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
292;detect_dataset_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.
293;detect_dataset_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.
294;detect_dataset_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
295;detect_dataset_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
296;detect_dataset_pca.png;The first 2 principal components are enough for explaining half the data variance.
297;detect_dataset_pca.png;Using the first 3 principal components would imply an error between 10 and 20%.
298;detect_dataset_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.
299;detect_dataset_correlation_heatmap.png;One of the variables Vc or Va can be discarded without losing information.
300;detect_dataset_correlation_heatmap.png;The variable Ic can be discarded without risking losing information.
301;detect_dataset_correlation_heatmap.png;Variables Ia and Ic are redundant, but we can’t say the same for the pair Vc and Vb.
302;detect_dataset_correlation_heatmap.png;Variables Ib and Vc are redundant.
303;detect_dataset_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
304;detect_dataset_correlation_heatmap.png;Variable Vb seems to be relevant for the majority of mining tasks.
305;detect_dataset_correlation_heatmap.png;Variables Ib and Ic seem to be useful for classification tasks.
306;detect_dataset_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
307;detect_dataset_correlation_heatmap.png;Removing variable Ic might improve the training of decision trees .
308;detect_dataset_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Ic previously than variable Va.
309;detect_dataset_boxplots.png;Variable Vb is balanced.
310;detect_dataset_boxplots.png;Those boxplots show that the data is not normalized.
311;detect_dataset_boxplots.png;It is clear that variable Vb shows some outliers, but we can’t be sure of the same for variable Va.
312;detect_dataset_boxplots.png;Outliers seem to be a problem in the dataset.
313;detect_dataset_boxplots.png;Variable Vb shows some outlier values.
314;detect_dataset_boxplots.png;Variable Vb doesn’t have any outliers.
315;detect_dataset_boxplots.png;Variable Ia presents some outliers.
316;detect_dataset_boxplots.png;At least 75 of the variables present outliers.
317;detect_dataset_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
318;detect_dataset_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
319;detect_dataset_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
320;detect_dataset_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
321;detect_dataset_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
322;detect_dataset_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
323;detect_dataset_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
324;detect_dataset_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.
325;detect_dataset_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
326;detect_dataset_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
327;detect_dataset_histograms_numeric.png;All variables, but the class, should be dealt with as date.
328;detect_dataset_histograms_numeric.png;The variable Ic can be seen as ordinal.
329;detect_dataset_histograms_numeric.png;The variable Vc can be seen as ordinal without losing information.
330;detect_dataset_histograms_numeric.png;Variable Ia is balanced.
331;detect_dataset_histograms_numeric.png;It is clear that variable Va shows some outliers, but we can’t be sure of the same for variable Vc.
332;detect_dataset_histograms_numeric.png;Outliers seem to be a problem in the dataset.
333;detect_dataset_histograms_numeric.png;Variable Ia shows a high number of outlier values.
334;detect_dataset_histograms_numeric.png;Variable Ic doesn’t have any outliers.
335;detect_dataset_histograms_numeric.png;Variable Ic presents some outliers.
336;detect_dataset_histograms_numeric.png;At least 60 of the variables present outliers.
337;detect_dataset_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
338;detect_dataset_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
339;detect_dataset_histograms_numeric.png;Considering the common semantics for Ia and Ib variables, dummification if applied would increase the risk of facing the curse of dimensionality.
340;detect_dataset_histograms_numeric.png;Considering the common semantics for Vc variable, dummification would be the most adequate encoding.
341;detect_dataset_histograms_numeric.png;The variable Vb can be coded as ordinal without losing information.
342;detect_dataset_histograms_numeric.png;Feature generation based on variable Vb seems to be promising.
343;detect_dataset_histograms_numeric.png;Feature generation based on the use of variable Ic wouldn’t be useful, but the use of Ia seems to be promising.
344;detect_dataset_histograms_numeric.png;Given the usual semantics of Ib variable, dummification would have been a better codification.
345;detect_dataset_histograms_numeric.png;It is better to drop the variable Ia than removing all records with missing values.
346;detect_dataset_histograms_numeric.png;Not knowing the semantics of Ia variable, dummification could have been a more adequate codification.
347;diabetes_decision_tree.png;The variable BMI discriminates between the target values, as shown in the decision tree.
348;diabetes_decision_tree.png;Variable BMI is one of the most relevant variables.
349;diabetes_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
350;diabetes_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
351;diabetes_decision_tree.png;The precision for the presented tree is higher than 60%.
352;diabetes_decision_tree.png;The number of True Positives reported in the same tree is 30.
353;diabetes_decision_tree.png;The number of False Positives is lower than the number of False Negatives for the presented tree.
354;diabetes_decision_tree.png;The accuracy for the presented tree is higher than its specificity.
355;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], the Decision Tree presented classifies (not A, not B) as 1.
356;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], the Decision Tree presented classifies (not A, not B) as 1.
357;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 98.
358;diabetes_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
359;diabetes_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
360;diabetes_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.
361;diabetes_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
362;diabetes_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
363;diabetes_overfitting_knn.png;KNN is in overfitting for k larger than 13.
364;diabetes_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
365;diabetes_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.
366;diabetes_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
367;diabetes_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.
368;diabetes_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.
369;diabetes_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.
370;diabetes_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
371;diabetes_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
372;diabetes_pca.png;The first 7 principal components are enough for explaining half the data variance.
373;diabetes_pca.png;Using the first 2 principal components would imply an error between 10 and 20%.
374;diabetes_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.
375;diabetes_correlation_heatmap.png;One of the variables Age or Insulin can be discarded without losing information.
376;diabetes_correlation_heatmap.png;The variable DiabetesPedigreeFunction can be discarded without risking losing information.
377;diabetes_correlation_heatmap.png;Variables Age and SkinThickness are redundant, but we can’t say the same for the pair BMI and BloodPressure.
378;diabetes_correlation_heatmap.png;Variables DiabetesPedigreeFunction and Age are redundant.
379;diabetes_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
380;diabetes_correlation_heatmap.png;Variable SkinThickness seems to be relevant for the majority of mining tasks.
381;diabetes_correlation_heatmap.png;Variables Insulin and Glucose seem to be useful for classification tasks.
382;diabetes_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
383;diabetes_correlation_heatmap.png;Removing variable Insulin might improve the training of decision trees .
384;diabetes_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable DiabetesPedigreeFunction previously than variable Pregnancies.
385;diabetes_boxplots.png;Variable DiabetesPedigreeFunction is balanced.
386;diabetes_boxplots.png;Those boxplots show that the data is not normalized.
387;diabetes_boxplots.png;It is clear that variable Glucose shows some outliers, but we can’t be sure of the same for variable Age.
388;diabetes_boxplots.png;Outliers seem to be a problem in the dataset.
389;diabetes_boxplots.png;Variable Pregnancies shows some outlier values.
390;diabetes_boxplots.png;Variable Insulin doesn’t have any outliers.
391;diabetes_boxplots.png;Variable BMI presents some outliers.
392;diabetes_boxplots.png;At least 85 of the variables present outliers.
393;diabetes_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
394;diabetes_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
395;diabetes_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
396;diabetes_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
397;diabetes_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
398;diabetes_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
399;diabetes_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
400;diabetes_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.
401;diabetes_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
402;diabetes_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
403;diabetes_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
404;diabetes_histograms_numeric.png;The variable Age can be seen as ordinal.
405;diabetes_histograms_numeric.png;The variable Age can be seen as ordinal without losing information.
406;diabetes_histograms_numeric.png;Variable Pregnancies is balanced.
407;diabetes_histograms_numeric.png;It is clear that variable DiabetesPedigreeFunction shows some outliers, but we can’t be sure of the same for variable Glucose.
408;diabetes_histograms_numeric.png;Outliers seem to be a problem in the dataset.
409;diabetes_histograms_numeric.png;Variable Age shows a high number of outlier values.
410;diabetes_histograms_numeric.png;Variable BMI doesn’t have any outliers.
411;diabetes_histograms_numeric.png;Variable BloodPressure presents some outliers.
412;diabetes_histograms_numeric.png;At least 60 of the variables present outliers.
413;diabetes_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
414;diabetes_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
415;diabetes_histograms_numeric.png;Considering the common semantics for BloodPressure and Pregnancies variables, dummification if applied would increase the risk of facing the curse of dimensionality.
416;diabetes_histograms_numeric.png;Considering the common semantics for BMI variable, dummification would be the most adequate encoding.
417;diabetes_histograms_numeric.png;The variable Age can be coded as ordinal without losing information.
418;diabetes_histograms_numeric.png;Feature generation based on variable BMI seems to be promising.
419;diabetes_histograms_numeric.png;Feature generation based on the use of variable Age wouldn’t be useful, but the use of Pregnancies seems to be promising.
420;diabetes_histograms_numeric.png;Given the usual semantics of BMI variable, dummification would have been a better codification.
421;diabetes_histograms_numeric.png;It is better to drop the variable BMI than removing all records with missing values.
422;diabetes_histograms_numeric.png;Not knowing the semantics of SkinThickness variable, dummification could have been a more adequate codification.
423;Placement_decision_tree.png;The variable ssc_p discriminates between the target values, as shown in the decision tree.
424;Placement_decision_tree.png;Variable hsc_p is one of the most relevant variables.
425;Placement_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.
426;Placement_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
427;Placement_decision_tree.png;The recall for the presented tree is higher than 90%.
428;Placement_decision_tree.png;The number of False Negatives is lower than the number of True Negatives for the presented tree.
429;Placement_decision_tree.png;The number of True Negatives is lower than the number of True Positives for the presented tree.
430;Placement_decision_tree.png;The accuracy for the presented tree is higher than 75%.
431;Placement_decision_tree.png;Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], it is possible to state that KNN algorithm classifies (not A, not B) as Placed for any k ≤ 68.
432;Placement_decision_tree.png;Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], it is possible to state that KNN algorithm classifies (not A, not B) as Placed for any k ≤ 68.
433;Placement_decision_tree.png;Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], the Decision Tree presented classifies (A, not B) as Placed.
434;Placement_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
435;Placement_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
436;Placement_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.
437;Placement_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
438;Placement_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
439;Placement_overfitting_knn.png;KNN is in overfitting for k less than 13.
440;Placement_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
441;Placement_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
442;Placement_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.
443;Placement_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.
444;Placement_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.
445;Placement_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.
446;Placement_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.
447;Placement_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
448;Placement_pca.png;The first 3 principal components are enough for explaining half the data variance.
449;Placement_pca.png;Using the first 2 principal components would imply an error between 10 and 30%.
450;Placement_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.
451;Placement_correlation_heatmap.png;One of the variables hsc_p or mba_p can be discarded without losing information.
452;Placement_correlation_heatmap.png;The variable mba_p can be discarded without risking losing information.
453;Placement_correlation_heatmap.png;Variables hsc_p and ssc_p are redundant, but we can’t say the same for the pair degree_p and etest_p.
454;Placement_correlation_heatmap.png;Variables hsc_p and etest_p are redundant.
455;Placement_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
456;Placement_correlation_heatmap.png;Variable ssc_p seems to be relevant for the majority of mining tasks.
457;Placement_correlation_heatmap.png;Variables hsc_p and degree_p seem to be useful for classification tasks.
458;Placement_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
459;Placement_correlation_heatmap.png;Removing variable degree_p might improve the training of decision trees .
460;Placement_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable hsc_p previously than variable mba_p.
461;Placement_boxplots.png;Variable etest_p is balanced.
462;Placement_boxplots.png;Those boxplots show that the data is not normalized.
463;Placement_boxplots.png;It is clear that variable mba_p shows some outliers, but we can’t be sure of the same for variable ssc_p.
464;Placement_boxplots.png;Outliers seem to be a problem in the dataset.
465;Placement_boxplots.png;Variable hsc_p shows some outlier values.
466;Placement_boxplots.png;Variable hsc_p doesn’t have any outliers.
467;Placement_boxplots.png;Variable hsc_p presents some outliers.
468;Placement_boxplots.png;At least 75 of the variables present outliers.
469;Placement_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
470;Placement_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
471;Placement_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
472;Placement_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
473;Placement_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
474;Placement_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
475;Placement_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.
476;Placement_histograms_symbolic.png;The variable degree_t can be seen as ordinal.
477;Placement_histograms_symbolic.png;The variable specialisation can be seen as ordinal without losing information.
478;Placement_histograms_symbolic.png;Considering the common semantics for specialisation and hsc_s variables, dummification if applied would increase the risk of facing the curse of dimensionality.
479;Placement_histograms_symbolic.png;Considering the common semantics for specialisation variable, dummification would be the most adequate encoding.
480;Placement_histograms_symbolic.png;The variable ssc_b can be coded as ordinal without losing information.
481;Placement_histograms_symbolic.png;Feature generation based on variable hsc_s seems to be promising.
482;Placement_histograms_symbolic.png;Feature generation based on the use of variable hsc_s wouldn’t be useful, but the use of degree_t seems to be promising.
483;Placement_histograms_symbolic.png;Given the usual semantics of hsc_s variable, dummification would have been a better codification.
484;Placement_histograms_symbolic.png;It is better to drop the variable ssc_b than removing all records with missing values.
485;Placement_histograms_symbolic.png;Not knowing the semantics of hsc_b variable, dummification could have been a more adequate codification.
486;Placement_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
487;Placement_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.
488;Placement_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
489;Placement_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
490;Placement_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
491;Placement_histograms_numeric.png;The variable etest_p can be seen as ordinal.
492;Placement_histograms_numeric.png;The variable mba_p can be seen as ordinal without losing information.
493;Placement_histograms_numeric.png;Variable degree_p is balanced.
494;Placement_histograms_numeric.png;It is clear that variable mba_p shows some outliers, but we can’t be sure of the same for variable hsc_p.
495;Placement_histograms_numeric.png;Outliers seem to be a problem in the dataset.
496;Placement_histograms_numeric.png;Variable mba_p shows some outlier values.
497;Placement_histograms_numeric.png;Variable ssc_p doesn’t have any outliers.
498;Placement_histograms_numeric.png;Variable degree_p presents some outliers.
499;Placement_histograms_numeric.png;At least 85 of the variables present outliers.
500;Placement_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
501;Placement_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
502;Placement_histograms_numeric.png;Considering the common semantics for ssc_p and hsc_p variables, dummification if applied would increase the risk of facing the curse of dimensionality.
503;Placement_histograms_numeric.png;Considering the common semantics for ssc_p variable, dummification would be the most adequate encoding.
504;Placement_histograms_numeric.png;The variable degree_p can be coded as ordinal without losing information.
505;Placement_histograms_numeric.png;Feature generation based on variable ssc_p seems to be promising.
506;Placement_histograms_numeric.png;Feature generation based on the use of variable etest_p wouldn’t be useful, but the use of ssc_p seems to be promising.
507;Placement_histograms_numeric.png;Given the usual semantics of degree_p variable, dummification would have been a better codification.
508;Placement_histograms_numeric.png;It is better to drop the variable etest_p than removing all records with missing values.
509;Placement_histograms_numeric.png;Not knowing the semantics of hsc_p variable, dummification could have been a more adequate codification.
510;Liver_Patient_decision_tree.png;The variable Sgot discriminates between the target values, as shown in the decision tree.
511;Liver_Patient_decision_tree.png;Variable Alkphos is one of the most relevant variables.
512;Liver_Patient_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
513;Liver_Patient_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
514;Liver_Patient_decision_tree.png;The recall for the presented tree is higher than 90%.
515;Liver_Patient_decision_tree.png;The number of True Negatives is higher than the number of True Positives for the presented tree.
516;Liver_Patient_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree.
517;Liver_Patient_decision_tree.png;The precision for the presented tree is higher than its recall.
518;Liver_Patient_decision_tree.png;Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 1.
519;Liver_Patient_decision_tree.png;Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], it is possible to state that KNN algorithm classifies (not A, not B) as 2 for any k ≤ 94.
520;Liver_Patient_decision_tree.png;Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], the Decision Tree presented classifies (not A, B) as 1.
521;Liver_Patient_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
522;Liver_Patient_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
523;Liver_Patient_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.
524;Liver_Patient_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
525;Liver_Patient_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
526;Liver_Patient_overfitting_knn.png;KNN is in overfitting for k less than 13.
527;Liver_Patient_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
528;Liver_Patient_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
529;Liver_Patient_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.
530;Liver_Patient_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.
531;Liver_Patient_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.
532;Liver_Patient_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.
533;Liver_Patient_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
534;Liver_Patient_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
535;Liver_Patient_pca.png;The first 3 principal components are enough for explaining half the data variance.
536;Liver_Patient_pca.png;Using the first 3 principal components would imply an error between 10 and 25%.
537;Liver_Patient_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 8.
538;Liver_Patient_correlation_heatmap.png;One of the variables ALB or DB can be discarded without losing information.
539;Liver_Patient_correlation_heatmap.png;The variable AG_Ratio can be discarded without risking losing information.
540;Liver_Patient_correlation_heatmap.png;Variables AG_Ratio and DB are redundant, but we can’t say the same for the pair Sgpt and Sgot.
541;Liver_Patient_correlation_heatmap.png;Variables Sgpt and AG_Ratio are redundant.
542;Liver_Patient_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
543;Liver_Patient_correlation_heatmap.png;Variable Sgpt seems to be relevant for the majority of mining tasks.
544;Liver_Patient_correlation_heatmap.png;Variables Age and DB seem to be useful for classification tasks.
545;Liver_Patient_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
546;Liver_Patient_correlation_heatmap.png;Removing variable DB might improve the training of decision trees .
547;Liver_Patient_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable DB previously than variable TB.
548;Liver_Patient_boxplots.png;Variable ALB is balanced.
549;Liver_Patient_boxplots.png;Those boxplots show that the data is not normalized.
550;Liver_Patient_boxplots.png;It is clear that variable Sgpt shows some outliers, but we can’t be sure of the same for variable TP.
551;Liver_Patient_boxplots.png;Outliers seem to be a problem in the dataset.
552;Liver_Patient_boxplots.png;Variable Sgot shows a high number of outlier values.
553;Liver_Patient_boxplots.png;Variable TP doesn’t have any outliers.
554;Liver_Patient_boxplots.png;Variable Age presents some outliers.
555;Liver_Patient_boxplots.png;At least 75 of the variables present outliers.
556;Liver_Patient_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
557;Liver_Patient_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
558;Liver_Patient_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
559;Liver_Patient_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
560;Liver_Patient_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
561;Liver_Patient_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
562;Liver_Patient_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.
563;Liver_Patient_histograms_symbolic.png;The variable Gender can be seen as ordinal.
564;Liver_Patient_histograms_symbolic.png;The variable Gender can be seen as ordinal without losing information.
565;Liver_Patient_histograms_symbolic.png;Considering the common semantics for Gender and <all_variables> variables, dummification if applied would increase the risk of facing the curse of dimensionality.
566;Liver_Patient_histograms_symbolic.png;Considering the common semantics for Gender variable, dummification would be the most adequate encoding.
567;Liver_Patient_histograms_symbolic.png;The variable Gender can be coded as ordinal without losing information.
568;Liver_Patient_histograms_symbolic.png;Feature generation based on variable Gender seems to be promising.
569;Liver_Patient_histograms_symbolic.png;Feature generation based on the use of variable Gender wouldn’t be useful, but the use of <all_variables> seems to be promising.
570;Liver_Patient_histograms_symbolic.png;Given the usual semantics of Gender variable, dummification would have been a better codification.
571;Liver_Patient_histograms_symbolic.png;It is better to drop the variable Gender than removing all records with missing values.
572;Liver_Patient_histograms_symbolic.png;Not knowing the semantics of Gender variable, dummification could have been a more adequate codification.
573;Liver_Patient_mv.png;Discarding variable AG_Ratio would be better than discarding all the records with missing values for that variable.
574;Liver_Patient_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.
575;Liver_Patient_mv.png;Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.
576;Liver_Patient_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.
577;Liver_Patient_mv.png;Feature generation based on variable AG_Ratio seems to be promising.
578;Liver_Patient_mv.png;It is better to drop the variable AG_Ratio than removing all records with missing values.
579;Liver_Patient_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
580;Liver_Patient_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
581;Liver_Patient_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
582;Liver_Patient_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
583;Liver_Patient_histograms_numeric.png;All variables, but the class, should be dealt with as binary.
584;Liver_Patient_histograms_numeric.png;The variable ALB can be seen as ordinal.
585;Liver_Patient_histograms_numeric.png;The variable AG_Ratio can be seen as ordinal without losing information.
586;Liver_Patient_histograms_numeric.png;Variable Age is balanced.
587;Liver_Patient_histograms_numeric.png;It is clear that variable Sgot shows some outliers, but we can’t be sure of the same for variable Age.
588;Liver_Patient_histograms_numeric.png;Outliers seem to be a problem in the dataset.
589;Liver_Patient_histograms_numeric.png;Variable ALB shows a high number of outlier values.
590;Liver_Patient_histograms_numeric.png;Variable DB doesn’t have any outliers.
591;Liver_Patient_histograms_numeric.png;Variable Alkphos presents some outliers.
592;Liver_Patient_histograms_numeric.png;At least 50 of the variables present outliers.
593;Liver_Patient_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
594;Liver_Patient_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
595;Liver_Patient_histograms_numeric.png;Considering the common semantics for Age and TB variables, dummification if applied would increase the risk of facing the curse of dimensionality.
596;Liver_Patient_histograms_numeric.png;Considering the common semantics for TB variable, dummification would be the most adequate encoding.
597;Liver_Patient_histograms_numeric.png;The variable AG_Ratio can be coded as ordinal without losing information.
598;Liver_Patient_histograms_numeric.png;Feature generation based on variable ALB seems to be promising.
599;Liver_Patient_histograms_numeric.png;Feature generation based on the use of variable Sgpt wouldn’t be useful, but the use of Age seems to be promising.
600;Liver_Patient_histograms_numeric.png;Given the usual semantics of Alkphos variable, dummification would have been a better codification.
601;Liver_Patient_histograms_numeric.png;It is better to drop the variable AG_Ratio than removing all records with missing values.
602;Liver_Patient_histograms_numeric.png;Not knowing the semantics of AG_Ratio variable, dummification could have been a more adequate codification.
603;Hotel_Reservations_decision_tree.png;The variable lead_time discriminates between the target values, as shown in the decision tree.
604;Hotel_Reservations_decision_tree.png;Variable no_of_special_requests is one of the most relevant variables.
605;Hotel_Reservations_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
606;Hotel_Reservations_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
607;Hotel_Reservations_decision_tree.png;The recall for the presented tree is lower than 75%.
608;Hotel_Reservations_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree.
609;Hotel_Reservations_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree.
610;Hotel_Reservations_decision_tree.png;The variable lead_time discriminates between the target values, as shown in the decision tree.
611;Hotel_Reservations_decision_tree.png;Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A, not B) as Canceled for any k ≤ 4955.
612;Hotel_Reservations_decision_tree.png;Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A,B) as Not_Canceled for any k ≤ 10612.
613;Hotel_Reservations_decision_tree.png;Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A,B) as Canceled for any k ≤ 9756.
614;Hotel_Reservations_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
615;Hotel_Reservations_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
616;Hotel_Reservations_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.
617;Hotel_Reservations_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
618;Hotel_Reservations_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
619;Hotel_Reservations_overfitting_knn.png;KNN is in overfitting for k less than 5.
620;Hotel_Reservations_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
621;Hotel_Reservations_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
622;Hotel_Reservations_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.
623;Hotel_Reservations_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.
624;Hotel_Reservations_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.
625;Hotel_Reservations_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.
626;Hotel_Reservations_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.
627;Hotel_Reservations_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
628;Hotel_Reservations_pca.png;The first 3 principal components are enough for explaining half the data variance.
629;Hotel_Reservations_pca.png;Using the first 5 principal components would imply an error between 10 and 20%.
630;Hotel_Reservations_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.
631;Hotel_Reservations_correlation_heatmap.png;One of the variables arrival_month or no_of_special_requests can be discarded without losing information.
632;Hotel_Reservations_correlation_heatmap.png;The variable no_of_adults can be discarded without risking losing information.
633;Hotel_Reservations_correlation_heatmap.png;Variables no_of_adults and arrival_month are redundant, but we can’t say the same for the pair no_of_week_nights and no_of_weekend_nights.
634;Hotel_Reservations_correlation_heatmap.png;Variables no_of_adults and no_of_week_nights are redundant.
635;Hotel_Reservations_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
636;Hotel_Reservations_correlation_heatmap.png;Variable arrival_month seems to be relevant for the majority of mining tasks.
637;Hotel_Reservations_correlation_heatmap.png;Variables arrival_month and no_of_adults seem to be useful for classification tasks.
638;Hotel_Reservations_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
639;Hotel_Reservations_correlation_heatmap.png;Removing variable no_of_adults might improve the training of decision trees .
640;Hotel_Reservations_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable arrival_date previously than variable no_of_week_nights.
641;Hotel_Reservations_boxplots.png;Variable arrival_date is balanced.
642;Hotel_Reservations_boxplots.png;Those boxplots show that the data is not normalized.
643;Hotel_Reservations_boxplots.png;It is clear that variable no_of_weekend_nights shows some outliers, but we can’t be sure of the same for variable lead_time.
644;Hotel_Reservations_boxplots.png;Outliers seem to be a problem in the dataset.
645;Hotel_Reservations_boxplots.png;Variable no_of_week_nights shows a high number of outlier values.
646;Hotel_Reservations_boxplots.png;Variable no_of_week_nights doesn’t have any outliers.
647;Hotel_Reservations_boxplots.png;Variable avg_price_per_room presents some outliers.
648;Hotel_Reservations_boxplots.png;At least 85 of the variables present outliers.
649;Hotel_Reservations_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
650;Hotel_Reservations_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
651;Hotel_Reservations_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
652;Hotel_Reservations_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
653;Hotel_Reservations_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
654;Hotel_Reservations_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
655;Hotel_Reservations_histograms_symbolic.png;All variables, but the class, should be dealt with as symbolic.
656;Hotel_Reservations_histograms_symbolic.png;The variable room_type_reserved can be seen as ordinal.
657;Hotel_Reservations_histograms_symbolic.png;The variable type_of_meal_plan can be seen as ordinal without losing information.
658;Hotel_Reservations_histograms_symbolic.png;Considering the common semantics for arrival_year and type_of_meal_plan variables, dummification if applied would increase the risk of facing the curse of dimensionality.
659;Hotel_Reservations_histograms_symbolic.png;Considering the common semantics for type_of_meal_plan variable, dummification would be the most adequate encoding.
660;Hotel_Reservations_histograms_symbolic.png;The variable type_of_meal_plan can be coded as ordinal without losing information.
661;Hotel_Reservations_histograms_symbolic.png;Feature generation based on variable arrival_year seems to be promising.
662;Hotel_Reservations_histograms_symbolic.png;Feature generation based on the use of variable required_car_parking_space wouldn’t be useful, but the use of type_of_meal_plan seems to be promising.
663;Hotel_Reservations_histograms_symbolic.png;Given the usual semantics of required_car_parking_space variable, dummification would have been a better codification.
664;Hotel_Reservations_histograms_symbolic.png;It is better to drop the variable required_car_parking_space than removing all records with missing values.
665;Hotel_Reservations_histograms_symbolic.png;Not knowing the semantics of arrival_year variable, dummification could have been a more adequate codification.
666;Hotel_Reservations_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
667;Hotel_Reservations_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.
668;Hotel_Reservations_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
669;Hotel_Reservations_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
670;Hotel_Reservations_histograms_numeric.png;All variables, but the class, should be dealt with as date.
671;Hotel_Reservations_histograms_numeric.png;The variable arrival_date can be seen as ordinal.
672;Hotel_Reservations_histograms_numeric.png;The variable no_of_children can be seen as ordinal without losing information.
673;Hotel_Reservations_histograms_numeric.png;Variable no_of_children is balanced.
674;Hotel_Reservations_histograms_numeric.png;It is clear that variable no_of_special_requests shows some outliers, but we can’t be sure of the same for variable avg_price_per_room.
675;Hotel_Reservations_histograms_numeric.png;Outliers seem to be a problem in the dataset.
676;Hotel_Reservations_histograms_numeric.png;Variable arrival_date shows a high number of outlier values.
677;Hotel_Reservations_histograms_numeric.png;Variable no_of_adults doesn’t have any outliers.
678;Hotel_Reservations_histograms_numeric.png;Variable no_of_weekend_nights presents some outliers.
679;Hotel_Reservations_histograms_numeric.png;At least 75 of the variables present outliers.
680;Hotel_Reservations_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
681;Hotel_Reservations_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
682;Hotel_Reservations_histograms_numeric.png;Considering the common semantics for arrival_date and no_of_adults variables, dummification if applied would increase the risk of facing the curse of dimensionality.
683;Hotel_Reservations_histograms_numeric.png;Considering the common semantics for no_of_special_requests variable, dummification would be the most adequate encoding.
684;Hotel_Reservations_histograms_numeric.png;The variable avg_price_per_room can be coded as ordinal without losing information.
685;Hotel_Reservations_histograms_numeric.png;Feature generation based on variable no_of_special_requests seems to be promising.
686;Hotel_Reservations_histograms_numeric.png;Feature generation based on the use of variable no_of_week_nights wouldn’t be useful, but the use of no_of_adults seems to be promising.
687;Hotel_Reservations_histograms_numeric.png;Given the usual semantics of no_of_adults variable, dummification would have been a better codification.
688;Hotel_Reservations_histograms_numeric.png;It is better to drop the variable arrival_date than removing all records with missing values.
689;Hotel_Reservations_histograms_numeric.png;Not knowing the semantics of no_of_week_nights variable, dummification could have been a more adequate codification.
690;StressLevelDataset_decision_tree.png;The variable bullying discriminates between the target values, as shown in the decision tree.
691;StressLevelDataset_decision_tree.png;Variable basic_needs is one of the most relevant variables.
692;StressLevelDataset_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.
693;StressLevelDataset_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
694;StressLevelDataset_decision_tree.png;The precision for the presented tree is higher than 60%.
695;StressLevelDataset_decision_tree.png;The number of False Positives reported in the same tree is 30.
696;StressLevelDataset_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree.
697;StressLevelDataset_decision_tree.png;The variable basic_needs seems to be one of the four most relevant features.
698;StressLevelDataset_decision_tree.png;Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], the Decision Tree presented classifies (A, not B) as 2.
699;StressLevelDataset_decision_tree.png;Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], the Decision Tree presented classifies (not A, B) as 1.
700;StressLevelDataset_decision_tree.png;Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 271.
701;StressLevelDataset_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
702;StressLevelDataset_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
703;StressLevelDataset_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.
704;StressLevelDataset_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
705;StressLevelDataset_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
706;StressLevelDataset_overfitting_knn.png;KNN is in overfitting for k larger than 17.
707;StressLevelDataset_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
708;StressLevelDataset_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.
709;StressLevelDataset_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.
710;StressLevelDataset_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.
711;StressLevelDataset_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.
712;StressLevelDataset_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.
713;StressLevelDataset_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.
714;StressLevelDataset_pca.png;The first 4 principal components are enough for explaining half the data variance.
715;StressLevelDataset_pca.png;Using the first 2 principal components would imply an error between 5 and 25%.
716;StressLevelDataset_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 6.
717;StressLevelDataset_correlation_heatmap.png;One of the variables headache or bullying can be discarded without losing information.
718;StressLevelDataset_correlation_heatmap.png;The variable breathing_problem can be discarded without risking losing information.
719;StressLevelDataset_correlation_heatmap.png;Variables anxiety_level and bullying are redundant, but we can’t say the same for the pair study_load and living_conditions.
720;StressLevelDataset_correlation_heatmap.png;Variables bullying and depression are redundant.
721;StressLevelDataset_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
722;StressLevelDataset_correlation_heatmap.png;Variable breathing_problem seems to be relevant for the majority of mining tasks.
723;StressLevelDataset_correlation_heatmap.png;Variables living_conditions and breathing_problem seem to be useful for classification tasks.
724;StressLevelDataset_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
725;StressLevelDataset_correlation_heatmap.png;Removing variable basic_needs might improve the training of decision trees .
726;StressLevelDataset_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable basic_needs previously than variable self_esteem.
727;StressLevelDataset_boxplots.png;Variable study_load is balanced.
728;StressLevelDataset_boxplots.png;Those boxplots show that the data is not normalized.
729;StressLevelDataset_boxplots.png;It is clear that variable self_esteem shows some outliers, but we can’t be sure of the same for variable anxiety_level.
730;StressLevelDataset_boxplots.png;Outliers seem to be a problem in the dataset.
731;StressLevelDataset_boxplots.png;Variable basic_needs shows some outlier values.
732;StressLevelDataset_boxplots.png;Variable headache doesn’t have any outliers.
733;StressLevelDataset_boxplots.png;Variable depression presents some outliers.
734;StressLevelDataset_boxplots.png;At least 60 of the variables present outliers.
735;StressLevelDataset_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
736;StressLevelDataset_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
737;StressLevelDataset_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
738;StressLevelDataset_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
739;StressLevelDataset_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
740;StressLevelDataset_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
741;StressLevelDataset_histograms_symbolic.png;All variables, but the class, should be dealt with as symbolic.
742;StressLevelDataset_histograms_symbolic.png;The variable mental_health_history can be seen as ordinal.
743;StressLevelDataset_histograms_symbolic.png;The variable mental_health_history can be seen as ordinal without losing information.
744;StressLevelDataset_histograms_symbolic.png;Considering the common semantics for mental_health_history and <all_variables> variables, dummification if applied would increase the risk of facing the curse of dimensionality.
745;StressLevelDataset_histograms_symbolic.png;Considering the common semantics for mental_health_history variable, dummification would be the most adequate encoding.
746;StressLevelDataset_histograms_symbolic.png;The variable mental_health_history can be coded as ordinal without losing information.
747;StressLevelDataset_histograms_symbolic.png;Feature generation based on variable mental_health_history seems to be promising.
748;StressLevelDataset_histograms_symbolic.png;Feature generation based on the use of variable mental_health_history wouldn’t be useful, but the use of <all_variables> seems to be promising.
749;StressLevelDataset_histograms_symbolic.png;Given the usual semantics of mental_health_history variable, dummification would have been a better codification.
750;StressLevelDataset_histograms_symbolic.png;It is better to drop the variable mental_health_history than removing all records with missing values.
751;StressLevelDataset_histograms_symbolic.png;Not knowing the semantics of mental_health_history variable, dummification could have been a more adequate codification.
752;StressLevelDataset_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
753;StressLevelDataset_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.
754;StressLevelDataset_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
755;StressLevelDataset_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
756;StressLevelDataset_histograms_numeric.png;All variables, but the class, should be dealt with as symbolic.
757;StressLevelDataset_histograms_numeric.png;The variable living_conditions can be seen as ordinal.
758;StressLevelDataset_histograms_numeric.png;The variable breathing_problem can be seen as ordinal without losing information.
759;StressLevelDataset_histograms_numeric.png;Variable breathing_problem is balanced.
760;StressLevelDataset_histograms_numeric.png;It is clear that variable depression shows some outliers, but we can’t be sure of the same for variable study_load.
761;StressLevelDataset_histograms_numeric.png;Outliers seem to be a problem in the dataset.
762;StressLevelDataset_histograms_numeric.png;Variable bullying shows a high number of outlier values.
763;StressLevelDataset_histograms_numeric.png;Variable headache doesn’t have any outliers.
764;StressLevelDataset_histograms_numeric.png;Variable anxiety_level presents some outliers.
765;StressLevelDataset_histograms_numeric.png;At least 60 of the variables present outliers.
766;StressLevelDataset_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
767;StressLevelDataset_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
768;StressLevelDataset_histograms_numeric.png;Considering the common semantics for sleep_quality and anxiety_level variables, dummification if applied would increase the risk of facing the curse of dimensionality.
769;StressLevelDataset_histograms_numeric.png;Considering the common semantics for headache variable, dummification would be the most adequate encoding.
770;StressLevelDataset_histograms_numeric.png;The variable breathing_problem can be coded as ordinal without losing information.
771;StressLevelDataset_histograms_numeric.png;Feature generation based on variable self_esteem seems to be promising.
772;StressLevelDataset_histograms_numeric.png;Feature generation based on the use of variable anxiety_level wouldn’t be useful, but the use of self_esteem seems to be promising.
773;StressLevelDataset_histograms_numeric.png;Given the usual semantics of study_load variable, dummification would have been a better codification.
774;StressLevelDataset_histograms_numeric.png;It is better to drop the variable depression than removing all records with missing values.
775;StressLevelDataset_histograms_numeric.png;Not knowing the semantics of basic_needs variable, dummification could have been a more adequate codification.
776;WineQT_decision_tree.png;The variable chlorides discriminates between the target values, as shown in the decision tree.
777;WineQT_decision_tree.png;Variable density is one of the most relevant variables.
778;WineQT_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
779;WineQT_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
780;WineQT_decision_tree.png;The precision for the presented tree is higher than 75%.
781;WineQT_decision_tree.png;The number of False Negatives is lower than the number of True Negatives for the presented tree.
782;WineQT_decision_tree.png;The number of False Positives is lower than the number of False Negatives for the presented tree.
783;WineQT_decision_tree.png;The number of False Positives reported in the same tree is 10.
784;WineQT_decision_tree.png;Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that KNN algorithm classifies (not A, not B) as 6 for any k ≤ 447.
785;WineQT_decision_tree.png;Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 3.
786;WineQT_decision_tree.png;Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that KNN algorithm classifies (not A, not B) as 5 for any k ≤ 172.
787;WineQT_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
788;WineQT_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
789;WineQT_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.
790;WineQT_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
791;WineQT_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
792;WineQT_overfitting_knn.png;KNN is in overfitting for k larger than 13.
793;WineQT_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
794;WineQT_overfitting_knn.png;KNN with more than 7 neighbours is in overfitting.
795;WineQT_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.
796;WineQT_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
797;WineQT_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.
798;WineQT_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 10.
799;WineQT_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
800;WineQT_pca.png;The first 8 principal components are enough for explaining half the data variance.
801;WineQT_pca.png;Using the first 6 principal components would imply an error between 15 and 25%.
802;WineQT_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.
803;WineQT_correlation_heatmap.png;One of the variables citric acid or residual sugar can be discarded without losing information.
804;WineQT_correlation_heatmap.png;The variable chlorides can be discarded without risking losing information.
805;WineQT_correlation_heatmap.png;Variables sulphates and pH are redundant, but we can’t say the same for the pair free sulfur dioxide and volatile acidity.
806;WineQT_correlation_heatmap.png;Variables free sulfur dioxide and total sulfur dioxide are redundant.
807;WineQT_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
808;WineQT_correlation_heatmap.png;Variable volatile acidity seems to be relevant for the majority of mining tasks.
809;WineQT_correlation_heatmap.png;Variables chlorides and citric acid seem to be useful for classification tasks.
810;WineQT_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
811;WineQT_correlation_heatmap.png;Removing variable fixed acidity might improve the training of decision trees .
812;WineQT_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable pH previously than variable chlorides.
813;WineQT_boxplots.png;Variable citric acid is balanced.
814;WineQT_boxplots.png;Those boxplots show that the data is not normalized.
815;WineQT_boxplots.png;It is clear that variable pH shows some outliers, but we can’t be sure of the same for variable volatile acidity.
816;WineQT_boxplots.png;Outliers seem to be a problem in the dataset.
817;WineQT_boxplots.png;Variable free sulfur dioxide shows a high number of outlier values.
818;WineQT_boxplots.png;Variable chlorides doesn’t have any outliers.
819;WineQT_boxplots.png;Variable total sulfur dioxide presents some outliers.
820;WineQT_boxplots.png;At least 75 of the variables present outliers.
821;WineQT_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
822;WineQT_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
823;WineQT_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
824;WineQT_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
825;WineQT_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
826;WineQT_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
827;WineQT_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
828;WineQT_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.
829;WineQT_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
830;WineQT_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
831;WineQT_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
832;WineQT_histograms_numeric.png;The variable fixed acidity can be seen as ordinal.
833;WineQT_histograms_numeric.png;The variable pH can be seen as ordinal without losing information.
834;WineQT_histograms_numeric.png;Variable free sulfur dioxide is balanced.
835;WineQT_histograms_numeric.png;It is clear that variable alcohol shows some outliers, but we can’t be sure of the same for variable sulphates.
836;WineQT_histograms_numeric.png;Outliers seem to be a problem in the dataset.
837;WineQT_histograms_numeric.png;Variable sulphates shows a high number of outlier values.
838;WineQT_histograms_numeric.png;Variable pH doesn’t have any outliers.
839;WineQT_histograms_numeric.png;Variable citric acid presents some outliers.
840;WineQT_histograms_numeric.png;At least 75 of the variables present outliers.
841;WineQT_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
842;WineQT_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
843;WineQT_histograms_numeric.png;Considering the common semantics for citric acid and fixed acidity variables, dummification if applied would increase the risk of facing the curse of dimensionality.
844;WineQT_histograms_numeric.png;Considering the common semantics for citric acid variable, dummification would be the most adequate encoding.
845;WineQT_histograms_numeric.png;The variable pH can be coded as ordinal without losing information.
846;WineQT_histograms_numeric.png;Feature generation based on variable density seems to be promising.
847;WineQT_histograms_numeric.png;Feature generation based on the use of variable sulphates wouldn’t be useful, but the use of fixed acidity seems to be promising.
848;WineQT_histograms_numeric.png;Given the usual semantics of citric acid variable, dummification would have been a better codification.
849;WineQT_histograms_numeric.png;It is better to drop the variable free sulfur dioxide than removing all records with missing values.
850;WineQT_histograms_numeric.png;Not knowing the semantics of pH variable, dummification could have been a more adequate codification.
851;loan_data_decision_tree.png;The variable ApplicantIncome discriminates between the target values, as shown in the decision tree.
852;loan_data_decision_tree.png;Variable ApplicantIncome is one of the most relevant variables.
853;loan_data_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
854;loan_data_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
855;loan_data_decision_tree.png;The recall for the presented tree is lower than 90%.
856;loan_data_decision_tree.png;The number of False Positives is higher than the number of True Negatives for the presented tree.
857;loan_data_decision_tree.png;The number of False Negatives is higher than the number of True Positives for the presented tree.
858;loan_data_decision_tree.png;The recall for the presented tree is higher than its accuracy.
859;loan_data_decision_tree.png;Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that KNN algorithm classifies (not A, not B) as Y for any k ≤ 3.
860;loan_data_decision_tree.png;Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that KNN algorithm classifies (not A, B) as N for any k ≤ 204.
861;loan_data_decision_tree.png;Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as N.
862;loan_data_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
863;loan_data_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
864;loan_data_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.
865;loan_data_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
866;loan_data_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
867;loan_data_overfitting_knn.png;KNN is in overfitting for k larger than 13.
868;loan_data_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
869;loan_data_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
870;loan_data_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.
871;loan_data_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
872;loan_data_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.
873;loan_data_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 10.
874;loan_data_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.
875;loan_data_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
876;loan_data_pca.png;The first 2 principal components are enough for explaining half the data variance.
877;loan_data_pca.png;Using the first 2 principal components would imply an error between 10 and 20%.
878;loan_data_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
879;loan_data_correlation_heatmap.png;One of the variables CoapplicantIncome or ApplicantIncome can be discarded without losing information.
880;loan_data_correlation_heatmap.png;The variable CoapplicantIncome can be discarded without risking losing information.
881;loan_data_correlation_heatmap.png;Variables ApplicantIncome and LoanAmount seem to be useful for classification tasks.
882;loan_data_correlation_heatmap.png;Variables Loan_Amount_Term and CoapplicantIncome are redundant.
883;loan_data_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
884;loan_data_correlation_heatmap.png;Variable ApplicantIncome seems to be relevant for the majority of mining tasks.
885;loan_data_correlation_heatmap.png;Variables CoapplicantIncome and ApplicantIncome seem to be useful for classification tasks.
886;loan_data_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
887;loan_data_correlation_heatmap.png;Removing variable LoanAmount might improve the training of decision trees .
888;loan_data_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable CoapplicantIncome previously than variable Loan_Amount_Term.
889;loan_data_boxplots.png;Variable Loan_Amount_Term is balanced.
890;loan_data_boxplots.png;Those boxplots show that the data is not normalized.
891;loan_data_boxplots.png;It is clear that variable Loan_Amount_Term shows some outliers, but we can’t be sure of the same for variable ApplicantIncome.
892;loan_data_boxplots.png;Outliers seem to be a problem in the dataset.
893;loan_data_boxplots.png;Variable ApplicantIncome shows a high number of outlier values.
894;loan_data_boxplots.png;Variable Loan_Amount_Term doesn’t have any outliers.
895;loan_data_boxplots.png;Variable ApplicantIncome presents some outliers.
896;loan_data_boxplots.png;At least 50 of the variables present outliers.
897;loan_data_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
898;loan_data_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
899;loan_data_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
900;loan_data_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
901;loan_data_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
902;loan_data_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
903;loan_data_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.
904;loan_data_histograms_symbolic.png;The variable Credit_History can be seen as ordinal.
905;loan_data_histograms_symbolic.png;The variable Married can be seen as ordinal without losing information.
906;loan_data_histograms_symbolic.png;Considering the common semantics for Credit_History and Dependents variables, dummification if applied would increase the risk of facing the curse of dimensionality.
907;loan_data_histograms_symbolic.png;Considering the common semantics for Property_Area variable, dummification would be the most adequate encoding.
908;loan_data_histograms_symbolic.png;The variable Dependents can be coded as ordinal without losing information.
909;loan_data_histograms_symbolic.png;Feature generation based on variable Dependents seems to be promising.
910;loan_data_histograms_symbolic.png;Feature generation based on the use of variable Self_Employed wouldn’t be useful, but the use of Dependents seems to be promising.
911;loan_data_histograms_symbolic.png;Given the usual semantics of Education variable, dummification would have been a better codification.
912;loan_data_histograms_symbolic.png;It is better to drop the variable Property_Area than removing all records with missing values.
913;loan_data_histograms_symbolic.png;Not knowing the semantics of Dependents variable, dummification could have been a more adequate codification.
914;loan_data_mv.png;Discarding variable Gender would be better than discarding all the records with missing values for that variable.
915;loan_data_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.
916;loan_data_mv.png;Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.
917;loan_data_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.
918;loan_data_mv.png;Feature generation based on variable Dependents seems to be promising.
919;loan_data_mv.png;It is better to drop the variable Self_Employed than removing all records with missing values.
920;loan_data_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
921;loan_data_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.
922;loan_data_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
923;loan_data_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
924;loan_data_histograms_numeric.png;All variables, but the class, should be dealt with as date.
925;loan_data_histograms_numeric.png;The variable Loan_Amount_Term can be seen as ordinal.
926;loan_data_histograms_numeric.png;The variable CoapplicantIncome can be seen as ordinal without losing information.
927;loan_data_histograms_numeric.png;Variable LoanAmount is balanced.
928;loan_data_histograms_numeric.png;It is clear that variable LoanAmount shows some outliers, but we can’t be sure of the same for variable Loan_Amount_Term.
929;loan_data_histograms_numeric.png;Outliers seem to be a problem in the dataset.
930;loan_data_histograms_numeric.png;Variable Loan_Amount_Term shows a high number of outlier values.
931;loan_data_histograms_numeric.png;Variable Loan_Amount_Term doesn’t have any outliers.
932;loan_data_histograms_numeric.png;Variable LoanAmount presents some outliers.
933;loan_data_histograms_numeric.png;At least 85 of the variables present outliers.
934;loan_data_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
935;loan_data_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
936;loan_data_histograms_numeric.png;Considering the common semantics for LoanAmount and ApplicantIncome variables, dummification if applied would increase the risk of facing the curse of dimensionality.
937;loan_data_histograms_numeric.png;Considering the common semantics for LoanAmount variable, dummification would be the most adequate encoding.
938;loan_data_histograms_numeric.png;The variable LoanAmount can be coded as ordinal without losing information.
939;loan_data_histograms_numeric.png;Feature generation based on variable LoanAmount seems to be promising.
940;loan_data_histograms_numeric.png;Feature generation based on the use of variable CoapplicantIncome wouldn’t be useful, but the use of ApplicantIncome seems to be promising.
941;loan_data_histograms_numeric.png;Given the usual semantics of ApplicantIncome variable, dummification would have been a better codification.
942;loan_data_histograms_numeric.png;It is better to drop the variable ApplicantIncome than removing all records with missing values.
943;loan_data_histograms_numeric.png;Not knowing the semantics of Loan_Amount_Term variable, dummification could have been a more adequate codification.
944;Dry_Bean_Dataset_decision_tree.png;The variable Area discriminates between the target values, as shown in the decision tree.
945;Dry_Bean_Dataset_decision_tree.png;Variable AspectRation is one of the most relevant variables.
946;Dry_Bean_Dataset_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.
947;Dry_Bean_Dataset_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
948;Dry_Bean_Dataset_decision_tree.png;The recall for the presented tree is higher than 60%.
949;Dry_Bean_Dataset_decision_tree.png;The number of True Positives is lower than the number of False Negatives for the presented tree.
950;Dry_Bean_Dataset_decision_tree.png;The number of True Positives is higher than the number of False Positives for the presented tree.
951;Dry_Bean_Dataset_decision_tree.png;The precision for the presented tree is higher than its specificity.
952;Dry_Bean_Dataset_decision_tree.png;Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], it is possible to state that KNN algorithm classifies (not A, not B) as SEKER for any k ≤ 1284.
953;Dry_Bean_Dataset_decision_tree.png;Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], the Decision Tree presented classifies (not A, B) as BOMBAY.
954;Dry_Bean_Dataset_decision_tree.png;Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], it is possible to state that KNN algorithm classifies (A,B) as DERMASON for any k ≤ 2501.
955;Dry_Bean_Dataset_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
956;Dry_Bean_Dataset_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
957;Dry_Bean_Dataset_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.
958;Dry_Bean_Dataset_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
959;Dry_Bean_Dataset_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
960;Dry_Bean_Dataset_overfitting_knn.png;KNN is in overfitting for k larger than 17.
961;Dry_Bean_Dataset_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
962;Dry_Bean_Dataset_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
963;Dry_Bean_Dataset_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.
964;Dry_Bean_Dataset_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
965;Dry_Bean_Dataset_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.
966;Dry_Bean_Dataset_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.
967;Dry_Bean_Dataset_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.
968;Dry_Bean_Dataset_pca.png;The first 6 principal components are enough for explaining half the data variance.
969;Dry_Bean_Dataset_pca.png;Using the first 2 principal components would imply an error between 15 and 25%.
970;Dry_Bean_Dataset_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 9.
971;Dry_Bean_Dataset_correlation_heatmap.png;One of the variables MinorAxisLength or Eccentricity can be discarded without losing information.
972;Dry_Bean_Dataset_correlation_heatmap.png;The variable Eccentricity can be discarded without risking losing information.
973;Dry_Bean_Dataset_correlation_heatmap.png;Variables MinorAxisLength and Solidity are redundant, but we can’t say the same for the pair ShapeFactor1 and Extent.
974;Dry_Bean_Dataset_correlation_heatmap.png;Variables roundness and ShapeFactor1 are redundant.
975;Dry_Bean_Dataset_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
976;Dry_Bean_Dataset_correlation_heatmap.png;Variable ShapeFactor1 seems to be relevant for the majority of mining tasks.
977;Dry_Bean_Dataset_correlation_heatmap.png;Variables Perimeter and Eccentricity seem to be useful for classification tasks.
978;Dry_Bean_Dataset_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
979;Dry_Bean_Dataset_correlation_heatmap.png;Removing variable Solidity might improve the training of decision trees .
980;Dry_Bean_Dataset_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Eccentricity previously than variable EquivDiameter.
981;Dry_Bean_Dataset_boxplots.png;Variable MinorAxisLength is balanced.
982;Dry_Bean_Dataset_boxplots.png;Those boxplots show that the data is not normalized.
983;Dry_Bean_Dataset_boxplots.png;It is clear that variable Solidity shows some outliers, but we can’t be sure of the same for variable EquivDiameter.
984;Dry_Bean_Dataset_boxplots.png;Outliers seem to be a problem in the dataset.
985;Dry_Bean_Dataset_boxplots.png;Variable Solidity shows some outlier values.
986;Dry_Bean_Dataset_boxplots.png;Variable roundness doesn’t have any outliers.
987;Dry_Bean_Dataset_boxplots.png;Variable Eccentricity presents some outliers.
988;Dry_Bean_Dataset_boxplots.png;At least 50 of the variables present outliers.
989;Dry_Bean_Dataset_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
990;Dry_Bean_Dataset_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
991;Dry_Bean_Dataset_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
992;Dry_Bean_Dataset_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
993;Dry_Bean_Dataset_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
994;Dry_Bean_Dataset_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
995;Dry_Bean_Dataset_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
996;Dry_Bean_Dataset_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
997;Dry_Bean_Dataset_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
998;Dry_Bean_Dataset_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
999;Dry_Bean_Dataset_histograms_numeric.png;All variables, but the class, should be dealt with as date.
1000;Dry_Bean_Dataset_histograms_numeric.png;The variable Perimeter can be seen as ordinal.
1001;Dry_Bean_Dataset_histograms_numeric.png;The variable Extent can be seen as ordinal without losing information.
1002;Dry_Bean_Dataset_histograms_numeric.png;Variable Solidity is balanced.
1003;Dry_Bean_Dataset_histograms_numeric.png;It is clear that variable EquivDiameter shows some outliers, but we can’t be sure of the same for variable MinorAxisLength.
1004;Dry_Bean_Dataset_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1005;Dry_Bean_Dataset_histograms_numeric.png;Variable Area shows some outlier values.
1006;Dry_Bean_Dataset_histograms_numeric.png;Variable roundness doesn’t have any outliers.
1007;Dry_Bean_Dataset_histograms_numeric.png;Variable Solidity presents some outliers.
1008;Dry_Bean_Dataset_histograms_numeric.png;At least 85 of the variables present outliers.
1009;Dry_Bean_Dataset_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
1010;Dry_Bean_Dataset_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1011;Dry_Bean_Dataset_histograms_numeric.png;Considering the common semantics for AspectRation and Area variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1012;Dry_Bean_Dataset_histograms_numeric.png;Considering the common semantics for EquivDiameter variable, dummification would be the most adequate encoding.
1013;Dry_Bean_Dataset_histograms_numeric.png;The variable roundness can be coded as ordinal without losing information.
1014;Dry_Bean_Dataset_histograms_numeric.png;Feature generation based on variable EquivDiameter seems to be promising.
1015;Dry_Bean_Dataset_histograms_numeric.png;Feature generation based on the use of variable MinorAxisLength wouldn’t be useful, but the use of Area seems to be promising.
1016;Dry_Bean_Dataset_histograms_numeric.png;Given the usual semantics of roundness variable, dummification would have been a better codification.
1017;Dry_Bean_Dataset_histograms_numeric.png;It is better to drop the variable Solidity than removing all records with missing values.
1018;Dry_Bean_Dataset_histograms_numeric.png;Not knowing the semantics of Perimeter variable, dummification could have been a more adequate codification.
1019;credit_customers_decision_tree.png;The variable residence_since discriminates between the target values, as shown in the decision tree.
1020;credit_customers_decision_tree.png;Variable residence_since is one of the most relevant variables.
1021;credit_customers_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
1022;credit_customers_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
1023;credit_customers_decision_tree.png;The accuracy for the presented tree is higher than 90%.
1024;credit_customers_decision_tree.png;The number of False Negatives is lower than the number of True Positives for the presented tree.
1025;credit_customers_decision_tree.png;The number of True Positives is higher than the number of False Positives for the presented tree.
1026;credit_customers_decision_tree.png;The accuracy for the presented tree is higher than its recall.
1027;credit_customers_decision_tree.png;Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 107.
1028;credit_customers_decision_tree.png;Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as bad.
1029;credit_customers_decision_tree.png;Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 264.
1030;credit_customers_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
1031;credit_customers_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
1032;credit_customers_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.
1033;credit_customers_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1034;credit_customers_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
1035;credit_customers_overfitting_knn.png;KNN is in overfitting for k less than 13.
1036;credit_customers_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
1037;credit_customers_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
1038;credit_customers_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.
1039;credit_customers_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
1040;credit_customers_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.
1041;credit_customers_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.
1042;credit_customers_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
1043;credit_customers_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1044;credit_customers_pca.png;The first 3 principal components are enough for explaining half the data variance.
1045;credit_customers_pca.png;Using the first 4 principal components would imply an error between 5 and 20%.
1046;credit_customers_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
1047;credit_customers_correlation_heatmap.png;One of the variables age or credit_amount can be discarded without losing information.
1048;credit_customers_correlation_heatmap.png;The variable existing_credits can be discarded without risking losing information.
1049;credit_customers_correlation_heatmap.png;Variables existing_credits and credit_amount are redundant, but we can’t say the same for the pair duration and installment_commitment.
1050;credit_customers_correlation_heatmap.png;Variables residence_since and existing_credits are redundant.
1051;credit_customers_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1052;credit_customers_correlation_heatmap.png;Variable age seems to be relevant for the majority of mining tasks.
1053;credit_customers_correlation_heatmap.png;Variables age and installment_commitment seem to be useful for classification tasks.
1054;credit_customers_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1055;credit_customers_correlation_heatmap.png;Removing variable credit_amount might improve the training of decision trees .
1056;credit_customers_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable existing_credits previously than variable credit_amount.
1057;credit_customers_boxplots.png;Variable existing_credits is balanced.
1058;credit_customers_boxplots.png;Those boxplots show that the data is not normalized.
1059;credit_customers_boxplots.png;It is clear that variable age shows some outliers, but we can’t be sure of the same for variable residence_since.
1060;credit_customers_boxplots.png;Outliers seem to be a problem in the dataset.
1061;credit_customers_boxplots.png;Variable age shows some outlier values.
1062;credit_customers_boxplots.png;Variable residence_since doesn’t have any outliers.
1063;credit_customers_boxplots.png;Variable age presents some outliers.
1064;credit_customers_boxplots.png;At least 50 of the variables present outliers.
1065;credit_customers_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1066;credit_customers_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1067;credit_customers_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
1068;credit_customers_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1069;credit_customers_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1070;credit_customers_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1071;credit_customers_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.
1072;credit_customers_histograms_symbolic.png;The variable other_parties can be seen as ordinal.
1073;credit_customers_histograms_symbolic.png;The variable employment can be seen as ordinal without losing information.
1074;credit_customers_histograms_symbolic.png;Considering the common semantics for checking_status and employment variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1075;credit_customers_histograms_symbolic.png;Considering the common semantics for housing variable, dummification would be the most adequate encoding.
1076;credit_customers_histograms_symbolic.png;The variable checking_status can be coded as ordinal without losing information.
1077;credit_customers_histograms_symbolic.png;Feature generation based on variable num_dependents seems to be promising.
1078;credit_customers_histograms_symbolic.png;Feature generation based on the use of variable employment wouldn’t be useful, but the use of checking_status seems to be promising.
1079;credit_customers_histograms_symbolic.png;Given the usual semantics of own_telephone variable, dummification would have been a better codification.
1080;credit_customers_histograms_symbolic.png;It is better to drop the variable num_dependents than removing all records with missing values.
1081;credit_customers_histograms_symbolic.png;Not knowing the semantics of num_dependents variable, dummification could have been a more adequate codification.
1082;credit_customers_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1083;credit_customers_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.
1084;credit_customers_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1085;credit_customers_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1086;credit_customers_histograms_numeric.png;All variables, but the class, should be dealt with as binary.
1087;credit_customers_histograms_numeric.png;The variable credit_amount can be seen as ordinal.
1088;credit_customers_histograms_numeric.png;The variable age can be seen as ordinal without losing information.
1089;credit_customers_histograms_numeric.png;Variable duration is balanced.
1090;credit_customers_histograms_numeric.png;It is clear that variable age shows some outliers, but we can’t be sure of the same for variable credit_amount.
1091;credit_customers_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1092;credit_customers_histograms_numeric.png;Variable residence_since shows some outlier values.
1093;credit_customers_histograms_numeric.png;Variable credit_amount doesn’t have any outliers.
1094;credit_customers_histograms_numeric.png;Variable existing_credits presents some outliers.
1095;credit_customers_histograms_numeric.png;At least 60 of the variables present outliers.
1096;credit_customers_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
1097;credit_customers_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1098;credit_customers_histograms_numeric.png;Considering the common semantics for residence_since and duration variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1099;credit_customers_histograms_numeric.png;Considering the common semantics for installment_commitment variable, dummification would be the most adequate encoding.
1100;credit_customers_histograms_numeric.png;The variable age can be coded as ordinal without losing information.
1101;credit_customers_histograms_numeric.png;Feature generation based on variable residence_since seems to be promising.
1102;credit_customers_histograms_numeric.png;Feature generation based on the use of variable credit_amount wouldn’t be useful, but the use of duration seems to be promising.
1103;credit_customers_histograms_numeric.png;Given the usual semantics of age variable, dummification would have been a better codification.
1104;credit_customers_histograms_numeric.png;It is better to drop the variable residence_since than removing all records with missing values.
1105;credit_customers_histograms_numeric.png;Not knowing the semantics of installment_commitment variable, dummification could have been a more adequate codification.
1106;weatherAUS_decision_tree.png;The variable Pressure3pm discriminates between the target values, as shown in the decision tree.
1107;weatherAUS_decision_tree.png;Variable Rainfall is one of the most relevant variables.
1108;weatherAUS_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.
1109;weatherAUS_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1110;weatherAUS_decision_tree.png;The precision for the presented tree is lower than 60%.
1111;weatherAUS_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree.
1112;weatherAUS_decision_tree.png;The number of False Negatives is lower than the number of False Positives for the presented tree.
1113;weatherAUS_decision_tree.png;The accuracy for the presented tree is higher than 75%.
1114;weatherAUS_decision_tree.png;Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], the Decision Tree presented classifies (not A, not B) as No.
1115;weatherAUS_decision_tree.png;Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], the Decision Tree presented classifies (not A, B) as Yes.
1116;weatherAUS_decision_tree.png;Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as No.
1117;weatherAUS_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
1118;weatherAUS_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
1119;weatherAUS_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.
1120;weatherAUS_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1121;weatherAUS_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
1122;weatherAUS_overfitting_knn.png;KNN is in overfitting for k less than 5.
1123;weatherAUS_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
1124;weatherAUS_overfitting_knn.png;KNN with less than 17 neighbours is in overfitting.
1125;weatherAUS_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
1126;weatherAUS_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.
1127;weatherAUS_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.
1128;weatherAUS_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.
1129;weatherAUS_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.
1130;weatherAUS_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1131;weatherAUS_pca.png;The first 3 principal components are enough for explaining half the data variance.
1132;weatherAUS_pca.png;Using the first 6 principal components would imply an error between 5 and 20%.
1133;weatherAUS_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.
1134;weatherAUS_correlation_heatmap.png;One of the variables Pressure9am or Pressure3pm can be discarded without losing information.
1135;weatherAUS_correlation_heatmap.png;The variable Pressure9am can be discarded without risking losing information.
1136;weatherAUS_correlation_heatmap.png;Variables Rainfall and Pressure3pm are redundant, but we can’t say the same for the pair Pressure9am and Cloud3pm.
1137;weatherAUS_correlation_heatmap.png;Variables Temp3pm and Rainfall are redundant.
1138;weatherAUS_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1139;weatherAUS_correlation_heatmap.png;Variable Temp3pm seems to be relevant for the majority of mining tasks.
1140;weatherAUS_correlation_heatmap.png;Variables Pressure9am and Cloud3pm seem to be useful for classification tasks.
1141;weatherAUS_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1142;weatherAUS_correlation_heatmap.png;Removing variable Cloud9am might improve the training of decision trees .
1143;weatherAUS_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Cloud9am previously than variable Pressure9am.
1144;weatherAUS_boxplots.png;Variable Pressure9am is balanced.
1145;weatherAUS_boxplots.png;Those boxplots show that the data is not normalized.
1146;weatherAUS_boxplots.png;It is clear that variable Cloud9am shows some outliers, but we can’t be sure of the same for variable WindSpeed9am.
1147;weatherAUS_boxplots.png;Outliers seem to be a problem in the dataset.
1148;weatherAUS_boxplots.png;Variable Rainfall shows a high number of outlier values.
1149;weatherAUS_boxplots.png;Variable Cloud9am doesn’t have any outliers.
1150;weatherAUS_boxplots.png;Variable Cloud9am presents some outliers.
1151;weatherAUS_boxplots.png;At least 60 of the variables present outliers.
1152;weatherAUS_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1153;weatherAUS_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1154;weatherAUS_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
1155;weatherAUS_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1156;weatherAUS_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1157;weatherAUS_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1158;weatherAUS_histograms_symbolic.png;All variables, but the class, should be dealt with as symbolic.
1159;weatherAUS_histograms_symbolic.png;The variable WindDir9am can be seen as ordinal.
1160;weatherAUS_histograms_symbolic.png;The variable WindDir3pm can be seen as ordinal without losing information.
1161;weatherAUS_histograms_symbolic.png;Considering the common semantics for Location and WindGustDir variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1162;weatherAUS_histograms_symbolic.png;Considering the common semantics for WindGustDir variable, dummification would be the most adequate encoding.
1163;weatherAUS_histograms_symbolic.png;The variable WindDir3pm can be coded as ordinal without losing information.
1164;weatherAUS_histograms_symbolic.png;Feature generation based on variable WindDir3pm seems to be promising.
1165;weatherAUS_histograms_symbolic.png;Feature generation based on the use of variable WindDir3pm wouldn’t be useful, but the use of Location seems to be promising.
1166;weatherAUS_histograms_symbolic.png;Given the usual semantics of RainToday variable, dummification would have been a better codification.
1167;weatherAUS_histograms_symbolic.png;It is better to drop the variable Location than removing all records with missing values.
1168;weatherAUS_histograms_symbolic.png;Not knowing the semantics of WindGustDir variable, dummification could have been a more adequate codification.
1169;weatherAUS_mv.png;Discarding variable Pressure9am would be better than discarding all the records with missing values for that variable.
1170;weatherAUS_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.
1171;weatherAUS_mv.png;Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.
1172;weatherAUS_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.
1173;weatherAUS_mv.png;Feature generation based on variable RainToday seems to be promising.
1174;weatherAUS_mv.png;It is better to drop the variable Cloud9am than removing all records with missing values.
1175;weatherAUS_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1176;weatherAUS_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.
1177;weatherAUS_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1178;weatherAUS_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1179;weatherAUS_histograms_numeric.png;All variables, but the class, should be dealt with as date.
1180;weatherAUS_histograms_numeric.png;The variable Rainfall can be seen as ordinal.
1181;weatherAUS_histograms_numeric.png;The variable Pressure3pm can be seen as ordinal without losing information.
1182;weatherAUS_histograms_numeric.png;Variable Cloud3pm is balanced.
1183;weatherAUS_histograms_numeric.png;It is clear that variable Pressure9am shows some outliers, but we can’t be sure of the same for variable Rainfall.
1184;weatherAUS_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1185;weatherAUS_histograms_numeric.png;Variable Pressure9am shows some outlier values.
1186;weatherAUS_histograms_numeric.png;Variable Cloud3pm doesn’t have any outliers.
1187;weatherAUS_histograms_numeric.png;Variable Pressure9am presents some outliers.
1188;weatherAUS_histograms_numeric.png;At least 85 of the variables present outliers.
1189;weatherAUS_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
1190;weatherAUS_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1191;weatherAUS_histograms_numeric.png;Considering the common semantics for Pressure3pm and Rainfall variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1192;weatherAUS_histograms_numeric.png;Considering the common semantics for Rainfall variable, dummification would be the most adequate encoding.
1193;weatherAUS_histograms_numeric.png;The variable Cloud9am can be coded as ordinal without losing information.
1194;weatherAUS_histograms_numeric.png;Feature generation based on variable Temp3pm seems to be promising.
1195;weatherAUS_histograms_numeric.png;Feature generation based on the use of variable Cloud9am wouldn’t be useful, but the use of Rainfall seems to be promising.
1196;weatherAUS_histograms_numeric.png;Given the usual semantics of WindSpeed9am variable, dummification would have been a better codification.
1197;weatherAUS_histograms_numeric.png;It is better to drop the variable Rainfall than removing all records with missing values.
1198;weatherAUS_histograms_numeric.png;Not knowing the semantics of Rainfall variable, dummification could have been a more adequate codification.
1199;car_insurance_decision_tree.png;The variable displacement discriminates between the target values, as shown in the decision tree.
1200;car_insurance_decision_tree.png;Variable displacement is one of the most relevant variables.
1201;car_insurance_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.
1202;car_insurance_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1203;car_insurance_decision_tree.png;The specificity for the presented tree is higher than 60%.
1204;car_insurance_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree.
1205;car_insurance_decision_tree.png;The number of True Negatives is lower than the number of True Positives for the presented tree.
1206;car_insurance_decision_tree.png;The number of True Positives is lower than the number of False Positives for the presented tree.
1207;car_insurance_decision_tree.png;Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 2141.
1208;car_insurance_decision_tree.png;Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], the Decision Tree presented classifies (not A, B) as 1.
1209;car_insurance_decision_tree.png;Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 686.
1210;car_insurance_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
1211;car_insurance_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
1212;car_insurance_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.
1213;car_insurance_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1214;car_insurance_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
1215;car_insurance_overfitting_knn.png;KNN is in overfitting for k larger than 5.
1216;car_insurance_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
1217;car_insurance_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.
1218;car_insurance_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.
1219;car_insurance_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
1220;car_insurance_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.
1221;car_insurance_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.
1222;car_insurance_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.
1223;car_insurance_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1224;car_insurance_pca.png;The first 4 principal components are enough for explaining half the data variance.
1225;car_insurance_pca.png;Using the first 6 principal components would imply an error between 5 and 25%.
1226;car_insurance_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 6.
1227;car_insurance_correlation_heatmap.png;One of the variables age_of_car or airbags can be discarded without losing information.
1228;car_insurance_correlation_heatmap.png;The variable length can be discarded without risking losing information.
1229;car_insurance_correlation_heatmap.png;Variables age_of_car and policy_tenure are redundant, but we can’t say the same for the pair height and length.
1230;car_insurance_correlation_heatmap.png;Variables age_of_car and gross_weight are redundant.
1231;car_insurance_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1232;car_insurance_correlation_heatmap.png;Variable height seems to be relevant for the majority of mining tasks.
1233;car_insurance_correlation_heatmap.png;Variables gross_weight and width seem to be useful for classification tasks.
1234;car_insurance_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1235;car_insurance_correlation_heatmap.png;Removing variable length might improve the training of decision trees .
1236;car_insurance_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable length previously than variable gross_weight.
1237;car_insurance_boxplots.png;Variable height is balanced.
1238;car_insurance_boxplots.png;Those boxplots show that the data is not normalized.
1239;car_insurance_boxplots.png;It is clear that variable displacement shows some outliers, but we can’t be sure of the same for variable policy_tenure.
1240;car_insurance_boxplots.png;Outliers seem to be a problem in the dataset.
1241;car_insurance_boxplots.png;Variable airbags shows some outlier values.
1242;car_insurance_boxplots.png;Variable width doesn’t have any outliers.
1243;car_insurance_boxplots.png;Variable length presents some outliers.
1244;car_insurance_boxplots.png;At least 50 of the variables present outliers.
1245;car_insurance_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1246;car_insurance_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1247;car_insurance_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
1248;car_insurance_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1249;car_insurance_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1250;car_insurance_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1251;car_insurance_histograms_symbolic.png;All variables, but the class, should be dealt with as symbolic.
1252;car_insurance_histograms_symbolic.png;The variable segment can be seen as ordinal.
1253;car_insurance_histograms_symbolic.png;The variable is_esc can be seen as ordinal without losing information.
1254;car_insurance_histograms_symbolic.png;Considering the common semantics for segment and area_cluster variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1255;car_insurance_histograms_symbolic.png;Considering the common semantics for max_torque variable, dummification would be the most adequate encoding.
1256;car_insurance_histograms_symbolic.png;The variable max_torque can be coded as ordinal without losing information.
1257;car_insurance_histograms_symbolic.png;Feature generation based on variable area_cluster seems to be promising.
1258;car_insurance_histograms_symbolic.png;Feature generation based on the use of variable steering_type wouldn’t be useful, but the use of area_cluster seems to be promising.
1259;car_insurance_histograms_symbolic.png;Given the usual semantics of model variable, dummification would have been a better codification.
1260;car_insurance_histograms_symbolic.png;It is better to drop the variable steering_type than removing all records with missing values.
1261;car_insurance_histograms_symbolic.png;Not knowing the semantics of is_esc variable, dummification could have been a more adequate codification.
1262;car_insurance_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1263;car_insurance_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.
1264;car_insurance_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1265;car_insurance_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1266;car_insurance_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
1267;car_insurance_histograms_numeric.png;The variable age_of_car can be seen as ordinal.
1268;car_insurance_histograms_numeric.png;The variable height can be seen as ordinal without losing information.
1269;car_insurance_histograms_numeric.png;Variable displacement is balanced.
1270;car_insurance_histograms_numeric.png;It is clear that variable displacement shows some outliers, but we can’t be sure of the same for variable age_of_car.
1271;car_insurance_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1272;car_insurance_histograms_numeric.png;Variable displacement shows some outlier values.
1273;car_insurance_histograms_numeric.png;Variable width doesn’t have any outliers.
1274;car_insurance_histograms_numeric.png;Variable height presents some outliers.
1275;car_insurance_histograms_numeric.png;At least 60 of the variables present outliers.
1276;car_insurance_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
1277;car_insurance_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1278;car_insurance_histograms_numeric.png;Considering the common semantics for displacement and policy_tenure variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1279;car_insurance_histograms_numeric.png;Considering the common semantics for length variable, dummification would be the most adequate encoding.
1280;car_insurance_histograms_numeric.png;The variable age_of_car can be coded as ordinal without losing information.
1281;car_insurance_histograms_numeric.png;Feature generation based on variable height seems to be promising.
1282;car_insurance_histograms_numeric.png;Feature generation based on the use of variable age_of_car wouldn’t be useful, but the use of policy_tenure seems to be promising.
1283;car_insurance_histograms_numeric.png;Given the usual semantics of age_of_policyholder variable, dummification would have been a better codification.
1284;car_insurance_histograms_numeric.png;It is better to drop the variable gross_weight than removing all records with missing values.
1285;car_insurance_histograms_numeric.png;Not knowing the semantics of displacement variable, dummification could have been a more adequate codification.
1286;heart_decision_tree.png;The variable slope discriminates between the target values, as shown in the decision tree.
1287;heart_decision_tree.png;Variable slope is one of the most relevant variables.
1288;heart_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
1289;heart_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1290;heart_decision_tree.png;The specificity for the presented tree is lower than 75%.
1291;heart_decision_tree.png;The number of True Negatives is higher than the number of False Positives for the presented tree.
1292;heart_decision_tree.png;The number of False Positives is lower than the number of True Negatives for the presented tree.
1293;heart_decision_tree.png;The precision for the presented tree is lower than its specificity.
1294;heart_decision_tree.png;Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], the Decision Tree presented classifies (not A, B) as 1.
1295;heart_decision_tree.png;Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], the Decision Tree presented classifies (not A, B) as 1.
1296;heart_decision_tree.png;Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as 0.
1297;heart_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
1298;heart_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
1299;heart_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.
1300;heart_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1301;heart_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
1302;heart_overfitting_knn.png;KNN is in overfitting for k less than 17.
1303;heart_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
1304;heart_overfitting_knn.png;KNN with more than 7 neighbours is in overfitting.
1305;heart_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.
1306;heart_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.
1307;heart_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.
1308;heart_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.
1309;heart_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.
1310;heart_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1311;heart_pca.png;The first 4 principal components are enough for explaining half the data variance.
1312;heart_pca.png;Using the first 9 principal components would imply an error between 15 and 20%.
1313;heart_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
1314;heart_correlation_heatmap.png;One of the variables restecg or age can be discarded without losing information.
1315;heart_correlation_heatmap.png;The variable trestbps can be discarded without risking losing information.
1316;heart_correlation_heatmap.png;Variables cp and age are redundant, but we can’t say the same for the pair ca and trestbps.
1317;heart_correlation_heatmap.png;Variables restecg and oldpeak are redundant.
1318;heart_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1319;heart_correlation_heatmap.png;Variable thalach seems to be relevant for the majority of mining tasks.
1320;heart_correlation_heatmap.png;Variables cp and chol seem to be useful for classification tasks.
1321;heart_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1322;heart_correlation_heatmap.png;Removing variable age might improve the training of decision trees .
1323;heart_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable restecg previously than variable slope.
1324;heart_boxplots.png;Variable thal is balanced.
1325;heart_boxplots.png;Those boxplots show that the data is not normalized.
1326;heart_boxplots.png;It is clear that variable trestbps shows some outliers, but we can’t be sure of the same for variable restecg.
1327;heart_boxplots.png;Outliers seem to be a problem in the dataset.
1328;heart_boxplots.png;Variable chol shows some outlier values.
1329;heart_boxplots.png;Variable restecg doesn’t have any outliers.
1330;heart_boxplots.png;Variable restecg presents some outliers.
1331;heart_boxplots.png;At least 85 of the variables present outliers.
1332;heart_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1333;heart_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1334;heart_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
1335;heart_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1336;heart_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1337;heart_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1338;heart_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.
1339;heart_histograms_symbolic.png;The variable sex can be seen as ordinal.
1340;heart_histograms_symbolic.png;The variable sex can be seen as ordinal without losing information.
1341;heart_histograms_symbolic.png;Considering the common semantics for fbs and sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1342;heart_histograms_symbolic.png;Considering the common semantics for sex variable, dummification would be the most adequate encoding.
1343;heart_histograms_symbolic.png;The variable sex can be coded as ordinal without losing information.
1344;heart_histograms_symbolic.png;Feature generation based on variable exang seems to be promising.
1345;heart_histograms_symbolic.png;Feature generation based on the use of variable exang wouldn’t be useful, but the use of sex seems to be promising.
1346;heart_histograms_symbolic.png;Given the usual semantics of sex variable, dummification would have been a better codification.
1347;heart_histograms_symbolic.png;It is better to drop the variable exang than removing all records with missing values.
1348;heart_histograms_symbolic.png;Not knowing the semantics of sex variable, dummification could have been a more adequate codification.
1349;heart_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1350;heart_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
1351;heart_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1352;heart_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1353;heart_histograms_numeric.png;All variables, but the class, should be dealt with as binary.
1354;heart_histograms_numeric.png;The variable chol can be seen as ordinal.
1355;heart_histograms_numeric.png;The variable age can be seen as ordinal without losing information.
1356;heart_histograms_numeric.png;Variable restecg is balanced.
1357;heart_histograms_numeric.png;It is clear that variable chol shows some outliers, but we can’t be sure of the same for variable age.
1358;heart_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1359;heart_histograms_numeric.png;Variable age shows some outlier values.
1360;heart_histograms_numeric.png;Variable chol doesn’t have any outliers.
1361;heart_histograms_numeric.png;Variable ca presents some outliers.
1362;heart_histograms_numeric.png;At least 50 of the variables present outliers.
1363;heart_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
1364;heart_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1365;heart_histograms_numeric.png;Considering the common semantics for chol and age variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1366;heart_histograms_numeric.png;Considering the common semantics for restecg variable, dummification would be the most adequate encoding.
1367;heart_histograms_numeric.png;The variable thal can be coded as ordinal without losing information.
1368;heart_histograms_numeric.png;Feature generation based on variable cp seems to be promising.
1369;heart_histograms_numeric.png;Feature generation based on the use of variable thalach wouldn’t be useful, but the use of age seems to be promising.
1370;heart_histograms_numeric.png;Given the usual semantics of restecg variable, dummification would have been a better codification.
1371;heart_histograms_numeric.png;It is better to drop the variable trestbps than removing all records with missing values.
1372;heart_histograms_numeric.png;Not knowing the semantics of trestbps variable, dummification could have been a more adequate codification.
1373;Breast_Cancer_decision_tree.png;The variable texture_worst discriminates between the target values, as shown in the decision tree.
1374;Breast_Cancer_decision_tree.png;Variable texture_worst is one of the most relevant variables.
1375;Breast_Cancer_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
1376;Breast_Cancer_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
1377;Breast_Cancer_decision_tree.png;The recall for the presented tree is higher than 60%.
1378;Breast_Cancer_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree.
1379;Breast_Cancer_decision_tree.png;The number of False Positives is lower than the number of False Negatives for the presented tree.
1380;Breast_Cancer_decision_tree.png;The number of True Positives is lower than the number of False Positives for the presented tree.
1381;Breast_Cancer_decision_tree.png;Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that Naive Bayes algorithm classifies (A, not B), as M.
1382;Breast_Cancer_decision_tree.png;Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that Naive Bayes algorithm classifies (not A, B), as M.
1383;Breast_Cancer_decision_tree.png;Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that KNN algorithm classifies (not A, B) as M for any k ≤ 20.
1384;Breast_Cancer_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
1385;Breast_Cancer_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
1386;Breast_Cancer_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.
1387;Breast_Cancer_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1388;Breast_Cancer_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
1389;Breast_Cancer_overfitting_knn.png;KNN is in overfitting for k larger than 5.
1390;Breast_Cancer_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
1391;Breast_Cancer_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
1392;Breast_Cancer_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.
1393;Breast_Cancer_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.
1394;Breast_Cancer_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.
1395;Breast_Cancer_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.
1396;Breast_Cancer_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
1397;Breast_Cancer_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1398;Breast_Cancer_pca.png;The first 6 principal components are enough for explaining half the data variance.
1399;Breast_Cancer_pca.png;Using the first 6 principal components would imply an error between 10 and 30%.
1400;Breast_Cancer_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.
1401;Breast_Cancer_correlation_heatmap.png;One of the variables symmetry_se or area_se can be discarded without losing information.
1402;Breast_Cancer_correlation_heatmap.png;The variable perimeter_worst can be discarded without risking losing information.
1403;Breast_Cancer_correlation_heatmap.png;Variables texture_worst and radius_worst are redundant, but we can’t say the same for the pair perimeter_worst and texture_se.
1404;Breast_Cancer_correlation_heatmap.png;Variables texture_worst and perimeter_se are redundant.
1405;Breast_Cancer_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1406;Breast_Cancer_correlation_heatmap.png;Variable area_se seems to be relevant for the majority of mining tasks.
1407;Breast_Cancer_correlation_heatmap.png;Variables symmetry_se and perimeter_mean seem to be useful for classification tasks.
1408;Breast_Cancer_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1409;Breast_Cancer_correlation_heatmap.png;Removing variable texture_se might improve the training of decision trees .
1410;Breast_Cancer_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable texture_mean previously than variable perimeter_se.
1411;Breast_Cancer_boxplots.png;Variable perimeter_se is balanced.
1412;Breast_Cancer_boxplots.png;Those boxplots show that the data is not normalized.
1413;Breast_Cancer_boxplots.png;It is clear that variable texture_mean shows some outliers, but we can’t be sure of the same for variable perimeter_se.
1414;Breast_Cancer_boxplots.png;Outliers seem to be a problem in the dataset.
1415;Breast_Cancer_boxplots.png;Variable texture_se shows a high number of outlier values.
1416;Breast_Cancer_boxplots.png;Variable texture_mean doesn’t have any outliers.
1417;Breast_Cancer_boxplots.png;Variable radius_worst presents some outliers.
1418;Breast_Cancer_boxplots.png;At least 60 of the variables present outliers.
1419;Breast_Cancer_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1420;Breast_Cancer_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1421;Breast_Cancer_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
1422;Breast_Cancer_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1423;Breast_Cancer_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1424;Breast_Cancer_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1425;Breast_Cancer_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1426;Breast_Cancer_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.
1427;Breast_Cancer_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1428;Breast_Cancer_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1429;Breast_Cancer_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
1430;Breast_Cancer_histograms_numeric.png;The variable perimeter_mean can be seen as ordinal.
1431;Breast_Cancer_histograms_numeric.png;The variable radius_worst can be seen as ordinal without losing information.
1432;Breast_Cancer_histograms_numeric.png;Variable texture_se is balanced.
1433;Breast_Cancer_histograms_numeric.png;It is clear that variable radius_worst shows some outliers, but we can’t be sure of the same for variable perimeter_worst.
1434;Breast_Cancer_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1435;Breast_Cancer_histograms_numeric.png;Variable smoothness_se shows some outlier values.
1436;Breast_Cancer_histograms_numeric.png;Variable smoothness_se doesn’t have any outliers.
1437;Breast_Cancer_histograms_numeric.png;Variable texture_worst presents some outliers.
1438;Breast_Cancer_histograms_numeric.png;At least 60 of the variables present outliers.
1439;Breast_Cancer_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
1440;Breast_Cancer_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1441;Breast_Cancer_histograms_numeric.png;Considering the common semantics for perimeter_mean and texture_mean variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1442;Breast_Cancer_histograms_numeric.png;Considering the common semantics for area_se variable, dummification would be the most adequate encoding.
1443;Breast_Cancer_histograms_numeric.png;The variable smoothness_se can be coded as ordinal without losing information.
1444;Breast_Cancer_histograms_numeric.png;Feature generation based on variable perimeter_worst seems to be promising.
1445;Breast_Cancer_histograms_numeric.png;Feature generation based on the use of variable area_se wouldn’t be useful, but the use of texture_mean seems to be promising.
1446;Breast_Cancer_histograms_numeric.png;Given the usual semantics of perimeter_worst variable, dummification would have been a better codification.
1447;Breast_Cancer_histograms_numeric.png;It is better to drop the variable texture_mean than removing all records with missing values.
1448;Breast_Cancer_histograms_numeric.png;Not knowing the semantics of smoothness_se variable, dummification could have been a more adequate codification.
1449;e-commerce_decision_tree.png;The variable Prior_purchases discriminates between the target values, as shown in the decision tree.
1450;e-commerce_decision_tree.png;Variable Prior_purchases is one of the most relevant variables.
1451;e-commerce_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.
1452;e-commerce_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1453;e-commerce_decision_tree.png;The accuracy for the presented tree is lower than 60%.
1454;e-commerce_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree.
1455;e-commerce_decision_tree.png;The number of True Negatives is lower than the number of False Negatives for the presented tree.
1456;e-commerce_decision_tree.png;The accuracy for the presented tree is higher than 60%.
1457;e-commerce_decision_tree.png;Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (A,B) as Yes for any k ≤ 1596.
1458;e-commerce_decision_tree.png;Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (not A, B) as No for any k ≤ 3657.
1459;e-commerce_decision_tree.png;Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (not A, B) as No for any k ≤ 1596.
1460;e-commerce_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
1461;e-commerce_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
1462;e-commerce_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.
1463;e-commerce_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1464;e-commerce_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
1465;e-commerce_overfitting_knn.png;KNN is in overfitting for k less than 17.
1466;e-commerce_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
1467;e-commerce_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.
1468;e-commerce_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.
1469;e-commerce_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.
1470;e-commerce_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.
1471;e-commerce_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.
1472;e-commerce_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.
1473;e-commerce_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1474;e-commerce_pca.png;The first 2 principal components are enough for explaining half the data variance.
1475;e-commerce_pca.png;Using the first 4 principal components would imply an error between 10 and 25%.
1476;e-commerce_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.
1477;e-commerce_correlation_heatmap.png;One of the variables Discount_offered or Prior_purchases can be discarded without losing information.
1478;e-commerce_correlation_heatmap.png;The variable Customer_rating can be discarded without risking losing information.
1479;e-commerce_correlation_heatmap.png;Variables Customer_care_calls and Cost_of_the_Product are redundant.
1480;e-commerce_correlation_heatmap.png;Variables Prior_purchases and Cost_of_the_Product are redundant.
1481;e-commerce_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1482;e-commerce_correlation_heatmap.png;Variable Discount_offered seems to be relevant for the majority of mining tasks.
1483;e-commerce_correlation_heatmap.png;Variables Weight_in_gms and Prior_purchases seem to be useful for classification tasks.
1484;e-commerce_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1485;e-commerce_correlation_heatmap.png;Removing variable Cost_of_the_Product might improve the training of decision trees .
1486;e-commerce_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Discount_offered previously than variable Cost_of_the_Product.
1487;e-commerce_boxplots.png;Variable Discount_offered is balanced.
1488;e-commerce_boxplots.png;Those boxplots show that the data is not normalized.
1489;e-commerce_boxplots.png;It is clear that variable Customer_rating shows some outliers, but we can’t be sure of the same for variable Prior_purchases.
1490;e-commerce_boxplots.png;Outliers seem to be a problem in the dataset.
1491;e-commerce_boxplots.png;Variable Discount_offered shows some outlier values.
1492;e-commerce_boxplots.png;Variable Customer_rating doesn’t have any outliers.
1493;e-commerce_boxplots.png;Variable Prior_purchases presents some outliers.
1494;e-commerce_boxplots.png;At least 60 of the variables present outliers.
1495;e-commerce_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1496;e-commerce_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1497;e-commerce_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
1498;e-commerce_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1499;e-commerce_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1500;e-commerce_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1501;e-commerce_histograms_symbolic.png;All variables, but the class, should be dealt with as symbolic.
1502;e-commerce_histograms_symbolic.png;The variable Warehouse_block can be seen as ordinal.
1503;e-commerce_histograms_symbolic.png;The variable Product_importance can be seen as ordinal without losing information.
1504;e-commerce_histograms_symbolic.png;Considering the common semantics for Mode_of_Shipment and Warehouse_block variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1505;e-commerce_histograms_symbolic.png;Considering the common semantics for Mode_of_Shipment variable, dummification would be the most adequate encoding.
1506;e-commerce_histograms_symbolic.png;The variable Product_importance can be coded as ordinal without losing information.
1507;e-commerce_histograms_symbolic.png;Feature generation based on variable Gender seems to be promising.
1508;e-commerce_histograms_symbolic.png;Feature generation based on the use of variable Warehouse_block wouldn’t be useful, but the use of Mode_of_Shipment seems to be promising.
1509;e-commerce_histograms_symbolic.png;Given the usual semantics of Warehouse_block variable, dummification would have been a better codification.
1510;e-commerce_histograms_symbolic.png;It is better to drop the variable Product_importance than removing all records with missing values.
1511;e-commerce_histograms_symbolic.png;Not knowing the semantics of Product_importance variable, dummification could have been a more adequate codification.
1512;e-commerce_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1513;e-commerce_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.
1514;e-commerce_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1515;e-commerce_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1516;e-commerce_histograms_numeric.png;All variables, but the class, should be dealt with as date.
1517;e-commerce_histograms_numeric.png;The variable Weight_in_gms can be seen as ordinal.
1518;e-commerce_histograms_numeric.png;The variable Weight_in_gms can be seen as ordinal without losing information.
1519;e-commerce_histograms_numeric.png;Variable Customer_care_calls is balanced.
1520;e-commerce_histograms_numeric.png;It is clear that variable Discount_offered shows some outliers, but we can’t be sure of the same for variable Customer_care_calls.
1521;e-commerce_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1522;e-commerce_histograms_numeric.png;Variable Prior_purchases shows a high number of outlier values.
1523;e-commerce_histograms_numeric.png;Variable Prior_purchases doesn’t have any outliers.
1524;e-commerce_histograms_numeric.png;Variable Discount_offered presents some outliers.
1525;e-commerce_histograms_numeric.png;At least 85 of the variables present outliers.
1526;e-commerce_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
1527;e-commerce_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1528;e-commerce_histograms_numeric.png;Considering the common semantics for Prior_purchases and Customer_care_calls variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1529;e-commerce_histograms_numeric.png;Considering the common semantics for Customer_care_calls variable, dummification would be the most adequate encoding.
1530;e-commerce_histograms_numeric.png;The variable Customer_care_calls can be coded as ordinal without losing information.
1531;e-commerce_histograms_numeric.png;Feature generation based on variable Discount_offered seems to be promising.
1532;e-commerce_histograms_numeric.png;Feature generation based on the use of variable Discount_offered wouldn’t be useful, but the use of Customer_care_calls seems to be promising.
1533;e-commerce_histograms_numeric.png;Given the usual semantics of Discount_offered variable, dummification would have been a better codification.
1534;e-commerce_histograms_numeric.png;It is better to drop the variable Discount_offered than removing all records with missing values.
1535;e-commerce_histograms_numeric.png;Not knowing the semantics of Cost_of_the_Product variable, dummification could have been a more adequate codification.
1536;maintenance_decision_tree.png;The variable Rotational speed [rpm] discriminates between the target values, as shown in the decision tree.
1537;maintenance_decision_tree.png;Variable Rotational speed [rpm] is one of the most relevant variables.
1538;maintenance_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
1539;maintenance_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1540;maintenance_decision_tree.png;The precision for the presented tree is lower than 60%.
1541;maintenance_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree.
1542;maintenance_decision_tree.png;The number of True Negatives is higher than the number of False Negatives for the presented tree.
1543;maintenance_decision_tree.png;The number of True Negatives reported in the same tree is 50.
1544;maintenance_decision_tree.png;Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 5990.
1545;maintenance_decision_tree.png;Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 46.
1546;maintenance_decision_tree.png;Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 46.
1547;maintenance_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
1548;maintenance_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
1549;maintenance_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.
1550;maintenance_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1551;maintenance_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
1552;maintenance_overfitting_knn.png;KNN is in overfitting for k larger than 13.
1553;maintenance_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
1554;maintenance_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.
1555;maintenance_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.
1556;maintenance_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.
1557;maintenance_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.
1558;maintenance_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.
1559;maintenance_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
1560;maintenance_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1561;maintenance_pca.png;The first 2 principal components are enough for explaining half the data variance.
1562;maintenance_pca.png;Using the first 2 principal components would imply an error between 10 and 25%.
1563;maintenance_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
1564;maintenance_correlation_heatmap.png;One of the variables Process temperature [K] or Torque [Nm] can be discarded without losing information.
1565;maintenance_correlation_heatmap.png;The variable Rotational speed [rpm] can be discarded without risking losing information.
1566;maintenance_correlation_heatmap.png;Variables Air temperature [K] and Tool wear [min] seem to be useful for classification tasks.
1567;maintenance_correlation_heatmap.png;Variables Rotational speed [rpm] and Process temperature [K] are redundant.
1568;maintenance_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1569;maintenance_correlation_heatmap.png;Variable Tool wear [min] seems to be relevant for the majority of mining tasks.
1570;maintenance_correlation_heatmap.png;Variables Torque [Nm] and Tool wear [min] seem to be useful for classification tasks.
1571;maintenance_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1572;maintenance_correlation_heatmap.png;Removing variable Torque [Nm] might improve the training of decision trees .
1573;maintenance_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Rotational speed [rpm] previously than variable Torque [Nm].
1574;maintenance_boxplots.png;Variable Process temperature [K] is balanced.
1575;maintenance_boxplots.png;Those boxplots show that the data is not normalized.
1576;maintenance_boxplots.png;It is clear that variable Rotational speed [rpm] shows some outliers, but we can’t be sure of the same for variable Torque [Nm].
1577;maintenance_boxplots.png;Outliers seem to be a problem in the dataset.
1578;maintenance_boxplots.png;Variable Tool wear [min] shows a high number of outlier values.
1579;maintenance_boxplots.png;Variable Air temperature [K] doesn’t have any outliers.
1580;maintenance_boxplots.png;Variable Tool wear [min] presents some outliers.
1581;maintenance_boxplots.png;At least 85 of the variables present outliers.
1582;maintenance_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1583;maintenance_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1584;maintenance_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
1585;maintenance_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1586;maintenance_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1587;maintenance_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1588;maintenance_histograms_symbolic.png;All variables, but the class, should be dealt with as date.
1589;maintenance_histograms_symbolic.png;The variable TWF can be seen as ordinal.
1590;maintenance_histograms_symbolic.png;The variable HDF can be seen as ordinal without losing information.
1591;maintenance_histograms_symbolic.png;Considering the common semantics for PWF and Type variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1592;maintenance_histograms_symbolic.png;Considering the common semantics for Type variable, dummification would be the most adequate encoding.
1593;maintenance_histograms_symbolic.png;The variable Type can be coded as ordinal without losing information.
1594;maintenance_histograms_symbolic.png;Feature generation based on variable OSF seems to be promising.
1595;maintenance_histograms_symbolic.png;Feature generation based on the use of variable RNF wouldn’t be useful, but the use of Type seems to be promising.
1596;maintenance_histograms_symbolic.png;Given the usual semantics of OSF variable, dummification would have been a better codification.
1597;maintenance_histograms_symbolic.png;It is better to drop the variable PWF than removing all records with missing values.
1598;maintenance_histograms_symbolic.png;Not knowing the semantics of RNF variable, dummification could have been a more adequate codification.
1599;maintenance_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1600;maintenance_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.
1601;maintenance_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1602;maintenance_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1603;maintenance_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
1604;maintenance_histograms_numeric.png;The variable Rotational speed [rpm] can be seen as ordinal.
1605;maintenance_histograms_numeric.png;The variable Air temperature [K] can be seen as ordinal without losing information.
1606;maintenance_histograms_numeric.png;Variable Rotational speed [rpm] is balanced.
1607;maintenance_histograms_numeric.png;It is clear that variable Air temperature [K] shows some outliers, but we can’t be sure of the same for variable Torque [Nm].
1608;maintenance_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1609;maintenance_histograms_numeric.png;Variable Torque [Nm] shows some outlier values.
1610;maintenance_histograms_numeric.png;Variable Air temperature [K] doesn’t have any outliers.
1611;maintenance_histograms_numeric.png;Variable Process temperature [K] presents some outliers.
1612;maintenance_histograms_numeric.png;At least 85 of the variables present outliers.
1613;maintenance_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
1614;maintenance_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1615;maintenance_histograms_numeric.png;Considering the common semantics for Torque [Nm] and Air temperature [K] variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1616;maintenance_histograms_numeric.png;Considering the common semantics for Torque [Nm] variable, dummification would be the most adequate encoding.
1617;maintenance_histograms_numeric.png;The variable Rotational speed [rpm] can be coded as ordinal without losing information.
1618;maintenance_histograms_numeric.png;Feature generation based on variable Rotational speed [rpm] seems to be promising.
1619;maintenance_histograms_numeric.png;Feature generation based on the use of variable Air temperature [K] wouldn’t be useful, but the use of Process temperature [K] seems to be promising.
1620;maintenance_histograms_numeric.png;Given the usual semantics of Rotational speed [rpm] variable, dummification would have been a better codification.
1621;maintenance_histograms_numeric.png;It is better to drop the variable Process temperature [K] than removing all records with missing values.
1622;maintenance_histograms_numeric.png;Not knowing the semantics of Tool wear [min] variable, dummification could have been a more adequate codification.
1623;Churn_Modelling_decision_tree.png;The variable NumOfProducts discriminates between the target values, as shown in the decision tree.
1624;Churn_Modelling_decision_tree.png;Variable Age is one of the most relevant variables.
1625;Churn_Modelling_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.
1626;Churn_Modelling_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1627;Churn_Modelling_decision_tree.png;The specificity for the presented tree is lower than 90%.
1628;Churn_Modelling_decision_tree.png;The number of True Positives reported in the same tree is 50.
1629;Churn_Modelling_decision_tree.png;The number of True Positives is lower than the number of False Negatives for the presented tree.
1630;Churn_Modelling_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree.
1631;Churn_Modelling_decision_tree.png;Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 124.
1632;Churn_Modelling_decision_tree.png;Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.
1633;Churn_Modelling_decision_tree.png;Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 1.
1634;Churn_Modelling_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
1635;Churn_Modelling_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
1636;Churn_Modelling_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.
1637;Churn_Modelling_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1638;Churn_Modelling_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
1639;Churn_Modelling_overfitting_knn.png;KNN is in overfitting for k larger than 13.
1640;Churn_Modelling_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
1641;Churn_Modelling_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.
1642;Churn_Modelling_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
1643;Churn_Modelling_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
1644;Churn_Modelling_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.
1645;Churn_Modelling_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.
1646;Churn_Modelling_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.
1647;Churn_Modelling_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1648;Churn_Modelling_pca.png;The first 5 principal components are enough for explaining half the data variance.
1649;Churn_Modelling_pca.png;Using the first 5 principal components would imply an error between 10 and 25%.
1650;Churn_Modelling_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.
1651;Churn_Modelling_correlation_heatmap.png;One of the variables EstimatedSalary or NumOfProducts can be discarded without losing information.
1652;Churn_Modelling_correlation_heatmap.png;The variable EstimatedSalary can be discarded without risking losing information.
1653;Churn_Modelling_correlation_heatmap.png;Variables Age and CreditScore are redundant, but we can’t say the same for the pair Tenure and NumOfProducts.
1654;Churn_Modelling_correlation_heatmap.png;Variables NumOfProducts and CreditScore are redundant.
1655;Churn_Modelling_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1656;Churn_Modelling_correlation_heatmap.png;Variable EstimatedSalary seems to be relevant for the majority of mining tasks.
1657;Churn_Modelling_correlation_heatmap.png;Variables NumOfProducts and CreditScore seem to be useful for classification tasks.
1658;Churn_Modelling_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1659;Churn_Modelling_correlation_heatmap.png;Removing variable Balance might improve the training of decision trees .
1660;Churn_Modelling_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Tenure previously than variable CreditScore.
1661;Churn_Modelling_boxplots.png;Variable Tenure is balanced.
1662;Churn_Modelling_boxplots.png;Those boxplots show that the data is not normalized.
1663;Churn_Modelling_boxplots.png;It is clear that variable Tenure shows some outliers, but we can’t be sure of the same for variable NumOfProducts.
1664;Churn_Modelling_boxplots.png;Outliers seem to be a problem in the dataset.
1665;Churn_Modelling_boxplots.png;Variable EstimatedSalary shows some outlier values.
1666;Churn_Modelling_boxplots.png;Variable EstimatedSalary doesn’t have any outliers.
1667;Churn_Modelling_boxplots.png;Variable Age presents some outliers.
1668;Churn_Modelling_boxplots.png;At least 60 of the variables present outliers.
1669;Churn_Modelling_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1670;Churn_Modelling_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1671;Churn_Modelling_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
1672;Churn_Modelling_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1673;Churn_Modelling_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1674;Churn_Modelling_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1675;Churn_Modelling_histograms_symbolic.png;All variables, but the class, should be dealt with as binary.
1676;Churn_Modelling_histograms_symbolic.png;The variable IsActiveMember can be seen as ordinal.
1677;Churn_Modelling_histograms_symbolic.png;The variable Gender can be seen as ordinal without losing information.
1678;Churn_Modelling_histograms_symbolic.png;Considering the common semantics for Gender and Geography variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1679;Churn_Modelling_histograms_symbolic.png;Considering the common semantics for IsActiveMember variable, dummification would be the most adequate encoding.
1680;Churn_Modelling_histograms_symbolic.png;The variable IsActiveMember can be coded as ordinal without losing information.
1681;Churn_Modelling_histograms_symbolic.png;Feature generation based on variable IsActiveMember seems to be promising.
1682;Churn_Modelling_histograms_symbolic.png;Feature generation based on the use of variable Gender wouldn’t be useful, but the use of Geography seems to be promising.
1683;Churn_Modelling_histograms_symbolic.png;Given the usual semantics of Geography variable, dummification would have been a better codification.
1684;Churn_Modelling_histograms_symbolic.png;It is better to drop the variable Gender than removing all records with missing values.
1685;Churn_Modelling_histograms_symbolic.png;Not knowing the semantics of Gender variable, dummification could have been a more adequate codification.
1686;Churn_Modelling_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1687;Churn_Modelling_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.
1688;Churn_Modelling_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1689;Churn_Modelling_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1690;Churn_Modelling_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
1691;Churn_Modelling_histograms_numeric.png;The variable Age can be seen as ordinal.
1692;Churn_Modelling_histograms_numeric.png;The variable EstimatedSalary can be seen as ordinal without losing information.
1693;Churn_Modelling_histograms_numeric.png;Variable Age is balanced.
1694;Churn_Modelling_histograms_numeric.png;It is clear that variable NumOfProducts shows some outliers, but we can’t be sure of the same for variable Tenure.
1695;Churn_Modelling_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1696;Churn_Modelling_histograms_numeric.png;Variable EstimatedSalary shows some outlier values.
1697;Churn_Modelling_histograms_numeric.png;Variable Age doesn’t have any outliers.
1698;Churn_Modelling_histograms_numeric.png;Variable NumOfProducts presents some outliers.
1699;Churn_Modelling_histograms_numeric.png;At least 85 of the variables present outliers.
1700;Churn_Modelling_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
1701;Churn_Modelling_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1702;Churn_Modelling_histograms_numeric.png;Considering the common semantics for NumOfProducts and CreditScore variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1703;Churn_Modelling_histograms_numeric.png;Considering the common semantics for Balance variable, dummification would be the most adequate encoding.
1704;Churn_Modelling_histograms_numeric.png;The variable CreditScore can be coded as ordinal without losing information.
1705;Churn_Modelling_histograms_numeric.png;Feature generation based on variable Age seems to be promising.
1706;Churn_Modelling_histograms_numeric.png;Feature generation based on the use of variable EstimatedSalary wouldn’t be useful, but the use of CreditScore seems to be promising.
1707;Churn_Modelling_histograms_numeric.png;Given the usual semantics of CreditScore variable, dummification would have been a better codification.
1708;Churn_Modelling_histograms_numeric.png;It is better to drop the variable EstimatedSalary than removing all records with missing values.
1709;Churn_Modelling_histograms_numeric.png;Not knowing the semantics of CreditScore variable, dummification could have been a more adequate codification.
1710;vehicle_decision_tree.png;The variable MAJORSKEWNESS discriminates between the target values, as shown in the decision tree.
1711;vehicle_decision_tree.png;Variable MAJORSKEWNESS is one of the most relevant variables.
1712;vehicle_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.
1713;vehicle_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1714;vehicle_decision_tree.png;The accuracy for the presented tree is lower than 90%.
1715;vehicle_decision_tree.png;The number of False Negatives is lower than the number of True Positives for the presented tree.
1716;vehicle_decision_tree.png;The number of True Positives is lower than the number of False Negatives for the presented tree.
1717;vehicle_decision_tree.png;The variable MAJORSKEWNESS seems to be one of the five most relevant features.
1718;vehicle_decision_tree.png;Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 4.
1719;vehicle_decision_tree.png;Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that KNN algorithm classifies (A, not B) as 2 for any k ≤ 3.
1720;vehicle_decision_tree.png;Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that KNN algorithm classifies (A,B) as 4 for any k ≤ 3.
1721;vehicle_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
1722;vehicle_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
1723;vehicle_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.
1724;vehicle_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1725;vehicle_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
1726;vehicle_overfitting_knn.png;KNN is in overfitting for k larger than 17.
1727;vehicle_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
1728;vehicle_overfitting_knn.png;KNN with more than 7 neighbours is in overfitting.
1729;vehicle_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.
1730;vehicle_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.
1731;vehicle_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.
1732;vehicle_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.
1733;vehicle_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.
1734;vehicle_pca.png;The first 10 principal components are enough for explaining half the data variance.
1735;vehicle_pca.png;Using the first 8 principal components would imply an error between 5 and 20%.
1736;vehicle_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.
1737;vehicle_correlation_heatmap.png;One of the variables MAJORSKEWNESS or CIRCULARITY can be discarded without losing information.
1738;vehicle_correlation_heatmap.png;The variable GYRATIONRADIUS can be discarded without risking losing information.
1739;vehicle_correlation_heatmap.png;Variables CIRCULARITY and COMPACTNESS are redundant, but we can’t say the same for the pair MINORVARIANCE and MAJORVARIANCE.
1740;vehicle_correlation_heatmap.png;Variables MINORVARIANCE and MINORKURTOSIS are redundant.
1741;vehicle_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1742;vehicle_correlation_heatmap.png;Variable MAJORVARIANCE seems to be relevant for the majority of mining tasks.
1743;vehicle_correlation_heatmap.png;Variables MINORKURTOSIS and MINORSKEWNESS seem to be useful for classification tasks.
1744;vehicle_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1745;vehicle_correlation_heatmap.png;Removing variable MAJORKURTOSIS might improve the training of decision trees .
1746;vehicle_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable MINORKURTOSIS previously than variable MAJORSKEWNESS.
1747;vehicle_boxplots.png;Variable COMPACTNESS is balanced.
1748;vehicle_boxplots.png;Those boxplots show that the data is not normalized.
1749;vehicle_boxplots.png;It is clear that variable MINORSKEWNESS shows some outliers, but we can’t be sure of the same for variable MINORVARIANCE.
1750;vehicle_boxplots.png;Outliers seem to be a problem in the dataset.
1751;vehicle_boxplots.png;Variable MINORKURTOSIS shows some outlier values.
1752;vehicle_boxplots.png;Variable COMPACTNESS doesn’t have any outliers.
1753;vehicle_boxplots.png;Variable CIRCULARITY presents some outliers.
1754;vehicle_boxplots.png;At least 75 of the variables present outliers.
1755;vehicle_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1756;vehicle_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1757;vehicle_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
1758;vehicle_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1759;vehicle_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1760;vehicle_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1761;vehicle_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1762;vehicle_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
1763;vehicle_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1764;vehicle_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1765;vehicle_histograms_numeric.png;All variables, but the class, should be dealt with as date.
1766;vehicle_histograms_numeric.png;The variable MINORSKEWNESS can be seen as ordinal.
1767;vehicle_histograms_numeric.png;The variable GYRATIONRADIUS can be seen as ordinal without losing information.
1768;vehicle_histograms_numeric.png;Variable COMPACTNESS is balanced.
1769;vehicle_histograms_numeric.png;It is clear that variable MAJORSKEWNESS shows some outliers, but we can’t be sure of the same for variable MAJORVARIANCE.
1770;vehicle_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1771;vehicle_histograms_numeric.png;Variable MINORKURTOSIS shows a high number of outlier values.
1772;vehicle_histograms_numeric.png;Variable MINORSKEWNESS doesn’t have any outliers.
1773;vehicle_histograms_numeric.png;Variable CIRCULARITY presents some outliers.
1774;vehicle_histograms_numeric.png;At least 60 of the variables present outliers.
1775;vehicle_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
1776;vehicle_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1777;vehicle_histograms_numeric.png;Considering the common semantics for RADIUS RATIO and COMPACTNESS variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1778;vehicle_histograms_numeric.png;Considering the common semantics for MINORKURTOSIS variable, dummification would be the most adequate encoding.
1779;vehicle_histograms_numeric.png;The variable DISTANCE CIRCULARITY can be coded as ordinal without losing information.
1780;vehicle_histograms_numeric.png;Feature generation based on variable GYRATIONRADIUS seems to be promising.
1781;vehicle_histograms_numeric.png;Feature generation based on the use of variable MAJORSKEWNESS wouldn’t be useful, but the use of COMPACTNESS seems to be promising.
1782;vehicle_histograms_numeric.png;Given the usual semantics of GYRATIONRADIUS variable, dummification would have been a better codification.
1783;vehicle_histograms_numeric.png;It is better to drop the variable COMPACTNESS than removing all records with missing values.
1784;vehicle_histograms_numeric.png;Not knowing the semantics of MAJORSKEWNESS variable, dummification could have been a more adequate codification.
1785;adult_decision_tree.png;The variable capital-loss discriminates between the target values, as shown in the decision tree.
1786;adult_decision_tree.png;Variable hours-per-week is one of the most relevant variables.
1787;adult_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
1788;adult_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1789;adult_decision_tree.png;The accuracy for the presented tree is higher than 60%.
1790;adult_decision_tree.png;The number of True Negatives is higher than the number of False Negatives for the presented tree.
1791;adult_decision_tree.png;The number of False Negatives is lower than the number of False Positives for the presented tree.
1792;adult_decision_tree.png;The number of False Negatives is lower than the number of False Positives for the presented tree.
1793;adult_decision_tree.png;Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as >50K.
1794;adult_decision_tree.png;Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that KNN algorithm classifies (A, not B) as >50K for any k ≤ 541.
1795;adult_decision_tree.png;Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that KNN algorithm classifies (not A, B) as >50K for any k ≤ 21974.
1796;adult_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
1797;adult_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
1798;adult_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.
1799;adult_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1800;adult_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
1801;adult_overfitting_knn.png;KNN is in overfitting for k larger than 17.
1802;adult_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
1803;adult_overfitting_knn.png;KNN with more than 7 neighbours is in overfitting.
1804;adult_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
1805;adult_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.
1806;adult_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.
1807;adult_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.
1808;adult_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.
1809;adult_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1810;adult_pca.png;The first 4 principal components are enough for explaining half the data variance.
1811;adult_pca.png;Using the first 5 principal components would imply an error between 15 and 30%.
1812;adult_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.
1813;adult_correlation_heatmap.png;One of the variables fnlwgt or hours-per-week can be discarded without losing information.
1814;adult_correlation_heatmap.png;The variable hours-per-week can be discarded without risking losing information.
1815;adult_correlation_heatmap.png;Variables capital-loss and age are redundant.
1816;adult_correlation_heatmap.png;Variables age and educational-num are redundant.
1817;adult_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1818;adult_correlation_heatmap.png;Variable capital-gain seems to be relevant for the majority of mining tasks.
1819;adult_correlation_heatmap.png;Variables fnlwgt and age seem to be useful for classification tasks.
1820;adult_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1821;adult_correlation_heatmap.png;Removing variable fnlwgt might improve the training of decision trees .
1822;adult_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable capital-gain previously than variable fnlwgt.
1823;adult_boxplots.png;Variable hours-per-week is balanced.
1824;adult_boxplots.png;Those boxplots show that the data is not normalized.
1825;adult_boxplots.png;It is clear that variable educational-num shows some outliers, but we can’t be sure of the same for variable fnlwgt.
1826;adult_boxplots.png;Outliers seem to be a problem in the dataset.
1827;adult_boxplots.png;Variable capital-loss shows a high number of outlier values.
1828;adult_boxplots.png;Variable capital-gain doesn’t have any outliers.
1829;adult_boxplots.png;Variable capital-gain presents some outliers.
1830;adult_boxplots.png;At least 75 of the variables present outliers.
1831;adult_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
1832;adult_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1833;adult_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
1834;adult_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1835;adult_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1836;adult_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1837;adult_histograms_symbolic.png;All variables, but the class, should be dealt with as date.
1838;adult_histograms_symbolic.png;The variable gender can be seen as ordinal.
1839;adult_histograms_symbolic.png;The variable education can be seen as ordinal without losing information.
1840;adult_histograms_symbolic.png;Considering the common semantics for marital-status and workclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1841;adult_histograms_symbolic.png;Considering the common semantics for marital-status variable, dummification would be the most adequate encoding.
1842;adult_histograms_symbolic.png;The variable education can be coded as ordinal without losing information.
1843;adult_histograms_symbolic.png;Feature generation based on variable marital-status seems to be promising.
1844;adult_histograms_symbolic.png;Feature generation based on the use of variable occupation wouldn’t be useful, but the use of workclass seems to be promising.
1845;adult_histograms_symbolic.png;Given the usual semantics of education variable, dummification would have been a better codification.
1846;adult_histograms_symbolic.png;It is better to drop the variable relationship than removing all records with missing values.
1847;adult_histograms_symbolic.png;Not knowing the semantics of occupation variable, dummification could have been a more adequate codification.
1848;adult_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1849;adult_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.
1850;adult_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1851;adult_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1852;adult_histograms_numeric.png;All variables, but the class, should be dealt with as date.
1853;adult_histograms_numeric.png;The variable fnlwgt can be seen as ordinal.
1854;adult_histograms_numeric.png;The variable hours-per-week can be seen as ordinal without losing information.
1855;adult_histograms_numeric.png;Variable fnlwgt is balanced.
1856;adult_histograms_numeric.png;It is clear that variable educational-num shows some outliers, but we can’t be sure of the same for variable capital-loss.
1857;adult_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1858;adult_histograms_numeric.png;Variable educational-num shows some outlier values.
1859;adult_histograms_numeric.png;Variable capital-loss doesn’t have any outliers.
1860;adult_histograms_numeric.png;Variable age presents some outliers.
1861;adult_histograms_numeric.png;At least 85 of the variables present outliers.
1862;adult_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
1863;adult_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1864;adult_histograms_numeric.png;Considering the common semantics for capital-gain and age variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1865;adult_histograms_numeric.png;Considering the common semantics for fnlwgt variable, dummification would be the most adequate encoding.
1866;adult_histograms_numeric.png;The variable educational-num can be coded as ordinal without losing information.
1867;adult_histograms_numeric.png;Feature generation based on variable educational-num seems to be promising.
1868;adult_histograms_numeric.png;Feature generation based on the use of variable capital-loss wouldn’t be useful, but the use of age seems to be promising.
1869;adult_histograms_numeric.png;Given the usual semantics of capital-gain variable, dummification would have been a better codification.
1870;adult_histograms_numeric.png;It is better to drop the variable hours-per-week than removing all records with missing values.
1871;adult_histograms_numeric.png;Not knowing the semantics of fnlwgt variable, dummification could have been a more adequate codification.
1872;Covid_Data_decision_tree.png;The variable ASHTMA discriminates between the target values, as shown in the decision tree.
1873;Covid_Data_decision_tree.png;Variable ASHTMA is one of the most relevant variables.
1874;Covid_Data_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
1875;Covid_Data_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
1876;Covid_Data_decision_tree.png;The precision for the presented tree is higher than 75%.
1877;Covid_Data_decision_tree.png;The number of False Positives is higher than the number of True Negatives for the presented tree.
1878;Covid_Data_decision_tree.png;The number of True Negatives is higher than the number of True Positives for the presented tree.
1879;Covid_Data_decision_tree.png;The recall for the presented tree is lower than 90%.
1880;Covid_Data_decision_tree.png;Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (not A, B) as Yes for any k ≤ 46.
1881;Covid_Data_decision_tree.png;Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (A,B) as No for any k ≤ 7971.
1882;Covid_Data_decision_tree.png;Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (A, not B) as Yes for any k ≤ 173.
1883;Covid_Data_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
1884;Covid_Data_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
1885;Covid_Data_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.
1886;Covid_Data_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1887;Covid_Data_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
1888;Covid_Data_overfitting_knn.png;KNN is in overfitting for k larger than 13.
1889;Covid_Data_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
1890;Covid_Data_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.
1891;Covid_Data_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.
1892;Covid_Data_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.
1893;Covid_Data_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.
1894;Covid_Data_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.
1895;Covid_Data_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.
1896;Covid_Data_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
1897;Covid_Data_pca.png;The first 2 principal components are enough for explaining half the data variance.
1898;Covid_Data_pca.png;Using the first 11 principal components would imply an error between 15 and 25%.
1899;Covid_Data_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.
1900;Covid_Data_correlation_heatmap.png;One of the variables HIPERTENSION or RENAL_CHRONIC can be discarded without losing information.
1901;Covid_Data_correlation_heatmap.png;The variable MEDICAL_UNIT can be discarded without risking losing information.
1902;Covid_Data_correlation_heatmap.png;Variables PREGNANT and TOBACCO are redundant, but we can’t say the same for the pair MEDICAL_UNIT and ASTHMA.
1903;Covid_Data_correlation_heatmap.png;Variables COPD and AGE are redundant.
1904;Covid_Data_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1905;Covid_Data_correlation_heatmap.png;Variable ICU seems to be relevant for the majority of mining tasks.
1906;Covid_Data_correlation_heatmap.png;Variables HIPERTENSION and TOBACCO seem to be useful for classification tasks.
1907;Covid_Data_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1908;Covid_Data_correlation_heatmap.png;Removing variable COPD might improve the training of decision trees .
1909;Covid_Data_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable ICU previously than variable PREGNANT.
1910;Covid_Data_boxplots.png;Variable OTHER_DISEASE is balanced.
1911;Covid_Data_boxplots.png;Those boxplots show that the data is not normalized.
1912;Covid_Data_boxplots.png;It is clear that variable ASTHMA shows some outliers, but we can’t be sure of the same for variable COPD.
1913;Covid_Data_boxplots.png;Outliers seem to be a problem in the dataset.
1914;Covid_Data_boxplots.png;Variable AGE shows some outlier values.
1915;Covid_Data_boxplots.png;Variable ASTHMA doesn’t have any outliers.
1916;Covid_Data_boxplots.png;Variable OTHER_DISEASE presents some outliers.
1917;Covid_Data_boxplots.png;At least 75 of the variables present outliers.
1918;Covid_Data_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
1919;Covid_Data_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
1920;Covid_Data_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
1921;Covid_Data_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
1922;Covid_Data_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
1923;Covid_Data_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
1924;Covid_Data_histograms_symbolic.png;All variables, but the class, should be dealt with as date.
1925;Covid_Data_histograms_symbolic.png;The variable PATIENT_TYPE can be seen as ordinal.
1926;Covid_Data_histograms_symbolic.png;The variable USMER can be seen as ordinal without losing information.
1927;Covid_Data_histograms_symbolic.png;Considering the common semantics for USMER and SEX variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1928;Covid_Data_histograms_symbolic.png;Considering the common semantics for PATIENT_TYPE variable, dummification would be the most adequate encoding.
1929;Covid_Data_histograms_symbolic.png;The variable PATIENT_TYPE can be coded as ordinal without losing information.
1930;Covid_Data_histograms_symbolic.png;Feature generation based on variable SEX seems to be promising.
1931;Covid_Data_histograms_symbolic.png;Feature generation based on the use of variable PATIENT_TYPE wouldn’t be useful, but the use of USMER seems to be promising.
1932;Covid_Data_histograms_symbolic.png;Given the usual semantics of PATIENT_TYPE variable, dummification would have been a better codification.
1933;Covid_Data_histograms_symbolic.png;It is better to drop the variable PATIENT_TYPE than removing all records with missing values.
1934;Covid_Data_histograms_symbolic.png;Not knowing the semantics of SEX variable, dummification could have been a more adequate codification.
1935;Covid_Data_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
1936;Covid_Data_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
1937;Covid_Data_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
1938;Covid_Data_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
1939;Covid_Data_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
1940;Covid_Data_histograms_numeric.png;The variable TOBACCO can be seen as ordinal.
1941;Covid_Data_histograms_numeric.png;The variable MEDICAL_UNIT can be seen as ordinal without losing information.
1942;Covid_Data_histograms_numeric.png;Variable ICU is balanced.
1943;Covid_Data_histograms_numeric.png;It is clear that variable RENAL_CHRONIC shows some outliers, but we can’t be sure of the same for variable ICU.
1944;Covid_Data_histograms_numeric.png;Outliers seem to be a problem in the dataset.
1945;Covid_Data_histograms_numeric.png;Variable OTHER_DISEASE shows some outlier values.
1946;Covid_Data_histograms_numeric.png;Variable MEDICAL_UNIT doesn’t have any outliers.
1947;Covid_Data_histograms_numeric.png;Variable PREGNANT presents some outliers.
1948;Covid_Data_histograms_numeric.png;At least 85 of the variables present outliers.
1949;Covid_Data_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
1950;Covid_Data_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
1951;Covid_Data_histograms_numeric.png;Considering the common semantics for COPD and MEDICAL_UNIT variables, dummification if applied would increase the risk of facing the curse of dimensionality.
1952;Covid_Data_histograms_numeric.png;Considering the common semantics for ASTHMA variable, dummification would be the most adequate encoding.
1953;Covid_Data_histograms_numeric.png;The variable PREGNANT can be coded as ordinal without losing information.
1954;Covid_Data_histograms_numeric.png;Feature generation based on variable ICU seems to be promising.
1955;Covid_Data_histograms_numeric.png;Feature generation based on the use of variable PNEUMONIA wouldn’t be useful, but the use of MEDICAL_UNIT seems to be promising.
1956;Covid_Data_histograms_numeric.png;Given the usual semantics of HIPERTENSION variable, dummification would have been a better codification.
1957;Covid_Data_histograms_numeric.png;It is better to drop the variable PREGNANT than removing all records with missing values.
1958;Covid_Data_histograms_numeric.png;Not knowing the semantics of COPD variable, dummification could have been a more adequate codification.
1959;sky_survey_decision_tree.png;The variable mjd discriminates between the target values, as shown in the decision tree.
1960;sky_survey_decision_tree.png;Variable mjd is one of the most relevant variables.
1961;sky_survey_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.
1962;sky_survey_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
1963;sky_survey_decision_tree.png;The specificity for the presented tree is lower than 75%.
1964;sky_survey_decision_tree.png;The number of False Negatives is higher than the number of True Negatives for the presented tree.
1965;sky_survey_decision_tree.png;The number of False Positives is higher than the number of True Negatives for the presented tree.
1966;sky_survey_decision_tree.png;The number of False Negatives is higher than the number of False Positives for the presented tree.
1967;sky_survey_decision_tree.png;Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], the Decision Tree presented classifies (A, not B) as QSO.
1968;sky_survey_decision_tree.png;Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], the Decision Tree presented classifies (A, not B) as QSO.
1969;sky_survey_decision_tree.png;Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], it is possible to state that KNN algorithm classifies (A,B) as GALAXY for any k ≤ 945.
1970;sky_survey_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
1971;sky_survey_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
1972;sky_survey_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.
1973;sky_survey_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
1974;sky_survey_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
1975;sky_survey_overfitting_knn.png;KNN is in overfitting for k less than 5.
1976;sky_survey_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
1977;sky_survey_overfitting_knn.png;KNN with less than 15 neighbours is in overfitting.
1978;sky_survey_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
1979;sky_survey_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.
1980;sky_survey_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.
1981;sky_survey_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.
1982;sky_survey_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.
1983;sky_survey_pca.png;The first 7 principal components are enough for explaining half the data variance.
1984;sky_survey_pca.png;Using the first 4 principal components would imply an error between 15 and 30%.
1985;sky_survey_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.
1986;sky_survey_correlation_heatmap.png;One of the variables redshift or plate can be discarded without losing information.
1987;sky_survey_correlation_heatmap.png;The variable camcol can be discarded without risking losing information.
1988;sky_survey_correlation_heatmap.png;Variables run and ra are redundant, but we can’t say the same for the pair mjd and dec.
1989;sky_survey_correlation_heatmap.png;Variables run and redshift are redundant.
1990;sky_survey_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
1991;sky_survey_correlation_heatmap.png;Variable dec seems to be relevant for the majority of mining tasks.
1992;sky_survey_correlation_heatmap.png;Variables camcol and mjd seem to be useful for classification tasks.
1993;sky_survey_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
1994;sky_survey_correlation_heatmap.png;Removing variable ra might improve the training of decision trees .
1995;sky_survey_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable camcol previously than variable mjd.
1996;sky_survey_boxplots.png;Variable plate is balanced.
1997;sky_survey_boxplots.png;Those boxplots show that the data is not normalized.
1998;sky_survey_boxplots.png;It is clear that variable field shows some outliers, but we can’t be sure of the same for variable ra.
1999;sky_survey_boxplots.png;Outliers seem to be a problem in the dataset.
2000;sky_survey_boxplots.png;Variable field shows some outlier values.
2001;sky_survey_boxplots.png;Variable field doesn’t have any outliers.
2002;sky_survey_boxplots.png;Variable redshift presents some outliers.
2003;sky_survey_boxplots.png;At least 50 of the variables present outliers.
2004;sky_survey_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
2005;sky_survey_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2006;sky_survey_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
2007;sky_survey_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2008;sky_survey_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2009;sky_survey_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2010;sky_survey_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2011;sky_survey_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
2012;sky_survey_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2013;sky_survey_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2014;sky_survey_histograms_numeric.png;All variables, but the class, should be dealt with as date.
2015;sky_survey_histograms_numeric.png;The variable run can be seen as ordinal.
2016;sky_survey_histograms_numeric.png;The variable field can be seen as ordinal without losing information.
2017;sky_survey_histograms_numeric.png;Variable ra is balanced.
2018;sky_survey_histograms_numeric.png;It is clear that variable camcol shows some outliers, but we can’t be sure of the same for variable mjd.
2019;sky_survey_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2020;sky_survey_histograms_numeric.png;Variable redshift shows a high number of outlier values.
2021;sky_survey_histograms_numeric.png;Variable field doesn’t have any outliers.
2022;sky_survey_histograms_numeric.png;Variable plate presents some outliers.
2023;sky_survey_histograms_numeric.png;At least 60 of the variables present outliers.
2024;sky_survey_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2025;sky_survey_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2026;sky_survey_histograms_numeric.png;Considering the common semantics for ra and dec variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2027;sky_survey_histograms_numeric.png;Considering the common semantics for field variable, dummification would be the most adequate encoding.
2028;sky_survey_histograms_numeric.png;The variable camcol can be coded as ordinal without losing information.
2029;sky_survey_histograms_numeric.png;Feature generation based on variable redshift seems to be promising.
2030;sky_survey_histograms_numeric.png;Feature generation based on the use of variable camcol wouldn’t be useful, but the use of ra seems to be promising.
2031;sky_survey_histograms_numeric.png;Given the usual semantics of ra variable, dummification would have been a better codification.
2032;sky_survey_histograms_numeric.png;It is better to drop the variable redshift than removing all records with missing values.
2033;sky_survey_histograms_numeric.png;Not knowing the semantics of plate variable, dummification could have been a more adequate codification.
2034;Wine_decision_tree.png;The variable Proanthocyanins discriminates between the target values, as shown in the decision tree.
2035;Wine_decision_tree.png;Variable Proanthocyanins is one of the most relevant variables.
2036;Wine_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.
2037;Wine_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
2038;Wine_decision_tree.png;The specificity for the presented tree is higher than 75%.
2039;Wine_decision_tree.png;The number of True Positives is lower than the number of False Positives for the presented tree.
2040;Wine_decision_tree.png;The number of False Negatives is lower than the number of True Negatives for the presented tree.
2041;Wine_decision_tree.png;The number of False Negatives is higher than the number of True Positives for the presented tree.
2042;Wine_decision_tree.png;Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A,B) as 3 for any k ≤ 60.
2043;Wine_decision_tree.png;Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 60.
2044;Wine_decision_tree.png;Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A, not B) as 2 for any k ≤ 49.
2045;Wine_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
2046;Wine_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
2047;Wine_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.
2048;Wine_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2049;Wine_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
2050;Wine_overfitting_knn.png;KNN is in overfitting for k larger than 17.
2051;Wine_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
2052;Wine_overfitting_knn.png;KNN with less than 15 neighbours is in overfitting.
2053;Wine_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.
2054;Wine_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
2055;Wine_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.
2056;Wine_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.
2057;Wine_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.
2058;Wine_pca.png;The first 4 principal components are enough for explaining half the data variance.
2059;Wine_pca.png;Using the first 10 principal components would imply an error between 15 and 30%.
2060;Wine_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
2061;Wine_correlation_heatmap.png;One of the variables Flavanoids or Hue can be discarded without losing information.
2062;Wine_correlation_heatmap.png;The variable Color intensity can be discarded without risking losing information.
2063;Wine_correlation_heatmap.png;Variables Color intensity and Alcohol are redundant, but we can’t say the same for the pair Flavanoids and Alcalinity of ash.
2064;Wine_correlation_heatmap.png;Variables Flavanoids and Total phenols are redundant.
2065;Wine_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2066;Wine_correlation_heatmap.png;Variable Ash seems to be relevant for the majority of mining tasks.
2067;Wine_correlation_heatmap.png;Variables Alcalinity of ash and Malic acid seem to be useful for classification tasks.
2068;Wine_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2069;Wine_correlation_heatmap.png;Removing variable Alcohol might improve the training of decision trees .
2070;Wine_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable OD280-OD315 of diluted wines previously than variable Total phenols.
2071;Wine_boxplots.png;Variable OD280-OD315 of diluted wines is balanced.
2072;Wine_boxplots.png;Those boxplots show that the data is not normalized.
2073;Wine_boxplots.png;It is clear that variable Nonflavanoid phenols shows some outliers, but we can’t be sure of the same for variable Color intensity.
2074;Wine_boxplots.png;Outliers seem to be a problem in the dataset.
2075;Wine_boxplots.png;Variable Hue shows some outlier values.
2076;Wine_boxplots.png;Variable Malic acid doesn’t have any outliers.
2077;Wine_boxplots.png;Variable OD280-OD315 of diluted wines presents some outliers.
2078;Wine_boxplots.png;At least 50 of the variables present outliers.
2079;Wine_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
2080;Wine_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2081;Wine_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
2082;Wine_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2083;Wine_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2084;Wine_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2085;Wine_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2086;Wine_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.
2087;Wine_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2088;Wine_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2089;Wine_histograms_numeric.png;All variables, but the class, should be dealt with as binary.
2090;Wine_histograms_numeric.png;The variable Total phenols can be seen as ordinal.
2091;Wine_histograms_numeric.png;The variable Alcohol can be seen as ordinal without losing information.
2092;Wine_histograms_numeric.png;Variable Flavanoids is balanced.
2093;Wine_histograms_numeric.png;It is clear that variable Color intensity shows some outliers, but we can’t be sure of the same for variable Total phenols.
2094;Wine_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2095;Wine_histograms_numeric.png;Variable Alcalinity of ash shows some outlier values.
2096;Wine_histograms_numeric.png;Variable Alcohol doesn’t have any outliers.
2097;Wine_histograms_numeric.png;Variable Ash presents some outliers.
2098;Wine_histograms_numeric.png;At least 50 of the variables present outliers.
2099;Wine_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
2100;Wine_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2101;Wine_histograms_numeric.png;Considering the common semantics for OD280-OD315 of diluted wines and Alcohol variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2102;Wine_histograms_numeric.png;Considering the common semantics for OD280-OD315 of diluted wines variable, dummification would be the most adequate encoding.
2103;Wine_histograms_numeric.png;The variable Hue can be coded as ordinal without losing information.
2104;Wine_histograms_numeric.png;Feature generation based on variable Malic acid seems to be promising.
2105;Wine_histograms_numeric.png;Feature generation based on the use of variable Nonflavanoid phenols wouldn’t be useful, but the use of Alcohol seems to be promising.
2106;Wine_histograms_numeric.png;Given the usual semantics of Total phenols variable, dummification would have been a better codification.
2107;Wine_histograms_numeric.png;It is better to drop the variable Alcalinity of ash than removing all records with missing values.
2108;Wine_histograms_numeric.png;Not knowing the semantics of Alcalinity of ash variable, dummification could have been a more adequate codification.
2109;water_potability_decision_tree.png;The variable Hardness discriminates between the target values, as shown in the decision tree.
2110;water_potability_decision_tree.png;Variable Chloramines is one of the most relevant variables.
2111;water_potability_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.
2112;water_potability_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
2113;water_potability_decision_tree.png;The specificity for the presented tree is higher than 90%.
2114;water_potability_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree.
2115;water_potability_decision_tree.png;The number of False Negatives is higher than the number of True Positives for the presented tree.
2116;water_potability_decision_tree.png;The specificity for the presented tree is lower than 60%.
2117;water_potability_decision_tree.png;Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 1388.
2118;water_potability_decision_tree.png;Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 6.
2119;water_potability_decision_tree.png;Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 1.
2120;water_potability_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
2121;water_potability_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
2122;water_potability_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.
2123;water_potability_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2124;water_potability_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
2125;water_potability_overfitting_knn.png;KNN is in overfitting for k larger than 5.
2126;water_potability_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
2127;water_potability_overfitting_knn.png;KNN with less than 15 neighbours is in overfitting.
2128;water_potability_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.
2129;water_potability_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.
2130;water_potability_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.
2131;water_potability_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.
2132;water_potability_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.
2133;water_potability_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
2134;water_potability_pca.png;The first 4 principal components are enough for explaining half the data variance.
2135;water_potability_pca.png;Using the first 3 principal components would imply an error between 5 and 30%.
2136;water_potability_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
2137;water_potability_correlation_heatmap.png;One of the variables Hardness or Conductivity can be discarded without losing information.
2138;water_potability_correlation_heatmap.png;The variable Turbidity can be discarded without risking losing information.
2139;water_potability_correlation_heatmap.png;Variables Trihalomethanes and Hardness are redundant, but we can’t say the same for the pair Chloramines and Sulfate.
2140;water_potability_correlation_heatmap.png;Variables Hardness and ph are redundant.
2141;water_potability_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2142;water_potability_correlation_heatmap.png;Variable Turbidity seems to be relevant for the majority of mining tasks.
2143;water_potability_correlation_heatmap.png;Variables Conductivity and Turbidity seem to be useful for classification tasks.
2144;water_potability_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2145;water_potability_correlation_heatmap.png;Removing variable Hardness might improve the training of decision trees .
2146;water_potability_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Turbidity previously than variable Chloramines.
2147;water_potability_boxplots.png;Variable Trihalomethanes is balanced.
2148;water_potability_boxplots.png;Those boxplots show that the data is not normalized.
2149;water_potability_boxplots.png;It is clear that variable Turbidity shows some outliers, but we can’t be sure of the same for variable Sulfate.
2150;water_potability_boxplots.png;Outliers seem to be a problem in the dataset.
2151;water_potability_boxplots.png;Variable ph shows some outlier values.
2152;water_potability_boxplots.png;Variable Turbidity doesn’t have any outliers.
2153;water_potability_boxplots.png;Variable Trihalomethanes presents some outliers.
2154;water_potability_boxplots.png;At least 75 of the variables present outliers.
2155;water_potability_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
2156;water_potability_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2157;water_potability_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
2158;water_potability_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2159;water_potability_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2160;water_potability_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2161;water_potability_mv.png;Discarding variable Trihalomethanes would be better than discarding all the records with missing values for that variable.
2162;water_potability_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.
2163;water_potability_mv.png;Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.
2164;water_potability_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.
2165;water_potability_mv.png;Feature generation based on variable Sulfate seems to be promising.
2166;water_potability_mv.png;It is better to drop the variable Trihalomethanes than removing all records with missing values.
2167;water_potability_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2168;water_potability_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.
2169;water_potability_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2170;water_potability_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2171;water_potability_histograms_numeric.png;All variables, but the class, should be dealt with as date.
2172;water_potability_histograms_numeric.png;The variable Trihalomethanes can be seen as ordinal.
2173;water_potability_histograms_numeric.png;The variable Chloramines can be seen as ordinal without losing information.
2174;water_potability_histograms_numeric.png;Variable Turbidity is balanced.
2175;water_potability_histograms_numeric.png;It is clear that variable Chloramines shows some outliers, but we can’t be sure of the same for variable Trihalomethanes.
2176;water_potability_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2177;water_potability_histograms_numeric.png;Variable Trihalomethanes shows a high number of outlier values.
2178;water_potability_histograms_numeric.png;Variable Turbidity doesn’t have any outliers.
2179;water_potability_histograms_numeric.png;Variable Sulfate presents some outliers.
2180;water_potability_histograms_numeric.png;At least 50 of the variables present outliers.
2181;water_potability_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
2182;water_potability_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2183;water_potability_histograms_numeric.png;Considering the common semantics for ph and Hardness variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2184;water_potability_histograms_numeric.png;Considering the common semantics for Turbidity variable, dummification would be the most adequate encoding.
2185;water_potability_histograms_numeric.png;The variable Conductivity can be coded as ordinal without losing information.
2186;water_potability_histograms_numeric.png;Feature generation based on variable Chloramines seems to be promising.
2187;water_potability_histograms_numeric.png;Feature generation based on the use of variable Conductivity wouldn’t be useful, but the use of ph seems to be promising.
2188;water_potability_histograms_numeric.png;Given the usual semantics of Chloramines variable, dummification would have been a better codification.
2189;water_potability_histograms_numeric.png;It is better to drop the variable Hardness than removing all records with missing values.
2190;water_potability_histograms_numeric.png;Not knowing the semantics of ph variable, dummification could have been a more adequate codification.
2191;abalone_decision_tree.png;The variable Diameter discriminates between the target values, as shown in the decision tree.
2192;abalone_decision_tree.png;Variable Diameter is one of the most relevant variables.
2193;abalone_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
2194;abalone_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
2195;abalone_decision_tree.png;The recall for the presented tree is higher than 60%.
2196;abalone_decision_tree.png;The number of False Negatives is higher than the number of False Positives for the presented tree.
2197;abalone_decision_tree.png;The number of True Positives is higher than the number of False Positives for the presented tree.
2198;abalone_decision_tree.png;The number of False Negatives is lower than the number of True Negatives for the presented tree.
2199;abalone_decision_tree.png;Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], the Decision Tree presented classifies (not A, B) as I.
2200;abalone_decision_tree.png;Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (A, not B) as M for any k ≤ 117.
2201;abalone_decision_tree.png;Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (not A, not B) as M for any k ≤ 1191.
2202;abalone_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.
2203;abalone_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
2204;abalone_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.
2205;abalone_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2206;abalone_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
2207;abalone_overfitting_knn.png;KNN is in overfitting for k less than 5.
2208;abalone_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
2209;abalone_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.
2210;abalone_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.
2211;abalone_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.
2212;abalone_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.
2213;abalone_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.
2214;abalone_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.
2215;abalone_pca.png;The first 6 principal components are enough for explaining half the data variance.
2216;abalone_pca.png;Using the first 3 principal components would imply an error between 15 and 30%.
2217;abalone_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 6.
2218;abalone_correlation_heatmap.png;One of the variables Whole weight or Length can be discarded without losing information.
2219;abalone_correlation_heatmap.png;The variable Whole weight can be discarded without risking losing information.
2220;abalone_correlation_heatmap.png;Variables Length and Height are redundant, but we can’t say the same for the pair Whole weight and Viscera weight.
2221;abalone_correlation_heatmap.png;Variables Diameter and Length are redundant.
2222;abalone_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2223;abalone_correlation_heatmap.png;Variable Whole weight seems to be relevant for the majority of mining tasks.
2224;abalone_correlation_heatmap.png;Variables Whole weight and Length seem to be useful for classification tasks.
2225;abalone_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2226;abalone_correlation_heatmap.png;Removing variable Rings might improve the training of decision trees .
2227;abalone_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Length previously than variable Height.
2228;abalone_boxplots.png;Variable Rings is balanced.
2229;abalone_boxplots.png;Those boxplots show that the data is not normalized.
2230;abalone_boxplots.png;It is clear that variable Shell weight shows some outliers, but we can’t be sure of the same for variable Viscera weight.
2231;abalone_boxplots.png;Outliers seem to be a problem in the dataset.
2232;abalone_boxplots.png;Variable Rings shows a high number of outlier values.
2233;abalone_boxplots.png;Variable Shell weight doesn’t have any outliers.
2234;abalone_boxplots.png;Variable Shell weight presents some outliers.
2235;abalone_boxplots.png;At least 50 of the variables present outliers.
2236;abalone_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2237;abalone_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2238;abalone_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
2239;abalone_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2240;abalone_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2241;abalone_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2242;abalone_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2243;abalone_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
2244;abalone_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2245;abalone_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2246;abalone_histograms_numeric.png;All variables, but the class, should be dealt with as date.
2247;abalone_histograms_numeric.png;The variable Shucked weight can be seen as ordinal.
2248;abalone_histograms_numeric.png;The variable Shucked weight can be seen as ordinal without losing information.
2249;abalone_histograms_numeric.png;Variable Shell weight is balanced.
2250;abalone_histograms_numeric.png;It is clear that variable Rings shows some outliers, but we can’t be sure of the same for variable Whole weight.
2251;abalone_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2252;abalone_histograms_numeric.png;Variable Viscera weight shows some outlier values.
2253;abalone_histograms_numeric.png;Variable Diameter doesn’t have any outliers.
2254;abalone_histograms_numeric.png;Variable Length presents some outliers.
2255;abalone_histograms_numeric.png;At least 75 of the variables present outliers.
2256;abalone_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2257;abalone_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2258;abalone_histograms_numeric.png;Considering the common semantics for Diameter and Length variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2259;abalone_histograms_numeric.png;Considering the common semantics for Length variable, dummification would be the most adequate encoding.
2260;abalone_histograms_numeric.png;The variable Diameter can be coded as ordinal without losing information.
2261;abalone_histograms_numeric.png;Feature generation based on variable Shucked weight seems to be promising.
2262;abalone_histograms_numeric.png;Feature generation based on the use of variable Diameter wouldn’t be useful, but the use of Length seems to be promising.
2263;abalone_histograms_numeric.png;Given the usual semantics of Viscera weight variable, dummification would have been a better codification.
2264;abalone_histograms_numeric.png;It is better to drop the variable Shucked weight than removing all records with missing values.
2265;abalone_histograms_numeric.png;Not knowing the semantics of Shell weight variable, dummification could have been a more adequate codification.
2266;smoking_drinking_decision_tree.png;The variable SMK_stat_type_cd discriminates between the target values, as shown in the decision tree.
2267;smoking_drinking_decision_tree.png;Variable gamma_GTP is one of the most relevant variables.
2268;smoking_drinking_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
2269;smoking_drinking_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
2270;smoking_drinking_decision_tree.png;The specificity for the presented tree is higher than 90%.
2271;smoking_drinking_decision_tree.png;The number of False Negatives reported in the same tree is 10.
2272;smoking_drinking_decision_tree.png;The number of True Positives is lower than the number of False Negatives for the presented tree.
2273;smoking_drinking_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree.
2274;smoking_drinking_decision_tree.png;Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that KNN algorithm classifies (A,B) as N for any k ≤ 3135.
2275;smoking_drinking_decision_tree.png;Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as N.
2276;smoking_drinking_decision_tree.png;Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], the Decision Tree presented classifies (A, not B) as Y.
2277;smoking_drinking_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
2278;smoking_drinking_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
2279;smoking_drinking_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.
2280;smoking_drinking_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2281;smoking_drinking_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
2282;smoking_drinking_overfitting_knn.png;KNN is in overfitting for k less than 13.
2283;smoking_drinking_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
2284;smoking_drinking_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.
2285;smoking_drinking_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
2286;smoking_drinking_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.
2287;smoking_drinking_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.
2288;smoking_drinking_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 3.
2289;smoking_drinking_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
2290;smoking_drinking_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
2291;smoking_drinking_pca.png;The first 6 principal components are enough for explaining half the data variance.
2292;smoking_drinking_pca.png;Using the first 8 principal components would imply an error between 15 and 30%.
2293;smoking_drinking_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 7.
2294;smoking_drinking_correlation_heatmap.png;One of the variables waistline or age can be discarded without losing information.
2295;smoking_drinking_correlation_heatmap.png;The variable hemoglobin can be discarded without risking losing information.
2296;smoking_drinking_correlation_heatmap.png;Variables BLDS and weight are redundant, but we can’t say the same for the pair waistline and LDL_chole.
2297;smoking_drinking_correlation_heatmap.png;Variables age and height are redundant.
2298;smoking_drinking_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2299;smoking_drinking_correlation_heatmap.png;Variable age seems to be relevant for the majority of mining tasks.
2300;smoking_drinking_correlation_heatmap.png;Variables height and waistline seem to be useful for classification tasks.
2301;smoking_drinking_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2302;smoking_drinking_correlation_heatmap.png;Removing variable waistline might improve the training of decision trees .
2303;smoking_drinking_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable LDL_chole previously than variable SMK_stat_type_cd.
2304;smoking_drinking_boxplots.png;Variable waistline is balanced.
2305;smoking_drinking_boxplots.png;Those boxplots show that the data is not normalized.
2306;smoking_drinking_boxplots.png;It is clear that variable tot_chole shows some outliers, but we can’t be sure of the same for variable waistline.
2307;smoking_drinking_boxplots.png;Outliers seem to be a problem in the dataset.
2308;smoking_drinking_boxplots.png;Variable tot_chole shows a high number of outlier values.
2309;smoking_drinking_boxplots.png;Variable SMK_stat_type_cd doesn’t have any outliers.
2310;smoking_drinking_boxplots.png;Variable BLDS presents some outliers.
2311;smoking_drinking_boxplots.png;At least 85 of the variables present outliers.
2312;smoking_drinking_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
2313;smoking_drinking_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2314;smoking_drinking_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
2315;smoking_drinking_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2316;smoking_drinking_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2317;smoking_drinking_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2318;smoking_drinking_histograms_symbolic.png;All variables, but the class, should be dealt with as binary.
2319;smoking_drinking_histograms_symbolic.png;The variable hear_right can be seen as ordinal.
2320;smoking_drinking_histograms_symbolic.png;The variable hear_right can be seen as ordinal without losing information.
2321;smoking_drinking_histograms_symbolic.png;Considering the common semantics for hear_left and sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2322;smoking_drinking_histograms_symbolic.png;Considering the common semantics for hear_right variable, dummification would be the most adequate encoding.
2323;smoking_drinking_histograms_symbolic.png;The variable sex can be coded as ordinal without losing information.
2324;smoking_drinking_histograms_symbolic.png;Feature generation based on variable hear_left seems to be promising.
2325;smoking_drinking_histograms_symbolic.png;Feature generation based on the use of variable hear_right wouldn’t be useful, but the use of sex seems to be promising.
2326;smoking_drinking_histograms_symbolic.png;Given the usual semantics of hear_right variable, dummification would have been a better codification.
2327;smoking_drinking_histograms_symbolic.png;It is better to drop the variable hear_left than removing all records with missing values.
2328;smoking_drinking_histograms_symbolic.png;Not knowing the semantics of hear_right variable, dummification could have been a more adequate codification.
2329;smoking_drinking_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2330;smoking_drinking_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
2331;smoking_drinking_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2332;smoking_drinking_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2333;smoking_drinking_histograms_numeric.png;All variables, but the class, should be dealt with as date.
2334;smoking_drinking_histograms_numeric.png;The variable SBP can be seen as ordinal.
2335;smoking_drinking_histograms_numeric.png;The variable tot_chole can be seen as ordinal without losing information.
2336;smoking_drinking_histograms_numeric.png;Variable weight is balanced.
2337;smoking_drinking_histograms_numeric.png;It is clear that variable height shows some outliers, but we can’t be sure of the same for variable age.
2338;smoking_drinking_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2339;smoking_drinking_histograms_numeric.png;Variable LDL_chole shows some outlier values.
2340;smoking_drinking_histograms_numeric.png;Variable tot_chole doesn’t have any outliers.
2341;smoking_drinking_histograms_numeric.png;Variable gamma_GTP presents some outliers.
2342;smoking_drinking_histograms_numeric.png;At least 60 of the variables present outliers.
2343;smoking_drinking_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
2344;smoking_drinking_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2345;smoking_drinking_histograms_numeric.png;Considering the common semantics for age and height variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2346;smoking_drinking_histograms_numeric.png;Considering the common semantics for weight variable, dummification would be the most adequate encoding.
2347;smoking_drinking_histograms_numeric.png;The variable hemoglobin can be coded as ordinal without losing information.
2348;smoking_drinking_histograms_numeric.png;Feature generation based on variable waistline seems to be promising.
2349;smoking_drinking_histograms_numeric.png;Feature generation based on the use of variable height wouldn’t be useful, but the use of age seems to be promising.
2350;smoking_drinking_histograms_numeric.png;Given the usual semantics of BLDS variable, dummification would have been a better codification.
2351;smoking_drinking_histograms_numeric.png;It is better to drop the variable SMK_stat_type_cd than removing all records with missing values.
2352;smoking_drinking_histograms_numeric.png;Not knowing the semantics of hemoglobin variable, dummification could have been a more adequate codification.
2353;BankNoteAuthentication_decision_tree.png;The variable curtosis discriminates between the target values, as shown in the decision tree.
2354;BankNoteAuthentication_decision_tree.png;Variable skewness is one of the most relevant variables.
2355;BankNoteAuthentication_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
2356;BankNoteAuthentication_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
2357;BankNoteAuthentication_decision_tree.png;The recall for the presented tree is higher than 75%.
2358;BankNoteAuthentication_decision_tree.png;The number of False Positives reported in the same tree is 10.
2359;BankNoteAuthentication_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree.
2360;BankNoteAuthentication_decision_tree.png;The recall for the presented tree is lower than its accuracy.
2361;BankNoteAuthentication_decision_tree.png;Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 214.
2362;BankNoteAuthentication_decision_tree.png;Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 214.
2363;BankNoteAuthentication_decision_tree.png;Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 131.
2364;BankNoteAuthentication_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
2365;BankNoteAuthentication_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
2366;BankNoteAuthentication_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.
2367;BankNoteAuthentication_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2368;BankNoteAuthentication_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
2369;BankNoteAuthentication_overfitting_knn.png;KNN is in overfitting for k larger than 5.
2370;BankNoteAuthentication_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
2371;BankNoteAuthentication_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
2372;BankNoteAuthentication_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
2373;BankNoteAuthentication_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.
2374;BankNoteAuthentication_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.
2375;BankNoteAuthentication_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 10.
2376;BankNoteAuthentication_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.
2377;BankNoteAuthentication_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
2378;BankNoteAuthentication_pca.png;The first 2 principal components are enough for explaining half the data variance.
2379;BankNoteAuthentication_pca.png;Using the first 2 principal components would imply an error between 15 and 30%.
2380;BankNoteAuthentication_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.
2381;BankNoteAuthentication_correlation_heatmap.png;One of the variables variance or curtosis can be discarded without losing information.
2382;BankNoteAuthentication_correlation_heatmap.png;The variable skewness can be discarded without risking losing information.
2383;BankNoteAuthentication_correlation_heatmap.png;Variables entropy and curtosis are redundant, but we can’t say the same for the pair variance and skewness.
2384;BankNoteAuthentication_correlation_heatmap.png;Variables curtosis and entropy are redundant.
2385;BankNoteAuthentication_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2386;BankNoteAuthentication_correlation_heatmap.png;Variable skewness seems to be relevant for the majority of mining tasks.
2387;BankNoteAuthentication_correlation_heatmap.png;Variables curtosis and variance seem to be useful for classification tasks.
2388;BankNoteAuthentication_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2389;BankNoteAuthentication_correlation_heatmap.png;Removing variable variance might improve the training of decision trees .
2390;BankNoteAuthentication_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable variance previously than variable skewness.
2391;BankNoteAuthentication_boxplots.png;Variable curtosis is balanced.
2392;BankNoteAuthentication_boxplots.png;Those boxplots show that the data is not normalized.
2393;BankNoteAuthentication_boxplots.png;It is clear that variable curtosis shows some outliers, but we can’t be sure of the same for variable entropy.
2394;BankNoteAuthentication_boxplots.png;Outliers seem to be a problem in the dataset.
2395;BankNoteAuthentication_boxplots.png;Variable skewness shows a high number of outlier values.
2396;BankNoteAuthentication_boxplots.png;Variable skewness doesn’t have any outliers.
2397;BankNoteAuthentication_boxplots.png;Variable variance presents some outliers.
2398;BankNoteAuthentication_boxplots.png;At least 75 of the variables present outliers.
2399;BankNoteAuthentication_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
2400;BankNoteAuthentication_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2401;BankNoteAuthentication_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
2402;BankNoteAuthentication_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2403;BankNoteAuthentication_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2404;BankNoteAuthentication_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2405;BankNoteAuthentication_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2406;BankNoteAuthentication_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
2407;BankNoteAuthentication_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2408;BankNoteAuthentication_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2409;BankNoteAuthentication_histograms_numeric.png;All variables, but the class, should be dealt with as symbolic.
2410;BankNoteAuthentication_histograms_numeric.png;The variable skewness can be seen as ordinal.
2411;BankNoteAuthentication_histograms_numeric.png;The variable skewness can be seen as ordinal without losing information.
2412;BankNoteAuthentication_histograms_numeric.png;Variable variance is balanced.
2413;BankNoteAuthentication_histograms_numeric.png;It is clear that variable variance shows some outliers, but we can’t be sure of the same for variable entropy.
2414;BankNoteAuthentication_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2415;BankNoteAuthentication_histograms_numeric.png;Variable variance shows a high number of outlier values.
2416;BankNoteAuthentication_histograms_numeric.png;Variable skewness doesn’t have any outliers.
2417;BankNoteAuthentication_histograms_numeric.png;Variable curtosis presents some outliers.
2418;BankNoteAuthentication_histograms_numeric.png;At least 60 of the variables present outliers.
2419;BankNoteAuthentication_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2420;BankNoteAuthentication_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2421;BankNoteAuthentication_histograms_numeric.png;Considering the common semantics for variance and skewness variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2422;BankNoteAuthentication_histograms_numeric.png;Considering the common semantics for curtosis variable, dummification would be the most adequate encoding.
2423;BankNoteAuthentication_histograms_numeric.png;The variable entropy can be coded as ordinal without losing information.
2424;BankNoteAuthentication_histograms_numeric.png;Feature generation based on variable skewness seems to be promising.
2425;BankNoteAuthentication_histograms_numeric.png;Feature generation based on the use of variable curtosis wouldn’t be useful, but the use of variance seems to be promising.
2426;BankNoteAuthentication_histograms_numeric.png;Given the usual semantics of curtosis variable, dummification would have been a better codification.
2427;BankNoteAuthentication_histograms_numeric.png;It is better to drop the variable skewness than removing all records with missing values.
2428;BankNoteAuthentication_histograms_numeric.png;Not knowing the semantics of entropy variable, dummification could have been a more adequate codification.
2429;Iris_decision_tree.png;The variable PetalWidthCm discriminates between the target values, as shown in the decision tree.
2430;Iris_decision_tree.png;Variable PetalWidthCm is one of the most relevant variables.
2431;Iris_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
2432;Iris_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
2433;Iris_decision_tree.png;The recall for the presented tree is lower than 75%.
2434;Iris_decision_tree.png;The number of True Negatives reported in the same tree is 30.
2435;Iris_decision_tree.png;The number of True Positives is lower than the number of False Negatives for the presented tree.
2436;Iris_decision_tree.png;The precision for the presented tree is lower than 90%.
2437;Iris_decision_tree.png;Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], the Decision Tree presented classifies (not A, not B) as Iris-versicolor.
2438;Iris_decision_tree.png;Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], it is possible to state that KNN algorithm classifies (A,B) as Iris-virginica for any k ≤ 38.
2439;Iris_decision_tree.png;Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], it is possible to state that KNN algorithm classifies (A,B) as Iris-setosa for any k ≤ 32.
2440;Iris_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
2441;Iris_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.
2442;Iris_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.
2443;Iris_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2444;Iris_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
2445;Iris_overfitting_knn.png;KNN is in overfitting for k larger than 17.
2446;Iris_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
2447;Iris_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.
2448;Iris_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.
2449;Iris_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
2450;Iris_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.
2451;Iris_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 10.
2452;Iris_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.
2453;Iris_pca.png;The first 2 principal components are enough for explaining half the data variance.
2454;Iris_pca.png;Using the first 2 principal components would imply an error between 10 and 25%.
2455;Iris_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
2456;Iris_correlation_heatmap.png;One of the variables PetalWidthCm or SepalLengthCm can be discarded without losing information.
2457;Iris_correlation_heatmap.png;The variable PetalLengthCm can be discarded without risking losing information.
2458;Iris_correlation_heatmap.png;Variables SepalLengthCm and SepalWidthCm are redundant, but we can’t say the same for the pair PetalLengthCm and PetalWidthCm.
2459;Iris_correlation_heatmap.png;Variables PetalLengthCm and SepalLengthCm are redundant.
2460;Iris_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2461;Iris_correlation_heatmap.png;Variable SepalLengthCm seems to be relevant for the majority of mining tasks.
2462;Iris_correlation_heatmap.png;Variables PetalLengthCm and SepalLengthCm seem to be useful for classification tasks.
2463;Iris_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2464;Iris_correlation_heatmap.png;Removing variable PetalWidthCm might improve the training of decision trees .
2465;Iris_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable SepalWidthCm.
2466;Iris_boxplots.png;Variable PetalWidthCm is balanced.
2467;Iris_boxplots.png;Those boxplots show that the data is not normalized.
2468;Iris_boxplots.png;It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.
2469;Iris_boxplots.png;Outliers seem to be a problem in the dataset.
2470;Iris_boxplots.png;Variable PetalLengthCm shows some outlier values.
2471;Iris_boxplots.png;Variable SepalLengthCm doesn’t have any outliers.
2472;Iris_boxplots.png;Variable PetalWidthCm presents some outliers.
2473;Iris_boxplots.png;At least 75 of the variables present outliers.
2474;Iris_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2475;Iris_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2476;Iris_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
2477;Iris_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2478;Iris_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2479;Iris_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2480;Iris_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2481;Iris_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.
2482;Iris_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2483;Iris_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2484;Iris_histograms_numeric.png;All variables, but the class, should be dealt with as date.
2485;Iris_histograms_numeric.png;The variable PetalWidthCm can be seen as ordinal.
2486;Iris_histograms_numeric.png;The variable SepalLengthCm can be seen as ordinal without losing information.
2487;Iris_histograms_numeric.png;Variable PetalWidthCm is balanced.
2488;Iris_histograms_numeric.png;It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.
2489;Iris_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2490;Iris_histograms_numeric.png;Variable SepalWidthCm shows a high number of outlier values.
2491;Iris_histograms_numeric.png;Variable SepalWidthCm doesn’t have any outliers.
2492;Iris_histograms_numeric.png;Variable PetalLengthCm presents some outliers.
2493;Iris_histograms_numeric.png;At least 75 of the variables present outliers.
2494;Iris_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2495;Iris_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2496;Iris_histograms_numeric.png;Considering the common semantics for PetalWidthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2497;Iris_histograms_numeric.png;Considering the common semantics for PetalLengthCm variable, dummification would be the most adequate encoding.
2498;Iris_histograms_numeric.png;The variable PetalLengthCm can be coded as ordinal without losing information.
2499;Iris_histograms_numeric.png;Feature generation based on variable SepalWidthCm seems to be promising.
2500;Iris_histograms_numeric.png;Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.
2501;Iris_histograms_numeric.png;Given the usual semantics of PetalLengthCm variable, dummification would have been a better codification.
2502;Iris_histograms_numeric.png;It is better to drop the variable SepalWidthCm than removing all records with missing values.
2503;Iris_histograms_numeric.png;Not knowing the semantics of SepalWidthCm variable, dummification could have been a more adequate codification.
2504;phone_decision_tree.png;The variable mobile_wt discriminates between the target values, as shown in the decision tree.
2505;phone_decision_tree.png;Variable mobile_wt is one of the most relevant variables.
2506;phone_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.
2507;phone_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
2508;phone_decision_tree.png;The specificity for the presented tree is lower than 60%.
2509;phone_decision_tree.png;The number of False Positives is lower than the number of False Negatives for the presented tree.
2510;phone_decision_tree.png;The number of True Negatives is higher than the number of True Positives for the presented tree.
2511;phone_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree.
2512;phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], it is possible to state that KNN algorithm classifies (A,B) as 2 for any k ≤ 636.
2513;phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], the Decision Tree presented classifies (not A, B) as 1.
2514;phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], the Decision Tree presented classifies (A, not B) as 0.
2515;phone_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.
2516;phone_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
2517;phone_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.
2518;phone_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2519;phone_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
2520;phone_overfitting_knn.png;KNN is in overfitting for k less than 13.
2521;phone_overfitting_knn.png;KNN with 5 neighbour is in overfitting.
2522;phone_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
2523;phone_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.
2524;phone_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.
2525;phone_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.
2526;phone_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.
2527;phone_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.
2528;phone_pca.png;The first 8 principal components are enough for explaining half the data variance.
2529;phone_pca.png;Using the first 11 principal components would imply an error between 10 and 25%.
2530;phone_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 11.
2531;phone_correlation_heatmap.png;One of the variables px_height or battery_power can be discarded without losing information.
2532;phone_correlation_heatmap.png;The variable battery_power can be discarded without risking losing information.
2533;phone_correlation_heatmap.png;Variables ram and px_width are redundant, but we can’t say the same for the pair mobile_wt and sc_h.
2534;phone_correlation_heatmap.png;Variables px_height and sc_w are redundant.
2535;phone_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2536;phone_correlation_heatmap.png;Variable n_cores seems to be relevant for the majority of mining tasks.
2537;phone_correlation_heatmap.png;Variables sc_h and fc seem to be useful for classification tasks.
2538;phone_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2539;phone_correlation_heatmap.png;Removing variable sc_h might improve the training of decision trees .
2540;phone_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable px_height previously than variable px_width.
2541;phone_boxplots.png;Variable n_cores is balanced.
2542;phone_boxplots.png;Those boxplots show that the data is not normalized.
2543;phone_boxplots.png;It is clear that variable talk_time shows some outliers, but we can’t be sure of the same for variable px_width.
2544;phone_boxplots.png;Outliers seem to be a problem in the dataset.
2545;phone_boxplots.png;Variable px_height shows some outlier values.
2546;phone_boxplots.png;Variable sc_w doesn’t have any outliers.
2547;phone_boxplots.png;Variable pc presents some outliers.
2548;phone_boxplots.png;At least 50 of the variables present outliers.
2549;phone_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
2550;phone_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2551;phone_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
2552;phone_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2553;phone_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2554;phone_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2555;phone_histograms_symbolic.png;All variables, but the class, should be dealt with as date.
2556;phone_histograms_symbolic.png;The variable four_g can be seen as ordinal.
2557;phone_histograms_symbolic.png;The variable wifi can be seen as ordinal without losing information.
2558;phone_histograms_symbolic.png;Considering the common semantics for touch_screen and blue variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2559;phone_histograms_symbolic.png;Considering the common semantics for three_g variable, dummification would be the most adequate encoding.
2560;phone_histograms_symbolic.png;The variable three_g can be coded as ordinal without losing information.
2561;phone_histograms_symbolic.png;Feature generation based on variable four_g seems to be promising.
2562;phone_histograms_symbolic.png;Feature generation based on the use of variable three_g wouldn’t be useful, but the use of blue seems to be promising.
2563;phone_histograms_symbolic.png;Given the usual semantics of three_g variable, dummification would have been a better codification.
2564;phone_histograms_symbolic.png;It is better to drop the variable three_g than removing all records with missing values.
2565;phone_histograms_symbolic.png;Not knowing the semantics of four_g variable, dummification could have been a more adequate codification.
2566;phone_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2567;phone_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.
2568;phone_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2569;phone_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2570;phone_histograms_numeric.png;All variables, but the class, should be dealt with as binary.
2571;phone_histograms_numeric.png;The variable int_memory can be seen as ordinal.
2572;phone_histograms_numeric.png;The variable fc can be seen as ordinal without losing information.
2573;phone_histograms_numeric.png;Variable sc_h is balanced.
2574;phone_histograms_numeric.png;It is clear that variable sc_w shows some outliers, but we can’t be sure of the same for variable sc_h.
2575;phone_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2576;phone_histograms_numeric.png;Variable pc shows a high number of outlier values.
2577;phone_histograms_numeric.png;Variable ram doesn’t have any outliers.
2578;phone_histograms_numeric.png;Variable fc presents some outliers.
2579;phone_histograms_numeric.png;At least 60 of the variables present outliers.
2580;phone_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
2581;phone_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2582;phone_histograms_numeric.png;Considering the common semantics for px_height and battery_power variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2583;phone_histograms_numeric.png;Considering the common semantics for px_height variable, dummification would be the most adequate encoding.
2584;phone_histograms_numeric.png;The variable battery_power can be coded as ordinal without losing information.
2585;phone_histograms_numeric.png;Feature generation based on variable mobile_wt seems to be promising.
2586;phone_histograms_numeric.png;Feature generation based on the use of variable sc_h wouldn’t be useful, but the use of battery_power seems to be promising.
2587;phone_histograms_numeric.png;Given the usual semantics of mobile_wt variable, dummification would have been a better codification.
2588;phone_histograms_numeric.png;It is better to drop the variable mobile_wt than removing all records with missing values.
2589;phone_histograms_numeric.png;Not knowing the semantics of talk_time variable, dummification could have been a more adequate codification.
2590;Titanic_decision_tree.png;The variable Parch discriminates between the target values, as shown in the decision tree.
2591;Titanic_decision_tree.png;Variable Parch is one of the most relevant variables.
2592;Titanic_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.
2593;Titanic_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.
2594;Titanic_decision_tree.png;The accuracy for the presented tree is lower than 75%.
2595;Titanic_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree.
2596;Titanic_decision_tree.png;The number of True Negatives is higher than the number of False Positives for the presented tree.
2597;Titanic_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree.
2598;Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 181.
2599;Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 1.
2600;Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.
2601;Titanic_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
2602;Titanic_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
2603;Titanic_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.
2604;Titanic_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2605;Titanic_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.
2606;Titanic_overfitting_knn.png;KNN is in overfitting for k larger than 13.
2607;Titanic_overfitting_knn.png;KNN with 11 neighbour is in overfitting.
2608;Titanic_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.
2609;Titanic_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.
2610;Titanic_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.
2611;Titanic_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.
2612;Titanic_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.
2613;Titanic_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.
2614;Titanic_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
2615;Titanic_pca.png;The first 4 principal components are enough for explaining half the data variance.
2616;Titanic_pca.png;Using the first 2 principal components would imply an error between 15 and 20%.
2617;Titanic_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.
2618;Titanic_correlation_heatmap.png;One of the variables SibSp or Parch can be discarded without losing information.
2619;Titanic_correlation_heatmap.png;The variable Parch can be discarded without risking losing information.
2620;Titanic_correlation_heatmap.png;Variables Fare and Age seem to be useful for classification tasks.
2621;Titanic_correlation_heatmap.png;Variables Age and Fare are redundant.
2622;Titanic_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2623;Titanic_correlation_heatmap.png;Variable Pclass seems to be relevant for the majority of mining tasks.
2624;Titanic_correlation_heatmap.png;Variables Parch and SibSp seem to be useful for classification tasks.
2625;Titanic_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2626;Titanic_correlation_heatmap.png;Removing variable SibSp might improve the training of decision trees .
2627;Titanic_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Fare previously than variable Age.
2628;Titanic_boxplots.png;Variable Age is balanced.
2629;Titanic_boxplots.png;Those boxplots show that the data is not normalized.
2630;Titanic_boxplots.png;It is clear that variable Pclass shows some outliers, but we can’t be sure of the same for variable Fare.
2631;Titanic_boxplots.png;Outliers seem to be a problem in the dataset.
2632;Titanic_boxplots.png;Variable Fare shows a high number of outlier values.
2633;Titanic_boxplots.png;Variable Fare doesn’t have any outliers.
2634;Titanic_boxplots.png;Variable Parch presents some outliers.
2635;Titanic_boxplots.png;At least 50 of the variables present outliers.
2636;Titanic_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2637;Titanic_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2638;Titanic_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
2639;Titanic_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2640;Titanic_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2641;Titanic_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2642;Titanic_histograms_symbolic.png;All variables, but the class, should be dealt with as date.
2643;Titanic_histograms_symbolic.png;The variable Sex can be seen as ordinal.
2644;Titanic_histograms_symbolic.png;The variable Sex can be seen as ordinal without losing information.
2645;Titanic_histograms_symbolic.png;Considering the common semantics for Sex and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2646;Titanic_histograms_symbolic.png;Considering the common semantics for Embarked variable, dummification would be the most adequate encoding.
2647;Titanic_histograms_symbolic.png;The variable Embarked can be coded as ordinal without losing information.
2648;Titanic_histograms_symbolic.png;Feature generation based on variable Sex seems to be promising.
2649;Titanic_histograms_symbolic.png;Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Embarked seems to be promising.
2650;Titanic_histograms_symbolic.png;Given the usual semantics of Sex variable, dummification would have been a better codification.
2651;Titanic_histograms_symbolic.png;It is better to drop the variable Embarked than removing all records with missing values.
2652;Titanic_histograms_symbolic.png;Not knowing the semantics of Embarked variable, dummification could have been a more adequate codification.
2653;Titanic_mv.png;Discarding variable Embarked would be better than discarding all the records with missing values for that variable.
2654;Titanic_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.
2655;Titanic_mv.png;Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.
2656;Titanic_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.
2657;Titanic_mv.png;Feature generation based on variable Embarked seems to be promising.
2658;Titanic_mv.png;It is better to drop the variable Age than removing all records with missing values.
2659;Titanic_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2660;Titanic_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
2661;Titanic_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2662;Titanic_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2663;Titanic_histograms_numeric.png;All variables, but the class, should be dealt with as symbolic.
2664;Titanic_histograms_numeric.png;The variable Parch can be seen as ordinal.
2665;Titanic_histograms_numeric.png;The variable Fare can be seen as ordinal without losing information.
2666;Titanic_histograms_numeric.png;Variable Pclass is balanced.
2667;Titanic_histograms_numeric.png;It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable SibSp.
2668;Titanic_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2669;Titanic_histograms_numeric.png;Variable Age shows a high number of outlier values.
2670;Titanic_histograms_numeric.png;Variable Fare doesn’t have any outliers.
2671;Titanic_histograms_numeric.png;Variable Parch presents some outliers.
2672;Titanic_histograms_numeric.png;At least 60 of the variables present outliers.
2673;Titanic_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2674;Titanic_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2675;Titanic_histograms_numeric.png;Considering the common semantics for Age and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2676;Titanic_histograms_numeric.png;Considering the common semantics for SibSp variable, dummification would be the most adequate encoding.
2677;Titanic_histograms_numeric.png;The variable Pclass can be coded as ordinal without losing information.
2678;Titanic_histograms_numeric.png;Feature generation based on variable Parch seems to be promising.
2679;Titanic_histograms_numeric.png;Feature generation based on the use of variable Age wouldn’t be useful, but the use of Pclass seems to be promising.
2680;Titanic_histograms_numeric.png;Given the usual semantics of Age variable, dummification would have been a better codification.
2681;Titanic_histograms_numeric.png;It is better to drop the variable Pclass than removing all records with missing values.
2682;Titanic_histograms_numeric.png;Not knowing the semantics of SibSp variable, dummification could have been a more adequate codification.
2683;apple_quality_decision_tree.png;The variable Crunchiness discriminates between the target values, as shown in the decision tree.
2684;apple_quality_decision_tree.png;Variable Juiciness is one of the most relevant variables.
2685;apple_quality_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
2686;apple_quality_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
2687;apple_quality_decision_tree.png;The recall for the presented tree is higher than 75%.
2688;apple_quality_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree.
2689;apple_quality_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree.
2690;apple_quality_decision_tree.png;The specificity for the presented tree is higher than 90%.
2691;apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], the Decision Tree presented classifies (not A, not B) as bad.
2692;apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 1625.
2693;apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as good.
2694;apple_quality_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
2695;apple_quality_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.
2696;apple_quality_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.
2697;apple_quality_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2698;apple_quality_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.
2699;apple_quality_overfitting_knn.png;KNN is in overfitting for k larger than 17.
2700;apple_quality_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
2701;apple_quality_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.
2702;apple_quality_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.
2703;apple_quality_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.
2704;apple_quality_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.
2705;apple_quality_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.
2706;apple_quality_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.
2707;apple_quality_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
2708;apple_quality_pca.png;The first 5 principal components are enough for explaining half the data variance.
2709;apple_quality_pca.png;Using the first 2 principal components would imply an error between 15 and 20%.
2710;apple_quality_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
2711;apple_quality_correlation_heatmap.png;One of the variables Crunchiness or Acidity can be discarded without losing information.
2712;apple_quality_correlation_heatmap.png;The variable Ripeness can be discarded without risking losing information.
2713;apple_quality_correlation_heatmap.png;Variables Juiciness and Crunchiness are redundant, but we can’t say the same for the pair Sweetness and Ripeness.
2714;apple_quality_correlation_heatmap.png;Variables Juiciness and Crunchiness are redundant.
2715;apple_quality_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2716;apple_quality_correlation_heatmap.png;Variable Juiciness seems to be relevant for the majority of mining tasks.
2717;apple_quality_correlation_heatmap.png;Variables Crunchiness and Weight seem to be useful for classification tasks.
2718;apple_quality_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2719;apple_quality_correlation_heatmap.png;Removing variable Juiciness might improve the training of decision trees .
2720;apple_quality_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Juiciness previously than variable Ripeness.
2721;apple_quality_boxplots.png;Variable Weight is balanced.
2722;apple_quality_boxplots.png;Those boxplots show that the data is not normalized.
2723;apple_quality_boxplots.png;It is clear that variable Sweetness shows some outliers, but we can’t be sure of the same for variable Crunchiness.
2724;apple_quality_boxplots.png;Outliers seem to be a problem in the dataset.
2725;apple_quality_boxplots.png;Variable Ripeness shows a high number of outlier values.
2726;apple_quality_boxplots.png;Variable Acidity doesn’t have any outliers.
2727;apple_quality_boxplots.png;Variable Juiciness presents some outliers.
2728;apple_quality_boxplots.png;At least 75 of the variables present outliers.
2729;apple_quality_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.
2730;apple_quality_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2731;apple_quality_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.
2732;apple_quality_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2733;apple_quality_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2734;apple_quality_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2735;apple_quality_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2736;apple_quality_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.
2737;apple_quality_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2738;apple_quality_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2739;apple_quality_histograms_numeric.png;All variables, but the class, should be dealt with as numeric.
2740;apple_quality_histograms_numeric.png;The variable Acidity can be seen as ordinal.
2741;apple_quality_histograms_numeric.png;The variable Size can be seen as ordinal without losing information.
2742;apple_quality_histograms_numeric.png;Variable Juiciness is balanced.
2743;apple_quality_histograms_numeric.png;It is clear that variable Weight shows some outliers, but we can’t be sure of the same for variable Sweetness.
2744;apple_quality_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2745;apple_quality_histograms_numeric.png;Variable Juiciness shows a high number of outlier values.
2746;apple_quality_histograms_numeric.png;Variable Size doesn’t have any outliers.
2747;apple_quality_histograms_numeric.png;Variable Weight presents some outliers.
2748;apple_quality_histograms_numeric.png;At least 50 of the variables present outliers.
2749;apple_quality_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2750;apple_quality_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2751;apple_quality_histograms_numeric.png;Considering the common semantics for Crunchiness and Size variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2752;apple_quality_histograms_numeric.png;Considering the common semantics for Sweetness variable, dummification would be the most adequate encoding.
2753;apple_quality_histograms_numeric.png;The variable Juiciness can be coded as ordinal without losing information.
2754;apple_quality_histograms_numeric.png;Feature generation based on variable Acidity seems to be promising.
2755;apple_quality_histograms_numeric.png;Feature generation based on the use of variable Acidity wouldn’t be useful, but the use of Size seems to be promising.
2756;apple_quality_histograms_numeric.png;Given the usual semantics of Acidity variable, dummification would have been a better codification.
2757;apple_quality_histograms_numeric.png;It is better to drop the variable Ripeness than removing all records with missing values.
2758;apple_quality_histograms_numeric.png;Not knowing the semantics of Acidity variable, dummification could have been a more adequate codification.
2759;Employee_decision_tree.png;The variable JoiningYear discriminates between the target values, as shown in the decision tree.
2760;Employee_decision_tree.png;Variable ExperienceInCurrentDomain is one of the most relevant variables.
2761;Employee_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.
2762;Employee_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.
2763;Employee_decision_tree.png;The recall for the presented tree is lower than 60%.
2764;Employee_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree.
2765;Employee_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree.
2766;Employee_decision_tree.png;The number of True Negatives is higher than the number of False Negatives for the presented tree.
2767;Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 44.
2768;Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], the Decision Tree presented classifies (not A, B) as 1.
2769;Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], the Decision Tree presented classifies (A,B) as 0.
2770;Employee_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.
2771;Employee_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.
2772;Employee_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.
2773;Employee_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.
2774;Employee_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.
2775;Employee_overfitting_knn.png;KNN is in overfitting for k less than 13.
2776;Employee_overfitting_knn.png;KNN with 7 neighbour is in overfitting.
2777;Employee_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.
2778;Employee_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.
2779;Employee_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.
2780;Employee_overfitting_decision_tree.png;The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.
2781;Employee_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.
2782;Employee_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.
2783;Employee_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.
2784;Employee_pca.png;The first 3 principal components are enough for explaining half the data variance.
2785;Employee_pca.png;Using the first 3 principal components would imply an error between 15 and 25%.
2786;Employee_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.
2787;Employee_correlation_heatmap.png;One of the variables PaymentTier or JoiningYear can be discarded without losing information.
2788;Employee_correlation_heatmap.png;The variable JoiningYear can be discarded without risking losing information.
2789;Employee_correlation_heatmap.png;Variables Age and PaymentTier are redundant, but we can’t say the same for the pair ExperienceInCurrentDomain and JoiningYear.
2790;Employee_correlation_heatmap.png;Variables PaymentTier and JoiningYear are redundant.
2791;Employee_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.
2792;Employee_correlation_heatmap.png;Variable JoiningYear seems to be relevant for the majority of mining tasks.
2793;Employee_correlation_heatmap.png;Variables Age and ExperienceInCurrentDomain seem to be useful for classification tasks.
2794;Employee_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.
2795;Employee_correlation_heatmap.png;Removing variable PaymentTier might improve the training of decision trees .
2796;Employee_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable ExperienceInCurrentDomain previously than variable PaymentTier.
2797;Employee_boxplots.png;Variable ExperienceInCurrentDomain is balanced.
2798;Employee_boxplots.png;Those boxplots show that the data is not normalized.
2799;Employee_boxplots.png;It is clear that variable PaymentTier shows some outliers, but we can’t be sure of the same for variable Age.
2800;Employee_boxplots.png;Outliers seem to be a problem in the dataset.
2801;Employee_boxplots.png;Variable JoiningYear shows a high number of outlier values.
2802;Employee_boxplots.png;Variable JoiningYear doesn’t have any outliers.
2803;Employee_boxplots.png;Variable PaymentTier presents some outliers.
2804;Employee_boxplots.png;At least 60 of the variables present outliers.
2805;Employee_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.
2806;Employee_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.
2807;Employee_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.
2808;Employee_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.
2809;Employee_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.
2810;Employee_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.
2811;Employee_histograms_symbolic.png;All variables, but the class, should be dealt with as date.
2812;Employee_histograms_symbolic.png;The variable Gender can be seen as ordinal.
2813;Employee_histograms_symbolic.png;The variable EverBenched can be seen as ordinal without losing information.
2814;Employee_histograms_symbolic.png;Considering the common semantics for Education and City variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2815;Employee_histograms_symbolic.png;Considering the common semantics for City variable, dummification would be the most adequate encoding.
2816;Employee_histograms_symbolic.png;The variable City can be coded as ordinal without losing information.
2817;Employee_histograms_symbolic.png;Feature generation based on variable City seems to be promising.
2818;Employee_histograms_symbolic.png;Feature generation based on the use of variable EverBenched wouldn’t be useful, but the use of Education seems to be promising.
2819;Employee_histograms_symbolic.png;Given the usual semantics of Gender variable, dummification would have been a better codification.
2820;Employee_histograms_symbolic.png;It is better to drop the variable EverBenched than removing all records with missing values.
2821;Employee_histograms_symbolic.png;Not knowing the semantics of Education variable, dummification could have been a more adequate codification.
2822;Employee_class_histogram.png;Balancing this dataset would be mandatory to improve the results.
2823;Employee_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.
2824;Employee_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.
2825;Employee_nr_records_nr_variables.png;Balancing this dataset by SMOTE would most probably be preferable over undersampling.
2826;Employee_histograms_numeric.png;All variables, but the class, should be dealt with as date.
2827;Employee_histograms_numeric.png;The variable PaymentTier can be seen as ordinal.
2828;Employee_histograms_numeric.png;The variable Age can be seen as ordinal without losing information.
2829;Employee_histograms_numeric.png;Variable Age is balanced.
2830;Employee_histograms_numeric.png;It is clear that variable PaymentTier shows some outliers, but we can’t be sure of the same for variable Age.
2831;Employee_histograms_numeric.png;Outliers seem to be a problem in the dataset.
2832;Employee_histograms_numeric.png;Variable JoiningYear shows some outlier values.
2833;Employee_histograms_numeric.png;Variable ExperienceInCurrentDomain doesn’t have any outliers.
2834;Employee_histograms_numeric.png;Variable PaymentTier presents some outliers.
2835;Employee_histograms_numeric.png;At least 50 of the variables present outliers.
2836;Employee_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.
2837;Employee_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.
2838;Employee_histograms_numeric.png;Considering the common semantics for JoiningYear and PaymentTier variables, dummification if applied would increase the risk of facing the curse of dimensionality.
2839;Employee_histograms_numeric.png;Considering the common semantics for PaymentTier variable, dummification would be the most adequate encoding.
2840;Employee_histograms_numeric.png;The variable PaymentTier can be coded as ordinal without losing information.
2841;Employee_histograms_numeric.png;Feature generation based on variable PaymentTier seems to be promising.
2842;Employee_histograms_numeric.png;Feature generation based on the use of variable ExperienceInCurrentDomain wouldn’t be useful, but the use of JoiningYear seems to be promising.
2843;Employee_histograms_numeric.png;Given the usual semantics of PaymentTier variable, dummification would have been a better codification.
2844;Employee_histograms_numeric.png;It is better to drop the variable JoiningYear than removing all records with missing values.
2845;Employee_histograms_numeric.png;Not knowing the semantics of Age variable, dummification could have been a more adequate codification.
