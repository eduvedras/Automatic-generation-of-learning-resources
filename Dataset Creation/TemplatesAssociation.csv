Chart;Templates
'nr_records_nr_variables';['Given the number of records and that some variables are [1], we might be facing the curse of dimensionality.', 'We face the curse of dimensionality when training a classifier with this dataset.', 'Balancing this dataset by SMOTE would most probably be preferable over undersampling.']
'correlation_heatmap';['The intrinsic dimensionality of this dataset is [1].', 'One of the variables [1] or [2] can be discarded without losing information.', 'The variable [1] can be discarded without risking losing information.', 'Variables [1] and [2] are redundant, but we can’t say the same for the pair [3] and [4].', 'Variables [1] and [2] are redundant.', 'From the correlation analysis alone, it is clear that there are relevant variables.', 'Variable [1] seems to be relevant for the majority of mining tasks.', 'Variables [1] and [2] seem to be useful for classification tasks.', 'Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.', 'Removing variable [1] might improve the training of decision trees .', 'There is evidence in favour for sequential backward selection to select variable [1] previously than variable [2].']
'histograms_numeric';['All variables, but the class, should be dealt with as [1].', 'The variable [1] can be seen as ordinal.', 'The variable [1] can be seen as ordinal without losing information.', 'Variable [1] is balanced.', 'It is clear that variable [1] shows some outliers, but we can’t be sure of the same for variable [2].', 'Outliers seem to be a problem in the dataset.', 'Variable [1] shows [2] outlier values.', 'Variable [1] doesn’t have any outliers.', 'Variable [1] presents some outliers.', 'At least [1] of the variables present outliers.', 'The [1] presented show a large number of outliers for most of the numeric variables.', 'The existence of outliers is one of the problems to tackle in this dataset.', 'Considering the common semantics for [1] and [2] variables, dummification if applied would increase the risk of facing the curse of dimensionality.', 'Considering the common semantics for [1] variable, dummification would be the most adequate encoding.', 'The variable [1] can be coded as ordinal without losing information.', 'Feature generation based on variable [1] seems to be promising.', 'Feature generation based on the use of variable [1] wouldn’t be useful, but the use of [2] seems to be promising.', 'Given the usual semantics of [1] variable, dummification would have been a better codification.', 'It is better to drop the variable [1] than removing all records with missing values.', 'Not knowing the semantics of [1] variable, dummification could have been a more adequate codification.']
'histograms_symbolic';['All variables, but the class, should be dealt with as [1].', 'The variable [1] can be seen as ordinal.', 'The variable [1] can be seen as ordinal without losing information.', 'Considering the common semantics for [1] and [2] variables, dummification if applied would increase the risk of facing the curse of dimensionality.', 'Considering the common semantics for [1] variable, dummification would be the most adequate encoding.', 'The variable [1] can be coded as ordinal without losing information.', 'Feature generation based on variable [1] seems to be promising.', 'Feature generation based on the use of variable [1] wouldn’t be useful, but the use of [2] seems to be promising.', 'Given the usual semantics of [1] variable, dummification would have been a better codification.', 'It is better to drop the variable [1] than removing all records with missing values.', 'Not knowing the semantics of [1] variable, dummification could have been a more adequate codification.']
'boxplots';['Variable [1] is balanced.', 'Those boxplots show that the data is not normalized.', 'It is clear that variable [1] shows some outliers, but we can’t be sure of the same for variable [2].', 'Outliers seem to be a problem in the dataset.', 'Variable [1] shows [2] outlier values.', 'Variable [1] doesn’t have any outliers.', 'Variable [1] presents some outliers.', 'At least [1] of the variables present outliers.', 'The [1] presented show a large number of outliers for most of the numeric variables.', 'The existence of outliers is one of the problems to tackle in this dataset.', 'A scaling transformation is mandatory, in order to improve the [1] performance in this dataset.', 'Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.', 'Normalization of this dataset could not have impact on a KNN classifier.', 'Scaling this dataset would be mandatory to improve the results with distance-based methods.']
'decision_tree';['The variable [1] discriminates between the target values, as shown in the decision tree.', 'Variable [1] is one of the most relevant variables.', 'A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of [1]%.', 'As reported in the tree, the number of False Positive is [1] than the number of False Negatives.', 'The [1] for the presented tree is [2] than [3]%.', 'The number of [1] reported in the same tree is [2].', 'The number of [1] is [2] than the number of [3] for the presented tree.', 'The [1] for the presented tree is [2] than its [3].', 'Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], the Decision Tree presented classifies [1] as [2].', 'Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], it is possible to state that KNN algorithm classifies [1] as [2] for any k ≤ [3].', 'Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], it is possible to state that Naive Bayes algorithm classifies [1], as [2].']
'class_histogram';['Balancing this dataset would be mandatory to improve the results.']
'mv';['Discarding variable [1] would be better than discarding all the records with missing values for that variable.', 'Dropping all records with missing values would be better than to drop the variables with missing values.', 'Dropping all rows with missing values can lead to a dataset with less than [1]% of the original data.', 'There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.', 'Feature generation based on variable [1] seems to be promising.', 'It is better to drop the variable [1] than removing all records with missing values.']
'pca';['The first [1] principal components are enough for explaining half the data variance.', 'Using the first [1] principal components would imply an error between [2] and [3]%.']
'overfitting_decision_tree';['According to the decision tree overfitting chart, the tree with [1] nodes of depth is in overfitting.', 'The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than [1].', 'The decision tree is in overfitting for depths above [1].', 'We are able to identify the existence of overfitting for decision tree models with more than [1] nodes of depth.']
'overfitting_knn';['KNN is in overfitting for k [1] than [2].', 'KNN with [1] neighbour is in overfitting.', 'KNN with [1] than [2] neighbours is in overfitting.', 'We are able to identify the existence of overfitting for KNN models with less than [1] neighbors.']
'overfitting_rf';['Results for Random Forests identified as [1], may be explained by its estimators being in [2].', 'The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.', 'We are able to identify the existence of overfitting for random forest models with more than [1] estimators.']
'overfitting_dt_acc_rec';['The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.']
'overfitting_gb';['We are able to identify the existence of overfitting for gradient boosting models with more than [1] estimators.']
'overfitting_mlp';['We are able to identify the existence of overfitting for MLP models trained longer than [1] episodes.']
