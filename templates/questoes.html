 
<html> 
<head></head> 
<body> 
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>0: It is clear that variable Age is one of the three most relevant features.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>1: The variable TUE seems to be one of the two most relevant features.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>2: The variable Age discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>3: It is possible to state that CH2O is the first most discriminative variable regarding the class.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>4: Variable TUE is one of the most relevant variables.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>5: Variable Weight seems to be relevant for the majority of mining tasks.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>6: Variables TUE and Age seem to be useful for classification tasks.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>7: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>8: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>9: The accuracy for the presented tree is higher than 90%.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>10: The number of False Positives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>11: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>12: The variable Age seems to be one of the five most relevant features.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>13: Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that Naive Bayes algorithm classifies (not A, B), as Obesity_Type_II.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>14: Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that KNN algorithm classifies (A,B) as Obesity_Type_II for any k ≤ 370.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>15: Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that KNN algorithm classifies (not A, B) as Obesity_Type_I for any k ≤ 840.</p>
    <img src="images/ObesityDataSet_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>16: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/ObesityDataSet_overfitting_gb.png" width="auto" height = "600"/> 
    <p>17: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/ObesityDataSet_overfitting_rf.png" width="auto" height = "600"/> 
    <p>18: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/ObesityDataSet_overfitting_rf.png" width="auto" height = "600"/> 
    <p>19: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/ObesityDataSet_overfitting_rf.png" width="auto" height = "600"/> 
    <p>20: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/ObesityDataSet_overfitting_knn.png" width="auto" height = "600"/> 
    <p>21: KNN is in overfitting for k larger than 5.</p>
    <img src="images/ObesityDataSet_overfitting_knn.png" width="auto" height = "600"/> 
    <p>22: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/ObesityDataSet_overfitting_knn.png" width="auto" height = "600"/> 
    <p>23: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/ObesityDataSet_overfitting_knn.png" width="auto" height = "600"/> 
    <p>24: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/ObesityDataSet_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>25: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/ObesityDataSet_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>26: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/ObesityDataSet_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>27: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/ObesityDataSet_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>28: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/ObesityDataSet_pca.png" width="auto" height = "600"/> 
    <p>29: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/ObesityDataSet_pca.png" width="auto" height = "600"/> 
    <p>30: Using the first 5 principal components would imply an error between 10 and 30%.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>31: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>32: One of the variables CH2O or NCP can be discarded without losing information.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>33: The variable FAF can be discarded without risking losing information.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>34: Variables TUE and FAF are redundant, but we can’t say the same for the pair Height and FCVC.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>35: Variables Weight and FAF are redundant.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>36: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>37: Variable Weight seems to be relevant for the majority of mining tasks.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>38: Variables Age and Height seem to be useful for classification tasks.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>39: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>40: Removing variable FAF might improve the training of decision trees .</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>41: There is evidence in favour for sequential backward selection to select variable NCP previously than variable Weight.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>42: Variable FCVC is balanced.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>43: Those boxplots show that the data is not normalized.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>44: It is clear that variable TUE shows some outliers, but we can’t be sure of the same for variable NCP.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>45: Outliers seem to be a problem in the dataset.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>46: Variable FAF shows a high number of outlier values.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>47: Variable Age doesn’t have any outliers.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>48: Variable TUE presents some outliers.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>49: At least 75 of the variables present outliers.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>50: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>51: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>52: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>53: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>54: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>55: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>56: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>57: The variable FAVC can be seen as ordinal.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>58: The variable FAVC can be seen as ordinal without losing information.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>59: Considering the common semantics for FAVC and CAEC variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>60: Considering the common semantics for family_history_with_overweight variable, dummification would be the most adequate encoding.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>61: The variable CALC can be coded as ordinal without losing information.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>62: Feature generation based on variable MTRANS seems to be promising.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>63: Feature generation based on the use of variable Gender wouldn’t be useful, but the use of CAEC seems to be promising.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>64: Given the usual semantics of SMOKE variable, dummification would have been a better codification.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>65: It is better to drop the variable CAEC than removing all records with missing values.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>66: Not knowing the semantics of CALC variable, dummification could have been a more adequate codification.</p>
    <img src="images/ObesityDataSet_class_histogram.png" width="auto" height = "600"/> 
    <p>67: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/ObesityDataSet_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>68: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/ObesityDataSet_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>69: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/ObesityDataSet_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>70: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>71: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>72: The variable Age can be seen as ordinal.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>73: The variable Weight can be seen as ordinal without losing information.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>74: Variable NCP is balanced.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>75: It is clear that variable FAF shows some outliers, but we can’t be sure of the same for variable FCVC.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>76: Outliers seem to be a problem in the dataset.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>77: Variable TUE shows a high number of outlier values.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>78: Variable FAF doesn’t have any outliers.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>79: Variable Height presents some outliers.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>80: At least 75 of the variables present outliers.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>81: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>82: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>83: Considering the common semantics for TUE and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>84: Considering the common semantics for Height variable, dummification would be the most adequate encoding.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>85: The variable NCP can be coded as ordinal without losing information.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>86: Feature generation based on variable Height seems to be promising.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>87: Feature generation based on the use of variable FAF wouldn’t be useful, but the use of Age seems to be promising.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>88: Given the usual semantics of FAF variable, dummification would have been a better codification.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>89: It is better to drop the variable TUE than removing all records with missing values.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>90: Not knowing the semantics of Weight variable, dummification could have been a more adequate codification.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>91: It is clear that variable Work_Experience is one of the four most relevant features.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>92: The variable Work_Experience seems to be one of the three most relevant features.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>93: The variable Work_Experience discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>94: It is possible to state that Work_Experience is the second most discriminative variable regarding the class.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>95: Variable Work_Experience is one of the most relevant variables.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>96: Variable Work_Experience seems to be relevant for the majority of mining tasks.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>97: Variables Work_Experience and Family_Size seem to be useful for classification tasks.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>98: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>99: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>100: The recall for the presented tree is lower than 60%.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>101: The number of False Positives reported in the same tree is 30.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>102: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>103: The number of True Negatives reported in the same tree is 10.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>104: Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A,B) as D for any k ≤ 11.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>105: Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], the Decision Tree presented classifies (not A, not B) as C.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>106: Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A,B) as A for any k ≤ 249.</p>
    <img src="images/customer_segmentation_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>107: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/customer_segmentation_overfitting_gb.png" width="auto" height = "600"/> 
    <p>108: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/customer_segmentation_overfitting_rf.png" width="auto" height = "600"/> 
    <p>109: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/customer_segmentation_overfitting_rf.png" width="auto" height = "600"/> 
    <p>110: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/customer_segmentation_overfitting_rf.png" width="auto" height = "600"/> 
    <p>111: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/customer_segmentation_overfitting_knn.png" width="auto" height = "600"/> 
    <p>112: KNN is in overfitting for k less than 17.</p>
    <img src="images/customer_segmentation_overfitting_knn.png" width="auto" height = "600"/> 
    <p>113: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/customer_segmentation_overfitting_knn.png" width="auto" height = "600"/> 
    <p>114: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/customer_segmentation_overfitting_knn.png" width="auto" height = "600"/> 
    <p>115: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/customer_segmentation_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>116: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/customer_segmentation_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>117: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/customer_segmentation_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>118: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/customer_segmentation_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>119: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/customer_segmentation_pca.png" width="auto" height = "600"/> 
    <p>120: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/customer_segmentation_pca.png" width="auto" height = "600"/> 
    <p>121: Using the first 2 principal components would imply an error between 15 and 30%.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>122: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>123: One of the variables Age or Family_Size can be discarded without losing information.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>124: The variable Family_Size can be discarded without risking losing information.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>125: Variables Age and Family_Size seem to be useful for classification tasks.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>126: Variables Work_Experience and Family_Size are redundant.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>127: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>128: Variable Family_Size seems to be relevant for the majority of mining tasks.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>129: Variables Family_Size and Age seem to be useful for classification tasks.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>130: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>131: Removing variable Family_Size might improve the training of decision trees .</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>132: There is evidence in favour for sequential backward selection to select variable Family_Size previously than variable Age.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>133: Variable Age is balanced.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>134: Those boxplots show that the data is not normalized.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>135: It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Work_Experience.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>136: Outliers seem to be a problem in the dataset.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>137: Variable Work_Experience shows a high number of outlier values.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>138: Variable Work_Experience doesn’t have any outliers.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>139: Variable Work_Experience presents some outliers.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>140: At least 50 of the variables present outliers.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>141: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>142: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>143: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>144: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>145: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>146: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>147: All variables, but the class, should be dealt with as date.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>148: The variable Spending_Score can be seen as ordinal.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>149: The variable Profession can be seen as ordinal without losing information.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>150: Considering the common semantics for Profession and Spending_Score variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>151: Considering the common semantics for Var_1 variable, dummification would be the most adequate encoding.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>152: The variable Profession can be coded as ordinal without losing information.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>153: Feature generation based on variable Var_1 seems to be promising.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>154: Feature generation based on the use of variable Profession wouldn’t be useful, but the use of Spending_Score seems to be promising.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>155: Given the usual semantics of Gender variable, dummification would have been a better codification.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>156: It is better to drop the variable Graduated than removing all records with missing values.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>157: Not knowing the semantics of Graduated variable, dummification could have been a more adequate codification.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>158: Discarding variable Ever_Married would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>159: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>160: Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>161: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>162: Feature generation based on variable Graduated seems to be promising.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>163: It is better to drop the variable Var_1 than removing all records with missing values.</p>
    <img src="images/customer_segmentation_class_histogram.png" width="auto" height = "600"/> 
    <p>164: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/customer_segmentation_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>165: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/customer_segmentation_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>166: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/customer_segmentation_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>167: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>168: All variables, but the class, should be dealt with as date.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>169: The variable Family_Size can be seen as ordinal.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>170: The variable Age can be seen as ordinal without losing information.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>171: Variable Work_Experience is balanced.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>172: It is clear that variable Family_Size shows some outliers, but we can’t be sure of the same for variable Work_Experience.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>173: Outliers seem to be a problem in the dataset.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>174: Variable Work_Experience shows some outlier values.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>175: Variable Work_Experience doesn’t have any outliers.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>176: Variable Age presents some outliers.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>177: At least 50 of the variables present outliers.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>178: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>179: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>180: Considering the common semantics for Family_Size and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>181: Considering the common semantics for Work_Experience variable, dummification would be the most adequate encoding.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>182: The variable Work_Experience can be coded as ordinal without losing information.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>183: Feature generation based on variable Family_Size seems to be promising.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>184: Feature generation based on the use of variable Family_Size wouldn’t be useful, but the use of Age seems to be promising.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>185: Given the usual semantics of Family_Size variable, dummification would have been a better codification.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>186: It is better to drop the variable Work_Experience than removing all records with missing values.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>187: Not knowing the semantics of Work_Experience variable, dummification could have been a more adequate codification.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>188: It is clear that variable pH is one of the three most relevant features.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>189: The variable Age seems to be one of the four most relevant features.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>190: The variable Age discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>191: It is possible to state that pH is the first most discriminative variable regarding the class.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>192: Variable Specific Gravity is one of the most relevant variables.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>193: Variable Age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>194: Variables Specific Gravity and pH seem to be useful for classification tasks.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>195: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>196: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>197: The accuracy for the presented tree is lower than 75%.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>198: The number of False Positives reported in the same tree is 10.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>199: The number of True Positives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>200: The number of True Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>201: Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], it is possible to state that KNN algorithm classifies (A,B) as POSITIVE for any k ≤ 215.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>202: Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as POSITIVE.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>203: Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as NEGATIVE.</p>
    <img src="images/urinalysis_tests_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>204: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/urinalysis_tests_overfitting_gb.png" width="auto" height = "600"/> 
    <p>205: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/urinalysis_tests_overfitting_rf.png" width="auto" height = "600"/> 
    <p>206: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/urinalysis_tests_overfitting_rf.png" width="auto" height = "600"/> 
    <p>207: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/urinalysis_tests_overfitting_rf.png" width="auto" height = "600"/> 
    <p>208: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/urinalysis_tests_overfitting_knn.png" width="auto" height = "600"/> 
    <p>209: KNN is in overfitting for k less than 5.</p>
    <img src="images/urinalysis_tests_overfitting_knn.png" width="auto" height = "600"/> 
    <p>210: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/urinalysis_tests_overfitting_knn.png" width="auto" height = "600"/> 
    <p>211: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/urinalysis_tests_overfitting_knn.png" width="auto" height = "600"/> 
    <p>212: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/urinalysis_tests_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>213: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/urinalysis_tests_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>214: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.</p>
    <img src="images/urinalysis_tests_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>215: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/urinalysis_tests_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>216: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/urinalysis_tests_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>217: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/urinalysis_tests_pca.png" width="auto" height = "600"/> 
    <p>218: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/urinalysis_tests_pca.png" width="auto" height = "600"/> 
    <p>219: Using the first 2 principal components would imply an error between 15 and 25%.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>220: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>221: One of the variables pH or Specific Gravity can be discarded without losing information.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>222: The variable Specific Gravity can be discarded without risking losing information.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>223: Variables pH and Age seem to be useful for classification tasks.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>224: Variables pH and Specific Gravity are redundant.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>225: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>226: Variable Specific Gravity seems to be relevant for the majority of mining tasks.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>227: Variables Specific Gravity and pH seem to be useful for classification tasks.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>228: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>229: Removing variable pH might improve the training of decision trees .</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>230: There is evidence in favour for sequential backward selection to select variable Specific Gravity previously than variable Age.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>231: Variable pH is balanced.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>232: Those boxplots show that the data is not normalized.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>233: It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Specific Gravity.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>234: Outliers seem to be a problem in the dataset.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>235: Variable pH shows a high number of outlier values.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>236: Variable Specific Gravity doesn’t have any outliers.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>237: Variable Specific Gravity presents some outliers.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>238: At least 85 of the variables present outliers.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>239: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>240: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>241: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>242: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>243: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>244: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>245: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>246: The variable Transparency can be seen as ordinal.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>247: The variable Mucous Threads can be seen as ordinal without losing information.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>248: Considering the common semantics for Amorphous Urates and Color variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>249: Considering the common semantics for Glucose variable, dummification would be the most adequate encoding.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>250: The variable Transparency can be coded as ordinal without losing information.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>251: Feature generation based on variable Gender seems to be promising.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>252: Feature generation based on the use of variable Protein wouldn’t be useful, but the use of Color seems to be promising.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>253: Given the usual semantics of Bacteria variable, dummification would have been a better codification.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>254: It is better to drop the variable Mucous Threads than removing all records with missing values.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>255: Not knowing the semantics of Transparency variable, dummification could have been a more adequate codification.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>256: Discarding variable Color would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>257: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>258: Dropping all rows with missing values can lead to a dataset with less than 40% of the original data.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>259: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>260: Feature generation based on variable Color seems to be promising.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>261: It is better to drop the variable Color than removing all records with missing values.</p>
    <img src="images/urinalysis_tests_class_histogram.png" width="auto" height = "600"/> 
    <p>262: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/urinalysis_tests_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>263: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/urinalysis_tests_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>264: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/urinalysis_tests_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>265: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>266: All variables, but the class, should be dealt with as date.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>267: The variable Age can be seen as ordinal.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>268: The variable Age can be seen as ordinal without losing information.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>269: Variable Age is balanced.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>270: It is clear that variable Specific Gravity shows some outliers, but we can’t be sure of the same for variable Age.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>271: Outliers seem to be a problem in the dataset.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>272: Variable Age shows a high number of outlier values.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>273: Variable Specific Gravity doesn’t have any outliers.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>274: Variable Age presents some outliers.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>275: At least 85 of the variables present outliers.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>276: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>277: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>278: Considering the common semantics for pH and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>279: Considering the common semantics for Specific Gravity variable, dummification would be the most adequate encoding.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>280: The variable Age can be coded as ordinal without losing information.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>281: Feature generation based on variable Specific Gravity seems to be promising.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>282: Feature generation based on the use of variable Specific Gravity wouldn’t be useful, but the use of Age seems to be promising.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>283: Given the usual semantics of pH variable, dummification would have been a better codification.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>284: It is better to drop the variable Specific Gravity than removing all records with missing values.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>285: Not knowing the semantics of Specific Gravity variable, dummification could have been a more adequate codification.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>286: It is clear that variable Vb is one of the four most relevant features.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>287: The variable Vc seems to be one of the five most relevant features.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>288: The variable Ib discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>289: It is possible to state that Vc is the second most discriminative variable regarding the class.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>290: Variable Ic is one of the most relevant variables.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>291: Variable Ic seems to be relevant for the majority of mining tasks.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>292: Variables Vc and Ib seem to be useful for classification tasks.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>293: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>294: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>295: The precision for the presented tree is higher than 60%.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>296: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>297: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>298: The variable Va seems to be one of the four most relevant features.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>299: Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 797.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>300: Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 1206.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>301: Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 3.</p>
    <img src="images/detect_dataset_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>302: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/detect_dataset_overfitting_gb.png" width="auto" height = "600"/> 
    <p>303: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/detect_dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>304: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/detect_dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>305: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/detect_dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>306: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/detect_dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>307: KNN is in overfitting for k larger than 17.</p>
    <img src="images/detect_dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>308: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/detect_dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>309: KNN with more than 7 neighbours is in overfitting.</p>
    <img src="images/detect_dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>310: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/detect_dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>311: According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.</p>
    <img src="images/detect_dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>312: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.</p>
    <img src="images/detect_dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>313: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/detect_dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>314: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/detect_dataset_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>315: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/detect_dataset_pca.png" width="auto" height = "600"/> 
    <p>316: The first 5 principal components are enough for explaining half the data variance.</p>
    <img src="images/detect_dataset_pca.png" width="auto" height = "600"/> 
    <p>317: Using the first 2 principal components would imply an error between 15 and 30%.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>318: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>319: One of the variables Ic or Ia can be discarded without losing information.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>320: The variable Ia can be discarded without risking losing information.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>321: Variables Vb and Ia are redundant, but we can’t say the same for the pair Va and Ic.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>322: Variables Ia and Ib are redundant.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>323: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>324: Variable Ic seems to be relevant for the majority of mining tasks.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>325: Variables Vc and Ic seem to be useful for classification tasks.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>326: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>327: Removing variable Va might improve the training of decision trees .</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>328: There is evidence in favour for sequential backward selection to select variable Vc previously than variable Ic.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>329: Variable Vb is balanced.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>330: Those boxplots show that the data is not normalized.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>331: It is clear that variable Ib shows some outliers, but we can’t be sure of the same for variable Vc.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>332: Outliers seem to be a problem in the dataset.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>333: Variable Vb shows a high number of outlier values.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>334: Variable Ia doesn’t have any outliers.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>335: Variable Ia presents some outliers.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>336: At least 50 of the variables present outliers.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>337: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>338: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>339: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>340: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>341: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>342: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/detect_dataset_class_histogram.png" width="auto" height = "600"/> 
    <p>343: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/detect_dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>344: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/detect_dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>345: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/detect_dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>346: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>347: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>348: The variable Ic can be seen as ordinal.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>349: The variable Ib can be seen as ordinal without losing information.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>350: Variable Va is balanced.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>351: It is clear that variable Vb shows some outliers, but we can’t be sure of the same for variable Vc.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>352: Outliers seem to be a problem in the dataset.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>353: Variable Ib shows some outlier values.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>354: Variable Ic doesn’t have any outliers.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>355: Variable Vc presents some outliers.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>356: At least 75 of the variables present outliers.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>357: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>358: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>359: Considering the common semantics for Vc and Ia variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>360: Considering the common semantics for Ic variable, dummification would be the most adequate encoding.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>361: The variable Ia can be coded as ordinal without losing information.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>362: Feature generation based on variable Vb seems to be promising.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>363: Feature generation based on the use of variable Vb wouldn’t be useful, but the use of Ia seems to be promising.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>364: Given the usual semantics of Ic variable, dummification would have been a better codification.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>365: It is better to drop the variable Ic than removing all records with missing values.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>366: Not knowing the semantics of Va variable, dummification could have been a more adequate codification.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>367: It is clear that variable Glucose is one of the five most relevant features.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>368: The variable Glucose seems to be one of the three most relevant features.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>369: The variable Insulin discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>370: It is possible to state that Age is the second most discriminative variable regarding the class.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>371: Variable DiabetesPedigreeFunction is one of the most relevant variables.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>372: Variable Age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>373: Variables Age and DiabetesPedigreeFunction seem to be useful for classification tasks.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>374: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>375: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>376: The specificity for the presented tree is higher than 75%.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>377: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>378: The number of False Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>379: The variable Insulin seems to be one of the three most relevant features.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>380: Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as 1.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>381: Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 161.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>382: Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 167.</p>
    <img src="images/diabetes_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>383: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/diabetes_overfitting_gb.png" width="auto" height = "600"/> 
    <p>384: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/diabetes_overfitting_rf.png" width="auto" height = "600"/> 
    <p>385: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/diabetes_overfitting_rf.png" width="auto" height = "600"/> 
    <p>386: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/diabetes_overfitting_rf.png" width="auto" height = "600"/> 
    <p>387: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/diabetes_overfitting_knn.png" width="auto" height = "600"/> 
    <p>388: KNN is in overfitting for k larger than 13.</p>
    <img src="images/diabetes_overfitting_knn.png" width="auto" height = "600"/> 
    <p>389: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/diabetes_overfitting_knn.png" width="auto" height = "600"/> 
    <p>390: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/diabetes_overfitting_knn.png" width="auto" height = "600"/> 
    <p>391: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/diabetes_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>392: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/diabetes_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>393: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.</p>
    <img src="images/diabetes_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>394: The decision tree is in overfitting for depths above 10.</p>
    <img src="images/diabetes_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>395: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/diabetes_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>396: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/diabetes_pca.png" width="auto" height = "600"/> 
    <p>397: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/diabetes_pca.png" width="auto" height = "600"/> 
    <p>398: Using the first 6 principal components would imply an error between 15 and 20%.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>399: The intrinsic dimensionality of this dataset is 7.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>400: One of the variables Age or Insulin can be discarded without losing information.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>401: The variable Glucose can be discarded without risking losing information.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>402: Variables Pregnancies and BMI are redundant, but we can’t say the same for the pair SkinThickness and Glucose.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>403: Variables BloodPressure and DiabetesPedigreeFunction are redundant.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>404: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>405: Variable Glucose seems to be relevant for the majority of mining tasks.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>406: Variables Age and DiabetesPedigreeFunction seem to be useful for classification tasks.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>407: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>408: Removing variable Insulin might improve the training of decision trees .</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>409: There is evidence in favour for sequential backward selection to select variable Pregnancies previously than variable Insulin.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>410: Variable Age is balanced.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>411: Those boxplots show that the data is not normalized.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>412: It is clear that variable DiabetesPedigreeFunction shows some outliers, but we can’t be sure of the same for variable BloodPressure.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>413: Outliers seem to be a problem in the dataset.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>414: Variable Pregnancies shows some outlier values.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>415: Variable Insulin doesn’t have any outliers.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>416: Variable BloodPressure presents some outliers.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>417: At least 60 of the variables present outliers.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>418: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>419: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>420: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>421: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>422: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>423: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/diabetes_class_histogram.png" width="auto" height = "600"/> 
    <p>424: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/diabetes_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>425: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/diabetes_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>426: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/diabetes_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>427: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>428: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>429: The variable DiabetesPedigreeFunction can be seen as ordinal.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>430: The variable BloodPressure can be seen as ordinal without losing information.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>431: Variable Insulin is balanced.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>432: It is clear that variable SkinThickness shows some outliers, but we can’t be sure of the same for variable BMI.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>433: Outliers seem to be a problem in the dataset.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>434: Variable DiabetesPedigreeFunction shows some outlier values.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>435: Variable Insulin doesn’t have any outliers.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>436: Variable DiabetesPedigreeFunction presents some outliers.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>437: At least 50 of the variables present outliers.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>438: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>439: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>440: Considering the common semantics for Age and Pregnancies variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>441: Considering the common semantics for Age variable, dummification would be the most adequate encoding.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>442: The variable BloodPressure can be coded as ordinal without losing information.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>443: Feature generation based on variable SkinThickness seems to be promising.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>444: Feature generation based on the use of variable Glucose wouldn’t be useful, but the use of Pregnancies seems to be promising.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>445: Given the usual semantics of DiabetesPedigreeFunction variable, dummification would have been a better codification.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>446: It is better to drop the variable Insulin than removing all records with missing values.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>447: Not knowing the semantics of DiabetesPedigreeFunction variable, dummification could have been a more adequate codification.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>448: It is clear that variable mba_p is one of the three most relevant features.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>449: The variable mba_p seems to be one of the three most relevant features.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>450: The variable degree_p discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>451: It is possible to state that mba_p is the second most discriminative variable regarding the class.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>452: Variable mba_p is one of the most relevant variables.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>453: Variable ssc_p seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>454: Variables degree_p and etest_p seem to be useful for classification tasks.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>455: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>456: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>457: The recall for the presented tree is lower than 60%.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>458: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>459: The number of True Negatives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>460: The variable etest_p seems to be one of the three most relevant features.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>461: Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], it is possible to state that KNN algorithm classifies (A,B) as Not Placed for any k ≤ 16.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>462: Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], the Decision Tree presented classifies (not A, B) as Not Placed.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>463: Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], it is possible to state that KNN algorithm classifies (not A, B) as Placed for any k ≤ 68.</p>
    <img src="images/Placement_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>464: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Placement_overfitting_gb.png" width="auto" height = "600"/> 
    <p>465: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/Placement_overfitting_rf.png" width="auto" height = "600"/> 
    <p>466: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/Placement_overfitting_rf.png" width="auto" height = "600"/> 
    <p>467: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Placement_overfitting_rf.png" width="auto" height = "600"/> 
    <p>468: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/Placement_overfitting_knn.png" width="auto" height = "600"/> 
    <p>469: KNN is in overfitting for k larger than 5.</p>
    <img src="images/Placement_overfitting_knn.png" width="auto" height = "600"/> 
    <p>470: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/Placement_overfitting_knn.png" width="auto" height = "600"/> 
    <p>471: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/Placement_overfitting_knn.png" width="auto" height = "600"/> 
    <p>472: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/Placement_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>473: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Placement_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>474: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/Placement_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>475: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/Placement_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>476: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/Placement_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>477: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Placement_pca.png" width="auto" height = "600"/> 
    <p>478: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/Placement_pca.png" width="auto" height = "600"/> 
    <p>479: Using the first 2 principal components would imply an error between 15 and 25%.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>480: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>481: One of the variables ssc_p or hsc_p can be discarded without losing information.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>482: The variable ssc_p can be discarded without risking losing information.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>483: Variables etest_p and ssc_p are redundant, but we can’t say the same for the pair mba_p and degree_p.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>484: Variables hsc_p and degree_p are redundant.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>485: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>486: Variable hsc_p seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>487: Variables mba_p and etest_p seem to be useful for classification tasks.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>488: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>489: Removing variable degree_p might improve the training of decision trees .</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>490: There is evidence in favour for sequential backward selection to select variable etest_p previously than variable ssc_p.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>491: Variable etest_p is balanced.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>492: Those boxplots show that the data is not normalized.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>493: It is clear that variable etest_p shows some outliers, but we can’t be sure of the same for variable ssc_p.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>494: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>495: Variable hsc_p shows a high number of outlier values.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>496: Variable ssc_p doesn’t have any outliers.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>497: Variable ssc_p presents some outliers.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>498: At least 75 of the variables present outliers.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>499: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>500: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>501: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>502: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>503: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>504: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>505: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>506: The variable ssc_b can be seen as ordinal.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>507: The variable workex can be seen as ordinal without losing information.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>508: Considering the common semantics for workex and hsc_s variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>509: Considering the common semantics for workex variable, dummification would be the most adequate encoding.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>510: The variable hsc_s can be coded as ordinal without losing information.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>511: Feature generation based on variable hsc_s seems to be promising.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>512: Feature generation based on the use of variable gender wouldn’t be useful, but the use of hsc_s seems to be promising.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>513: Given the usual semantics of hsc_s variable, dummification would have been a better codification.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>514: It is better to drop the variable specialisation than removing all records with missing values.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>515: Not knowing the semantics of workex variable, dummification could have been a more adequate codification.</p>
    <img src="images/Placement_class_histogram.png" width="auto" height = "600"/> 
    <p>516: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Placement_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>517: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/Placement_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>518: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Placement_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>519: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>520: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>521: The variable mba_p can be seen as ordinal.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>522: The variable hsc_p can be seen as ordinal without losing information.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>523: Variable etest_p is balanced.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>524: It is clear that variable ssc_p shows some outliers, but we can’t be sure of the same for variable etest_p.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>525: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>526: Variable mba_p shows a high number of outlier values.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>527: Variable etest_p doesn’t have any outliers.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>528: Variable mba_p presents some outliers.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>529: At least 85 of the variables present outliers.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>530: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>531: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>532: Considering the common semantics for ssc_p and hsc_p variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>533: Considering the common semantics for mba_p variable, dummification would be the most adequate encoding.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>534: The variable degree_p can be coded as ordinal without losing information.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>535: Feature generation based on variable etest_p seems to be promising.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>536: Feature generation based on the use of variable mba_p wouldn’t be useful, but the use of ssc_p seems to be promising.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>537: Given the usual semantics of degree_p variable, dummification would have been a better codification.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>538: It is better to drop the variable hsc_p than removing all records with missing values.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>539: Not knowing the semantics of mba_p variable, dummification could have been a more adequate codification.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>540: It is clear that variable ALB is one of the four most relevant features.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>541: The variable AG_Ratio seems to be one of the five most relevant features.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>542: The variable TB discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>543: It is possible to state that TP is the second most discriminative variable regarding the class.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>544: Variable TP is one of the most relevant variables.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>545: Variable Age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>546: Variables ALB and Age seem to be useful for classification tasks.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>547: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>548: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>549: The precision for the presented tree is higher than 60%.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>550: The number of False Positives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>551: The number of False Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>552: The recall for the presented tree is lower than 90%.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>553: Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 77.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>554: Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as 1.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>555: Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], the Decision Tree presented classifies (not A, not B) as 2.</p>
    <img src="images/Liver_Patient_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>556: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Liver_Patient_overfitting_gb.png" width="auto" height = "600"/> 
    <p>557: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Liver_Patient_overfitting_rf.png" width="auto" height = "600"/> 
    <p>558: Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.</p>
    <img src="images/Liver_Patient_overfitting_rf.png" width="auto" height = "600"/> 
    <p>559: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Liver_Patient_overfitting_rf.png" width="auto" height = "600"/> 
    <p>560: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Liver_Patient_overfitting_knn.png" width="auto" height = "600"/> 
    <p>561: KNN is in overfitting for k larger than 17.</p>
    <img src="images/Liver_Patient_overfitting_knn.png" width="auto" height = "600"/> 
    <p>562: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/Liver_Patient_overfitting_knn.png" width="auto" height = "600"/> 
    <p>563: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/Liver_Patient_overfitting_knn.png" width="auto" height = "600"/> 
    <p>564: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/Liver_Patient_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>565: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/Liver_Patient_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>566: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.</p>
    <img src="images/Liver_Patient_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>567: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/Liver_Patient_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>568: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Liver_Patient_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>569: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Liver_Patient_pca.png" width="auto" height = "600"/> 
    <p>570: The first 5 principal components are enough for explaining half the data variance.</p>
    <img src="images/Liver_Patient_pca.png" width="auto" height = "600"/> 
    <p>571: Using the first 8 principal components would imply an error between 15 and 30%.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>572: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>573: One of the variables TP or Sgpt can be discarded without losing information.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>574: The variable DB can be discarded without risking losing information.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>575: Variables AG_Ratio and TP are redundant, but we can’t say the same for the pair Sgot and Alkphos.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>576: Variables Sgot and TB are redundant.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>577: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>578: Variable TB seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>579: Variables Age and Sgpt seem to be useful for classification tasks.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>580: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>581: Removing variable DB might improve the training of decision trees .</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>582: There is evidence in favour for sequential backward selection to select variable TB previously than variable AG_Ratio.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>583: Variable TP is balanced.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>584: Those boxplots show that the data is not normalized.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>585: It is clear that variable TB shows some outliers, but we can’t be sure of the same for variable ALB.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>586: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>587: Variable TB shows a high number of outlier values.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>588: Variable AG_Ratio doesn’t have any outliers.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>589: Variable Sgot presents some outliers.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>590: At least 50 of the variables present outliers.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>591: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>592: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>593: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>594: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>595: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>596: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>597: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>598: The variable Gender can be seen as ordinal.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>599: The variable Gender can be seen as ordinal without losing information.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>600: Considering the common semantics for Gender and <all_variables> variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>601: Considering the common semantics for Gender variable, dummification would be the most adequate encoding.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>602: The variable Gender can be coded as ordinal without losing information.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>603: Feature generation based on variable Gender seems to be promising.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>604: Feature generation based on the use of variable Gender wouldn’t be useful, but the use of <all_variables> seems to be promising.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>605: Given the usual semantics of Gender variable, dummification would have been a better codification.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>606: It is better to drop the variable Gender than removing all records with missing values.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>607: Not knowing the semantics of Gender variable, dummification could have been a more adequate codification.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>608: Discarding variable AG_Ratio would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>609: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>610: Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>611: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>612: Feature generation based on variable AG_Ratio seems to be promising.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>613: It is better to drop the variable AG_Ratio than removing all records with missing values.</p>
    <img src="images/Liver_Patient_class_histogram.png" width="auto" height = "600"/> 
    <p>614: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Liver_Patient_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>615: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Liver_Patient_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>616: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Liver_Patient_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>617: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>618: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>619: The variable Sgpt can be seen as ordinal.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>620: The variable Alkphos can be seen as ordinal without losing information.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>621: Variable Sgpt is balanced.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>622: It is clear that variable ALB shows some outliers, but we can’t be sure of the same for variable DB.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>623: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>624: Variable AG_Ratio shows some outlier values.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>625: Variable AG_Ratio doesn’t have any outliers.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>626: Variable TB presents some outliers.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>627: At least 75 of the variables present outliers.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>628: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>629: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>630: Considering the common semantics for AG_Ratio and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>631: Considering the common semantics for Sgpt variable, dummification would be the most adequate encoding.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>632: The variable TB can be coded as ordinal without losing information.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>633: Feature generation based on variable Age seems to be promising.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>634: Feature generation based on the use of variable Alkphos wouldn’t be useful, but the use of Age seems to be promising.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>635: Given the usual semantics of Alkphos variable, dummification would have been a better codification.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>636: It is better to drop the variable AG_Ratio than removing all records with missing values.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>637: Not knowing the semantics of ALB variable, dummification could have been a more adequate codification.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>638: It is clear that variable no_of_special_requests is one of the five most relevant features.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>639: The variable no_of_weekend_nights seems to be one of the two most relevant features.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>640: The variable no_of_weekend_nights discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>641: It is possible to state that no_of_children is the first most discriminative variable regarding the class.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>642: Variable no_of_children is one of the most relevant variables.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>643: Variable avg_price_per_room seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>644: Variables no_of_weekend_nights and no_of_adults seem to be useful for classification tasks.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>645: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>646: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>647: The accuracy for the presented tree is lower than 75%.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>648: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>649: The number of True Positives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>650: The specificity for the presented tree is lower than its accuracy.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>651: Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], the Decision Tree presented classifies (not A, not B) as Canceled.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>652: Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], the Decision Tree presented classifies (A, not B) as Canceled.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>653: Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A,B) as Canceled for any k ≤ 9756.</p>
    <img src="images/Hotel_Reservations_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>654: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Hotel_Reservations_overfitting_gb.png" width="auto" height = "600"/> 
    <p>655: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/Hotel_Reservations_overfitting_rf.png" width="auto" height = "600"/> 
    <p>656: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/Hotel_Reservations_overfitting_rf.png" width="auto" height = "600"/> 
    <p>657: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Hotel_Reservations_overfitting_rf.png" width="auto" height = "600"/> 
    <p>658: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/Hotel_Reservations_overfitting_knn.png" width="auto" height = "600"/> 
    <p>659: KNN is in overfitting for k larger than 5.</p>
    <img src="images/Hotel_Reservations_overfitting_knn.png" width="auto" height = "600"/> 
    <p>660: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/Hotel_Reservations_overfitting_knn.png" width="auto" height = "600"/> 
    <p>661: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/Hotel_Reservations_overfitting_knn.png" width="auto" height = "600"/> 
    <p>662: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/Hotel_Reservations_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>663: According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.</p>
    <img src="images/Hotel_Reservations_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>664: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/Hotel_Reservations_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>665: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/Hotel_Reservations_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>666: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Hotel_Reservations_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>667: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Hotel_Reservations_pca.png" width="auto" height = "600"/> 
    <p>668: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/Hotel_Reservations_pca.png" width="auto" height = "600"/> 
    <p>669: Using the first 2 principal components would imply an error between 5 and 30%.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>670: The intrinsic dimensionality of this dataset is 7.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>671: One of the variables no_of_children or arrival_date can be discarded without losing information.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>672: The variable avg_price_per_room can be discarded without risking losing information.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>673: Variables no_of_adults and no_of_special_requests are redundant, but we can’t say the same for the pair no_of_children and lead_time.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>674: Variables no_of_week_nights and no_of_weekend_nights are redundant.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>675: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>676: Variable no_of_week_nights seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>677: Variables no_of_special_requests and no_of_week_nights seem to be useful for classification tasks.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>678: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>679: Removing variable no_of_special_requests might improve the training of decision trees .</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>680: There is evidence in favour for sequential backward selection to select variable no_of_special_requests previously than variable no_of_children.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>681: Variable no_of_adults is balanced.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>682: Those boxplots show that the data is not normalized.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>683: It is clear that variable no_of_weekend_nights shows some outliers, but we can’t be sure of the same for variable no_of_children.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>684: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>685: Variable no_of_special_requests shows a high number of outlier values.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>686: Variable no_of_week_nights doesn’t have any outliers.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>687: Variable arrival_date presents some outliers.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>688: At least 85 of the variables present outliers.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>689: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>690: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>691: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>692: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>693: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>694: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>695: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>696: The variable required_car_parking_space can be seen as ordinal.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>697: The variable repeated_guest can be seen as ordinal without losing information.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>698: Considering the common semantics for repeated_guest and type_of_meal_plan variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>699: Considering the common semantics for room_type_reserved variable, dummification would be the most adequate encoding.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>700: The variable type_of_meal_plan can be coded as ordinal without losing information.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>701: Feature generation based on variable room_type_reserved seems to be promising.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>702: Feature generation based on the use of variable arrival_year wouldn’t be useful, but the use of type_of_meal_plan seems to be promising.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>703: Given the usual semantics of type_of_meal_plan variable, dummification would have been a better codification.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>704: It is better to drop the variable repeated_guest than removing all records with missing values.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>705: Not knowing the semantics of arrival_year variable, dummification could have been a more adequate codification.</p>
    <img src="images/Hotel_Reservations_class_histogram.png" width="auto" height = "600"/> 
    <p>706: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Hotel_Reservations_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>707: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Hotel_Reservations_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>708: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Hotel_Reservations_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>709: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>710: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>711: The variable arrival_date can be seen as ordinal.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>712: The variable lead_time can be seen as ordinal without losing information.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>713: Variable arrival_date is balanced.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>714: It is clear that variable no_of_week_nights shows some outliers, but we can’t be sure of the same for variable lead_time.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>715: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>716: Variable no_of_adults shows some outlier values.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>717: Variable no_of_weekend_nights doesn’t have any outliers.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>718: Variable avg_price_per_room presents some outliers.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>719: At least 75 of the variables present outliers.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>720: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>721: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>722: Considering the common semantics for avg_price_per_room and no_of_adults variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>723: Considering the common semantics for no_of_children variable, dummification would be the most adequate encoding.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>724: The variable no_of_children can be coded as ordinal without losing information.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>725: Feature generation based on variable no_of_adults seems to be promising.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>726: Feature generation based on the use of variable no_of_adults wouldn’t be useful, but the use of no_of_children seems to be promising.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>727: Given the usual semantics of no_of_children variable, dummification would have been a better codification.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>728: It is better to drop the variable no_of_children than removing all records with missing values.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>729: Not knowing the semantics of no_of_special_requests variable, dummification could have been a more adequate codification.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>730: It is clear that variable self_esteem is one of the four most relevant features.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>731: The variable self_esteem seems to be one of the three most relevant features.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>732: The variable living_conditions discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>733: It is possible to state that headache is the second most discriminative variable regarding the class.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>734: Variable headache is one of the most relevant variables.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>735: Variable bullying seems to be relevant for the majority of mining tasks.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>736: Variables headache and depression seem to be useful for classification tasks.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>737: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>738: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>739: The precision for the presented tree is higher than 90%.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>740: The number of True Negatives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>741: The number of False Negatives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>742: The number of False Negatives reported in the same tree is 50.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>743: Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], it is possible to state that KNN algorithm classifies (A, not B) as 2 for any k ≤ 271.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>744: Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 1.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>745: Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], it is possible to state that KNN algorithm classifies (not A, not B) as 2 for any k ≤ 271.</p>
    <img src="images/StressLevelDataset_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>746: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/StressLevelDataset_overfitting_gb.png" width="auto" height = "600"/> 
    <p>747: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/StressLevelDataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>748: Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.</p>
    <img src="images/StressLevelDataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>749: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/StressLevelDataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>750: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/StressLevelDataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>751: KNN is in overfitting for k less than 17.</p>
    <img src="images/StressLevelDataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>752: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/StressLevelDataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>753: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/StressLevelDataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>754: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/StressLevelDataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>755: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/StressLevelDataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>756: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/StressLevelDataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>757: The decision tree is in overfitting for depths above 4.</p>
    <img src="images/StressLevelDataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>758: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/StressLevelDataset_pca.png" width="auto" height = "600"/> 
    <p>759: The first 9 principal components are enough for explaining half the data variance.</p>
    <img src="images/StressLevelDataset_pca.png" width="auto" height = "600"/> 
    <p>760: Using the first 8 principal components would imply an error between 15 and 30%.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>761: The intrinsic dimensionality of this dataset is 6.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>762: One of the variables depression or basic_needs can be discarded without losing information.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>763: The variable breathing_problem can be discarded without risking losing information.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>764: Variables bullying and study_load are redundant, but we can’t say the same for the pair breathing_problem and living_conditions.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>765: Variables headache and living_conditions are redundant.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>766: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>767: Variable living_conditions seems to be relevant for the majority of mining tasks.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>768: Variables sleep_quality and self_esteem seem to be useful for classification tasks.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>769: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>770: Removing variable self_esteem might improve the training of decision trees .</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>771: There is evidence in favour for sequential backward selection to select variable study_load previously than variable depression.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>772: Variable headache is balanced.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>773: Those boxplots show that the data is not normalized.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>774: It is clear that variable self_esteem shows some outliers, but we can’t be sure of the same for variable living_conditions.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>775: Outliers seem to be a problem in the dataset.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>776: Variable self_esteem shows a high number of outlier values.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>777: Variable bullying doesn’t have any outliers.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>778: Variable sleep_quality presents some outliers.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>779: At least 85 of the variables present outliers.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>780: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>781: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>782: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>783: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>784: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>785: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>786: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>787: The variable mental_health_history can be seen as ordinal.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>788: The variable mental_health_history can be seen as ordinal without losing information.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>789: Considering the common semantics for mental_health_history and <all_variables> variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>790: Considering the common semantics for mental_health_history variable, dummification would be the most adequate encoding.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>791: The variable mental_health_history can be coded as ordinal without losing information.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>792: Feature generation based on variable mental_health_history seems to be promising.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>793: Feature generation based on the use of variable mental_health_history wouldn’t be useful, but the use of <all_variables> seems to be promising.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>794: Given the usual semantics of mental_health_history variable, dummification would have been a better codification.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>795: It is better to drop the variable mental_health_history than removing all records with missing values.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>796: Not knowing the semantics of mental_health_history variable, dummification could have been a more adequate codification.</p>
    <img src="images/StressLevelDataset_class_histogram.png" width="auto" height = "600"/> 
    <p>797: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/StressLevelDataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>798: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/StressLevelDataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>799: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/StressLevelDataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>800: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>801: All variables, but the class, should be dealt with as date.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>802: The variable sleep_quality can be seen as ordinal.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>803: The variable sleep_quality can be seen as ordinal without losing information.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>804: Variable sleep_quality is balanced.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>805: It is clear that variable living_conditions shows some outliers, but we can’t be sure of the same for variable breathing_problem.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>806: Outliers seem to be a problem in the dataset.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>807: Variable basic_needs shows a high number of outlier values.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>808: Variable headache doesn’t have any outliers.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>809: Variable breathing_problem presents some outliers.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>810: At least 85 of the variables present outliers.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>811: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>812: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>813: Considering the common semantics for sleep_quality and anxiety_level variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>814: Considering the common semantics for study_load variable, dummification would be the most adequate encoding.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>815: The variable anxiety_level can be coded as ordinal without losing information.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>816: Feature generation based on variable living_conditions seems to be promising.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>817: Feature generation based on the use of variable breathing_problem wouldn’t be useful, but the use of anxiety_level seems to be promising.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>818: Given the usual semantics of self_esteem variable, dummification would have been a better codification.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>819: It is better to drop the variable bullying than removing all records with missing values.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>820: Not knowing the semantics of sleep_quality variable, dummification could have been a more adequate codification.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>821: It is clear that variable residual sugar is one of the four most relevant features.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>822: The variable pH seems to be one of the three most relevant features.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>823: The variable residual sugar discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>824: It is possible to state that alcohol is the second most discriminative variable regarding the class.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>825: Variable total sulfur dioxide is one of the most relevant variables.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>826: Variable sulphates seems to be relevant for the majority of mining tasks.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>827: Variables pH and sulphates seem to be useful for classification tasks.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>828: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>829: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>830: The specificity for the presented tree is lower than 90%.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>831: The number of False Positives reported in the same tree is 10.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>832: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>833: The variable free sulfur dioxide seems to be one of the five most relevant features.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>834: Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that KNN algorithm classifies (A, not B) as 8 for any k ≤ 154.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>835: Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that Naive Bayes algorithm classifies (not A, B), as 5.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>836: Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], the Decision Tree presented classifies (not A, not B) as 3.</p>
    <img src="images/WineQT_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>837: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/WineQT_overfitting_gb.png" width="auto" height = "600"/> 
    <p>838: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/WineQT_overfitting_rf.png" width="auto" height = "600"/> 
    <p>839: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/WineQT_overfitting_rf.png" width="auto" height = "600"/> 
    <p>840: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/WineQT_overfitting_rf.png" width="auto" height = "600"/> 
    <p>841: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/WineQT_overfitting_knn.png" width="auto" height = "600"/> 
    <p>842: KNN is in overfitting for k less than 13.</p>
    <img src="images/WineQT_overfitting_knn.png" width="auto" height = "600"/> 
    <p>843: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/WineQT_overfitting_knn.png" width="auto" height = "600"/> 
    <p>844: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/WineQT_overfitting_knn.png" width="auto" height = "600"/> 
    <p>845: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/WineQT_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>846: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/WineQT_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>847: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.</p>
    <img src="images/WineQT_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>848: The decision tree is in overfitting for depths above 3.</p>
    <img src="images/WineQT_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>849: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/WineQT_pca.png" width="auto" height = "600"/> 
    <p>850: The first 6 principal components are enough for explaining half the data variance.</p>
    <img src="images/WineQT_pca.png" width="auto" height = "600"/> 
    <p>851: Using the first 2 principal components would imply an error between 15 and 25%.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>852: The intrinsic dimensionality of this dataset is 5.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>853: One of the variables sulphates or free sulfur dioxide can be discarded without losing information.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>854: The variable density can be discarded without risking losing information.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>855: Variables fixed acidity and citric acid are redundant, but we can’t say the same for the pair free sulfur dioxide and density.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>856: Variables fixed acidity and free sulfur dioxide are redundant.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>857: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>858: Variable total sulfur dioxide seems to be relevant for the majority of mining tasks.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>859: Variables chlorides and volatile acidity seem to be useful for classification tasks.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>860: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>861: Removing variable fixed acidity might improve the training of decision trees .</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>862: There is evidence in favour for sequential backward selection to select variable residual sugar previously than variable free sulfur dioxide.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>863: Variable free sulfur dioxide is balanced.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>864: Those boxplots show that the data is not normalized.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>865: It is clear that variable free sulfur dioxide shows some outliers, but we can’t be sure of the same for variable citric acid.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>866: Outliers seem to be a problem in the dataset.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>867: Variable total sulfur dioxide shows some outlier values.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>868: Variable pH doesn’t have any outliers.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>869: Variable alcohol presents some outliers.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>870: At least 50 of the variables present outliers.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>871: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>872: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>873: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>874: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>875: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>876: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/WineQT_class_histogram.png" width="auto" height = "600"/> 
    <p>877: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/WineQT_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>878: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/WineQT_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>879: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/WineQT_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>880: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>881: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>882: The variable chlorides can be seen as ordinal.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>883: The variable citric acid can be seen as ordinal without losing information.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>884: Variable free sulfur dioxide is balanced.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>885: It is clear that variable residual sugar shows some outliers, but we can’t be sure of the same for variable alcohol.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>886: Outliers seem to be a problem in the dataset.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>887: Variable residual sugar shows some outlier values.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>888: Variable chlorides doesn’t have any outliers.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>889: Variable density presents some outliers.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>890: At least 50 of the variables present outliers.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>891: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>892: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>893: Considering the common semantics for total sulfur dioxide and fixed acidity variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>894: Considering the common semantics for sulphates variable, dummification would be the most adequate encoding.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>895: The variable pH can be coded as ordinal without losing information.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>896: Feature generation based on variable density seems to be promising.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>897: Feature generation based on the use of variable alcohol wouldn’t be useful, but the use of fixed acidity seems to be promising.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>898: Given the usual semantics of sulphates variable, dummification would have been a better codification.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>899: It is better to drop the variable volatile acidity than removing all records with missing values.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>900: Not knowing the semantics of density variable, dummification could have been a more adequate codification.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>901: It is clear that variable ApplicantIncome is one of the four most relevant features.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>902: The variable Loan_Amount_Term seems to be one of the five most relevant features.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>903: The variable ApplicantIncome discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>904: It is possible to state that Loan_Amount_Term is the first most discriminative variable regarding the class.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>905: Variable LoanAmount is one of the most relevant variables.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>906: Variable Loan_Amount_Term seems to be relevant for the majority of mining tasks.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>907: Variables LoanAmount and Loan_Amount_Term seem to be useful for classification tasks.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>908: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>909: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>910: The accuracy for the presented tree is lower than 60%.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>911: The number of False Positives reported in the same tree is 30.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>912: The number of False Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>913: The specificity for the presented tree is lower than 90%.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>914: Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that Naive Bayes algorithm classifies (not A, B), as Y.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>915: Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], the Decision Tree presented classifies (A,B) as N.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>916: Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that KNN algorithm classifies (not A, B) as N for any k ≤ 3.</p>
    <img src="images/loan_data_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>917: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/loan_data_overfitting_gb.png" width="auto" height = "600"/> 
    <p>918: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/loan_data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>919: Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.</p>
    <img src="images/loan_data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>920: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/loan_data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>921: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/loan_data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>922: KNN is in overfitting for k larger than 17.</p>
    <img src="images/loan_data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>923: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/loan_data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>924: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/loan_data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>925: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/loan_data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>926: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/loan_data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>927: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.</p>
    <img src="images/loan_data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>928: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/loan_data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>929: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/loan_data_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>930: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/loan_data_pca.png" width="auto" height = "600"/> 
    <p>931: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/loan_data_pca.png" width="auto" height = "600"/> 
    <p>932: Using the first 3 principal components would imply an error between 10 and 25%.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>933: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>934: One of the variables ApplicantIncome or LoanAmount can be discarded without losing information.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>935: The variable ApplicantIncome can be discarded without risking losing information.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>936: Variables Loan_Amount_Term and ApplicantIncome are redundant.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>937: Variables LoanAmount and CoapplicantIncome are redundant.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>938: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>939: Variable LoanAmount seems to be relevant for the majority of mining tasks.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>940: Variables ApplicantIncome and Loan_Amount_Term seem to be useful for classification tasks.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>941: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>942: Removing variable CoapplicantIncome might improve the training of decision trees .</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>943: There is evidence in favour for sequential backward selection to select variable ApplicantIncome previously than variable CoapplicantIncome.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>944: Variable CoapplicantIncome is balanced.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>945: Those boxplots show that the data is not normalized.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>946: It is clear that variable LoanAmount shows some outliers, but we can’t be sure of the same for variable Loan_Amount_Term.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>947: Outliers seem to be a problem in the dataset.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>948: Variable LoanAmount shows a high number of outlier values.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>949: Variable LoanAmount doesn’t have any outliers.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>950: Variable ApplicantIncome presents some outliers.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>951: At least 50 of the variables present outliers.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>952: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>953: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>954: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>955: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>956: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>957: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>958: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>959: The variable Gender can be seen as ordinal.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>960: The variable Gender can be seen as ordinal without losing information.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>961: Considering the common semantics for Married and Dependents variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>962: Considering the common semantics for Education variable, dummification would be the most adequate encoding.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>963: The variable Gender can be coded as ordinal without losing information.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>964: Feature generation based on variable Education seems to be promising.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>965: Feature generation based on the use of variable Credit_History wouldn’t be useful, but the use of Dependents seems to be promising.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>966: Given the usual semantics of Education variable, dummification would have been a better codification.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>967: It is better to drop the variable Married than removing all records with missing values.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>968: Not knowing the semantics of Property_Area variable, dummification could have been a more adequate codification.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>969: Discarding variable Dependents would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>970: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>971: Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>972: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>973: Feature generation based on variable Credit_History seems to be promising.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>974: It is better to drop the variable Gender than removing all records with missing values.</p>
    <img src="images/loan_data_class_histogram.png" width="auto" height = "600"/> 
    <p>975: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/loan_data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>976: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/loan_data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>977: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/loan_data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>978: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>979: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>980: The variable CoapplicantIncome can be seen as ordinal.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>981: The variable Loan_Amount_Term can be seen as ordinal without losing information.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>982: Variable CoapplicantIncome is balanced.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>983: It is clear that variable ApplicantIncome shows some outliers, but we can’t be sure of the same for variable Loan_Amount_Term.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>984: Outliers seem to be a problem in the dataset.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>985: Variable Loan_Amount_Term shows some outlier values.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>986: Variable ApplicantIncome doesn’t have any outliers.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>987: Variable Loan_Amount_Term presents some outliers.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>988: At least 60 of the variables present outliers.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>989: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>990: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>991: Considering the common semantics for Loan_Amount_Term and ApplicantIncome variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>992: Considering the common semantics for ApplicantIncome variable, dummification would be the most adequate encoding.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>993: The variable CoapplicantIncome can be coded as ordinal without losing information.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>994: Feature generation based on variable CoapplicantIncome seems to be promising.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>995: Feature generation based on the use of variable ApplicantIncome wouldn’t be useful, but the use of CoapplicantIncome seems to be promising.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>996: Given the usual semantics of Loan_Amount_Term variable, dummification would have been a better codification.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>997: It is better to drop the variable CoapplicantIncome than removing all records with missing values.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>998: Not knowing the semantics of Loan_Amount_Term variable, dummification could have been a more adequate codification.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>999: It is clear that variable ShapeFactor1 is one of the five most relevant features.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1000: The variable Extent seems to be one of the three most relevant features.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1001: The variable EquivDiameter discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1002: It is possible to state that ShapeFactor1 is the second most discriminative variable regarding the class.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1003: Variable AspectRation is one of the most relevant variables.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1004: Variable Perimeter seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1005: Variables Solidity and EquivDiameter seem to be useful for classification tasks.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1006: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1007: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1008: The accuracy for the presented tree is lower than 60%.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1009: The number of True Negatives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1010: The number of False Positives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1011: The precision for the presented tree is lower than 90%.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1012: Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], it is possible to state that KNN algorithm classifies (not A, not B) as SEKER for any k ≤ 2501.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1013: Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], it is possible to state that KNN algorithm classifies (not A, not B) as SEKER for any k ≤ 4982.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>1014: Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], the Decision Tree presented classifies (A,B) as HOROZ.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1015: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1016: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1017: Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1018: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1019: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1020: KNN is in overfitting for k larger than 5.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1021: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1022: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1023: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1024: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1025: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1026: The decision tree is in overfitting for depths above 10.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1027: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Dry_Bean_Dataset_pca.png" width="auto" height = "600"/> 
    <p>1028: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/Dry_Bean_Dataset_pca.png" width="auto" height = "600"/> 
    <p>1029: Using the first 4 principal components would imply an error between 5 and 25%.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1030: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1031: One of the variables Extent or Area can be discarded without losing information.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1032: The variable Solidity can be discarded without risking losing information.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1033: Variables roundness and Perimeter are redundant, but we can’t say the same for the pair MinorAxisLength and Eccentricity.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1034: Variables MinorAxisLength and Eccentricity are redundant.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1035: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1036: Variable Extent seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1037: Variables ShapeFactor1 and Area seem to be useful for classification tasks.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1038: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1039: Removing variable EquivDiameter might improve the training of decision trees .</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1040: There is evidence in favour for sequential backward selection to select variable Eccentricity previously than variable ShapeFactor1.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1041: Variable ShapeFactor1 is balanced.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1042: Those boxplots show that the data is not normalized.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1043: It is clear that variable Area shows some outliers, but we can’t be sure of the same for variable Perimeter.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1044: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1045: Variable AspectRation shows a high number of outlier values.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1046: Variable Extent doesn’t have any outliers.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1047: Variable Solidity presents some outliers.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1048: At least 50 of the variables present outliers.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1049: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1050: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1051: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1052: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1053: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>1054: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Dry_Bean_Dataset_class_histogram.png" width="auto" height = "600"/> 
    <p>1055: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Dry_Bean_Dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1056: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/Dry_Bean_Dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1057: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Dry_Bean_Dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1058: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1059: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1060: The variable Solidity can be seen as ordinal.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1061: The variable Area can be seen as ordinal without losing information.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1062: Variable Solidity is balanced.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1063: It is clear that variable MinorAxisLength shows some outliers, but we can’t be sure of the same for variable Solidity.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1064: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1065: Variable MinorAxisLength shows some outlier values.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1066: Variable MinorAxisLength doesn’t have any outliers.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1067: Variable Perimeter presents some outliers.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1068: At least 85 of the variables present outliers.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1069: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1070: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1071: Considering the common semantics for MinorAxisLength and Area variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1072: Considering the common semantics for Area variable, dummification would be the most adequate encoding.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1073: The variable ShapeFactor1 can be coded as ordinal without losing information.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1074: Feature generation based on variable AspectRation seems to be promising.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1075: Feature generation based on the use of variable Eccentricity wouldn’t be useful, but the use of Area seems to be promising.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1076: Given the usual semantics of Solidity variable, dummification would have been a better codification.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1077: It is better to drop the variable Area than removing all records with missing values.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1078: Not knowing the semantics of Area variable, dummification could have been a more adequate codification.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1079: It is clear that variable age is one of the five most relevant features.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1080: The variable installment_commitment seems to be one of the five most relevant features.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1081: The variable credit_amount discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1082: It is possible to state that age is the second most discriminative variable regarding the class.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1083: Variable duration is one of the most relevant variables.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1084: Variable age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1085: Variables credit_amount and age seem to be useful for classification tasks.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1086: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1087: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1088: The accuracy for the presented tree is higher than 60%.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1089: The number of True Negatives reported in the same tree is 50.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1090: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1091: The recall for the presented tree is higher than its accuracy.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1092: Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as good for any k ≤ 264.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1093: Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as good for any k ≤ 183.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1094: Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 146.</p>
    <img src="images/credit_customers_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1095: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/credit_customers_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1096: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/credit_customers_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1097: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/credit_customers_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1098: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/credit_customers_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1099: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/credit_customers_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1100: KNN is in overfitting for k less than 13.</p>
    <img src="images/credit_customers_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1101: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/credit_customers_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1102: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/credit_customers_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1103: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/credit_customers_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1104: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/credit_customers_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1105: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.</p>
    <img src="images/credit_customers_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1106: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/credit_customers_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1107: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/credit_customers_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1108: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/credit_customers_pca.png" width="auto" height = "600"/> 
    <p>1109: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/credit_customers_pca.png" width="auto" height = "600"/> 
    <p>1110: Using the first 3 principal components would imply an error between 5 and 25%.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1111: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1112: One of the variables age or existing_credits can be discarded without losing information.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1113: The variable existing_credits can be discarded without risking losing information.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1114: Variables residence_since and installment_commitment are redundant, but we can’t say the same for the pair credit_amount and age.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1115: Variables existing_credits and residence_since are redundant.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1116: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1117: Variable installment_commitment seems to be relevant for the majority of mining tasks.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1118: Variables installment_commitment and residence_since seem to be useful for classification tasks.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1119: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1120: Removing variable existing_credits might improve the training of decision trees .</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1121: There is evidence in favour for sequential backward selection to select variable installment_commitment previously than variable duration.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1122: Variable existing_credits is balanced.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1123: Those boxplots show that the data is not normalized.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1124: It is clear that variable duration shows some outliers, but we can’t be sure of the same for variable credit_amount.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1125: Outliers seem to be a problem in the dataset.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1126: Variable installment_commitment shows some outlier values.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1127: Variable residence_since doesn’t have any outliers.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1128: Variable residence_since presents some outliers.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1129: At least 75 of the variables present outliers.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1130: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1131: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1132: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1133: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1134: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1135: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1136: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1137: The variable other_payment_plans can be seen as ordinal.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1138: The variable num_dependents can be seen as ordinal without losing information.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1139: Considering the common semantics for housing and checking_status variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1140: Considering the common semantics for num_dependents variable, dummification would be the most adequate encoding.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1141: The variable foreign_worker can be coded as ordinal without losing information.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1142: Feature generation based on variable foreign_worker seems to be promising.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1143: Feature generation based on the use of variable employment wouldn’t be useful, but the use of checking_status seems to be promising.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1144: Given the usual semantics of foreign_worker variable, dummification would have been a better codification.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1145: It is better to drop the variable employment than removing all records with missing values.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1146: Not knowing the semantics of checking_status variable, dummification could have been a more adequate codification.</p>
    <img src="images/credit_customers_class_histogram.png" width="auto" height = "600"/> 
    <p>1147: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/credit_customers_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1148: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/credit_customers_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1149: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/credit_customers_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1150: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1151: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1152: The variable duration can be seen as ordinal.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1153: The variable duration can be seen as ordinal without losing information.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1154: Variable residence_since is balanced.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1155: It is clear that variable installment_commitment shows some outliers, but we can’t be sure of the same for variable residence_since.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1156: Outliers seem to be a problem in the dataset.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1157: Variable duration shows some outlier values.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1158: Variable age doesn’t have any outliers.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1159: Variable residence_since presents some outliers.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1160: At least 50 of the variables present outliers.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1161: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1162: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1163: Considering the common semantics for installment_commitment and duration variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1164: Considering the common semantics for credit_amount variable, dummification would be the most adequate encoding.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1165: The variable age can be coded as ordinal without losing information.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1166: Feature generation based on variable credit_amount seems to be promising.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1167: Feature generation based on the use of variable duration wouldn’t be useful, but the use of credit_amount seems to be promising.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1168: Given the usual semantics of credit_amount variable, dummification would have been a better codification.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1169: It is better to drop the variable installment_commitment than removing all records with missing values.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1170: Not knowing the semantics of existing_credits variable, dummification could have been a more adequate codification.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1171: It is clear that variable Cloud3pm is one of the three most relevant features.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1172: The variable Temp3pm seems to be one of the two most relevant features.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1173: The variable WindSpeed9am discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1174: It is possible to state that Cloud9am is the second most discriminative variable regarding the class.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1175: Variable Pressure3pm is one of the most relevant variables.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1176: Variable Cloud3pm seems to be relevant for the majority of mining tasks.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1177: Variables Cloud9am and WindSpeed9am seem to be useful for classification tasks.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1178: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1179: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1180: The specificity for the presented tree is higher than 75%.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1181: The number of False Negatives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1182: The number of False Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1183: The precision for the presented tree is lower than its recall.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1184: Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], it is possible to state that KNN algorithm classifies (A, not B) as No for any k ≤ 1686.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1185: Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], it is possible to state that KNN algorithm classifies (A, not B) as Yes for any k ≤ 1154.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1186: Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], the Decision Tree presented classifies (not A, B) as Yes.</p>
    <img src="images/weatherAUS_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1187: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/weatherAUS_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1188: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/weatherAUS_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1189: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/weatherAUS_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1190: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/weatherAUS_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1191: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/weatherAUS_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1192: KNN is in overfitting for k larger than 13.</p>
    <img src="images/weatherAUS_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1193: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/weatherAUS_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1194: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/weatherAUS_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1195: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/weatherAUS_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1196: According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.</p>
    <img src="images/weatherAUS_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1197: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.</p>
    <img src="images/weatherAUS_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1198: The decision tree is in overfitting for depths above 3.</p>
    <img src="images/weatherAUS_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1199: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/weatherAUS_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1200: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/weatherAUS_pca.png" width="auto" height = "600"/> 
    <p>1201: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/weatherAUS_pca.png" width="auto" height = "600"/> 
    <p>1202: Using the first 5 principal components would imply an error between 15 and 25%.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1203: The intrinsic dimensionality of this dataset is 6.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1204: One of the variables Cloud3pm or Pressure9am can be discarded without losing information.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1205: The variable Pressure3pm can be discarded without risking losing information.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1206: Variables Cloud9am and Temp3pm are redundant, but we can’t say the same for the pair Rainfall and Pressure3pm.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1207: Variables Rainfall and Cloud3pm are redundant.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1208: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1209: Variable WindSpeed9am seems to be relevant for the majority of mining tasks.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1210: Variables Cloud3pm and Rainfall seem to be useful for classification tasks.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1211: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1212: Removing variable Rainfall might improve the training of decision trees .</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1213: There is evidence in favour for sequential backward selection to select variable Temp3pm previously than variable Rainfall.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1214: Variable Pressure9am is balanced.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1215: Those boxplots show that the data is not normalized.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1216: It is clear that variable Cloud9am shows some outliers, but we can’t be sure of the same for variable Cloud3pm.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1217: Outliers seem to be a problem in the dataset.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1218: Variable Pressure3pm shows a high number of outlier values.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1219: Variable Temp3pm doesn’t have any outliers.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1220: Variable Pressure3pm presents some outliers.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1221: At least 85 of the variables present outliers.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1222: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1223: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1224: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1225: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1226: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1227: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1228: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1229: The variable RainToday can be seen as ordinal.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1230: The variable WindDir3pm can be seen as ordinal without losing information.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1231: Considering the common semantics for WindDir3pm and Location variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1232: Considering the common semantics for WindDir9am variable, dummification would be the most adequate encoding.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1233: The variable RainToday can be coded as ordinal without losing information.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1234: Feature generation based on variable Location seems to be promising.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1235: Feature generation based on the use of variable WindGustDir wouldn’t be useful, but the use of Location seems to be promising.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1236: Given the usual semantics of WindDir9am variable, dummification would have been a better codification.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1237: It is better to drop the variable WindDir9am than removing all records with missing values.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1238: Not knowing the semantics of WindDir9am variable, dummification could have been a more adequate codification.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1239: Discarding variable RainToday would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1240: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1241: Dropping all rows with missing values can lead to a dataset with less than 40% of the original data.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1242: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1243: Feature generation based on variable RainToday seems to be promising.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1244: It is better to drop the variable Pressure9am than removing all records with missing values.</p>
    <img src="images/weatherAUS_class_histogram.png" width="auto" height = "600"/> 
    <p>1245: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/weatherAUS_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1246: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/weatherAUS_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1247: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/weatherAUS_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1248: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1249: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1250: The variable Pressure3pm can be seen as ordinal.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1251: The variable Pressure3pm can be seen as ordinal without losing information.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1252: Variable WindSpeed9am is balanced.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1253: It is clear that variable Rainfall shows some outliers, but we can’t be sure of the same for variable Pressure3pm.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1254: Outliers seem to be a problem in the dataset.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1255: Variable Pressure9am shows a high number of outlier values.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1256: Variable Rainfall doesn’t have any outliers.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1257: Variable Cloud9am presents some outliers.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1258: At least 85 of the variables present outliers.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1259: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1260: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1261: Considering the common semantics for Rainfall and WindSpeed9am variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1262: Considering the common semantics for WindSpeed9am variable, dummification would be the most adequate encoding.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1263: The variable Pressure3pm can be coded as ordinal without losing information.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1264: Feature generation based on variable Rainfall seems to be promising.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1265: Feature generation based on the use of variable Pressure3pm wouldn’t be useful, but the use of Rainfall seems to be promising.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1266: Given the usual semantics of Temp3pm variable, dummification would have been a better codification.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1267: It is better to drop the variable Pressure9am than removing all records with missing values.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1268: Not knowing the semantics of Pressure3pm variable, dummification could have been a more adequate codification.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1269: It is clear that variable length is one of the three most relevant features.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1270: The variable age_of_car seems to be one of the three most relevant features.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1271: The variable displacement discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1272: It is possible to state that width is the first most discriminative variable regarding the class.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1273: Variable gross_weight is one of the most relevant variables.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1274: Variable airbags seems to be relevant for the majority of mining tasks.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1275: Variables length and age_of_car seem to be useful for classification tasks.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1276: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1277: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1278: The recall for the presented tree is lower than 90%.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1279: The number of False Negatives reported in the same tree is 10.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1280: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1281: The specificity for the presented tree is lower than its accuracy.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1282: Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 3813.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1283: Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that KNN algorithm classifies (not A, not B) as 1 for any k ≤ 3813.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1284: Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 1.</p>
    <img src="images/car_insurance_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1285: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/car_insurance_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1286: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/car_insurance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1287: Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.</p>
    <img src="images/car_insurance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1288: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/car_insurance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1289: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/car_insurance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1290: KNN is in overfitting for k larger than 17.</p>
    <img src="images/car_insurance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1291: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/car_insurance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1292: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/car_insurance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1293: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/car_insurance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1294: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/car_insurance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1295: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/car_insurance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1296: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/car_insurance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1297: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/car_insurance_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1298: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/car_insurance_pca.png" width="auto" height = "600"/> 
    <p>1299: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/car_insurance_pca.png" width="auto" height = "600"/> 
    <p>1300: Using the first 7 principal components would imply an error between 5 and 25%.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1301: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1302: One of the variables age_of_car or width can be discarded without losing information.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1303: The variable age_of_policyholder can be discarded without risking losing information.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1304: Variables gross_weight and length are redundant, but we can’t say the same for the pair policy_tenure and displacement.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1305: Variables policy_tenure and age_of_car are redundant.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1306: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1307: Variable age_of_car seems to be relevant for the majority of mining tasks.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1308: Variables policy_tenure and age_of_car seem to be useful for classification tasks.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1309: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1310: Removing variable width might improve the training of decision trees .</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1311: There is evidence in favour for sequential backward selection to select variable age_of_policyholder previously than variable height.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1312: Variable width is balanced.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1313: Those boxplots show that the data is not normalized.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1314: It is clear that variable age_of_policyholder shows some outliers, but we can’t be sure of the same for variable height.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1315: Outliers seem to be a problem in the dataset.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1316: Variable policy_tenure shows some outlier values.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1317: Variable displacement doesn’t have any outliers.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1318: Variable policy_tenure presents some outliers.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1319: At least 85 of the variables present outliers.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1320: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1321: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1322: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1323: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1324: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1325: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1326: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1327: The variable is_esc can be seen as ordinal.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1328: The variable fuel_type can be seen as ordinal without losing information.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1329: Considering the common semantics for max_power and area_cluster variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1330: Considering the common semantics for max_torque variable, dummification would be the most adequate encoding.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1331: The variable model can be coded as ordinal without losing information.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1332: Feature generation based on variable steering_type seems to be promising.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1333: Feature generation based on the use of variable fuel_type wouldn’t be useful, but the use of area_cluster seems to be promising.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1334: Given the usual semantics of model variable, dummification would have been a better codification.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1335: It is better to drop the variable steering_type than removing all records with missing values.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1336: Not knowing the semantics of max_power variable, dummification could have been a more adequate codification.</p>
    <img src="images/car_insurance_class_histogram.png" width="auto" height = "600"/> 
    <p>1337: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/car_insurance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1338: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/car_insurance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1339: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/car_insurance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1340: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1341: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1342: The variable age_of_policyholder can be seen as ordinal.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1343: The variable width can be seen as ordinal without losing information.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1344: Variable age_of_policyholder is balanced.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1345: It is clear that variable policy_tenure shows some outliers, but we can’t be sure of the same for variable height.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1346: Outliers seem to be a problem in the dataset.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1347: Variable age_of_car shows some outlier values.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1348: Variable airbags doesn’t have any outliers.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1349: Variable gross_weight presents some outliers.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1350: At least 85 of the variables present outliers.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1351: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1352: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1353: Considering the common semantics for width and policy_tenure variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1354: Considering the common semantics for gross_weight variable, dummification would be the most adequate encoding.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1355: The variable length can be coded as ordinal without losing information.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1356: Feature generation based on variable width seems to be promising.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1357: Feature generation based on the use of variable gross_weight wouldn’t be useful, but the use of policy_tenure seems to be promising.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1358: Given the usual semantics of width variable, dummification would have been a better codification.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1359: It is better to drop the variable height than removing all records with missing values.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1360: Not knowing the semantics of policy_tenure variable, dummification could have been a more adequate codification.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1361: It is clear that variable thal is one of the four most relevant features.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1362: The variable thal seems to be one of the four most relevant features.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1363: The variable trestbps discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1364: It is possible to state that thal is the second most discriminative variable regarding the class.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1365: Variable oldpeak is one of the most relevant variables.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1366: Variable restecg seems to be relevant for the majority of mining tasks.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1367: Variables slope and chol seem to be useful for classification tasks.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1368: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1369: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1370: The precision for the presented tree is lower than 60%.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1371: The number of True Negatives reported in the same tree is 50.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1372: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1373: The recall for the presented tree is higher than its specificity.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1374: Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1375: Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 202.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1376: Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="images/heart_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1377: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/heart_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1378: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/heart_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1379: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/heart_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1380: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/heart_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1381: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/heart_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1382: KNN is in overfitting for k less than 5.</p>
    <img src="images/heart_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1383: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/heart_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1384: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/heart_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1385: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/heart_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1386: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/heart_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1387: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.</p>
    <img src="images/heart_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1388: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/heart_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1389: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/heart_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1390: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/heart_pca.png" width="auto" height = "600"/> 
    <p>1391: The first 5 principal components are enough for explaining half the data variance.</p>
    <img src="images/heart_pca.png" width="auto" height = "600"/> 
    <p>1392: Using the first 6 principal components would imply an error between 10 and 30%.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1393: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1394: One of the variables restecg or thalach can be discarded without losing information.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1395: The variable trestbps can be discarded without risking losing information.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1396: Variables thalach and slope are redundant.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1397: Variables restecg and thal are redundant.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1398: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1399: Variable thalach seems to be relevant for the majority of mining tasks.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1400: Variables slope and age seem to be useful for classification tasks.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1401: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1402: Removing variable trestbps might improve the training of decision trees .</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1403: There is evidence in favour for sequential backward selection to select variable cp previously than variable ca.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1404: Variable ca is balanced.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1405: Those boxplots show that the data is not normalized.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1406: It is clear that variable ca shows some outliers, but we can’t be sure of the same for variable restecg.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1407: Outliers seem to be a problem in the dataset.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1408: Variable restecg shows a high number of outlier values.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1409: Variable thal doesn’t have any outliers.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1410: Variable ca presents some outliers.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1411: At least 85 of the variables present outliers.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1412: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1413: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1414: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1415: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1416: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1417: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1418: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1419: The variable fbs can be seen as ordinal.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1420: The variable exang can be seen as ordinal without losing information.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1421: Considering the common semantics for fbs and sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1422: Considering the common semantics for exang variable, dummification would be the most adequate encoding.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1423: The variable fbs can be coded as ordinal without losing information.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1424: Feature generation based on variable exang seems to be promising.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1425: Feature generation based on the use of variable sex wouldn’t be useful, but the use of fbs seems to be promising.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1426: Given the usual semantics of sex variable, dummification would have been a better codification.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1427: It is better to drop the variable sex than removing all records with missing values.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1428: Not knowing the semantics of sex variable, dummification could have been a more adequate codification.</p>
    <img src="images/heart_class_histogram.png" width="auto" height = "600"/> 
    <p>1429: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/heart_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1430: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/heart_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1431: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/heart_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1432: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1433: All variables, but the class, should be dealt with as date.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1434: The variable cp can be seen as ordinal.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1435: The variable thalach can be seen as ordinal without losing information.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1436: Variable thalach is balanced.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1437: It is clear that variable oldpeak shows some outliers, but we can’t be sure of the same for variable age.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1438: Outliers seem to be a problem in the dataset.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1439: Variable oldpeak shows some outlier values.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1440: Variable chol doesn’t have any outliers.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1441: Variable thalach presents some outliers.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1442: At least 85 of the variables present outliers.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1443: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1444: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1445: Considering the common semantics for age and cp variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1446: Considering the common semantics for age variable, dummification would be the most adequate encoding.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1447: The variable trestbps can be coded as ordinal without losing information.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1448: Feature generation based on variable restecg seems to be promising.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1449: Feature generation based on the use of variable trestbps wouldn’t be useful, but the use of age seems to be promising.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1450: Given the usual semantics of chol variable, dummification would have been a better codification.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1451: It is better to drop the variable oldpeak than removing all records with missing values.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1452: Not knowing the semantics of thal variable, dummification could have been a more adequate codification.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1453: It is clear that variable smoothness_se is one of the five most relevant features.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1454: The variable radius_worst seems to be one of the two most relevant features.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1455: The variable radius_worst discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1456: It is possible to state that texture_worst is the second most discriminative variable regarding the class.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1457: Variable perimeter_worst is one of the most relevant variables.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1458: Variable texture_worst seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1459: Variables area_se and perimeter_worst seem to be useful for classification tasks.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1460: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1461: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1462: The specificity for the presented tree is lower than 60%.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1463: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1464: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1465: The number of False Positives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1466: Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that KNN algorithm classifies (A,B) as B for any k ≤ 20.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1467: Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that KNN algorithm classifies (A,B) as B for any k ≤ 20.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1468: Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], the Decision Tree presented classifies (not A, B) as M.</p>
    <img src="images/Breast_Cancer_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1469: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Breast_Cancer_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1470: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/Breast_Cancer_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1471: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/Breast_Cancer_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1472: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Breast_Cancer_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1473: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/Breast_Cancer_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1474: KNN is in overfitting for k less than 17.</p>
    <img src="images/Breast_Cancer_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1475: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/Breast_Cancer_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1476: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/Breast_Cancer_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1477: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/Breast_Cancer_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1478: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/Breast_Cancer_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1479: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.</p>
    <img src="images/Breast_Cancer_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1480: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/Breast_Cancer_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1481: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/Breast_Cancer_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1482: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Breast_Cancer_pca.png" width="auto" height = "600"/> 
    <p>1483: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/Breast_Cancer_pca.png" width="auto" height = "600"/> 
    <p>1484: Using the first 9 principal components would imply an error between 10 and 20%.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1485: The intrinsic dimensionality of this dataset is 6.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1486: One of the variables perimeter_mean or symmetry_se can be discarded without losing information.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1487: The variable perimeter_worst can be discarded without risking losing information.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1488: Variables radius_worst and symmetry_se are redundant, but we can’t say the same for the pair perimeter_worst and perimeter_se.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1489: Variables texture_mean and texture_worst are redundant.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1490: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1491: Variable perimeter_mean seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1492: Variables perimeter_worst and perimeter_se seem to be useful for classification tasks.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1493: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1494: Removing variable texture_se might improve the training of decision trees .</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1495: There is evidence in favour for sequential backward selection to select variable radius_worst previously than variable area_se.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1496: Variable radius_worst is balanced.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1497: Those boxplots show that the data is not normalized.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1498: It is clear that variable radius_worst shows some outliers, but we can’t be sure of the same for variable perimeter_mean.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1499: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1500: Variable texture_mean shows a high number of outlier values.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1501: Variable symmetry_se doesn’t have any outliers.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1502: Variable perimeter_se presents some outliers.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1503: At least 75 of the variables present outliers.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1504: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1505: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1506: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1507: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1508: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1509: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Breast_Cancer_class_histogram.png" width="auto" height = "600"/> 
    <p>1510: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Breast_Cancer_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1511: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/Breast_Cancer_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1512: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Breast_Cancer_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1513: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1514: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1515: The variable perimeter_worst can be seen as ordinal.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1516: The variable texture_se can be seen as ordinal without losing information.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1517: Variable perimeter_se is balanced.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1518: It is clear that variable texture_worst shows some outliers, but we can’t be sure of the same for variable symmetry_se.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1519: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1520: Variable perimeter_se shows a high number of outlier values.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1521: Variable perimeter_worst doesn’t have any outliers.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1522: Variable texture_worst presents some outliers.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1523: At least 85 of the variables present outliers.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1524: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1525: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1526: Considering the common semantics for radius_worst and texture_mean variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1527: Considering the common semantics for area_se variable, dummification would be the most adequate encoding.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1528: The variable perimeter_se can be coded as ordinal without losing information.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1529: Feature generation based on variable radius_worst seems to be promising.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1530: Feature generation based on the use of variable texture_worst wouldn’t be useful, but the use of texture_mean seems to be promising.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1531: Given the usual semantics of perimeter_worst variable, dummification would have been a better codification.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1532: It is better to drop the variable perimeter_se than removing all records with missing values.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1533: Not knowing the semantics of perimeter_mean variable, dummification could have been a more adequate codification.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1534: It is clear that variable Customer_care_calls is one of the two most relevant features.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1535: The variable Customer_rating seems to be one of the three most relevant features.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1536: The variable Prior_purchases discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1537: It is possible to state that Discount_offered is the first most discriminative variable regarding the class.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1538: Variable Discount_offered is one of the most relevant variables.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1539: Variable Discount_offered seems to be relevant for the majority of mining tasks.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1540: Variables Cost_of_the_Product and Customer_rating seem to be useful for classification tasks.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1541: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1542: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1543: The specificity for the presented tree is higher than 90%.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1544: The number of True Negatives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1545: The number of True Positives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1546: The number of True Negatives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1547: Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as No.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1548: Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (A,B) as No for any k ≤ 906.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1549: Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as No.</p>
    <img src="images/e-commerce_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1550: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/e-commerce_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1551: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/e-commerce_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1552: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/e-commerce_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1553: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/e-commerce_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1554: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/e-commerce_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1555: KNN is in overfitting for k less than 5.</p>
    <img src="images/e-commerce_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1556: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/e-commerce_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1557: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/e-commerce_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1558: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/e-commerce_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1559: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/e-commerce_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1560: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/e-commerce_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1561: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/e-commerce_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1562: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/e-commerce_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1563: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/e-commerce_pca.png" width="auto" height = "600"/> 
    <p>1564: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/e-commerce_pca.png" width="auto" height = "600"/> 
    <p>1565: Using the first 4 principal components would imply an error between 15 and 25%.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1566: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1567: One of the variables Prior_purchases or Cost_of_the_Product can be discarded without losing information.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1568: The variable Weight_in_gms can be discarded without risking losing information.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1569: Variables Customer_care_calls and Prior_purchases are redundant, but we can’t say the same for the pair Cost_of_the_Product and Customer_rating.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1570: Variables Customer_rating and Cost_of_the_Product are redundant.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1571: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1572: Variable Customer_rating seems to be relevant for the majority of mining tasks.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1573: Variables Cost_of_the_Product and Prior_purchases seem to be useful for classification tasks.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1574: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1575: Removing variable Customer_care_calls might improve the training of decision trees .</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1576: There is evidence in favour for sequential backward selection to select variable Customer_care_calls previously than variable Weight_in_gms.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1577: Variable Customer_rating is balanced.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1578: Those boxplots show that the data is not normalized.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1579: It is clear that variable Cost_of_the_Product shows some outliers, but we can’t be sure of the same for variable Customer_rating.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1580: Outliers seem to be a problem in the dataset.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1581: Variable Weight_in_gms shows some outlier values.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1582: Variable Customer_care_calls doesn’t have any outliers.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1583: Variable Weight_in_gms presents some outliers.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1584: At least 50 of the variables present outliers.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1585: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1586: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1587: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1588: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1589: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1590: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1591: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1592: The variable Gender can be seen as ordinal.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1593: The variable Warehouse_block can be seen as ordinal without losing information.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1594: Considering the common semantics for Mode_of_Shipment and Warehouse_block variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1595: Considering the common semantics for Gender variable, dummification would be the most adequate encoding.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1596: The variable Product_importance can be coded as ordinal without losing information.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1597: Feature generation based on variable Gender seems to be promising.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1598: Feature generation based on the use of variable Mode_of_Shipment wouldn’t be useful, but the use of Warehouse_block seems to be promising.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1599: Given the usual semantics of Gender variable, dummification would have been a better codification.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1600: It is better to drop the variable Product_importance than removing all records with missing values.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1601: Not knowing the semantics of Mode_of_Shipment variable, dummification could have been a more adequate codification.</p>
    <img src="images/e-commerce_class_histogram.png" width="auto" height = "600"/> 
    <p>1602: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/e-commerce_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1603: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/e-commerce_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1604: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/e-commerce_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1605: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1606: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1607: The variable Weight_in_gms can be seen as ordinal.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1608: The variable Prior_purchases can be seen as ordinal without losing information.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1609: Variable Prior_purchases is balanced.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1610: It is clear that variable Prior_purchases shows some outliers, but we can’t be sure of the same for variable Customer_rating.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1611: Outliers seem to be a problem in the dataset.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1612: Variable Discount_offered shows some outlier values.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1613: Variable Weight_in_gms doesn’t have any outliers.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1614: Variable Customer_rating presents some outliers.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1615: At least 75 of the variables present outliers.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1616: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1617: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1618: Considering the common semantics for Customer_care_calls and Customer_rating variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1619: Considering the common semantics for Discount_offered variable, dummification would be the most adequate encoding.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1620: The variable Prior_purchases can be coded as ordinal without losing information.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1621: Feature generation based on variable Weight_in_gms seems to be promising.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1622: Feature generation based on the use of variable Discount_offered wouldn’t be useful, but the use of Customer_care_calls seems to be promising.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1623: Given the usual semantics of Discount_offered variable, dummification would have been a better codification.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1624: It is better to drop the variable Customer_rating than removing all records with missing values.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1625: Not knowing the semantics of Discount_offered variable, dummification could have been a more adequate codification.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1626: It is clear that variable Torque [Nm] is one of the two most relevant features.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1627: The variable Air temperature [K] seems to be one of the five most relevant features.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1628: The variable Tool wear [min] discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1629: It is possible to state that Process temperature [K] is the first most discriminative variable regarding the class.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1630: Variable Tool wear [min] is one of the most relevant variables.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1631: Variable Process temperature [K] seems to be relevant for the majority of mining tasks.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1632: Variables Tool wear [min] and Rotational speed [rpm] seem to be useful for classification tasks.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1633: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1634: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1635: The accuracy for the presented tree is higher than 60%.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1636: The number of False Positives reported in the same tree is 50.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1637: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1638: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1639: Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], the Decision Tree presented classifies (A, not B) as 0.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1640: Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 943.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1641: Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 5990.</p>
    <img src="images/maintenance_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1642: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/maintenance_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1643: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/maintenance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1644: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/maintenance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1645: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/maintenance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1646: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/maintenance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1647: KNN is in overfitting for k larger than 17.</p>
    <img src="images/maintenance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1648: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/maintenance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1649: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/maintenance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1650: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/maintenance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1651: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/maintenance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1652: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.</p>
    <img src="images/maintenance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1653: The decision tree is in overfitting for depths above 5.</p>
    <img src="images/maintenance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1654: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/maintenance_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1655: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/maintenance_pca.png" width="auto" height = "600"/> 
    <p>1656: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/maintenance_pca.png" width="auto" height = "600"/> 
    <p>1657: Using the first 2 principal components would imply an error between 10 and 30%.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1658: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1659: One of the variables Rotational speed [rpm] or Torque [Nm] can be discarded without losing information.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1660: The variable Process temperature [K] can be discarded without risking losing information.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1661: Variables Air temperature [K] and Tool wear [min] are redundant.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1662: Variables Rotational speed [rpm] and Torque [Nm] are redundant.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1663: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1664: Variable Rotational speed [rpm] seems to be relevant for the majority of mining tasks.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1665: Variables Rotational speed [rpm] and Process temperature [K] seem to be useful for classification tasks.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1666: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1667: Removing variable Process temperature [K] might improve the training of decision trees .</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1668: There is evidence in favour for sequential backward selection to select variable Rotational speed [rpm] previously than variable Tool wear [min].</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1669: Variable Torque [Nm] is balanced.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1670: Those boxplots show that the data is not normalized.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1671: It is clear that variable Rotational speed [rpm] shows some outliers, but we can’t be sure of the same for variable Process temperature [K].</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1672: Outliers seem to be a problem in the dataset.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1673: Variable Process temperature [K] shows some outlier values.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1674: Variable Torque [Nm] doesn’t have any outliers.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1675: Variable Process temperature [K] presents some outliers.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1676: At least 60 of the variables present outliers.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1677: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1678: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1679: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1680: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1681: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1682: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1683: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1684: The variable PWF can be seen as ordinal.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1685: The variable Type can be seen as ordinal without losing information.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1686: Considering the common semantics for Type and TWF variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1687: Considering the common semantics for Type variable, dummification would be the most adequate encoding.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1688: The variable Type can be coded as ordinal without losing information.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1689: Feature generation based on variable TWF seems to be promising.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1690: Feature generation based on the use of variable OSF wouldn’t be useful, but the use of Type seems to be promising.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1691: Given the usual semantics of RNF variable, dummification would have been a better codification.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1692: It is better to drop the variable TWF than removing all records with missing values.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1693: Not knowing the semantics of PWF variable, dummification could have been a more adequate codification.</p>
    <img src="images/maintenance_class_histogram.png" width="auto" height = "600"/> 
    <p>1694: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/maintenance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1695: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/maintenance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1696: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/maintenance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1697: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1698: All variables, but the class, should be dealt with as date.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1699: The variable Process temperature [K] can be seen as ordinal.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1700: The variable Air temperature [K] can be seen as ordinal without losing information.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1701: Variable Air temperature [K] is balanced.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1702: It is clear that variable Rotational speed [rpm] shows some outliers, but we can’t be sure of the same for variable Torque [Nm].</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1703: Outliers seem to be a problem in the dataset.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1704: Variable Tool wear [min] shows some outlier values.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1705: Variable Rotational speed [rpm] doesn’t have any outliers.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1706: Variable Tool wear [min] presents some outliers.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1707: At least 75 of the variables present outliers.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1708: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1709: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1710: Considering the common semantics for Tool wear [min] and Air temperature [K] variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1711: Considering the common semantics for Air temperature [K] variable, dummification would be the most adequate encoding.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1712: The variable Tool wear [min] can be coded as ordinal without losing information.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1713: Feature generation based on variable Rotational speed [rpm] seems to be promising.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1714: Feature generation based on the use of variable Air temperature [K] wouldn’t be useful, but the use of Process temperature [K] seems to be promising.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1715: Given the usual semantics of Rotational speed [rpm] variable, dummification would have been a better codification.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1716: It is better to drop the variable Rotational speed [rpm] than removing all records with missing values.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1717: Not knowing the semantics of Rotational speed [rpm] variable, dummification could have been a more adequate codification.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1718: It is clear that variable Tenure is one of the two most relevant features.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1719: The variable EstimatedSalary seems to be one of the three most relevant features.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1720: The variable Balance discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1721: It is possible to state that NumOfProducts is the first most discriminative variable regarding the class.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1722: Variable Tenure is one of the most relevant variables.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1723: Variable CreditScore seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1724: Variables CreditScore and EstimatedSalary seem to be useful for classification tasks.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1725: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1726: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1727: The specificity for the presented tree is lower than 90%.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1728: The number of True Negatives reported in the same tree is 10.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1729: The number of False Positives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1730: The variable CreditScore seems to be one of the two most relevant features.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1731: Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 0.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1732: Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that KNN algorithm classifies (not A, not B) as 1 for any k ≤ 114.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1733: Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 1931.</p>
    <img src="images/Churn_Modelling_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1734: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/Churn_Modelling_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1735: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Churn_Modelling_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1736: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/Churn_Modelling_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1737: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Churn_Modelling_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1738: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/Churn_Modelling_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1739: KNN is in overfitting for k less than 13.</p>
    <img src="images/Churn_Modelling_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1740: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/Churn_Modelling_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1741: KNN with less than 15 neighbours is in overfitting.</p>
    <img src="images/Churn_Modelling_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1742: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/Churn_Modelling_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1743: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/Churn_Modelling_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1744: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/Churn_Modelling_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1745: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/Churn_Modelling_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1746: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Churn_Modelling_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1747: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Churn_Modelling_pca.png" width="auto" height = "600"/> 
    <p>1748: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/Churn_Modelling_pca.png" width="auto" height = "600"/> 
    <p>1749: Using the first 4 principal components would imply an error between 15 and 30%.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1750: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1751: One of the variables Balance or EstimatedSalary can be discarded without losing information.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1752: The variable Tenure can be discarded without risking losing information.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1753: Variables EstimatedSalary and Age are redundant, but we can’t say the same for the pair Balance and NumOfProducts.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1754: Variables Age and NumOfProducts are redundant.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1755: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1756: Variable Tenure seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1757: Variables EstimatedSalary and Age seem to be useful for classification tasks.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1758: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1759: Removing variable Age might improve the training of decision trees .</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1760: There is evidence in favour for sequential backward selection to select variable Balance previously than variable Tenure.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1761: Variable Balance is balanced.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1762: Those boxplots show that the data is not normalized.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1763: It is clear that variable Balance shows some outliers, but we can’t be sure of the same for variable EstimatedSalary.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1764: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1765: Variable EstimatedSalary shows a high number of outlier values.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1766: Variable Tenure doesn’t have any outliers.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1767: Variable Balance presents some outliers.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1768: At least 60 of the variables present outliers.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1769: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1770: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1771: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1772: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1773: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1774: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1775: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1776: The variable IsActiveMember can be seen as ordinal.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1777: The variable IsActiveMember can be seen as ordinal without losing information.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1778: Considering the common semantics for Geography and Gender variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1779: Considering the common semantics for IsActiveMember variable, dummification would be the most adequate encoding.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1780: The variable Geography can be coded as ordinal without losing information.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1781: Feature generation based on variable Gender seems to be promising.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1782: Feature generation based on the use of variable HasCrCard wouldn’t be useful, but the use of Geography seems to be promising.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1783: Given the usual semantics of HasCrCard variable, dummification would have been a better codification.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1784: It is better to drop the variable IsActiveMember than removing all records with missing values.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1785: Not knowing the semantics of Geography variable, dummification could have been a more adequate codification.</p>
    <img src="images/Churn_Modelling_class_histogram.png" width="auto" height = "600"/> 
    <p>1786: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Churn_Modelling_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1787: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/Churn_Modelling_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1788: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Churn_Modelling_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1789: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1790: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1791: The variable Tenure can be seen as ordinal.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1792: The variable EstimatedSalary can be seen as ordinal without losing information.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1793: Variable EstimatedSalary is balanced.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1794: It is clear that variable Balance shows some outliers, but we can’t be sure of the same for variable CreditScore.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1795: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1796: Variable NumOfProducts shows some outlier values.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1797: Variable Balance doesn’t have any outliers.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1798: Variable Age presents some outliers.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1799: At least 50 of the variables present outliers.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1800: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1801: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1802: Considering the common semantics for Tenure and CreditScore variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1803: Considering the common semantics for Balance variable, dummification would be the most adequate encoding.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1804: The variable NumOfProducts can be coded as ordinal without losing information.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1805: Feature generation based on variable Balance seems to be promising.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1806: Feature generation based on the use of variable Balance wouldn’t be useful, but the use of CreditScore seems to be promising.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1807: Given the usual semantics of Age variable, dummification would have been a better codification.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1808: It is better to drop the variable EstimatedSalary than removing all records with missing values.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1809: Not knowing the semantics of Tenure variable, dummification could have been a more adequate codification.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1810: It is clear that variable MINORVARIANCE is one of the three most relevant features.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1811: The variable MINORKURTOSIS seems to be one of the four most relevant features.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1812: The variable DISTANCE CIRCULARITY discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1813: It is possible to state that CIRCULARITY is the second most discriminative variable regarding the class.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1814: Variable DISTANCE CIRCULARITY is one of the most relevant variables.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1815: Variable GYRATIONRADIUS seems to be relevant for the majority of mining tasks.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1816: Variables MAJORSKEWNESS and GYRATIONRADIUS seem to be useful for classification tasks.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1817: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1818: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1819: The recall for the presented tree is higher than 60%.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1820: The number of False Positives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1821: The number of True Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1822: The variable MAJORVARIANCE seems to be one of the four most relevant features.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1823: Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], the Decision Tree presented classifies (A,B) as 3.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1824: Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], the Decision Tree presented classifies (A, not B) as 4.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1825: Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that KNN algorithm classifies (not A, B) as 2 for any k ≤ 1.</p>
    <img src="images/vehicle_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1826: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/vehicle_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1827: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/vehicle_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1828: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/vehicle_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1829: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/vehicle_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1830: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/vehicle_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1831: KNN is in overfitting for k larger than 13.</p>
    <img src="images/vehicle_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1832: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/vehicle_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1833: KNN with more than 7 neighbours is in overfitting.</p>
    <img src="images/vehicle_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1834: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/vehicle_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1835: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/vehicle_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1836: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/vehicle_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1837: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/vehicle_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1838: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/vehicle_pca.png" width="auto" height = "600"/> 
    <p>1839: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/vehicle_pca.png" width="auto" height = "600"/> 
    <p>1840: Using the first 8 principal components would imply an error between 5 and 20%.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1841: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1842: One of the variables RADIUS RATIO or DISTANCE CIRCULARITY can be discarded without losing information.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1843: The variable MINORKURTOSIS can be discarded without risking losing information.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1844: Variables MINORVARIANCE and MAJORKURTOSIS are redundant, but we can’t say the same for the pair MAJORVARIANCE and CIRCULARITY.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1845: Variables GYRATIONRADIUS and DISTANCE CIRCULARITY are redundant.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1846: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1847: Variable RADIUS RATIO seems to be relevant for the majority of mining tasks.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1848: Variables DISTANCE CIRCULARITY and MINORKURTOSIS seem to be useful for classification tasks.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1849: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1850: Removing variable RADIUS RATIO might improve the training of decision trees .</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1851: There is evidence in favour for sequential backward selection to select variable CIRCULARITY previously than variable COMPACTNESS.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1852: Variable MAJORSKEWNESS is balanced.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1853: Those boxplots show that the data is not normalized.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1854: It is clear that variable MAJORSKEWNESS shows some outliers, but we can’t be sure of the same for variable COMPACTNESS.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1855: Outliers seem to be a problem in the dataset.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1856: Variable MINORVARIANCE shows some outlier values.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1857: Variable COMPACTNESS doesn’t have any outliers.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1858: Variable COMPACTNESS presents some outliers.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1859: At least 60 of the variables present outliers.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1860: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1861: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1862: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1863: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1864: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1865: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/vehicle_class_histogram.png" width="auto" height = "600"/> 
    <p>1866: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/vehicle_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1867: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/vehicle_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1868: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/vehicle_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1869: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1870: All variables, but the class, should be dealt with as date.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1871: The variable MAJORVARIANCE can be seen as ordinal.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1872: The variable MINORKURTOSIS can be seen as ordinal without losing information.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1873: Variable COMPACTNESS is balanced.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1874: It is clear that variable COMPACTNESS shows some outliers, but we can’t be sure of the same for variable MINORSKEWNESS.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1875: Outliers seem to be a problem in the dataset.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1876: Variable MINORSKEWNESS shows some outlier values.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1877: Variable MINORSKEWNESS doesn’t have any outliers.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1878: Variable CIRCULARITY presents some outliers.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1879: At least 50 of the variables present outliers.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1880: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1881: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1882: Considering the common semantics for GYRATIONRADIUS and COMPACTNESS variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1883: Considering the common semantics for COMPACTNESS variable, dummification would be the most adequate encoding.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1884: The variable MINORVARIANCE can be coded as ordinal without losing information.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1885: Feature generation based on variable MAJORSKEWNESS seems to be promising.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1886: Feature generation based on the use of variable MINORVARIANCE wouldn’t be useful, but the use of COMPACTNESS seems to be promising.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1887: Given the usual semantics of RADIUS RATIO variable, dummification would have been a better codification.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1888: It is better to drop the variable MINORVARIANCE than removing all records with missing values.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1889: Not knowing the semantics of MAJORSKEWNESS variable, dummification could have been a more adequate codification.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1890: It is clear that variable fnlwgt is one of the five most relevant features.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1891: The variable capital-gain seems to be one of the four most relevant features.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1892: The variable capital-loss discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1893: It is possible to state that fnlwgt is the first most discriminative variable regarding the class.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1894: Variable fnlwgt is one of the most relevant variables.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1895: Variable fnlwgt seems to be relevant for the majority of mining tasks.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1896: Variables capital-gain and educational-num seem to be useful for classification tasks.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1897: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1898: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1899: The recall for the presented tree is higher than 75%.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1900: The number of False Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1901: The number of True Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1902: The number of True Positives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1903: Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that KNN algorithm classifies (A, not B) as <=50K for any k ≤ 21974.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1904: Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that KNN algorithm classifies (not A, B) as >50K for any k ≤ 541.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1905: Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], the Decision Tree presented classifies (A, not B) as >50K.</p>
    <img src="images/adult_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1906: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/adult_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1907: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/adult_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1908: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/adult_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1909: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/adult_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1910: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/adult_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1911: KNN is in overfitting for k larger than 5.</p>
    <img src="images/adult_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1912: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/adult_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1913: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/adult_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1914: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/adult_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1915: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/adult_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1916: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/adult_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1917: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/adult_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1918: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/adult_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1919: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/adult_pca.png" width="auto" height = "600"/> 
    <p>1920: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/adult_pca.png" width="auto" height = "600"/> 
    <p>1921: Using the first 2 principal components would imply an error between 5 and 30%.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1922: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1923: One of the variables fnlwgt or educational-num can be discarded without losing information.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1924: The variable educational-num can be discarded without risking losing information.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1925: Variables capital-loss and capital-gain are redundant, but we can’t say the same for the pair hours-per-week and educational-num.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1926: Variables capital-gain and fnlwgt are redundant.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1927: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1928: Variable age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1929: Variables capital-gain and hours-per-week seem to be useful for classification tasks.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1930: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1931: Removing variable hours-per-week might improve the training of decision trees .</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1932: There is evidence in favour for sequential backward selection to select variable capital-gain previously than variable hours-per-week.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1933: Variable capital-gain is balanced.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1934: Those boxplots show that the data is not normalized.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1935: It is clear that variable age shows some outliers, but we can’t be sure of the same for variable capital-gain.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1936: Outliers seem to be a problem in the dataset.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1937: Variable capital-loss shows some outlier values.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1938: Variable hours-per-week doesn’t have any outliers.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1939: Variable educational-num presents some outliers.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1940: At least 85 of the variables present outliers.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1941: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1942: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1943: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1944: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1945: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1946: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1947: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1948: The variable relationship can be seen as ordinal.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1949: The variable relationship can be seen as ordinal without losing information.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1950: Considering the common semantics for workclass and education variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1951: Considering the common semantics for gender variable, dummification would be the most adequate encoding.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1952: The variable marital-status can be coded as ordinal without losing information.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1953: Feature generation based on variable education seems to be promising.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1954: Feature generation based on the use of variable race wouldn’t be useful, but the use of workclass seems to be promising.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1955: Given the usual semantics of education variable, dummification would have been a better codification.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1956: It is better to drop the variable workclass than removing all records with missing values.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1957: Not knowing the semantics of education variable, dummification could have been a more adequate codification.</p>
    <img src="images/adult_class_histogram.png" width="auto" height = "600"/> 
    <p>1958: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/adult_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1959: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/adult_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1960: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/adult_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1961: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1962: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1963: The variable capital-gain can be seen as ordinal.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1964: The variable age can be seen as ordinal without losing information.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1965: Variable hours-per-week is balanced.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1966: It is clear that variable capital-loss shows some outliers, but we can’t be sure of the same for variable capital-gain.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1967: Outliers seem to be a problem in the dataset.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1968: Variable capital-loss shows some outlier values.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1969: Variable fnlwgt doesn’t have any outliers.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1970: Variable educational-num presents some outliers.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1971: At least 85 of the variables present outliers.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1972: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1973: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1974: Considering the common semantics for capital-loss and age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1975: Considering the common semantics for educational-num variable, dummification would be the most adequate encoding.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1976: The variable age can be coded as ordinal without losing information.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1977: Feature generation based on variable capital-loss seems to be promising.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1978: Feature generation based on the use of variable hours-per-week wouldn’t be useful, but the use of age seems to be promising.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1979: Given the usual semantics of capital-gain variable, dummification would have been a better codification.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1980: It is better to drop the variable educational-num than removing all records with missing values.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1981: Not knowing the semantics of hours-per-week variable, dummification could have been a more adequate codification.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1982: It is clear that variable MEDICAL_UNIT is one of the five most relevant features.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1983: The variable ASTHMA seems to be one of the three most relevant features.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1984: The variable ASTHMA discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1985: It is possible to state that CARDIOVASCULAR is the first most discriminative variable regarding the class.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1986: Variable PREGNANT is one of the most relevant variables.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1987: Variable MEDICAL_UNIT seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1988: Variables AGE and PNEUMONIA seem to be useful for classification tasks.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1989: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1990: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1991: The accuracy for the presented tree is higher than 75%.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1992: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1993: The number of True Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1994: The accuracy for the presented tree is lower than 75%.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1995: Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (not A, B) as No for any k ≤ 16.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1996: Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (A,B) as No for any k ≤ 7971.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1997: Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (A,B) as Yes for any k ≤ 46.</p>
    <img src="images/Covid_Data_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1998: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Covid_Data_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1999: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Covid_Data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2000: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/Covid_Data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2001: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Covid_Data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2002: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Covid_Data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2003: KNN is in overfitting for k larger than 17.</p>
    <img src="images/Covid_Data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2004: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/Covid_Data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2005: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/Covid_Data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2006: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/Covid_Data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2007: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/Covid_Data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2008: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/Covid_Data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2009: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/Covid_Data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2010: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Covid_Data_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2011: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Covid_Data_pca.png" width="auto" height = "600"/> 
    <p>2012: The first 5 principal components are enough for explaining half the data variance.</p>
    <img src="images/Covid_Data_pca.png" width="auto" height = "600"/> 
    <p>2013: Using the first 10 principal components would imply an error between 15 and 25%.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2014: The intrinsic dimensionality of this dataset is 5.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2015: One of the variables RENAL_CHRONIC or OTHER_DISEASE can be discarded without losing information.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2016: The variable MEDICAL_UNIT can be discarded without risking losing information.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2017: Variables TOBACCO and PREGNANT are redundant, but we can’t say the same for the pair HIPERTENSION and RENAL_CHRONIC.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2018: Variables PREGNANT and HIPERTENSION are redundant.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2019: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2020: Variable TOBACCO seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2021: Variables AGE and ICU seem to be useful for classification tasks.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2022: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2023: Removing variable PREGNANT might improve the training of decision trees .</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2024: There is evidence in favour for sequential backward selection to select variable ICU previously than variable MEDICAL_UNIT.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2025: Variable OTHER_DISEASE is balanced.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2026: Those boxplots show that the data is not normalized.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2027: It is clear that variable HIPERTENSION shows some outliers, but we can’t be sure of the same for variable COPD.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2028: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2029: Variable HIPERTENSION shows a high number of outlier values.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2030: Variable MEDICAL_UNIT doesn’t have any outliers.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2031: Variable AGE presents some outliers.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2032: At least 50 of the variables present outliers.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2033: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2034: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2035: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2036: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2037: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>2038: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2039: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2040: The variable PATIENT_TYPE can be seen as ordinal.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2041: The variable PATIENT_TYPE can be seen as ordinal without losing information.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2042: Considering the common semantics for PATIENT_TYPE and USMER variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2043: Considering the common semantics for USMER variable, dummification would be the most adequate encoding.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2044: The variable PATIENT_TYPE can be coded as ordinal without losing information.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2045: Feature generation based on variable USMER seems to be promising.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2046: Feature generation based on the use of variable SEX wouldn’t be useful, but the use of USMER seems to be promising.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2047: Given the usual semantics of SEX variable, dummification would have been a better codification.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2048: It is better to drop the variable PATIENT_TYPE than removing all records with missing values.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2049: Not knowing the semantics of USMER variable, dummification could have been a more adequate codification.</p>
    <img src="images/Covid_Data_class_histogram.png" width="auto" height = "600"/> 
    <p>2050: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Covid_Data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2051: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/Covid_Data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2052: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Covid_Data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2053: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2054: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2055: The variable ICU can be seen as ordinal.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2056: The variable ICU can be seen as ordinal without losing information.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2057: Variable PNEUMONIA is balanced.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2058: It is clear that variable PNEUMONIA shows some outliers, but we can’t be sure of the same for variable HIPERTENSION.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2059: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2060: Variable COPD shows some outlier values.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2061: Variable COPD doesn’t have any outliers.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2062: Variable TOBACCO presents some outliers.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2063: At least 85 of the variables present outliers.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2064: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2065: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2066: Considering the common semantics for TOBACCO and MEDICAL_UNIT variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2067: Considering the common semantics for PNEUMONIA variable, dummification would be the most adequate encoding.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2068: The variable OTHER_DISEASE can be coded as ordinal without losing information.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2069: Feature generation based on variable COPD seems to be promising.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2070: Feature generation based on the use of variable TOBACCO wouldn’t be useful, but the use of MEDICAL_UNIT seems to be promising.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2071: Given the usual semantics of CARDIOVASCULAR variable, dummification would have been a better codification.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2072: It is better to drop the variable ASTHMA than removing all records with missing values.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2073: Not knowing the semantics of OTHER_DISEASE variable, dummification could have been a more adequate codification.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2074: It is clear that variable run is one of the two most relevant features.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2075: The variable run seems to be one of the five most relevant features.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2076: The variable run discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2077: It is possible to state that dec is the first most discriminative variable regarding the class.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2078: Variable redshift is one of the most relevant variables.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2079: Variable field seems to be relevant for the majority of mining tasks.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2080: Variables run and mjd seem to be useful for classification tasks.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2081: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2082: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2083: The accuracy for the presented tree is higher than 60%.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2084: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2085: The number of False Negatives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2086: The number of False Positives reported in the same tree is 10.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2087: Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as QSO.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2088: Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], it is possible to state that KNN algorithm classifies (A, not B) as QSO for any k ≤ 208.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>2089: Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], it is possible to state that KNN algorithm classifies (A, not B) as GALAXY for any k ≤ 1728.</p>
    <img src="images/sky_survey_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2090: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/sky_survey_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2091: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/sky_survey_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2092: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/sky_survey_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2093: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/sky_survey_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2094: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/sky_survey_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2095: KNN is in overfitting for k less than 5.</p>
    <img src="images/sky_survey_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2096: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/sky_survey_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2097: KNN with less than 15 neighbours is in overfitting.</p>
    <img src="images/sky_survey_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2098: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/sky_survey_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2099: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/sky_survey_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2100: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/sky_survey_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2101: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/sky_survey_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2102: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/sky_survey_pca.png" width="auto" height = "600"/> 
    <p>2103: The first 7 principal components are enough for explaining half the data variance.</p>
    <img src="images/sky_survey_pca.png" width="auto" height = "600"/> 
    <p>2104: Using the first 6 principal components would imply an error between 10 and 30%.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2105: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2106: One of the variables plate or ra can be discarded without losing information.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2107: The variable ra can be discarded without risking losing information.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2108: Variables run and dec are redundant, but we can’t say the same for the pair plate and field.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2109: Variables field and plate are redundant.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2110: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2111: Variable plate seems to be relevant for the majority of mining tasks.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2112: Variables mjd and dec seem to be useful for classification tasks.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2113: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2114: Removing variable run might improve the training of decision trees .</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2115: There is evidence in favour for sequential backward selection to select variable camcol previously than variable dec.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2116: Variable redshift is balanced.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2117: Those boxplots show that the data is not normalized.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2118: It is clear that variable ra shows some outliers, but we can’t be sure of the same for variable dec.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2119: Outliers seem to be a problem in the dataset.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2120: Variable redshift shows a high number of outlier values.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2121: Variable mjd doesn’t have any outliers.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2122: Variable field presents some outliers.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2123: At least 50 of the variables present outliers.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2124: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2125: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2126: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2127: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2128: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2129: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/sky_survey_class_histogram.png" width="auto" height = "600"/> 
    <p>2130: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/sky_survey_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2131: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/sky_survey_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2132: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/sky_survey_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2133: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2134: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2135: The variable dec can be seen as ordinal.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2136: The variable run can be seen as ordinal without losing information.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2137: Variable dec is balanced.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2138: It is clear that variable field shows some outliers, but we can’t be sure of the same for variable camcol.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2139: Outliers seem to be a problem in the dataset.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2140: Variable plate shows some outlier values.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2141: Variable field doesn’t have any outliers.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2142: Variable redshift presents some outliers.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2143: At least 85 of the variables present outliers.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2144: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2145: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2146: Considering the common semantics for redshift and ra variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2147: Considering the common semantics for plate variable, dummification would be the most adequate encoding.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2148: The variable ra can be coded as ordinal without losing information.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2149: Feature generation based on variable plate seems to be promising.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2150: Feature generation based on the use of variable run wouldn’t be useful, but the use of ra seems to be promising.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2151: Given the usual semantics of plate variable, dummification would have been a better codification.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2152: It is better to drop the variable mjd than removing all records with missing values.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2153: Not knowing the semantics of camcol variable, dummification could have been a more adequate codification.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2154: It is clear that variable Alcohol is one of the three most relevant features.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2155: The variable OD280-OD315 of diluted wines seems to be one of the four most relevant features.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2156: The variable Total phenols discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2157: It is possible to state that Hue is the second most discriminative variable regarding the class.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2158: Variable Alcalinity of ash is one of the most relevant variables.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2159: Variable Proanthocyanins seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2160: Variables Flavanoids and OD280-OD315 of diluted wines seem to be useful for classification tasks.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2161: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2162: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2163: The specificity for the presented tree is lower than 90%.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2164: The number of True Positives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2165: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2166: The accuracy for the presented tree is lower than its recall.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2167: Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (not A, B) as 3 for any k ≤ 2.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2168: Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 2.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2169: Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A, not B) as 2 for any k ≤ 49.</p>
    <img src="images/Wine_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2170: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Wine_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2171: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/Wine_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2172: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/Wine_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2173: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Wine_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2174: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/Wine_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2175: KNN is in overfitting for k less than 17.</p>
    <img src="images/Wine_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2176: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/Wine_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2177: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/Wine_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2178: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/Wine_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2179: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Wine_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2180: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/Wine_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2181: The decision tree is in overfitting for depths above 4.</p>
    <img src="images/Wine_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2182: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/Wine_pca.png" width="auto" height = "600"/> 
    <p>2183: The first 7 principal components are enough for explaining half the data variance.</p>
    <img src="images/Wine_pca.png" width="auto" height = "600"/> 
    <p>2184: Using the first 2 principal components would imply an error between 5 and 25%.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2185: The intrinsic dimensionality of this dataset is 5.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2186: One of the variables Alcalinity of ash or Flavanoids can be discarded without losing information.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2187: The variable Alcohol can be discarded without risking losing information.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2188: Variables Ash and Flavanoids seem to be useful for classification tasks.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2189: Variables Proanthocyanins and Nonflavanoid phenols are redundant.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2190: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2191: Variable Hue seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2192: Variables Color intensity and OD280-OD315 of diluted wines seem to be useful for classification tasks.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2193: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2194: Removing variable Malic acid might improve the training of decision trees .</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2195: There is evidence in favour for sequential backward selection to select variable Nonflavanoid phenols previously than variable Alcohol.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2196: Variable Flavanoids is balanced.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2197: Those boxplots show that the data is not normalized.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2198: It is clear that variable Flavanoids shows some outliers, but we can’t be sure of the same for variable Nonflavanoid phenols.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2199: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2200: Variable Alcalinity of ash shows some outlier values.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2201: Variable Alcohol doesn’t have any outliers.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2202: Variable Proanthocyanins presents some outliers.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2203: At least 85 of the variables present outliers.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2204: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2205: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2206: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2207: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2208: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2209: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Wine_class_histogram.png" width="auto" height = "600"/> 
    <p>2210: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Wine_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2211: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Wine_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2212: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Wine_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2213: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2214: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2215: The variable Hue can be seen as ordinal.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2216: The variable Color intensity can be seen as ordinal without losing information.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2217: Variable Nonflavanoid phenols is balanced.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2218: It is clear that variable Nonflavanoid phenols shows some outliers, but we can’t be sure of the same for variable Ash.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2219: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2220: Variable Ash shows a high number of outlier values.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2221: Variable Hue doesn’t have any outliers.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2222: Variable Proanthocyanins presents some outliers.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2223: At least 85 of the variables present outliers.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2224: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2225: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2226: Considering the common semantics for Hue and Alcohol variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2227: Considering the common semantics for Color intensity variable, dummification would be the most adequate encoding.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2228: The variable Nonflavanoid phenols can be coded as ordinal without losing information.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2229: Feature generation based on variable Alcalinity of ash seems to be promising.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2230: Feature generation based on the use of variable Ash wouldn’t be useful, but the use of Alcohol seems to be promising.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2231: Given the usual semantics of Proanthocyanins variable, dummification would have been a better codification.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2232: It is better to drop the variable Alcalinity of ash than removing all records with missing values.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2233: Not knowing the semantics of Flavanoids variable, dummification could have been a more adequate codification.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2234: It is clear that variable Turbidity is one of the three most relevant features.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2235: The variable Sulfate seems to be one of the three most relevant features.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2236: The variable ph discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2237: It is possible to state that Conductivity is the second most discriminative variable regarding the class.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2238: Variable Chloramines is one of the most relevant variables.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2239: Variable Trihalomethanes seems to be relevant for the majority of mining tasks.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2240: Variables Turbidity and Chloramines seem to be useful for classification tasks.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2241: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2242: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2243: The specificity for the presented tree is lower than 60%.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2244: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2245: The number of False Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2246: The number of True Positives reported in the same tree is 50.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2247: Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 8.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2248: Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], the Decision Tree presented classifies (A,B) as 0.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2249: Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 6.</p>
    <img src="images/water_potability_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2250: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/water_potability_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2251: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/water_potability_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2252: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/water_potability_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2253: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/water_potability_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2254: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/water_potability_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2255: KNN is in overfitting for k larger than 13.</p>
    <img src="images/water_potability_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2256: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/water_potability_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2257: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/water_potability_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2258: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/water_potability_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2259: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/water_potability_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2260: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/water_potability_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2261: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/water_potability_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2262: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/water_potability_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2263: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/water_potability_pca.png" width="auto" height = "600"/> 
    <p>2264: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/water_potability_pca.png" width="auto" height = "600"/> 
    <p>2265: Using the first 6 principal components would imply an error between 5 and 25%.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2266: The intrinsic dimensionality of this dataset is 5.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2267: One of the variables Sulfate or ph can be discarded without losing information.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2268: The variable Turbidity can be discarded without risking losing information.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2269: Variables Chloramines and Trihalomethanes are redundant, but we can’t say the same for the pair Conductivity and ph.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2270: Variables Hardness and Turbidity are redundant.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2271: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2272: Variable Turbidity seems to be relevant for the majority of mining tasks.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2273: Variables Trihalomethanes and ph seem to be useful for classification tasks.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2274: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2275: Removing variable Turbidity might improve the training of decision trees .</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2276: There is evidence in favour for sequential backward selection to select variable Chloramines previously than variable Conductivity.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2277: Variable Turbidity is balanced.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2278: Those boxplots show that the data is not normalized.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2279: It is clear that variable Hardness shows some outliers, but we can’t be sure of the same for variable Chloramines.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2280: Outliers seem to be a problem in the dataset.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2281: Variable Hardness shows some outlier values.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2282: Variable Chloramines doesn’t have any outliers.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2283: Variable Sulfate presents some outliers.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2284: At least 60 of the variables present outliers.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2285: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2286: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2287: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2288: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2289: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2290: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2291: Discarding variable Sulfate would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2292: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2293: Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2294: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2295: Feature generation based on variable ph seems to be promising.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2296: It is better to drop the variable ph than removing all records with missing values.</p>
    <img src="images/water_potability_class_histogram.png" width="auto" height = "600"/> 
    <p>2297: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/water_potability_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2298: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/water_potability_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2299: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/water_potability_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2300: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2301: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2302: The variable Hardness can be seen as ordinal.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2303: The variable ph can be seen as ordinal without losing information.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2304: Variable Turbidity is balanced.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2305: It is clear that variable Trihalomethanes shows some outliers, but we can’t be sure of the same for variable ph.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2306: Outliers seem to be a problem in the dataset.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2307: Variable Turbidity shows some outlier values.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2308: Variable Conductivity doesn’t have any outliers.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2309: Variable Sulfate presents some outliers.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2310: At least 50 of the variables present outliers.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2311: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2312: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2313: Considering the common semantics for Conductivity and ph variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2314: Considering the common semantics for Sulfate variable, dummification would be the most adequate encoding.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2315: The variable Hardness can be coded as ordinal without losing information.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2316: Feature generation based on variable Hardness seems to be promising.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2317: Feature generation based on the use of variable ph wouldn’t be useful, but the use of Hardness seems to be promising.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2318: Given the usual semantics of Sulfate variable, dummification would have been a better codification.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2319: It is better to drop the variable Trihalomethanes than removing all records with missing values.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2320: Not knowing the semantics of Sulfate variable, dummification could have been a more adequate codification.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2321: It is clear that variable Whole weight is one of the four most relevant features.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2322: The variable Rings seems to be one of the four most relevant features.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2323: The variable Rings discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2324: It is possible to state that Viscera weight is the first most discriminative variable regarding the class.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2325: Variable Viscera weight is one of the most relevant variables.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2326: Variable Shell weight seems to be relevant for the majority of mining tasks.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2327: Variables Shucked weight and Length seem to be useful for classification tasks.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2328: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2329: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2330: The precision for the presented tree is higher than 90%.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2331: The number of False Positives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2332: The number of True Negatives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2333: The number of True Negatives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2334: Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (A, not B) as F for any k ≤ 1191.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2335: Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (A,B) as I for any k ≤ 1191.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2336: Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (A,B) as F for any k ≤ 117.</p>
    <img src="images/abalone_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2337: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/abalone_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2338: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/abalone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2339: Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.</p>
    <img src="images/abalone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2340: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/abalone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2341: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/abalone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2342: KNN is in overfitting for k less than 17.</p>
    <img src="images/abalone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2343: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/abalone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2344: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/abalone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2345: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/abalone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2346: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/abalone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2347: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.</p>
    <img src="images/abalone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2348: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/abalone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2349: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/abalone_pca.png" width="auto" height = "600"/> 
    <p>2350: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/abalone_pca.png" width="auto" height = "600"/> 
    <p>2351: Using the first 3 principal components would imply an error between 15 and 30%.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2352: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2353: One of the variables Rings or Shucked weight can be discarded without losing information.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2354: The variable Height can be discarded without risking losing information.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2355: Variables Shucked weight and Whole weight are redundant, but we can’t say the same for the pair Diameter and Rings.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2356: Variables Viscera weight and Diameter are redundant.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2357: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2358: Variable Diameter seems to be relevant for the majority of mining tasks.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2359: Variables Shell weight and Length seem to be useful for classification tasks.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2360: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2361: Removing variable Length might improve the training of decision trees .</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2362: There is evidence in favour for sequential backward selection to select variable Diameter previously than variable Length.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2363: Variable Height is balanced.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2364: Those boxplots show that the data is not normalized.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2365: It is clear that variable Shucked weight shows some outliers, but we can’t be sure of the same for variable Shell weight.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2366: Outliers seem to be a problem in the dataset.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2367: Variable Rings shows a high number of outlier values.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2368: Variable Viscera weight doesn’t have any outliers.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2369: Variable Length presents some outliers.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2370: At least 75 of the variables present outliers.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2371: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2372: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2373: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2374: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2375: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2376: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/abalone_class_histogram.png" width="auto" height = "600"/> 
    <p>2377: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/abalone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2378: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/abalone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2379: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/abalone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2380: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2381: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2382: The variable Diameter can be seen as ordinal.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2383: The variable Whole weight can be seen as ordinal without losing information.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2384: Variable Rings is balanced.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2385: It is clear that variable Height shows some outliers, but we can’t be sure of the same for variable Shell weight.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2386: Outliers seem to be a problem in the dataset.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2387: Variable Viscera weight shows some outlier values.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2388: Variable Shucked weight doesn’t have any outliers.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2389: Variable Viscera weight presents some outliers.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2390: At least 75 of the variables present outliers.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2391: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2392: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2393: Considering the common semantics for Rings and Length variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2394: Considering the common semantics for Whole weight variable, dummification would be the most adequate encoding.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2395: The variable Height can be coded as ordinal without losing information.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2396: Feature generation based on variable Whole weight seems to be promising.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2397: Feature generation based on the use of variable Diameter wouldn’t be useful, but the use of Length seems to be promising.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2398: Given the usual semantics of Rings variable, dummification would have been a better codification.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2399: It is better to drop the variable Diameter than removing all records with missing values.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2400: Not knowing the semantics of Shell weight variable, dummification could have been a more adequate codification.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2401: It is clear that variable weight is one of the four most relevant features.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2402: The variable triglyceride seems to be one of the five most relevant features.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2403: The variable age discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2404: It is possible to state that gamma_GTP is the first most discriminative variable regarding the class.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2405: Variable height is one of the most relevant variables.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2406: Variable SMK_stat_type_cd seems to be relevant for the majority of mining tasks.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2407: Variables LDL_chole and hemoglobin seem to be useful for classification tasks.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2408: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2409: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2410: The precision for the presented tree is lower than 75%.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2411: The number of True Positives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2412: The number of True Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2413: The variable SBP seems to be one of the five most relevant features.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2414: Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that KNN algorithm classifies (not A, B) as N for any k ≤ 3135.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2415: Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as Y.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2416: Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that KNN algorithm classifies (not A, B) as Y for any k ≤ 2793.</p>
    <img src="images/smoking_drinking_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2417: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/smoking_drinking_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2418: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/smoking_drinking_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2419: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/smoking_drinking_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2420: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/smoking_drinking_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2421: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2422: KNN is in overfitting for k less than 17.</p>
    <img src="images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2423: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2424: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2425: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/smoking_drinking_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2426: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/smoking_drinking_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2427: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/smoking_drinking_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2428: The decision tree is in overfitting for depths above 3.</p>
    <img src="images/smoking_drinking_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2429: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/smoking_drinking_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2430: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/smoking_drinking_pca.png" width="auto" height = "600"/> 
    <p>2431: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/smoking_drinking_pca.png" width="auto" height = "600"/> 
    <p>2432: Using the first 2 principal components would imply an error between 15 and 25%.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2433: The intrinsic dimensionality of this dataset is 7.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2434: One of the variables SMK_stat_type_cd or SBP can be discarded without losing information.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2435: The variable SBP can be discarded without risking losing information.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2436: Variables waistline and height are redundant, but we can’t say the same for the pair triglyceride and SMK_stat_type_cd.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2437: Variables waistline and LDL_chole are redundant.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2438: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2439: Variable weight seems to be relevant for the majority of mining tasks.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2440: Variables tot_chole and triglyceride seem to be useful for classification tasks.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2441: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2442: Removing variable waistline might improve the training of decision trees .</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2443: There is evidence in favour for sequential backward selection to select variable height previously than variable weight.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2444: Variable tot_chole is balanced.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2445: Those boxplots show that the data is not normalized.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2446: It is clear that variable tot_chole shows some outliers, but we can’t be sure of the same for variable waistline.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2447: Outliers seem to be a problem in the dataset.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2448: Variable weight shows a high number of outlier values.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2449: Variable LDL_chole doesn’t have any outliers.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2450: Variable gamma_GTP presents some outliers.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2451: At least 60 of the variables present outliers.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2452: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2453: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2454: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2455: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2456: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2457: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2458: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2459: The variable sex can be seen as ordinal.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2460: The variable hear_left can be seen as ordinal without losing information.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2461: Considering the common semantics for hear_right and sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2462: Considering the common semantics for hear_left variable, dummification would be the most adequate encoding.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2463: The variable sex can be coded as ordinal without losing information.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2464: Feature generation based on variable hear_right seems to be promising.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2465: Feature generation based on the use of variable sex wouldn’t be useful, but the use of hear_left seems to be promising.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2466: Given the usual semantics of sex variable, dummification would have been a better codification.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2467: It is better to drop the variable sex than removing all records with missing values.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2468: Not knowing the semantics of hear_right variable, dummification could have been a more adequate codification.</p>
    <img src="images/smoking_drinking_class_histogram.png" width="auto" height = "600"/> 
    <p>2469: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/smoking_drinking_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2470: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/smoking_drinking_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2471: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/smoking_drinking_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2472: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2473: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2474: The variable SBP can be seen as ordinal.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2475: The variable SMK_stat_type_cd can be seen as ordinal without losing information.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2476: Variable gamma_GTP is balanced.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2477: It is clear that variable SBP shows some outliers, but we can’t be sure of the same for variable waistline.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2478: Outliers seem to be a problem in the dataset.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2479: Variable SMK_stat_type_cd shows a high number of outlier values.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2480: Variable gamma_GTP doesn’t have any outliers.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2481: Variable age presents some outliers.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2482: At least 50 of the variables present outliers.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2483: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2484: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2485: Considering the common semantics for age and height variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2486: Considering the common semantics for waistline variable, dummification would be the most adequate encoding.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2487: The variable BLDS can be coded as ordinal without losing information.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2488: Feature generation based on variable LDL_chole seems to be promising.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2489: Feature generation based on the use of variable SMK_stat_type_cd wouldn’t be useful, but the use of age seems to be promising.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2490: Given the usual semantics of age variable, dummification would have been a better codification.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2491: It is better to drop the variable SMK_stat_type_cd than removing all records with missing values.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2492: Not knowing the semantics of gamma_GTP variable, dummification could have been a more adequate codification.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2493: It is clear that variable entropy is one of the four most relevant features.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2494: The variable entropy seems to be one of the two most relevant features.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2495: The variable variance discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2496: It is possible to state that entropy is the second most discriminative variable regarding the class.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2497: Variable entropy is one of the most relevant variables.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2498: Variable variance seems to be relevant for the majority of mining tasks.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2499: Variables entropy and curtosis seem to be useful for classification tasks.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2500: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2501: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2502: The specificity for the presented tree is higher than 90%.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2503: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2504: The number of True Negatives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2505: The number of False Positives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2506: Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 214.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2507: Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 179.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2508: Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], the Decision Tree presented classifies (not A, B) as 0.</p>
    <img src="images/BankNoteAuthentication_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2509: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/BankNoteAuthentication_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2510: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/BankNoteAuthentication_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2511: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/BankNoteAuthentication_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2512: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/BankNoteAuthentication_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2513: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/BankNoteAuthentication_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2514: KNN is in overfitting for k less than 5.</p>
    <img src="images/BankNoteAuthentication_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2515: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/BankNoteAuthentication_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2516: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/BankNoteAuthentication_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2517: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/BankNoteAuthentication_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2518: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/BankNoteAuthentication_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2519: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/BankNoteAuthentication_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2520: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/BankNoteAuthentication_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2521: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/BankNoteAuthentication_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2522: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/BankNoteAuthentication_pca.png" width="auto" height = "600"/> 
    <p>2523: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/BankNoteAuthentication_pca.png" width="auto" height = "600"/> 
    <p>2524: Using the first 3 principal components would imply an error between 5 and 25%.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2525: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2526: One of the variables variance or curtosis can be discarded without losing information.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2527: The variable entropy can be discarded without risking losing information.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2528: Variables entropy and variance are redundant, but we can’t say the same for the pair skewness and curtosis.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2529: Variables variance and curtosis are redundant.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2530: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2531: Variable curtosis seems to be relevant for the majority of mining tasks.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2532: Variables entropy and skewness seem to be useful for classification tasks.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2533: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2534: Removing variable variance might improve the training of decision trees .</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2535: There is evidence in favour for sequential backward selection to select variable variance previously than variable entropy.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2536: Variable curtosis is balanced.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2537: Those boxplots show that the data is not normalized.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2538: It is clear that variable curtosis shows some outliers, but we can’t be sure of the same for variable skewness.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2539: Outliers seem to be a problem in the dataset.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2540: Variable entropy shows a high number of outlier values.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2541: Variable curtosis doesn’t have any outliers.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2542: Variable variance presents some outliers.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2543: At least 60 of the variables present outliers.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2544: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2545: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2546: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2547: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2548: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2549: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/BankNoteAuthentication_class_histogram.png" width="auto" height = "600"/> 
    <p>2550: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/BankNoteAuthentication_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2551: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/BankNoteAuthentication_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2552: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/BankNoteAuthentication_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2553: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2554: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2555: The variable skewness can be seen as ordinal.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2556: The variable curtosis can be seen as ordinal without losing information.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2557: Variable skewness is balanced.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2558: It is clear that variable variance shows some outliers, but we can’t be sure of the same for variable entropy.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2559: Outliers seem to be a problem in the dataset.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2560: Variable variance shows some outlier values.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2561: Variable curtosis doesn’t have any outliers.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2562: Variable skewness presents some outliers.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2563: At least 75 of the variables present outliers.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2564: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2565: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2566: Considering the common semantics for skewness and variance variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2567: Considering the common semantics for curtosis variable, dummification would be the most adequate encoding.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2568: The variable skewness can be coded as ordinal without losing information.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2569: Feature generation based on variable entropy seems to be promising.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2570: Feature generation based on the use of variable curtosis wouldn’t be useful, but the use of variance seems to be promising.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2571: Given the usual semantics of variance variable, dummification would have been a better codification.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2572: It is better to drop the variable curtosis than removing all records with missing values.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2573: Not knowing the semantics of variance variable, dummification could have been a more adequate codification.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2574: It is clear that variable SepalLengthCm is one of the four most relevant features.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2575: The variable SepalWidthCm seems to be one of the four most relevant features.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2576: The variable SepalLengthCm discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2577: It is possible to state that SepalLengthCm is the first most discriminative variable regarding the class.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2578: Variable SepalWidthCm is one of the most relevant variables.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2579: Variable SepalWidthCm seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2580: Variables SepalLengthCm and PetalLengthCm seem to be useful for classification tasks.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2581: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2582: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2583: The recall for the presented tree is higher than 90%.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2584: The number of False Negatives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2585: The number of False Positives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2586: The variable PetalWidthCm discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2587: Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], the Decision Tree presented classifies (not A, B) as Iris-versicolor.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2588: Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], it is possible to state that KNN algorithm classifies (not A, not B) as Iris-virginica for any k ≤ 38.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2589: Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], it is possible to state that KNN algorithm classifies (A, not B) as Iris-setosa for any k ≤ 35.</p>
    <img src="images/Iris_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2590: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Iris_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2591: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Iris_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2592: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/Iris_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2593: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Iris_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2594: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2595: KNN is in overfitting for k less than 5.</p>
    <img src="images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2596: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2597: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2598: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2599: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2600: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.</p>
    <img src="images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2601: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2602: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/Iris_pca.png" width="auto" height = "600"/> 
    <p>2603: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/Iris_pca.png" width="auto" height = "600"/> 
    <p>2604: Using the first 3 principal components would imply an error between 5 and 30%.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2605: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2606: One of the variables PetalWidthCm or SepalLengthCm can be discarded without losing information.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2607: The variable SepalLengthCm can be discarded without risking losing information.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2608: Variables PetalWidthCm and SepalLengthCm seem to be useful for classification tasks.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2609: Variables SepalWidthCm and PetalLengthCm are redundant.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2610: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2611: Variable PetalLengthCm seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2612: Variables PetalWidthCm and SepalWidthCm seem to be useful for classification tasks.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2613: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2614: Removing variable SepalLengthCm might improve the training of decision trees .</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2615: There is evidence in favour for sequential backward selection to select variable SepalWidthCm previously than variable PetalLengthCm.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2616: Variable PetalWidthCm is balanced.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2617: Those boxplots show that the data is not normalized.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2618: It is clear that variable SepalLengthCm shows some outliers, but we can’t be sure of the same for variable SepalWidthCm.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2619: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2620: Variable PetalWidthCm shows a high number of outlier values.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2621: Variable SepalWidthCm doesn’t have any outliers.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2622: Variable PetalWidthCm presents some outliers.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2623: At least 50 of the variables present outliers.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2624: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2625: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2626: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2627: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2628: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2629: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Iris_class_histogram.png" width="auto" height = "600"/> 
    <p>2630: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Iris_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2631: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/Iris_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2632: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Iris_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2633: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2634: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2635: The variable PetalWidthCm can be seen as ordinal.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2636: The variable SepalLengthCm can be seen as ordinal without losing information.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2637: Variable PetalWidthCm is balanced.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2638: It is clear that variable PetalLengthCm shows some outliers, but we can’t be sure of the same for variable SepalWidthCm.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2639: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2640: Variable SepalWidthCm shows some outlier values.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2641: Variable SepalWidthCm doesn’t have any outliers.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2642: Variable PetalWidthCm presents some outliers.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2643: At least 50 of the variables present outliers.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2644: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2645: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2646: Considering the common semantics for PetalWidthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2647: Considering the common semantics for SepalWidthCm variable, dummification would be the most adequate encoding.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2648: The variable PetalWidthCm can be coded as ordinal without losing information.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2649: Feature generation based on variable SepalLengthCm seems to be promising.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2650: Feature generation based on the use of variable PetalWidthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2651: Given the usual semantics of SepalWidthCm variable, dummification would have been a better codification.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2652: It is better to drop the variable PetalWidthCm than removing all records with missing values.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2653: Not knowing the semantics of SepalLengthCm variable, dummification could have been a more adequate codification.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2654: It is clear that variable sc_w is one of the four most relevant features.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2655: The variable fc seems to be one of the two most relevant features.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2656: The variable int_memory discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2657: It is possible to state that sc_h is the second most discriminative variable regarding the class.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2658: Variable pc is one of the most relevant variables.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2659: Variable n_cores seems to be relevant for the majority of mining tasks.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2660: Variables ram and fc seem to be useful for classification tasks.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2661: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2662: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2663: The recall for the presented tree is lower than 75%.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2664: The number of True Negatives reported in the same tree is 30.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2665: The number of False Positives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2666: The variable sc_w seems to be one of the three most relevant features.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2667: Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], it is possible to state that KNN algorithm classifies (not A, not B) as 2 for any k ≤ 636.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2668: Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 469.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2669: Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 469.</p>
    <img src="images/phone_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2670: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/phone_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2671: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/phone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2672: Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.</p>
    <img src="images/phone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2673: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/phone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2674: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2675: KNN is in overfitting for k less than 17.</p>
    <img src="images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2676: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2677: KNN with more than 7 neighbours is in overfitting.</p>
    <img src="images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2678: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/phone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2679: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/phone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2680: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.</p>
    <img src="images/phone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2681: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/phone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2682: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/phone_pca.png" width="auto" height = "600"/> 
    <p>2683: The first 8 principal components are enough for explaining half the data variance.</p>
    <img src="images/phone_pca.png" width="auto" height = "600"/> 
    <p>2684: Using the first 8 principal components would imply an error between 10 and 20%.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2685: The intrinsic dimensionality of this dataset is 9.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2686: One of the variables px_height or n_cores can be discarded without losing information.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2687: The variable px_width can be discarded without risking losing information.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2688: Variables battery_power and ram are redundant, but we can’t say the same for the pair px_width and pc.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2689: Variables sc_w and battery_power are redundant.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2690: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2691: Variable n_cores seems to be relevant for the majority of mining tasks.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2692: Variables sc_w and n_cores seem to be useful for classification tasks.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2693: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2694: Removing variable battery_power might improve the training of decision trees .</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2695: There is evidence in favour for sequential backward selection to select variable sc_w previously than variable sc_h.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2696: Variable sc_h is balanced.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2697: Those boxplots show that the data is not normalized.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2698: It is clear that variable battery_power shows some outliers, but we can’t be sure of the same for variable talk_time.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2699: Outliers seem to be a problem in the dataset.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2700: Variable sc_h shows some outlier values.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2701: Variable fc doesn’t have any outliers.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2702: Variable talk_time presents some outliers.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2703: At least 85 of the variables present outliers.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2704: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2705: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2706: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2707: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2708: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2709: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2710: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2711: The variable four_g can be seen as ordinal.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2712: The variable three_g can be seen as ordinal without losing information.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2713: Considering the common semantics for touch_screen and blue variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2714: Considering the common semantics for three_g variable, dummification would be the most adequate encoding.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2715: The variable dual_sim can be coded as ordinal without losing information.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2716: Feature generation based on variable four_g seems to be promising.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2717: Feature generation based on the use of variable touch_screen wouldn’t be useful, but the use of blue seems to be promising.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2718: Given the usual semantics of dual_sim variable, dummification would have been a better codification.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2719: It is better to drop the variable wifi than removing all records with missing values.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2720: Not knowing the semantics of touch_screen variable, dummification could have been a more adequate codification.</p>
    <img src="images/phone_class_histogram.png" width="auto" height = "600"/> 
    <p>2721: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/phone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2722: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/phone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2723: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/phone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2724: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2725: All variables, but the class, should be dealt with as date.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2726: The variable pc can be seen as ordinal.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2727: The variable int_memory can be seen as ordinal without losing information.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2728: Variable n_cores is balanced.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2729: It is clear that variable int_memory shows some outliers, but we can’t be sure of the same for variable sc_h.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2730: Outliers seem to be a problem in the dataset.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2731: Variable talk_time shows a high number of outlier values.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2732: Variable battery_power doesn’t have any outliers.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2733: Variable ram presents some outliers.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2734: At least 85 of the variables present outliers.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2735: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2736: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2737: Considering the common semantics for int_memory and battery_power variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2738: Considering the common semantics for mobile_wt variable, dummification would be the most adequate encoding.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2739: The variable fc can be coded as ordinal without losing information.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2740: Feature generation based on variable ram seems to be promising.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2741: Feature generation based on the use of variable sc_w wouldn’t be useful, but the use of battery_power seems to be promising.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2742: Given the usual semantics of px_width variable, dummification would have been a better codification.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2743: It is better to drop the variable sc_w than removing all records with missing values.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2744: Not knowing the semantics of battery_power variable, dummification could have been a more adequate codification.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2745: It is clear that variable Pclass is one of the five most relevant features.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2746: The variable Pclass seems to be one of the four most relevant features.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2747: The variable Age discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2748: It is possible to state that Age is the first most discriminative variable regarding the class.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2749: Variable Pclass is one of the most relevant variables.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2750: Variable Age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2751: Variables Parch and SibSp seem to be useful for classification tasks.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2752: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2753: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2754: The recall for the presented tree is higher than 60%.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2755: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2756: The number of True Negatives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2757: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2758: Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 0.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2759: Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 72.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2760: Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 181.</p>
    <img src="images/Titanic_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2761: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Titanic_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2762: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/Titanic_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2763: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/Titanic_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2764: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Titanic_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2765: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2766: KNN is in overfitting for k less than 17.</p>
    <img src="images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2767: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2768: KNN with more than 7 neighbours is in overfitting.</p>
    <img src="images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2769: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2770: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2771: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.</p>
    <img src="images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2772: The decision tree is in overfitting for depths above 3.</p>
    <img src="images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2773: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/Titanic_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2774: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Titanic_pca.png" width="auto" height = "600"/> 
    <p>2775: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/Titanic_pca.png" width="auto" height = "600"/> 
    <p>2776: Using the first 4 principal components would imply an error between 10 and 20%.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2777: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2778: One of the variables Fare or Pclass can be discarded without losing information.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2779: The variable Pclass can be discarded without risking losing information.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2780: Variables Age and Parch are redundant, but we can’t say the same for the pair Fare and Pclass.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2781: Variables SibSp and Fare are redundant.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2782: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2783: Variable Age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2784: Variables Parch and Fare seem to be useful for classification tasks.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2785: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2786: Removing variable Parch might improve the training of decision trees .</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2787: There is evidence in favour for sequential backward selection to select variable Parch previously than variable Age.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2788: Variable Fare is balanced.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2789: Those boxplots show that the data is not normalized.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2790: It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Pclass.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2791: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2792: Variable Parch shows some outlier values.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2793: Variable Parch doesn’t have any outliers.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2794: Variable Parch presents some outliers.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2795: At least 60 of the variables present outliers.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2796: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2797: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2798: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2799: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2800: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2801: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2802: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2803: The variable Embarked can be seen as ordinal.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2804: The variable Embarked can be seen as ordinal without losing information.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2805: Considering the common semantics for Sex and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2806: Considering the common semantics for Embarked variable, dummification would be the most adequate encoding.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2807: The variable Embarked can be coded as ordinal without losing information.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2808: Feature generation based on variable Sex seems to be promising.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2809: Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Embarked seems to be promising.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2810: Given the usual semantics of Embarked variable, dummification would have been a better codification.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2811: It is better to drop the variable Embarked than removing all records with missing values.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2812: Not knowing the semantics of Sex variable, dummification could have been a more adequate codification.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2813: Discarding variable Age would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2814: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2815: Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2816: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2817: Feature generation based on variable Embarked seems to be promising.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2818: It is better to drop the variable Embarked than removing all records with missing values.</p>
    <img src="images/Titanic_class_histogram.png" width="auto" height = "600"/> 
    <p>2819: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Titanic_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2820: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/Titanic_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2821: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Titanic_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2822: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2823: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2824: The variable Age can be seen as ordinal.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2825: The variable Fare can be seen as ordinal without losing information.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2826: Variable Age is balanced.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2827: It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable Age.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2828: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2829: Variable Parch shows some outlier values.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2830: Variable Fare doesn’t have any outliers.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2831: Variable Age presents some outliers.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2832: At least 60 of the variables present outliers.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2833: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2834: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2835: Considering the common semantics for Fare and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2836: Considering the common semantics for Fare variable, dummification would be the most adequate encoding.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2837: The variable Age can be coded as ordinal without losing information.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2838: Feature generation based on variable SibSp seems to be promising.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2839: Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Pclass seems to be promising.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2840: Given the usual semantics of Age variable, dummification would have been a better codification.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2841: It is better to drop the variable SibSp than removing all records with missing values.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2842: Not knowing the semantics of Parch variable, dummification could have been a more adequate codification.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2843: It is clear that variable Sweetness is one of the four most relevant features.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2844: The variable Sweetness seems to be one of the three most relevant features.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2845: The variable Size discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2846: It is possible to state that Crunchiness is the second most discriminative variable regarding the class.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2847: Variable Juiciness is one of the most relevant variables.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2848: Variable Crunchiness seems to be relevant for the majority of mining tasks.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2849: Variables Sweetness and Acidity seem to be useful for classification tasks.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2850: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2851: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2852: The precision for the presented tree is higher than 90%.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2853: The number of True Positives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2854: The number of False Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2855: The number of True Positives reported in the same tree is 50.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2856: Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that KNN algorithm classifies (not A, not B) as good for any k ≤ 1625.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2857: Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that KNN algorithm classifies (not A, not B) as good for any k ≤ 148.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2858: Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that KNN algorithm classifies (A, not B) as bad for any k ≤ 148.</p>
    <img src="images/apple_quality_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2859: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/apple_quality_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2860: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/apple_quality_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2861: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/apple_quality_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2862: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/apple_quality_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2863: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2864: KNN is in overfitting for k larger than 17.</p>
    <img src="images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2865: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2866: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2867: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/apple_quality_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2868: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/apple_quality_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2869: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/apple_quality_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2870: The decision tree is in overfitting for depths above 5.</p>
    <img src="images/apple_quality_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2871: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/apple_quality_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2872: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/apple_quality_pca.png" width="auto" height = "600"/> 
    <p>2873: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/apple_quality_pca.png" width="auto" height = "600"/> 
    <p>2874: Using the first 4 principal components would imply an error between 10 and 30%.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2875: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2876: One of the variables Weight or Ripeness can be discarded without losing information.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2877: The variable Juiciness can be discarded without risking losing information.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2878: Variables Sweetness and Ripeness seem to be useful for classification tasks.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2879: Variables Size and Ripeness are redundant.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2880: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2881: Variable Ripeness seems to be relevant for the majority of mining tasks.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2882: Variables Size and Juiciness seem to be useful for classification tasks.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2883: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2884: Removing variable Size might improve the training of decision trees .</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2885: There is evidence in favour for sequential backward selection to select variable Acidity previously than variable Size.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2886: Variable Ripeness is balanced.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2887: Those boxplots show that the data is not normalized.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2888: It is clear that variable Juiciness shows some outliers, but we can’t be sure of the same for variable Sweetness.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2889: Outliers seem to be a problem in the dataset.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2890: Variable Crunchiness shows a high number of outlier values.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2891: Variable Acidity doesn’t have any outliers.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2892: Variable Ripeness presents some outliers.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2893: At least 85 of the variables present outliers.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2894: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2895: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2896: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2897: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2898: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2899: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/apple_quality_class_histogram.png" width="auto" height = "600"/> 
    <p>2900: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/apple_quality_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2901: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/apple_quality_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2902: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/apple_quality_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2903: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2904: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2905: The variable Ripeness can be seen as ordinal.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2906: The variable Acidity can be seen as ordinal without losing information.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2907: Variable Sweetness is balanced.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2908: It is clear that variable Ripeness shows some outliers, but we can’t be sure of the same for variable Juiciness.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2909: Outliers seem to be a problem in the dataset.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2910: Variable Ripeness shows some outlier values.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2911: Variable Weight doesn’t have any outliers.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2912: Variable Juiciness presents some outliers.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2913: At least 75 of the variables present outliers.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2914: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2915: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2916: Considering the common semantics for Sweetness and Size variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2917: Considering the common semantics for Sweetness variable, dummification would be the most adequate encoding.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2918: The variable Juiciness can be coded as ordinal without losing information.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2919: Feature generation based on variable Juiciness seems to be promising.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2920: Feature generation based on the use of variable Acidity wouldn’t be useful, but the use of Size seems to be promising.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2921: Given the usual semantics of Ripeness variable, dummification would have been a better codification.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2922: It is better to drop the variable Juiciness than removing all records with missing values.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2923: Not knowing the semantics of Crunchiness variable, dummification could have been a more adequate codification.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2924: It is clear that variable Age is one of the four most relevant features.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2925: The variable JoiningYear seems to be one of the four most relevant features.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2926: The variable ExperienceInCurrentDomain discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2927: It is possible to state that Age is the second most discriminative variable regarding the class.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2928: Variable ExperienceInCurrentDomain is one of the most relevant variables.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2929: Variable JoiningYear seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2930: Variables JoiningYear and ExperienceInCurrentDomain seem to be useful for classification tasks.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2931: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2932: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2933: The precision for the presented tree is lower than 75%.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2934: The number of False Positives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2935: The number of True Positives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2936: The number of False Positives reported in the same tree is 10.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2937: Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 44.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2938: Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 1.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2939: Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 1215.</p>
    <img src="images/Employee_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2940: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Employee_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2941: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/Employee_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2942: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/Employee_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2943: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Employee_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2944: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Employee_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2945: KNN is in overfitting for k larger than 5.</p>
    <img src="images/Employee_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2946: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/Employee_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2947: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/Employee_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2948: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/Employee_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2949: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Employee_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2950: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.</p>
    <img src="images/Employee_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2951: The decision tree is in overfitting for depths above 3.</p>
    <img src="images/Employee_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2952: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Employee_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2953: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Employee_pca.png" width="auto" height = "600"/> 
    <p>2954: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/Employee_pca.png" width="auto" height = "600"/> 
    <p>2955: Using the first 2 principal components would imply an error between 15 and 20%.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2956: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2957: One of the variables JoiningYear or PaymentTier can be discarded without losing information.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2958: The variable PaymentTier can be discarded without risking losing information.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2959: Variables ExperienceInCurrentDomain and JoiningYear are redundant.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2960: Variables JoiningYear and ExperienceInCurrentDomain are redundant.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2961: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2962: Variable PaymentTier seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2963: Variables ExperienceInCurrentDomain and PaymentTier seem to be useful for classification tasks.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2964: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2965: Removing variable PaymentTier might improve the training of decision trees .</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2966: There is evidence in favour for sequential backward selection to select variable Age previously than variable ExperienceInCurrentDomain.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2967: Variable ExperienceInCurrentDomain is balanced.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2968: Those boxplots show that the data is not normalized.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2969: It is clear that variable ExperienceInCurrentDomain shows some outliers, but we can’t be sure of the same for variable PaymentTier.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2970: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2971: Variable PaymentTier shows a high number of outlier values.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2972: Variable ExperienceInCurrentDomain doesn’t have any outliers.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2973: Variable Age presents some outliers.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2974: At least 60 of the variables present outliers.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2975: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2976: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2977: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2978: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2979: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2980: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2981: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2982: The variable EverBenched can be seen as ordinal.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2983: The variable EverBenched can be seen as ordinal without losing information.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2984: Considering the common semantics for EverBenched and Education variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2985: Considering the common semantics for Gender variable, dummification would be the most adequate encoding.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2986: The variable Gender can be coded as ordinal without losing information.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2987: Feature generation based on variable Education seems to be promising.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2988: Feature generation based on the use of variable Gender wouldn’t be useful, but the use of Education seems to be promising.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2989: Given the usual semantics of Education variable, dummification would have been a better codification.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2990: It is better to drop the variable Gender than removing all records with missing values.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2991: Not knowing the semantics of City variable, dummification could have been a more adequate codification.</p>
    <img src="images/Employee_class_histogram.png" width="auto" height = "600"/> 
    <p>2992: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Employee_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2993: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Employee_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2994: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Employee_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2995: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2996: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2997: The variable JoiningYear can be seen as ordinal.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2998: The variable Age can be seen as ordinal without losing information.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2999: Variable PaymentTier is balanced.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3000: It is clear that variable JoiningYear shows some outliers, but we can’t be sure of the same for variable ExperienceInCurrentDomain.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3001: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3002: Variable JoiningYear shows a high number of outlier values.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3003: Variable JoiningYear doesn’t have any outliers.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3004: Variable ExperienceInCurrentDomain presents some outliers.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3005: At least 60 of the variables present outliers.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3006: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3007: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3008: Considering the common semantics for PaymentTier and JoiningYear variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3009: Considering the common semantics for ExperienceInCurrentDomain variable, dummification would be the most adequate encoding.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3010: The variable ExperienceInCurrentDomain can be coded as ordinal without losing information.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3011: Feature generation based on variable JoiningYear seems to be promising.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3012: Feature generation based on the use of variable Age wouldn’t be useful, but the use of JoiningYear seems to be promising.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3013: Given the usual semantics of PaymentTier variable, dummification would have been a better codification.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3014: It is better to drop the variable JoiningYear than removing all records with missing values.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>3015: Not knowing the semantics of Age variable, dummification could have been a more adequate codification.</p>
</body> 
</html> 
