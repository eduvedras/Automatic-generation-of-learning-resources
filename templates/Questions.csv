Question;Charts
Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;[Nr records x nr variables]
We face the curse of dimensionality when training a classifier with this dataset.;[Nr records x nr variables]
The intrinsic dimensionality of this dataset is 62.;[PCA,Correlation heatmap]
The intrinsic dimensionality of this dataset is 37.;[PCA,Correlation heatmap]
The intrinsic dimensionality of this dataset is 68.;[PCA,Correlation heatmap]
The intrinsic dimensionality of this dataset is 54.;[PCA,Correlation heatmap]
The intrinsic dimensionality of this dataset is 16.;[PCA,Correlation heatmap]
The figure doesn’t show any missing values for PetalWidthCm, but these may be hidden as some non-pre-identified value.;[Histograms]
The figure doesn’t show any missing values for PetalLengthCm, but these may be hidden as some non-pre-identified value.;[Histograms]
The figure doesn’t show any missing values for SepalWidthCm, but these may be hidden as some non-pre-identified value.;[Histograms]
The figure doesn’t show any missing values for SepalLengthCm, but these may be hidden as some non-pre-identified value.;[Histograms]
All variables, but the class, should be dealt with as symbolic.;[Scatter-plots,Histograms]
All variables, but the class, should be dealt with as date.;[Scatter-plots,Histograms]
All variables, but the class, should be dealt with as binary.;[Scatter-plots,Histograms]
All variables, but the class, should be dealt with as numeric.;[Scatter-plots,Histograms]
The variable PetalWidthCm can be seen as ordinal.;[Scatter-plots,Histograms]
The variable PetalLengthCm can be seen as ordinal.;[Scatter-plots,Histograms]
The variable SepalWidthCm can be seen as ordinal.;[Scatter-plots,Histograms]
The variable SepalLengthCm can be seen as ordinal.;[Scatter-plots,Histograms]
The variable PetalWidthCm can be seen as ordinal without losing information.;[Scatter-plots,Histograms]
The variable PetalLengthCm can be seen as ordinal without losing information.;[Scatter-plots,Histograms]
The variable SepalWidthCm can be seen as ordinal without losing information.;[Scatter-plots,Histograms]
The variable SepalLengthCm can be seen as ordinal without losing information.;[Scatter-plots,Histograms]
Variable PetalWidthCm is balanced.;[Histograms,Boxplots]
Variable PetalLengthCm is balanced.;[Histograms,Boxplots]
Variable SepalWidthCm is balanced.;[Histograms,Boxplots]
Variable SepalLengthCm is balanced.;[Histograms,Boxplots]
Those boxplots show that the data is not normalized.;[Boxplot global,Single boxplots]
It is clear that variable PetalLengthCm shows some outliers, but we can’t be sure of the same for variable PetalWidthCm.;[Boxplots,Histograms]
It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalWidthCm.;[Boxplots,Histograms]
It is clear that variable SepalLengthCm shows some outliers, but we can’t be sure of the same for variable PetalWidthCm.;[Boxplots,Histograms]
It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.;[Boxplots,Histograms]
It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.;[Boxplots,Histograms]
It is clear that variable SepalLengthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.;[Boxplots,Histograms]
It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalWidthCm.;[Boxplots,Histograms]
It is clear that variable PetalLengthCm shows some outliers, but we can’t be sure of the same for variable SepalWidthCm.;[Boxplots,Histograms]
It is clear that variable SepalLengthCm shows some outliers, but we can’t be sure of the same for variable SepalWidthCm.;[Boxplots,Histograms]
It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.;[Boxplots,Histograms]
It is clear that variable PetalLengthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.;[Boxplots,Histograms]
It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.;[Boxplots,Histograms]
Outliers seem to be a problem in the dataset.;[Boxplots,Histograms]
Variable PetalWidthCm shows a high number of outlier values.;[Boxplots,Histograms]
Variable PetalLengthCm shows a high number of outlier values.;[Boxplots,Histograms]
Variable SepalWidthCm shows a high number of outlier values.;[Boxplots,Histograms]
Variable SepalLengthCm shows a high number of outlier values.;[Boxplots,Histograms]
Variable PetalWidthCm shows some outlier values.;[Boxplots,Histograms]
Variable PetalLengthCm shows some outlier values.;[Boxplots,Histograms]
Variable SepalWidthCm shows some outlier values.;[Boxplots,Histograms]
Variable SepalLengthCm shows some outlier values.;[Boxplots,Histograms]
Variable PetalWidthCm doesn’t have any outliers.;[Boxplots,Histograms]
Variable PetalLengthCm doesn’t have any outliers.;[Boxplots,Histograms]
Variable SepalWidthCm doesn’t have any outliers.;[Boxplots,Histograms]
Variable SepalLengthCm doesn’t have any outliers.;[Boxplots,Histograms]
Variable PetalWidthCm presents some outliers.;[Boxplots,Histograms]
Variable PetalLengthCm presents some outliers.;[Boxplots,Histograms]
Variable SepalWidthCm presents some outliers.;[Boxplots,Histograms]
Variable SepalLengthCm presents some outliers.;[Boxplots,Histograms]
The histograms presented show a large number of outliers for most of the numeric variables.;[Boxplots,Histograms]
The boxplots presented show a large number of outliers for most of the numeric variables.;[Boxplots,Histograms]
The existence of outliers is one of the problems to tackle in this dataset.;[Boxplots,Histograms]
Variable Species is a false predictor.;[Description]
Variable PetalWidthCm is a false predictor.;[Description]
Variable PetalLengthCm is a false predictor.;[Description]
Variable SepalWidthCm is a false predictor.;[Description]
Variable SepalLengthCm is a false predictor.;[Description]
One of the variables PetalLengthCm or PetalWidthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables SepalWidthCm or PetalWidthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables SepalLengthCm or PetalWidthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables PetalWidthCm or PetalLengthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables SepalWidthCm or PetalLengthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables SepalLengthCm or PetalLengthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables PetalWidthCm or SepalWidthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables PetalLengthCm or SepalWidthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables SepalLengthCm or SepalWidthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables PetalWidthCm or SepalLengthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables PetalLengthCm or SepalLengthCm can be discarded without losing information.;[Correlation heatmap]
One of the variables SepalWidthCm or SepalLengthCm can be discarded without losing information.;[Correlation heatmap]
Variables SepalLengthCm and SepalWidthCm are redundant, but we can’t say the same for the pair PetalLengthCm and PetalWidthCm.;[Correlation heatmap]
Variables SepalWidthCm and SepalLengthCm are redundant, but we can’t say the same for the pair PetalLengthCm and PetalWidthCm.;[Correlation heatmap]
Variables SepalLengthCm and PetalLengthCm are redundant, but we can’t say the same for the pair SepalWidthCm and PetalWidthCm.;[Correlation heatmap]
Variables PetalLengthCm and SepalLengthCm are redundant, but we can’t say the same for the pair SepalWidthCm and PetalWidthCm.;[Correlation heatmap]
Variables SepalWidthCm and PetalLengthCm are redundant, but we can’t say the same for the pair SepalLengthCm and PetalWidthCm.;[Correlation heatmap]
Variables PetalLengthCm and SepalWidthCm are redundant, but we can’t say the same for the pair SepalLengthCm and PetalWidthCm.;[Correlation heatmap]
Variables SepalLengthCm and SepalWidthCm are redundant, but we can’t say the same for the pair PetalWidthCm and PetalLengthCm.;[Correlation heatmap]
Variables SepalWidthCm and SepalLengthCm are redundant, but we can’t say the same for the pair PetalWidthCm and PetalLengthCm.;[Correlation heatmap]
Variables SepalLengthCm and PetalWidthCm are redundant, but we can’t say the same for the pair SepalWidthCm and PetalLengthCm.;[Correlation heatmap]
Variables PetalWidthCm and SepalLengthCm are redundant, but we can’t say the same for the pair SepalWidthCm and PetalLengthCm.;[Correlation heatmap]
Variables SepalWidthCm and PetalWidthCm are redundant, but we can’t say the same for the pair SepalLengthCm and PetalLengthCm.;[Correlation heatmap]
Variables PetalWidthCm and SepalWidthCm are redundant, but we can’t say the same for the pair SepalLengthCm and PetalLengthCm.;[Correlation heatmap]
Variables SepalLengthCm and PetalLengthCm are redundant, but we can’t say the same for the pair PetalWidthCm and SepalWidthCm.;[Correlation heatmap]
Variables PetalLengthCm and SepalLengthCm are redundant, but we can’t say the same for the pair PetalWidthCm and SepalWidthCm.;[Correlation heatmap]
Variables SepalLengthCm and PetalWidthCm are redundant, but we can’t say the same for the pair PetalLengthCm and SepalWidthCm.;[Correlation heatmap]
Variables PetalWidthCm and SepalLengthCm are redundant, but we can’t say the same for the pair PetalLengthCm and SepalWidthCm.;[Correlation heatmap]
Variables PetalLengthCm and PetalWidthCm are redundant, but we can’t say the same for the pair SepalLengthCm and SepalWidthCm.;[Correlation heatmap]
Variables PetalWidthCm and PetalLengthCm are redundant, but we can’t say the same for the pair SepalLengthCm and SepalWidthCm.;[Correlation heatmap]
Variables SepalWidthCm and PetalLengthCm are redundant, but we can’t say the same for the pair PetalWidthCm and SepalLengthCm.;[Correlation heatmap]
Variables PetalLengthCm and SepalWidthCm are redundant, but we can’t say the same for the pair PetalWidthCm and SepalLengthCm.;[Correlation heatmap]
Variables SepalWidthCm and PetalWidthCm are redundant, but we can’t say the same for the pair PetalLengthCm and SepalLengthCm.;[Correlation heatmap]
Variables PetalWidthCm and SepalWidthCm are redundant, but we can’t say the same for the pair PetalLengthCm and SepalLengthCm.;[Correlation heatmap]
Variables PetalLengthCm and PetalWidthCm are redundant, but we can’t say the same for the pair SepalWidthCm and SepalLengthCm.;[Correlation heatmap]
Variables PetalWidthCm and PetalLengthCm are redundant, but we can’t say the same for the pair SepalWidthCm and SepalLengthCm.;[Correlation heatmap]
Variables PetalLengthCm and PetalWidthCm are redundant.;[Correlation heatmap]
Variables SepalWidthCm and PetalWidthCm are redundant.;[Correlation heatmap]
Variables SepalLengthCm and PetalWidthCm are redundant.;[Correlation heatmap]
Variables PetalWidthCm and PetalLengthCm are redundant.;[Correlation heatmap]
Variables SepalWidthCm and PetalLengthCm are redundant.;[Correlation heatmap]
Variables SepalLengthCm and PetalLengthCm are redundant.;[Correlation heatmap]
Variables PetalWidthCm and SepalWidthCm are redundant.;[Correlation heatmap]
Variables PetalLengthCm and SepalWidthCm are redundant.;[Correlation heatmap]
Variables SepalLengthCm and SepalWidthCm are redundant.;[Correlation heatmap]
Variables PetalWidthCm and SepalLengthCm are redundant.;[Correlation heatmap]
Variables PetalLengthCm and SepalLengthCm are redundant.;[Correlation heatmap]
Variables SepalWidthCm and SepalLengthCm are redundant.;[Correlation heatmap]
From the correlation analysis alone, it is clear that there are relevant variables.;[Correlation heatmap]
It is clear that variable PetalWidthCm is one of the five most relevant features.;[Decision tree]
It is clear that variable PetalLengthCm is one of the five most relevant features.;[Decision tree]
It is clear that variable SepalWidthCm is one of the five most relevant features.;[Decision tree]
It is clear that variable SepalLengthCm is one of the five most relevant features.;[Decision tree]
It is clear that variable PetalWidthCm is one of the four most relevant features.;[Decision tree]
It is clear that variable PetalLengthCm is one of the four most relevant features.;[Decision tree]
It is clear that variable SepalWidthCm is one of the four most relevant features.;[Decision tree]
It is clear that variable SepalLengthCm is one of the four most relevant features.;[Decision tree]
It is clear that variable PetalWidthCm is one of the three most relevant features.;[Decision tree]
It is clear that variable PetalLengthCm is one of the three most relevant features.;[Decision tree]
It is clear that variable SepalWidthCm is one of the three most relevant features.;[Decision tree]
It is clear that variable SepalLengthCm is one of the three most relevant features.;[Decision tree]
It is clear that variable PetalWidthCm is one of the two most relevant features.;[Decision tree]
It is clear that variable PetalLengthCm is one of the two most relevant features.;[Decision tree]
It is clear that variable SepalWidthCm is one of the two most relevant features.;[Decision tree]
It is clear that variable SepalLengthCm is one of the two most relevant features.;[Decision tree]
The variable PetalWidthCm seems to be one of the five most relevant features.;[Decision tree]
The variable PetalLengthCm seems to be one of the five most relevant features.;[Decision tree]
The variable SepalWidthCm seems to be one of the five most relevant features.;[Decision tree]
The variable SepalLengthCm seems to be one of the five most relevant features.;[Decision tree]
The variable PetalWidthCm seems to be one of the four most relevant features.;[Decision tree]
The variable PetalLengthCm seems to be one of the four most relevant features.;[Decision tree]
The variable SepalWidthCm seems to be one of the four most relevant features.;[Decision tree]
The variable SepalLengthCm seems to be one of the four most relevant features.;[Decision tree]
The variable PetalWidthCm seems to be one of the three most relevant features.;[Decision tree]
The variable PetalLengthCm seems to be one of the three most relevant features.;[Decision tree]
The variable SepalWidthCm seems to be one of the three most relevant features.;[Decision tree]
The variable SepalLengthCm seems to be one of the three most relevant features.;[Decision tree]
The variable PetalWidthCm seems to be one of the two most relevant features.;[Decision tree]
The variable PetalLengthCm seems to be one of the two most relevant features.;[Decision tree]
The variable SepalWidthCm seems to be one of the two most relevant features.;[Decision tree]
The variable SepalLengthCm seems to be one of the two most relevant features.;[Decision tree]
The variable PetalWidthCm discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable PetalLengthCm discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable SepalWidthCm discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable SepalLengthCm discriminates between the target values, as shown in the decision tree.;[Decision tree]
Variable PetalWidthCm is one of the most relevant variables.;[Decision tree]
Variable PetalLengthCm is one of the most relevant variables.;[Decision tree]
Variable SepalWidthCm is one of the most relevant variables.;[Decision tree]
Variable SepalLengthCm is one of the most relevant variables.;[Decision tree]
Variable PetalWidthCm seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable PetalLengthCm seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable SepalWidthCm seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable SepalLengthCm seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variables PetalLengthCm and PetalWidthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SepalWidthCm and PetalWidthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SepalLengthCm and PetalWidthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables PetalWidthCm and PetalLengthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SepalWidthCm and PetalLengthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SepalLengthCm and PetalLengthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables PetalWidthCm and SepalWidthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables PetalLengthCm and SepalWidthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SepalLengthCm and SepalWidthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables PetalWidthCm and SepalLengthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables PetalLengthCm and SepalLengthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SepalWidthCm and SepalLengthCm seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;[Boxplot global,Single boxplots]
A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;[Boxplot global,Single boxplots]
Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;[Correlation heatmap]
Balancing this dataset by SMOTE would be riskier than oversampling by replication.;[Scatter-plots]
Balancing this dataset by SMOTE would most probably be preferable over undersampling.;[Nr records x nr variables]
Balancing this dataset would be mandatory to improve the results.;[Class histogram]
Both PetalLengthCm and PetalWidthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SepalWidthCm and PetalWidthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SepalLengthCm and PetalWidthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both PetalWidthCm and PetalLengthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SepalWidthCm and PetalLengthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SepalLengthCm and PetalLengthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both PetalWidthCm and SepalWidthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both PetalLengthCm and SepalWidthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SepalLengthCm and SepalWidthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both PetalWidthCm and SepalLengthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both PetalLengthCm and SepalLengthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SepalWidthCm and SepalLengthCm variables could be used to derive a new variable using a concept hierarchy.;[Description]
Considering the common semantics for PetalLengthCm and PetalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for SepalWidthCm and PetalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for SepalLengthCm and PetalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for PetalWidthCm and PetalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for SepalWidthCm and PetalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for SepalLengthCm and PetalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for PetalWidthCm and SepalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for PetalLengthCm and SepalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for SepalLengthCm and SepalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for PetalWidthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for PetalLengthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for SepalWidthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,Histograms]
Considering the common semantics for PetalWidthCm variable, dummification would be the most adequate encoding.;[Description,Histograms]
Considering the common semantics for PetalLengthCm variable, dummification would be the most adequate encoding.;[Description,Histograms]
Considering the common semantics for SepalWidthCm variable, dummification would be the most adequate encoding.;[Description,Histograms]
Considering the common semantics for SepalLengthCm variable, dummification would be the most adequate encoding.;[Description,Histograms]
Discarding variables PetalLengthCm and PetalWidthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SepalWidthCm and PetalWidthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SepalLengthCm and PetalWidthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables PetalWidthCm and PetalLengthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SepalWidthCm and PetalLengthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SepalLengthCm and PetalLengthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables PetalWidthCm and SepalWidthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables PetalLengthCm and SepalWidthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SepalLengthCm and SepalWidthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables PetalWidthCm and SepalLengthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables PetalLengthCm and SepalLengthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SepalWidthCm and SepalLengthCm would be better than discarding all the records with missing values for those variables.;[Missing values]
Dropping all records with missing values would be better than to drop the variables with missing values.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 40% of the original data.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.;[Missing values]
Dummification is mandatory in this dataset.;[]
Dummifying the variables will improve the mining results.;[]
Feature generation based on both variables PetalLengthCm and PetalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables SepalWidthCm and PetalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables SepalLengthCm and PetalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables PetalWidthCm and PetalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables SepalWidthCm and PetalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables SepalLengthCm and PetalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables PetalWidthCm and SepalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables PetalLengthCm and SepalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables SepalLengthCm and SepalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables PetalWidthCm and SepalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables PetalLengthCm and SepalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on both variables SepalWidthCm and SepalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of PetalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable SepalWidthCm wouldn’t be useful, but the use of PetalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable SepalLengthCm wouldn’t be useful, but the use of PetalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable PetalWidthCm wouldn’t be useful, but the use of PetalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable SepalWidthCm wouldn’t be useful, but the use of PetalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable SepalLengthCm wouldn’t be useful, but the use of PetalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable PetalWidthCm wouldn’t be useful, but the use of SepalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of SepalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable SepalLengthCm wouldn’t be useful, but the use of SepalWidthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable PetalWidthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.;[Missing values,Histograms]
Feature generation based on the use of variable SepalWidthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.;[Missing values,Histograms]
Given the usual semantics of PetalWidthCm variable, dummification would have been a better codification.;[Histograms]
Given the usual semantics of PetalLengthCm variable, dummification would have been a better codification.;[Histograms]
Given the usual semantics of SepalWidthCm variable, dummification would have been a better codification.;[Histograms]
Given the usual semantics of SepalLengthCm variable, dummification would have been a better codification.;[Histograms]
If PetalLengthCm and PetalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalWidthCm and PetalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalLengthCm and PetalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalWidthCm and PetalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalWidthCm and PetalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalLengthCm and PetalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalWidthCm and SepalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalLengthCm and SepalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalLengthCm and SepalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalWidthCm and SepalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalLengthCm and SepalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalWidthCm and SepalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
It is better to drop the variable PetalWidthCm than removing all records with missing values.;[Missing values,Histograms]
It is better to drop the variable PetalLengthCm than removing all records with missing values.;[Missing values,Histograms]
It is better to drop the variable SepalWidthCm than removing all records with missing values.;[Missing values,Histograms]
It is better to drop the variable SepalLengthCm than removing all records with missing values.;[Missing values,Histograms]
Knowing that C and F are strongly correlated (correlation=1), we can say that removing one of those variables, would not have any impact on the performance of a KNN classifier.;[]
Missing value imputation using the mean value per class improves the quality of discovered patterns.;[]
Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;[Boxplots]
Normalization of this dataset could not have impact on a KNN classifier.;[Boxplot global,Single boxplots]
Normalization of this dataset should have a high impact on naïve Bayes classifier.;[]
Not knowing the semantics of PetalWidthCm variable, dummification could have been a more adequate codification.;[Histograms]
Not knowing the semantics of PetalLengthCm variable, dummification could have been a more adequate codification.;[Histograms]
Not knowing the semantics of SepalWidthCm variable, dummification could have been a more adequate codification.;[Histograms]
Not knowing the semantics of SepalLengthCm variable, dummification could have been a more adequate codification.;[Histograms]
Removing the Species variable from the training will improve model performance over any non-observed records.;[Description]
Removing the PetalWidthCm variable from the training will improve model performance over any non-observed records.;[Description]
Removing the PetalLengthCm variable from the training will improve model performance over any non-observed records.;[Description]
Removing the SepalWidthCm variable from the training will improve model performance over any non-observed records.;[Description]
Removing the SepalLengthCm variable from the training will improve model performance over any non-observed records.;[Description]
Removing variable PetalWidthCm might improve the training of decision trees .;[Correlation heatmap]
Removing variable PetalLengthCm might improve the training of decision trees .;[Correlation heatmap]
Removing variable SepalWidthCm might improve the training of decision trees .;[Correlation heatmap]
Removing variable SepalLengthCm might improve the training of decision trees .;[Correlation heatmap]
Scaling this dataset would be mandatory to improve the results with distance-based methods.;[Boxplots]
The first 10 principal components are enough for explaining half the data variance.;[PCA]
The first 5 principal components are enough for explaining half the data variance.;[PCA]
There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable PetalWidthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SepalWidthCm previously than variable PetalWidthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SepalLengthCm previously than variable PetalWidthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable PetalWidthCm previously than variable PetalLengthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SepalWidthCm previously than variable PetalLengthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SepalLengthCm previously than variable PetalLengthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable PetalWidthCm previously than variable SepalWidthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable SepalWidthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SepalLengthCm previously than variable SepalWidthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable PetalWidthCm previously than variable SepalLengthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable SepalLengthCm.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SepalWidthCm previously than variable SepalLengthCm.;[Correlation heatmap]
Using the first 30 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 20 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 10 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 30 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 20 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 10 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 30 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 20 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 10 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 30 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 20 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 10 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 30 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 20 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 10 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 30 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 20 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 10 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 30 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 20 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 10 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 30 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 20 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 10 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 30 principal components would imply an error between 5 and 20%.;[PCA]
Using the first 20 principal components would imply an error between 5 and 20%.;[PCA]
Using the first 10 principal components would imply an error between 5 and 20%.;[PCA]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 9%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 7%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;[Decision tree]
According to the decision tree overfitting chart, the tree with 8 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 7 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 6 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 4 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 3 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the charts, KNN and Decision Trees present a similar behaviour.;[[Overfitting Decision Tree,Overfitting KNN]]
As reported in the tree, the number of False Positive is bigger than the number of False Negatives.;[Decision tree]
As reported in the tree, the number of False Positive is smaller than the number of False Negatives.;[Decision tree]
Decision trees and KNN show similar behaviours.;[[Overfitting Decision Tree,Overfitting KNN]]
KNN and Decision Trees show a different trend in the majority of hyperparameters tested.;[[Overfitting Decision Tree,Overfitting KNN]]
KNN is in overfitting for k larger than 8.;[Overfitting KNN]
KNN is in overfitting for k less than 8.;[Overfitting KNN]
KNN is in overfitting for k larger than 7.;[Overfitting KNN]
KNN is in overfitting for k less than 7.;[Overfitting KNN]
KNN is in overfitting for k larger than 6.;[Overfitting KNN]
KNN is in overfitting for k less than 6.;[Overfitting KNN]
KNN is in overfitting for k larger than 5.;[Overfitting KNN]
KNN is in overfitting for k less than 5.;[Overfitting KNN]
KNN is in overfitting for k larger than 4.;[Overfitting KNN]
KNN is in overfitting for k less than 4.;[Overfitting KNN]
KNN is in overfitting for k larger than 3.;[Overfitting KNN]
KNN is in overfitting for k less than 3.;[Overfitting KNN]
KNN is in overfitting for k larger than 2.;[Overfitting KNN]
KNN is in overfitting for k less than 2.;[Overfitting KNN]
KNN with 10 neighbour is in overfitting.;[Overfitting KNN]
KNN with 9 neighbour is in overfitting.;[Overfitting KNN]
KNN with 8 neighbour is in overfitting.;[Overfitting KNN]
KNN with 7 neighbour is in overfitting.;[Overfitting KNN]
KNN with 6 neighbour is in overfitting.;[Overfitting KNN]
KNN with 5 neighbour is in overfitting.;[Overfitting KNN]
KNN with 4 neighbour is in overfitting.;[Overfitting KNN]
KNN with 3 neighbour is in overfitting.;[Overfitting KNN]
KNN with 2 neighbour is in overfitting.;[Overfitting KNN]
KNN with 1 neighbour is in overfitting.;[Overfitting KNN]
KNN with less than 8 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 8 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 7 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 7 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 6 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 6 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 5 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 5 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 4 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 4 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 3 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 3 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 2 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 2 neighbours is in overfitting.;[Overfitting KNN]
Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;[Overfitting RF]
Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;[Overfitting RF]
Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;[Overfitting RF]
The specificity for the presented tree is lower than 72%.;[Decision tree]
The precision for the presented tree is lower than 79%.;[Decision tree]
The recall for the presented tree is lower than 81%.;[Decision tree]
The accuracy for the presented tree is lower than 82%.;[Decision tree]
The specificity for the presented tree is higher than 60%.;[Decision tree]
The precision for the presented tree is higher than 80%.;[Decision tree]
The recall for the presented tree is higher than 66%.;[Decision tree]
The accuracy for the presented tree is higher than 90%.;[Decision tree]
The specificity for the presented tree is lower than 73%.;[Decision tree]
The precision for the presented tree is lower than 84%.;[Decision tree]
The recall for the presented tree is lower than 60%.;[Decision tree]
The accuracy for the presented tree is lower than 89%.;[Decision tree]
The specificity for the presented tree is higher than 62%.;[Decision tree]
The precision for the presented tree is higher than 63%.;[Decision tree]
The recall for the presented tree is higher than 83%.;[Decision tree]
The accuracy for the presented tree is higher than 82%.;[Decision tree]
The specificity for the presented tree is lower than 68%.;[Decision tree]
The precision for the presented tree is lower than 64%.;[Decision tree]
The recall for the presented tree is lower than 79%.;[Decision tree]
The accuracy for the presented tree is lower than 85%.;[Decision tree]
The specificity for the presented tree is higher than 61%.;[Decision tree]
The precision for the presented tree is higher than 81%.;[Decision tree]
The recall for the presented tree is higher than 90%.;[Decision tree]
The accuracy for the presented tree is higher than 86%.;[Decision tree]
The specificity for the presented tree is lower than 80%.;[Decision tree]
The precision for the presented tree is lower than 74%.;[Decision tree]
The recall for the presented tree is lower than 69%.;[Decision tree]
The accuracy for the presented tree is lower than 71%.;[Decision tree]
The specificity for the presented tree is higher than 73%.;[Decision tree]
The precision for the presented tree is higher than 88%.;[Decision tree]
The recall for the presented tree is higher than 78%.;[Decision tree]
The accuracy for the presented tree is higher than 75%.;[Decision tree]
The specificity for the presented tree is lower than 76%.;[Decision tree]
The precision for the presented tree is lower than 77%.;[Decision tree]
The recall for the presented tree is lower than 67%.;[Decision tree]
The accuracy for the presented tree is lower than 65%.;[Decision tree]
The specificity for the presented tree is higher than 87%.;[Decision tree]
The precision for the presented tree is higher than 72%.;[Decision tree]
The recall for the presented tree is higher than 70%.;[Decision tree]
The accuracy for the presented tree is higher than 66%.;[Decision tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 10.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 9.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 8.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 7.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 6.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 5.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 4.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 3.;[Overfitting Decision Tree]
The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;[Overfitting Accuracy + Recall]
The number of False Negatives reported in the same tree is 14.;[Decision tree]
The number of True Negatives reported in the same tree is 46.;[Decision tree]
The number of False Positives reported in the same tree is 26.;[Decision tree]
The number of True Positives reported in the same tree is 32.;[Decision tree]
The number of False Negatives reported in the same tree is 31.;[Decision tree]
The number of True Negatives reported in the same tree is 39.;[Decision tree]
The number of False Positives reported in the same tree is 40.;[Decision tree]
The number of True Positives reported in the same tree is 29.;[Decision tree]
The number of False Negatives reported in the same tree is 23.;[Decision tree]
The number of True Negatives reported in the same tree is 13.;[Decision tree]
The number of False Positives reported in the same tree is 17.;[Decision tree]
The number of True Positives reported in the same tree is 50.;[Decision tree]
The number of False Negatives reported in the same tree is 12.;[Decision tree]
The number of True Negatives reported in the same tree is 43.;[Decision tree]
The number of False Positives reported in the same tree is 18.;[Decision tree]
The number of True Positives reported in the same tree is 10.;[Decision tree]
The number of False Negatives reported in the same tree is 44.;[Decision tree]
The number of True Negatives reported in the same tree is 22.;[Decision tree]
The number of False Positives reported in the same tree is 20.;[Decision tree]
The number of True Positives reported in the same tree is 11.;[Decision tree]
The number of True Negatives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of True Negatives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of True Negatives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of True Positives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of True Positives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of True Positives for the presented tree.;[Decision tree]
The precision for the presented tree is lower than its specificity.;[Decision tree]
The recall for the presented tree is lower than its specificity.;[Decision tree]
The accuracy for the presented tree is lower than its specificity.;[Decision tree]
The precision for the presented tree is higher than its specificity.;[Decision tree]
The recall for the presented tree is higher than its specificity.;[Decision tree]
The accuracy for the presented tree is higher than its specificity.;[Decision tree]
The specificity for the presented tree is lower than its precision.;[Decision tree]
The recall for the presented tree is lower than its precision.;[Decision tree]
The accuracy for the presented tree is lower than its precision.;[Decision tree]
The specificity for the presented tree is higher than its precision.;[Decision tree]
The recall for the presented tree is higher than its precision.;[Decision tree]
The accuracy for the presented tree is higher than its precision.;[Decision tree]
The specificity for the presented tree is lower than its recall.;[Decision tree]
The precision for the presented tree is lower than its recall.;[Decision tree]
The accuracy for the presented tree is lower than its recall.;[Decision tree]
The specificity for the presented tree is higher than its recall.;[Decision tree]
The precision for the presented tree is higher than its recall.;[Decision tree]
The accuracy for the presented tree is higher than its recall.;[Decision tree]
The specificity for the presented tree is lower than its accuracy.;[Decision tree]
The precision for the presented tree is lower than its accuracy.;[Decision tree]
The recall for the presented tree is lower than its accuracy.;[Decision tree]
The specificity for the presented tree is higher than its accuracy.;[Decision tree]
The precision for the presented tree is higher than its accuracy.;[Decision tree]
The recall for the presented tree is higher than its accuracy.;[Decision tree]
The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;[Overfitting RF]
We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for random forest models with more than 71 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 84 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 65 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 107 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 51 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for gradient boosting models with more than 111 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 86 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 125 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 107 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 97 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for MLP models trained longer than 524 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 1167 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 864 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 703 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 932 episodes.;[Overfitting MLP]
