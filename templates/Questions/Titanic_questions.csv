Question;Charts
Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;[Nr records x nr variables]
We face the curse of dimensionality when training a classifier with this dataset.;[Nr records x nr variables]
The intrinsic dimensionality of this dataset is 36.;[Correlation heatmap]
The intrinsic dimensionality of this dataset is 30.;[Correlation heatmap]
The intrinsic dimensionality of this dataset is 81.;[Correlation heatmap]
The intrinsic dimensionality of this dataset is 42.;[Correlation heatmap]
The intrinsic dimensionality of this dataset is 76.;[Correlation heatmap]
The figure doesn’t show any missing values for Embarked, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for Fare, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for Parch, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for SibSp, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for Age, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for Sex, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for Pclass, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
All variables, but the class, should be dealt with as symbolic.;[Scatter-plots,All_Histograms]
All variables, but the class, should be dealt with as date.;[Scatter-plots,All_Histograms]
All variables, but the class, should be dealt with as binary.;[Scatter-plots,All_Histograms]
All variables, but the class, should be dealt with as numeric.;[Scatter-plots,All_Histograms]
The variable Embarked can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable Fare can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable Parch can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable SibSp can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable Age can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable Sex can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable Pclass can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable Embarked can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable Fare can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable Parch can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable SibSp can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable Age can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable Sex can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable Pclass can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
Variable Fare is balanced.;[Histograms,Boxplots]
Variable Parch is balanced.;[Histograms,Boxplots]
Variable SibSp is balanced.;[Histograms,Boxplots]
Variable Age is balanced.;[Histograms,Boxplots]
Variable Pclass is balanced.;[Histograms,Boxplots]
Those boxplots show that the data is not normalized.;[Boxplot global,Single boxplots]
It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable Fare.;[Boxplots,Histograms]
It is clear that variable SibSp shows some outliers, but we can’t be sure of the same for variable Fare.;[Boxplots,Histograms]
It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Fare.;[Boxplots,Histograms]
It is clear that variable Pclass shows some outliers, but we can’t be sure of the same for variable Fare.;[Boxplots,Histograms]
It is clear that variable Fare shows some outliers, but we can’t be sure of the same for variable Parch.;[Boxplots,Histograms]
It is clear that variable SibSp shows some outliers, but we can’t be sure of the same for variable Parch.;[Boxplots,Histograms]
It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Parch.;[Boxplots,Histograms]
It is clear that variable Pclass shows some outliers, but we can’t be sure of the same for variable Parch.;[Boxplots,Histograms]
It is clear that variable Fare shows some outliers, but we can’t be sure of the same for variable SibSp.;[Boxplots,Histograms]
It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable SibSp.;[Boxplots,Histograms]
It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable SibSp.;[Boxplots,Histograms]
It is clear that variable Pclass shows some outliers, but we can’t be sure of the same for variable SibSp.;[Boxplots,Histograms]
It is clear that variable Fare shows some outliers, but we can’t be sure of the same for variable Age.;[Boxplots,Histograms]
It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable Age.;[Boxplots,Histograms]
It is clear that variable SibSp shows some outliers, but we can’t be sure of the same for variable Age.;[Boxplots,Histograms]
It is clear that variable Pclass shows some outliers, but we can’t be sure of the same for variable Age.;[Boxplots,Histograms]
It is clear that variable Fare shows some outliers, but we can’t be sure of the same for variable Pclass.;[Boxplots,Histograms]
It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable Pclass.;[Boxplots,Histograms]
It is clear that variable SibSp shows some outliers, but we can’t be sure of the same for variable Pclass.;[Boxplots,Histograms]
It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Pclass.;[Boxplots,Histograms]
Outliers seem to be a problem in the dataset.;[Boxplots,Histograms]
Variable Fare shows a high number of outlier values.;[Boxplots,Histograms]
Variable Parch shows a high number of outlier values.;[Boxplots,Histograms]
Variable SibSp shows a high number of outlier values.;[Boxplots,Histograms]
Variable Age shows a high number of outlier values.;[Boxplots,Histograms]
Variable Pclass shows a high number of outlier values.;[Boxplots,Histograms]
Variable Fare shows some outlier values.;[Boxplots,Histograms]
Variable Parch shows some outlier values.;[Boxplots,Histograms]
Variable SibSp shows some outlier values.;[Boxplots,Histograms]
Variable Age shows some outlier values.;[Boxplots,Histograms]
Variable Pclass shows some outlier values.;[Boxplots,Histograms]
Variable Fare doesn’t have any outliers.;[Boxplots,Histograms]
Variable Parch doesn’t have any outliers.;[Boxplots,Histograms]
Variable SibSp doesn’t have any outliers.;[Boxplots,Histograms]
Variable Age doesn’t have any outliers.;[Boxplots,Histograms]
Variable Pclass doesn’t have any outliers.;[Boxplots,Histograms]
Variable Fare presents some outliers.;[Boxplots,Histograms]
Variable Parch presents some outliers.;[Boxplots,Histograms]
Variable SibSp presents some outliers.;[Boxplots,Histograms]
Variable Age presents some outliers.;[Boxplots,Histograms]
Variable Pclass presents some outliers.;[Boxplots,Histograms]
The histograms presented show a large number of outliers for most of the numeric variables.;[Boxplots,Histograms]
The boxplots presented show a large number of outliers for most of the numeric variables.;[Boxplots,Histograms]
The existence of outliers is one of the problems to tackle in this dataset.;[Boxplots,Histograms]
Variable Embarked is a false predictor.;[Description]
Variable Fare is a false predictor.;[Description]
Variable Parch is a false predictor.;[Description]
Variable SibSp is a false predictor.;[Description]
Variable Age is a false predictor.;[Description]
Variable Sex is a false predictor.;[Description]
Variable Pclass is a false predictor.;[Description]
Variable Survived is a false predictor.;[Description]
One of the variables Parch or Fare can be discarded without losing information.;[Correlation heatmap]
One of the variables SibSp or Fare can be discarded without losing information.;[Correlation heatmap]
One of the variables Age or Fare can be discarded without losing information.;[Correlation heatmap]
One of the variables Pclass or Fare can be discarded without losing information.;[Correlation heatmap]
One of the variables Fare or Parch can be discarded without losing information.;[Correlation heatmap]
One of the variables SibSp or Parch can be discarded without losing information.;[Correlation heatmap]
One of the variables Age or Parch can be discarded without losing information.;[Correlation heatmap]
One of the variables Pclass or Parch can be discarded without losing information.;[Correlation heatmap]
One of the variables Fare or SibSp can be discarded without losing information.;[Correlation heatmap]
One of the variables Parch or SibSp can be discarded without losing information.;[Correlation heatmap]
One of the variables Age or SibSp can be discarded without losing information.;[Correlation heatmap]
One of the variables Pclass or SibSp can be discarded without losing information.;[Correlation heatmap]
One of the variables Fare or Age can be discarded without losing information.;[Correlation heatmap]
One of the variables Parch or Age can be discarded without losing information.;[Correlation heatmap]
One of the variables SibSp or Age can be discarded without losing information.;[Correlation heatmap]
One of the variables Pclass or Age can be discarded without losing information.;[Correlation heatmap]
One of the variables Fare or Pclass can be discarded without losing information.;[Correlation heatmap]
One of the variables Parch or Pclass can be discarded without losing information.;[Correlation heatmap]
One of the variables SibSp or Pclass can be discarded without losing information.;[Correlation heatmap]
One of the variables Age or Pclass can be discarded without losing information.;[Correlation heatmap]
Variables Sex and Age are redundant, but we can’t say the same for the pair Pclass and SibSp.;[Correlation heatmap]
Variables Fare and Embarked are redundant, but we can’t say the same for the pair SibSp and Parch.;[Correlation heatmap]
Variables SibSp and Parch are redundant, but we can’t say the same for the pair Pclass and Sex.;[Correlation heatmap]
Variables Age and SibSp are redundant, but we can’t say the same for the pair Parch and Embarked.;[Correlation heatmap]
Variables SibSp and Embarked are redundant, but we can’t say the same for the pair Sex and Parch.;[Correlation heatmap]
Variables Sex and SibSp are redundant, but we can’t say the same for the pair Age and Fare.;[Correlation heatmap]
Variables Age and Parch are redundant, but we can’t say the same for the pair Pclass and Fare.;[Correlation heatmap]
Variables Pclass and Age are redundant, but we can’t say the same for the pair Sex and SibSp.;[Correlation heatmap]
Variables Fare and Embarked are redundant, but we can’t say the same for the pair Age and Parch.;[Correlation heatmap]
Variables Pclass and Fare are redundant, but we can’t say the same for the pair Age and SibSp.;[Correlation heatmap]
Variables Pclass and Parch are redundant, but we can’t say the same for the pair Sex and SibSp.;[Correlation heatmap]
Variables Age and SibSp are redundant, but we can’t say the same for the pair Pclass and Sex.;[Correlation heatmap]
Variables Sex and Fare are redundant, but we can’t say the same for the pair SibSp and Parch.;[Correlation heatmap]
Variables Sex and Parch are redundant, but we can’t say the same for the pair Fare and Embarked.;[Correlation heatmap]
Variables Pclass and Parch are redundant, but we can’t say the same for the pair Age and Fare.;[Correlation heatmap]
Variables SibSp and Parch are redundant, but we can’t say the same for the pair Pclass and Fare.;[Correlation heatmap]
Variables Parch and Embarked are redundant, but we can’t say the same for the pair Age and Fare.;[Correlation heatmap]
Variables Pclass and Sex are redundant, but we can’t say the same for the pair Age and Parch.;[Correlation heatmap]
Variables Pclass and Parch are redundant, but we can’t say the same for the pair SibSp and Embarked.;[Correlation heatmap]
Variables Sex and Parch are redundant, but we can’t say the same for the pair SibSp and Fare.;[Correlation heatmap]
Variables Age and Parch are redundant, but we can’t say the same for the pair Sex and SibSp.;[Correlation heatmap]
Variables Fare and Embarked are redundant, but we can’t say the same for the pair Sex and Age.;[Correlation heatmap]
Variables Age and SibSp are redundant, but we can’t say the same for the pair Pclass and Embarked.;[Correlation heatmap]
Variables Pclass and Embarked are redundant, but we can’t say the same for the pair Age and SibSp.;[Correlation heatmap]
Variables Pclass and Embarked are redundant, but we can’t say the same for the pair Sex and Fare.;[Correlation heatmap]
Variables Pclass and Age are redundant, but we can’t say the same for the pair Sex and Parch.;[Correlation heatmap]
Variables Sex and Parch are redundant, but we can’t say the same for the pair SibSp and Embarked.;[Correlation heatmap]
Variables Parch and Embarked are redundant, but we can’t say the same for the pair SibSp and Fare.;[Correlation heatmap]
Variables Fare and Embarked are redundant, but we can’t say the same for the pair Sex and SibSp.;[Correlation heatmap]
Variables SibSp and Parch are redundant, but we can’t say the same for the pair Fare and Embarked.;[Correlation heatmap]
Variables Parch and Fare are redundant, but we can’t say the same for the pair Sex and SibSp.;[Correlation heatmap]
Variables Sex and Embarked are redundant, but we can’t say the same for the pair Pclass and SibSp.;[Correlation heatmap]
Variables SibSp and Fare are redundant, but we can’t say the same for the pair Sex and Parch.;[Correlation heatmap]
Variables Age and Fare are redundant, but we can’t say the same for the pair Sex and SibSp.;[Correlation heatmap]
Variables Age and Fare are redundant, but we can’t say the same for the pair Pclass and Sex.;[Correlation heatmap]
Variables Fare and Embarked are redundant, but we can’t say the same for the pair Age and SibSp.;[Correlation heatmap]
Variables Sex and Embarked are redundant, but we can’t say the same for the pair SibSp and Parch.;[Correlation heatmap]
Variables Pclass and Age are redundant, but we can’t say the same for the pair Parch and Fare.;[Correlation heatmap]
Variables SibSp and Parch are redundant, but we can’t say the same for the pair Pclass and Embarked.;[Correlation heatmap]
Variables Sex and Fare are redundant, but we can’t say the same for the pair Pclass and Parch.;[Correlation heatmap]
Variables Age and Embarked are redundant, but we can’t say the same for the pair Pclass and SibSp.;[Correlation heatmap]
Variables SibSp and Fare are redundant, but we can’t say the same for the pair Pclass and Embarked.;[Correlation heatmap]
Variables Age and Embarked are redundant, but we can’t say the same for the pair Sex and Parch.;[Correlation heatmap]
Variables SibSp and Parch are redundant, but we can’t say the same for the pair Sex and Fare.;[Correlation heatmap]
Variables SibSp and Fare are redundant, but we can’t say the same for the pair Age and Parch.;[Correlation heatmap]
Variables Pclass and Fare are redundant, but we can’t say the same for the pair Parch and Embarked.;[Correlation heatmap]
Variables Sex and Embarked are redundant, but we can’t say the same for the pair Pclass and Parch.;[Correlation heatmap]
Variables Sex and Fare are redundant, but we can’t say the same for the pair Age and SibSp.;[Correlation heatmap]
Variables Fare and Embarked are redundant, but we can’t say the same for the pair Pclass and Sex.;[Correlation heatmap]
Variables Sex and Parch are redundant, but we can’t say the same for the pair Age and Embarked.;[Correlation heatmap]
Variables Sex and Age are redundant, but we can’t say the same for the pair Parch and Fare.;[Correlation heatmap]
Variables Pclass and Age are redundant, but we can’t say the same for the pair SibSp and Fare.;[Correlation heatmap]
Variables Pclass and Sex are redundant, but we can’t say the same for the pair Parch and Fare.;[Correlation heatmap]
Variables Pclass and SibSp are redundant, but we can’t say the same for the pair Parch and Fare.;[Correlation heatmap]
Variables Pclass and Fare are redundant, but we can’t say the same for the pair Age and Embarked.;[Correlation heatmap]
Variables Sex and Embarked are redundant, but we can’t say the same for the pair Pclass and Fare.;[Correlation heatmap]
Variables SibSp and Embarked are redundant, but we can’t say the same for the pair Pclass and Age.;[Correlation heatmap]
Variables Age and Fare are redundant, but we can’t say the same for the pair Pclass and SibSp.;[Correlation heatmap]
Variables Sex and Age are redundant, but we can’t say the same for the pair SibSp and Embarked.;[Correlation heatmap]
Variables Pclass and SibSp are redundant, but we can’t say the same for the pair Sex and Fare.;[Correlation heatmap]
Variables Sex and Age are redundant, but we can’t say the same for the pair SibSp and Parch.;[Correlation heatmap]
Variables Age and SibSp are redundant, but we can’t say the same for the pair Pclass and Fare.;[Correlation heatmap]
Variables Pclass and Age are redundant, but we can’t say the same for the pair SibSp and Embarked.;[Correlation heatmap]
Variables Parch and Embarked are redundant, but we can’t say the same for the pair Sex and Fare.;[Correlation heatmap]
Variables Pclass and Sex are redundant, but we can’t say the same for the pair SibSp and Embarked.;[Correlation heatmap]
Variables SibSp and Embarked are redundant, but we can’t say the same for the pair Sex and Age.;[Correlation heatmap]
Variables Age and Fare are redundant, but we can’t say the same for the pair Pclass and Embarked.;[Correlation heatmap]
Variables Sex and Fare are redundant, but we can’t say the same for the pair Pclass and Embarked.;[Correlation heatmap]
Variables Pclass and Age are redundant, but we can’t say the same for the pair Parch and Embarked.;[Correlation heatmap]
Variables Parch and Fare are redundant, but we can’t say the same for the pair Pclass and Age.;[Correlation heatmap]
Variables SibSp and Fare are redundant, but we can’t say the same for the pair Sex and Age.;[Correlation heatmap]
Variables Pclass and Age are redundant, but we can’t say the same for the pair SibSp and Parch.;[Correlation heatmap]
Variables SibSp and Fare are redundant, but we can’t say the same for the pair Pclass and Parch.;[Correlation heatmap]
Variables Sex and Embarked are redundant, but we can’t say the same for the pair Age and SibSp.;[Correlation heatmap]
Variables Pclass and Sex are redundant, but we can’t say the same for the pair SibSp and Fare.;[Correlation heatmap]
Variables SibSp and Embarked are redundant, but we can’t say the same for the pair Age and Parch.;[Correlation heatmap]
Variables Parch and Fare are redundant, but we can’t say the same for the pair SibSp and Embarked.;[Correlation heatmap]
Variables Sex and Age are redundant, but we can’t say the same for the pair SibSp and Fare.;[Correlation heatmap]
Variables Sex and Parch are redundant, but we can’t say the same for the pair Age and SibSp.;[Correlation heatmap]
Variables Fare and Embarked are redundant, but we can’t say the same for the pair Pclass and Age.;[Correlation heatmap]
Variables Fare and Embarked are redundant, but we can’t say the same for the pair Sex and Parch.;[Correlation heatmap]
Variables Age and SibSp are redundant, but we can’t say the same for the pair Sex and Parch.;[Correlation heatmap]
Variables SibSp and Embarked are redundant, but we can’t say the same for the pair Sex and Fare.;[Correlation heatmap]
Variables Parch and Fare are redundant, but we can’t say the same for the pair Sex and Embarked.;[Correlation heatmap]
Variables Parch and Fare are redundant.;[Correlation heatmap]
Variables SibSp and Fare are redundant.;[Correlation heatmap]
Variables Age and Fare are redundant.;[Correlation heatmap]
Variables Pclass and Fare are redundant.;[Correlation heatmap]
Variables Fare and Parch are redundant.;[Correlation heatmap]
Variables SibSp and Parch are redundant.;[Correlation heatmap]
Variables Age and Parch are redundant.;[Correlation heatmap]
Variables Pclass and Parch are redundant.;[Correlation heatmap]
Variables Fare and SibSp are redundant.;[Correlation heatmap]
Variables Parch and SibSp are redundant.;[Correlation heatmap]
Variables Age and SibSp are redundant.;[Correlation heatmap]
Variables Pclass and SibSp are redundant.;[Correlation heatmap]
Variables Fare and Age are redundant.;[Correlation heatmap]
Variables Parch and Age are redundant.;[Correlation heatmap]
Variables SibSp and Age are redundant.;[Correlation heatmap]
Variables Pclass and Age are redundant.;[Correlation heatmap]
Variables Fare and Pclass are redundant.;[Correlation heatmap]
Variables Parch and Pclass are redundant.;[Correlation heatmap]
Variables SibSp and Pclass are redundant.;[Correlation heatmap]
Variables Age and Pclass are redundant.;[Correlation heatmap]
From the correlation analysis alone, it is clear that there are relevant variables.;[Correlation heatmap]
It is clear that variable Fare is one of the five most relevant features.;[Decision tree]
It is clear that variable Parch is one of the five most relevant features.;[Decision tree]
It is clear that variable SibSp is one of the five most relevant features.;[Decision tree]
It is clear that variable Age is one of the five most relevant features.;[Decision tree]
It is clear that variable Pclass is one of the five most relevant features.;[Decision tree]
It is clear that variable Fare is one of the four most relevant features.;[Decision tree]
It is clear that variable Parch is one of the four most relevant features.;[Decision tree]
It is clear that variable SibSp is one of the four most relevant features.;[Decision tree]
It is clear that variable Age is one of the four most relevant features.;[Decision tree]
It is clear that variable Pclass is one of the four most relevant features.;[Decision tree]
It is clear that variable Fare is one of the three most relevant features.;[Decision tree]
It is clear that variable Parch is one of the three most relevant features.;[Decision tree]
It is clear that variable SibSp is one of the three most relevant features.;[Decision tree]
It is clear that variable Age is one of the three most relevant features.;[Decision tree]
It is clear that variable Pclass is one of the three most relevant features.;[Decision tree]
It is clear that variable Fare is one of the two most relevant features.;[Decision tree]
It is clear that variable Parch is one of the two most relevant features.;[Decision tree]
It is clear that variable SibSp is one of the two most relevant features.;[Decision tree]
It is clear that variable Age is one of the two most relevant features.;[Decision tree]
It is clear that variable Pclass is one of the two most relevant features.;[Decision tree]
The variable Fare seems to be one of the five most relevant features.;[Decision tree]
The variable Parch seems to be one of the five most relevant features.;[Decision tree]
The variable SibSp seems to be one of the five most relevant features.;[Decision tree]
The variable Age seems to be one of the five most relevant features.;[Decision tree]
The variable Pclass seems to be one of the five most relevant features.;[Decision tree]
The variable Fare seems to be one of the four most relevant features.;[Decision tree]
The variable Parch seems to be one of the four most relevant features.;[Decision tree]
The variable SibSp seems to be one of the four most relevant features.;[Decision tree]
The variable Age seems to be one of the four most relevant features.;[Decision tree]
The variable Pclass seems to be one of the four most relevant features.;[Decision tree]
The variable Fare seems to be one of the three most relevant features.;[Decision tree]
The variable Parch seems to be one of the three most relevant features.;[Decision tree]
The variable SibSp seems to be one of the three most relevant features.;[Decision tree]
The variable Age seems to be one of the three most relevant features.;[Decision tree]
The variable Pclass seems to be one of the three most relevant features.;[Decision tree]
The variable Fare seems to be one of the two most relevant features.;[Decision tree]
The variable Parch seems to be one of the two most relevant features.;[Decision tree]
The variable SibSp seems to be one of the two most relevant features.;[Decision tree]
The variable Age seems to be one of the two most relevant features.;[Decision tree]
The variable Pclass seems to be one of the two most relevant features.;[Decision tree]
The variable Fare discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable Parch discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable SibSp discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable Age discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable Pclass discriminates between the target values, as shown in the decision tree.;[Decision tree]
Variable Fare is one of the most relevant variables.;[Decision tree]
Variable Parch is one of the most relevant variables.;[Decision tree]
Variable SibSp is one of the most relevant variables.;[Decision tree]
Variable Age is one of the most relevant variables.;[Decision tree]
Variable Pclass is one of the most relevant variables.;[Decision tree]
Variable Fare seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable Parch seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable SibSp seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable Age seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable Pclass seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variables Parch and Fare seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SibSp and Fare seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Age and Fare seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Pclass and Fare seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Fare and Parch seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SibSp and Parch seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Age and Parch seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Pclass and Parch seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Fare and SibSp seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Parch and SibSp seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Age and SibSp seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Pclass and SibSp seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Fare and Age seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Parch and Age seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SibSp and Age seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Pclass and Age seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Fare and Pclass seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Parch and Pclass seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables SibSp and Pclass seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables Age and Pclass seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;[Boxplot global,Single boxplots]
A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;[Boxplot global,Single boxplots]
Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;[Correlation heatmap]
Balancing this dataset by SMOTE would be riskier than oversampling by replication.;[Scatter-plots]
Balancing this dataset by SMOTE would most probably be preferable over undersampling.;[Nr records x nr variables]
Balancing this dataset would be mandatory to improve the results.;[Class histogram]
Both Fare and Embarked variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Parch and Embarked variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SibSp and Embarked variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Age and Embarked variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Sex and Embarked variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Pclass and Embarked variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Embarked and Fare variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Parch and Fare variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SibSp and Fare variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Age and Fare variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Sex and Fare variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Pclass and Fare variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Embarked and Parch variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Fare and Parch variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SibSp and Parch variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Age and Parch variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Sex and Parch variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Pclass and Parch variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Embarked and SibSp variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Fare and SibSp variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Parch and SibSp variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Age and SibSp variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Sex and SibSp variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Pclass and SibSp variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Embarked and Age variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Fare and Age variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Parch and Age variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SibSp and Age variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Sex and Age variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Pclass and Age variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Embarked and Sex variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Fare and Sex variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Parch and Sex variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SibSp and Sex variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Age and Sex variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Pclass and Sex variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Embarked and Pclass variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Fare and Pclass variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Parch and Pclass variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both SibSp and Pclass variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Age and Pclass variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both Sex and Pclass variables could be used to derive a new variable using a concept hierarchy.;[Description]
Considering the common semantics for Fare and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Parch and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for SibSp and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Age and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Sex and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Pclass and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Embarked and Fare variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Parch and Fare variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for SibSp and Fare variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Age and Fare variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Sex and Fare variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Pclass and Fare variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Embarked and Parch variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Fare and Parch variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for SibSp and Parch variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Age and Parch variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Sex and Parch variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Pclass and Parch variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Embarked and SibSp variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Fare and SibSp variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Parch and SibSp variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Age and SibSp variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Sex and SibSp variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Pclass and SibSp variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Embarked and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Fare and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Parch and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for SibSp and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Sex and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Pclass and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Embarked and Sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Fare and Sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Parch and Sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for SibSp and Sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Age and Sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Pclass and Sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Embarked and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Fare and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Parch and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for SibSp and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Age and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Sex and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for Embarked variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for Fare variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for Parch variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for SibSp variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for Age variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for Sex variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for Pclass variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Discarding variables Fare and Embarked would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Parch and Embarked would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SibSp and Embarked would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Age and Embarked would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Sex and Embarked would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Pclass and Embarked would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Embarked and Fare would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Parch and Fare would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SibSp and Fare would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Age and Fare would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Sex and Fare would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Pclass and Fare would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Embarked and Parch would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Fare and Parch would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SibSp and Parch would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Age and Parch would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Sex and Parch would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Pclass and Parch would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Embarked and SibSp would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Fare and SibSp would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Parch and SibSp would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Age and SibSp would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Sex and SibSp would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Pclass and SibSp would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Embarked and Age would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Fare and Age would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Parch and Age would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SibSp and Age would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Sex and Age would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Pclass and Age would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Embarked and Sex would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Fare and Sex would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Parch and Sex would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SibSp and Sex would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Age and Sex would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Pclass and Sex would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Embarked and Pclass would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Fare and Pclass would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Parch and Pclass would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables SibSp and Pclass would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Age and Pclass would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables Sex and Pclass would be better than discarding all the records with missing values for those variables.;[Missing values]
Dropping all records with missing values would be better than to drop the variables with missing values.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 40% of the original data.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.;[Missing values]
Dummification is mandatory in this dataset.;[]
Dummifying the variables will improve the mining results.;[]
Feature generation based on both variables Fare and Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Parch and Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables SibSp and Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Age and Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Sex and Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Pclass and Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Embarked and Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Parch and Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables SibSp and Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Age and Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Sex and Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Pclass and Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Embarked and Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Fare and Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables SibSp and Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Age and Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Sex and Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Pclass and Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Embarked and SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Fare and SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Parch and SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Age and SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Sex and SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Pclass and SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Embarked and Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Fare and Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Parch and Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables SibSp and Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Sex and Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Pclass and Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Embarked and Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Fare and Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Parch and Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables SibSp and Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Age and Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Pclass and Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Embarked and Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Fare and Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Parch and Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables SibSp and Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Age and Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables Sex and Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Parch wouldn’t be useful, but the use of Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable SibSp wouldn’t be useful, but the use of Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Age wouldn’t be useful, but the use of Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Pclass wouldn’t be useful, but the use of Embarked seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Embarked wouldn’t be useful, but the use of Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Parch wouldn’t be useful, but the use of Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable SibSp wouldn’t be useful, but the use of Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Age wouldn’t be useful, but the use of Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Pclass wouldn’t be useful, but the use of Fare seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Embarked wouldn’t be useful, but the use of Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable SibSp wouldn’t be useful, but the use of Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Age wouldn’t be useful, but the use of Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Pclass wouldn’t be useful, but the use of Parch seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Embarked wouldn’t be useful, but the use of SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Fare wouldn’t be useful, but the use of SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Parch wouldn’t be useful, but the use of SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Age wouldn’t be useful, but the use of SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Sex wouldn’t be useful, but the use of SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Pclass wouldn’t be useful, but the use of SibSp seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Embarked wouldn’t be useful, but the use of Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Parch wouldn’t be useful, but the use of Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable SibSp wouldn’t be useful, but the use of Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Pclass wouldn’t be useful, but the use of Age seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Embarked wouldn’t be useful, but the use of Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Parch wouldn’t be useful, but the use of Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable SibSp wouldn’t be useful, but the use of Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Age wouldn’t be useful, but the use of Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Pclass wouldn’t be useful, but the use of Sex seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Embarked wouldn’t be useful, but the use of Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Parch wouldn’t be useful, but the use of Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable SibSp wouldn’t be useful, but the use of Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Age wouldn’t be useful, but the use of Pclass seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Pclass seems to be promising.;[Missing values,All_Histograms]
Given the usual semantics of Embarked variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of Fare variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of Parch variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of SibSp variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of Age variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of Sex variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of Pclass variable, dummification would have been a better codification.;[All_Histograms]
If Parch and Fare were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SibSp and Fare were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Age and Fare were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Pclass and Fare were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Fare and Parch were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SibSp and Parch were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Age and Parch were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Pclass and Parch were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Fare and SibSp were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Parch and SibSp were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Age and SibSp were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Pclass and SibSp were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Fare and Age were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Parch and Age were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SibSp and Age were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Pclass and Age were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Fare and Pclass were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Parch and Pclass were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SibSp and Pclass were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If Age and Pclass were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
It is better to drop the variable Embarked than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable Fare than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable Parch than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable SibSp than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable Age than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable Sex than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable Pclass than removing all records with missing values.;[Missing values,All_Histograms]
Knowing that C and F are strongly correlated (correlation=1), we can say that removing one of those variables, would not have any impact on the performance of a KNN classifier.;[]
Missing value imputation using the mean value per class improves the quality of discovered patterns.;[]
Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;[Boxplots]
Normalization of this dataset could not have impact on a KNN classifier.;[Boxplot global,Single boxplots]
Normalization of this dataset should have a high impact on naïve Bayes classifier.;[]
Not knowing the semantics of Embarked variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of Fare variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of Parch variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of SibSp variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of Age variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of Sex variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of Pclass variable, dummification could have been a more adequate codification.;[All_Histograms]
Removing the Embarked variable from the training will improve model performance over any non-observed records.;[Description]
Removing the Fare variable from the training will improve model performance over any non-observed records.;[Description]
Removing the Parch variable from the training will improve model performance over any non-observed records.;[Description]
Removing the SibSp variable from the training will improve model performance over any non-observed records.;[Description]
Removing the Age variable from the training will improve model performance over any non-observed records.;[Description]
Removing the Sex variable from the training will improve model performance over any non-observed records.;[Description]
Removing the Pclass variable from the training will improve model performance over any non-observed records.;[Description]
Removing the Survived variable from the training will improve model performance over any non-observed records.;[Description]
Removing variable Fare might improve the training of decision trees .;[Correlation heatmap]
Removing variable Parch might improve the training of decision trees .;[Correlation heatmap]
Removing variable SibSp might improve the training of decision trees .;[Correlation heatmap]
Removing variable Age might improve the training of decision trees .;[Correlation heatmap]
Removing variable Pclass might improve the training of decision trees .;[Correlation heatmap]
Scaling this dataset would be mandatory to improve the results with distance-based methods.;[Boxplots]
The first 4 principal components are enough for explaining half the data variance.;[PCA]
The first 3 principal components are enough for explaining half the data variance.;[PCA]
The first 2 principal components are enough for explaining half the data variance.;[PCA]
There is evidence in favour for sequential backward selection to select variable Parch previously than variable Fare.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SibSp previously than variable Fare.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Age previously than variable Fare.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Pclass previously than variable Fare.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Fare previously than variable Parch.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SibSp previously than variable Parch.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Age previously than variable Parch.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Pclass previously than variable Parch.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Fare previously than variable SibSp.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Parch previously than variable SibSp.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Age previously than variable SibSp.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Pclass previously than variable SibSp.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Fare previously than variable Age.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Parch previously than variable Age.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SibSp previously than variable Age.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Pclass previously than variable Age.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Fare previously than variable Pclass.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Parch previously than variable Pclass.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable SibSp previously than variable Pclass.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable Age previously than variable Pclass.;[Correlation heatmap]
Using the first 4 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 3 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 2 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 4 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 3 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 2 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 4 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 3 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 2 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 4 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 3 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 2 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 4 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 3 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 2 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 4 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 3 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 2 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 4 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 3 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 2 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 4 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 3 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 2 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 4 principal components would imply an error between 5 and 20%.;[PCA]
Using the first 3 principal components would imply an error between 5 and 20%.;[PCA]
Using the first 2 principal components would imply an error between 5 and 20%.;[PCA]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 9%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 7%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;[Decision tree]
According to the decision tree overfitting chart, the tree with 8 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 7 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 6 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 4 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 3 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the charts, KNN and Decision Trees present a similar behaviour.;[Overfitting Decision Tree + Overfitting KNN]
As reported in the tree, the number of False Positive is bigger than the number of False Negatives.;[Decision tree]
As reported in the tree, the number of False Positive is smaller than the number of False Negatives.;[Decision tree]
Decision trees and KNN show similar behaviours.;[Overfitting Decision Tree + Overfitting KNN]
KNN and Decision Trees show a different trend in the majority of hyperparameters tested.;[Overfitting Decision Tree + Overfitting KNN]
KNN is in overfitting for k larger than 8.;[Overfitting KNN]
KNN is in overfitting for k less than 8.;[Overfitting KNN]
KNN is in overfitting for k larger than 7.;[Overfitting KNN]
KNN is in overfitting for k less than 7.;[Overfitting KNN]
KNN is in overfitting for k larger than 6.;[Overfitting KNN]
KNN is in overfitting for k less than 6.;[Overfitting KNN]
KNN is in overfitting for k larger than 5.;[Overfitting KNN]
KNN is in overfitting for k less than 5.;[Overfitting KNN]
KNN is in overfitting for k larger than 4.;[Overfitting KNN]
KNN is in overfitting for k less than 4.;[Overfitting KNN]
KNN is in overfitting for k larger than 3.;[Overfitting KNN]
KNN is in overfitting for k less than 3.;[Overfitting KNN]
KNN is in overfitting for k larger than 2.;[Overfitting KNN]
KNN is in overfitting for k less than 2.;[Overfitting KNN]
KNN with 10 neighbour is in overfitting.;[Overfitting KNN]
KNN with 9 neighbour is in overfitting.;[Overfitting KNN]
KNN with 8 neighbour is in overfitting.;[Overfitting KNN]
KNN with 7 neighbour is in overfitting.;[Overfitting KNN]
KNN with 6 neighbour is in overfitting.;[Overfitting KNN]
KNN with 5 neighbour is in overfitting.;[Overfitting KNN]
KNN with 4 neighbour is in overfitting.;[Overfitting KNN]
KNN with 3 neighbour is in overfitting.;[Overfitting KNN]
KNN with 2 neighbour is in overfitting.;[Overfitting KNN]
KNN with 1 neighbour is in overfitting.;[Overfitting KNN]
KNN with less than 8 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 8 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 7 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 7 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 6 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 6 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 5 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 5 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 4 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 4 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 3 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 3 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 2 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 2 neighbours is in overfitting.;[Overfitting KNN]
Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;[Overfitting RF]
Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;[Overfitting RF]
Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;[Overfitting RF]
The specificity for the presented tree is lower than 90%.;[Decision tree]
The precision for the presented tree is lower than 90%.;[Decision tree]
The recall for the presented tree is lower than 85%.;[Decision tree]
The accuracy for the presented tree is lower than 71%.;[Decision tree]
The specificity for the presented tree is higher than 63%.;[Decision tree]
The precision for the presented tree is higher than 61%.;[Decision tree]
The recall for the presented tree is higher than 81%.;[Decision tree]
The accuracy for the presented tree is higher than 71%.;[Decision tree]
The specificity for the presented tree is lower than 75%.;[Decision tree]
The precision for the presented tree is lower than 79%.;[Decision tree]
The recall for the presented tree is lower than 82%.;[Decision tree]
The accuracy for the presented tree is lower than 85%.;[Decision tree]
The specificity for the presented tree is higher than 66%.;[Decision tree]
The precision for the presented tree is higher than 90%.;[Decision tree]
The recall for the presented tree is higher than 65%.;[Decision tree]
The accuracy for the presented tree is higher than 78%.;[Decision tree]
The specificity for the presented tree is lower than 87%.;[Decision tree]
The precision for the presented tree is lower than 76%.;[Decision tree]
The recall for the presented tree is lower than 75%.;[Decision tree]
The accuracy for the presented tree is lower than 88%.;[Decision tree]
The specificity for the presented tree is higher than 86%.;[Decision tree]
The precision for the presented tree is higher than 68%.;[Decision tree]
The recall for the presented tree is higher than 64%.;[Decision tree]
The accuracy for the presented tree is higher than 72%.;[Decision tree]
The specificity for the presented tree is lower than 73%.;[Decision tree]
The precision for the presented tree is lower than 80%.;[Decision tree]
The recall for the presented tree is lower than 69%.;[Decision tree]
The accuracy for the presented tree is lower than 63%.;[Decision tree]
The specificity for the presented tree is higher than 60%.;[Decision tree]
The precision for the presented tree is higher than 62%.;[Decision tree]
The recall for the presented tree is higher than 89%.;[Decision tree]
The accuracy for the presented tree is higher than 67%.;[Decision tree]
The specificity for the presented tree is lower than 77%.;[Decision tree]
The precision for the presented tree is lower than 70%.;[Decision tree]
The recall for the presented tree is lower than 83%.;[Decision tree]
The accuracy for the presented tree is lower than 61%.;[Decision tree]
The specificity for the presented tree is higher than 74%.;[Decision tree]
The precision for the presented tree is higher than 81%.;[Decision tree]
The recall for the presented tree is higher than 71%.;[Decision tree]
The accuracy for the presented tree is higher than 84%.;[Decision tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 10.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 9.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 8.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 7.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 6.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 5.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 4.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 3.;[Overfitting Decision Tree]
The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;[Overfitting Accuracy + Recall]
The number of False Negatives reported in the same tree is 23.;[Decision tree]
The number of True Negatives reported in the same tree is 22.;[Decision tree]
The number of False Positives reported in the same tree is 13.;[Decision tree]
The number of True Positives reported in the same tree is 43.;[Decision tree]
The number of False Negatives reported in the same tree is 37.;[Decision tree]
The number of True Negatives reported in the same tree is 24.;[Decision tree]
The number of False Positives reported in the same tree is 16.;[Decision tree]
The number of True Positives reported in the same tree is 14.;[Decision tree]
The number of False Negatives reported in the same tree is 32.;[Decision tree]
The number of True Negatives reported in the same tree is 15.;[Decision tree]
The number of False Positives reported in the same tree is 42.;[Decision tree]
The number of True Positives reported in the same tree is 20.;[Decision tree]
The number of False Negatives reported in the same tree is 19.;[Decision tree]
The number of True Negatives reported in the same tree is 39.;[Decision tree]
The number of False Positives reported in the same tree is 25.;[Decision tree]
The number of True Positives reported in the same tree is 38.;[Decision tree]
The number of False Negatives reported in the same tree is 44.;[Decision tree]
The number of True Negatives reported in the same tree is 28.;[Decision tree]
The number of False Positives reported in the same tree is 12.;[Decision tree]
The number of True Positives reported in the same tree is 26.;[Decision tree]
The number of True Negatives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of True Negatives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of True Negatives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of True Positives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of True Positives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of True Positives for the presented tree.;[Decision tree]
The precision for the presented tree is lower than its specificity.;[Decision tree]
The recall for the presented tree is lower than its specificity.;[Decision tree]
The accuracy for the presented tree is lower than its specificity.;[Decision tree]
The precision for the presented tree is higher than its specificity.;[Decision tree]
The recall for the presented tree is higher than its specificity.;[Decision tree]
The accuracy for the presented tree is higher than its specificity.;[Decision tree]
The specificity for the presented tree is lower than its precision.;[Decision tree]
The recall for the presented tree is lower than its precision.;[Decision tree]
The accuracy for the presented tree is lower than its precision.;[Decision tree]
The specificity for the presented tree is higher than its precision.;[Decision tree]
The recall for the presented tree is higher than its precision.;[Decision tree]
The accuracy for the presented tree is higher than its precision.;[Decision tree]
The specificity for the presented tree is lower than its recall.;[Decision tree]
The precision for the presented tree is lower than its recall.;[Decision tree]
The accuracy for the presented tree is lower than its recall.;[Decision tree]
The specificity for the presented tree is higher than its recall.;[Decision tree]
The precision for the presented tree is higher than its recall.;[Decision tree]
The accuracy for the presented tree is higher than its recall.;[Decision tree]
The specificity for the presented tree is lower than its accuracy.;[Decision tree]
The precision for the presented tree is lower than its accuracy.;[Decision tree]
The recall for the presented tree is lower than its accuracy.;[Decision tree]
The specificity for the presented tree is higher than its accuracy.;[Decision tree]
The precision for the presented tree is higher than its accuracy.;[Decision tree]
The recall for the presented tree is higher than its accuracy.;[Decision tree]
The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;[Overfitting RF]
We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for random forest models with more than 72 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 73 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 65 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 106 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 112 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for gradient boosting models with more than 140 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 118 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 55 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 68 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 95 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for MLP models trained longer than 1336 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 1490 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 788 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 1370 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 1288 episodes.;[Overfitting MLP]
