Question;Charts
Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;[Nr records x nr variables]
Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;[Nr records x nr variables]
We face the curse of dimensionality when training a classifier with this dataset.;[Nr records x nr variables]
The intrinsic dimensionality of this dataset is 71.;[Correlation heatmap]
The intrinsic dimensionality of this dataset is 22.;[Correlation heatmap]
The intrinsic dimensionality of this dataset is 19.;[Correlation heatmap]
The intrinsic dimensionality of this dataset is 51.;[Correlation heatmap]
The intrinsic dimensionality of this dataset is 62.;[Correlation heatmap]
The figure doesn’t show any missing values for entropy, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for curtosis, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for skewness, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
The figure doesn’t show any missing values for variance, but these may be hidden as some non-pre-identified value.;[Histograms + MV]
All variables, but the class, should be dealt with as symbolic.;[Scatter-plots,All_Histograms]
All variables, but the class, should be dealt with as date.;[Scatter-plots,All_Histograms]
All variables, but the class, should be dealt with as binary.;[Scatter-plots,All_Histograms]
All variables, but the class, should be dealt with as numeric.;[Scatter-plots,All_Histograms]
The variable entropy can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable curtosis can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable skewness can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable variance can be seen as ordinal.;[Scatter-plots,All_Histograms]
The variable entropy can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable curtosis can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable skewness can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
The variable variance can be seen as ordinal without losing information.;[Scatter-plots,All_Histograms]
Variable entropy is balanced.;[Histograms,Boxplots]
Variable curtosis is balanced.;[Histograms,Boxplots]
Variable skewness is balanced.;[Histograms,Boxplots]
Variable variance is balanced.;[Histograms,Boxplots]
Those boxplots show that the data is not normalized.;[Boxplot global,Single boxplots]
It is clear that variable curtosis shows some outliers, but we can’t be sure of the same for variable entropy.;[Boxplots,Histograms]
It is clear that variable skewness shows some outliers, but we can’t be sure of the same for variable entropy.;[Boxplots,Histograms]
It is clear that variable variance shows some outliers, but we can’t be sure of the same for variable entropy.;[Boxplots,Histograms]
It is clear that variable entropy shows some outliers, but we can’t be sure of the same for variable curtosis.;[Boxplots,Histograms]
It is clear that variable skewness shows some outliers, but we can’t be sure of the same for variable curtosis.;[Boxplots,Histograms]
It is clear that variable variance shows some outliers, but we can’t be sure of the same for variable curtosis.;[Boxplots,Histograms]
It is clear that variable entropy shows some outliers, but we can’t be sure of the same for variable skewness.;[Boxplots,Histograms]
It is clear that variable curtosis shows some outliers, but we can’t be sure of the same for variable skewness.;[Boxplots,Histograms]
It is clear that variable variance shows some outliers, but we can’t be sure of the same for variable skewness.;[Boxplots,Histograms]
It is clear that variable entropy shows some outliers, but we can’t be sure of the same for variable variance.;[Boxplots,Histograms]
It is clear that variable curtosis shows some outliers, but we can’t be sure of the same for variable variance.;[Boxplots,Histograms]
It is clear that variable skewness shows some outliers, but we can’t be sure of the same for variable variance.;[Boxplots,Histograms]
Outliers seem to be a problem in the dataset.;[Boxplots,Histograms]
Variable entropy shows a high number of outlier values.;[Boxplots,Histograms]
Variable curtosis shows a high number of outlier values.;[Boxplots,Histograms]
Variable skewness shows a high number of outlier values.;[Boxplots,Histograms]
Variable variance shows a high number of outlier values.;[Boxplots,Histograms]
Variable entropy shows some outlier values.;[Boxplots,Histograms]
Variable curtosis shows some outlier values.;[Boxplots,Histograms]
Variable skewness shows some outlier values.;[Boxplots,Histograms]
Variable variance shows some outlier values.;[Boxplots,Histograms]
Variable entropy doesn’t have any outliers.;[Boxplots,Histograms]
Variable curtosis doesn’t have any outliers.;[Boxplots,Histograms]
Variable skewness doesn’t have any outliers.;[Boxplots,Histograms]
Variable variance doesn’t have any outliers.;[Boxplots,Histograms]
Variable entropy presents some outliers.;[Boxplots,Histograms]
Variable curtosis presents some outliers.;[Boxplots,Histograms]
Variable skewness presents some outliers.;[Boxplots,Histograms]
Variable variance presents some outliers.;[Boxplots,Histograms]
At least 85 of the variables present outliers.;[Boxplots,Histograms]
At least 75 of the variables present outliers.;[Boxplots,Histograms]
At least 60 of the variables present outliers.;[Boxplots,Histograms]
At least 50 of the variables present outliers.;[Boxplots,Histograms]
The histograms presented show a large number of outliers for most of the numeric variables.;[Boxplots,Histograms]
The boxplots presented show a large number of outliers for most of the numeric variables.;[Boxplots,Histograms]
The existence of outliers is one of the problems to tackle in this dataset.;[Boxplots,Histograms]
Variable class is a false predictor.;[Description]
Variable entropy is a false predictor.;[Description]
Variable curtosis is a false predictor.;[Description]
Variable skewness is a false predictor.;[Description]
Variable variance is a false predictor.;[Description]
One of the variables curtosis or entropy can be discarded without losing information.;[Correlation heatmap]
One of the variables skewness or entropy can be discarded without losing information.;[Correlation heatmap]
One of the variables variance or entropy can be discarded without losing information.;[Correlation heatmap]
One of the variables entropy or curtosis can be discarded without losing information.;[Correlation heatmap]
One of the variables skewness or curtosis can be discarded without losing information.;[Correlation heatmap]
One of the variables variance or curtosis can be discarded without losing information.;[Correlation heatmap]
One of the variables entropy or skewness can be discarded without losing information.;[Correlation heatmap]
One of the variables curtosis or skewness can be discarded without losing information.;[Correlation heatmap]
One of the variables variance or skewness can be discarded without losing information.;[Correlation heatmap]
One of the variables entropy or variance can be discarded without losing information.;[Correlation heatmap]
One of the variables curtosis or variance can be discarded without losing information.;[Correlation heatmap]
One of the variables skewness or variance can be discarded without losing information.;[Correlation heatmap]
The variable entropy can be discarded without risking losing information.;[Correlation heatmap]
The variable curtosis can be discarded without risking losing information.;[Correlation heatmap]
The variable skewness can be discarded without risking losing information.;[Correlation heatmap]
The variable variance can be discarded without risking losing information.;[Correlation heatmap]
Variables variance and entropy are redundant, but we can’t say the same for the pair skewness and curtosis.;[Correlation heatmap]
Variables variance and curtosis are redundant, but we can’t say the same for the pair skewness and entropy.;[Correlation heatmap]
Variables curtosis and entropy are redundant, but we can’t say the same for the pair variance and skewness.;[Correlation heatmap]
Variables variance and skewness are redundant, but we can’t say the same for the pair curtosis and entropy.;[Correlation heatmap]
Variables skewness and curtosis are redundant, but we can’t say the same for the pair variance and entropy.;[Correlation heatmap]
Variables skewness and entropy are redundant, but we can’t say the same for the pair variance and curtosis.;[Correlation heatmap]
Variables curtosis and entropy are redundant.;[Correlation heatmap]
Variables skewness and entropy are redundant.;[Correlation heatmap]
Variables variance and entropy are redundant.;[Correlation heatmap]
Variables entropy and curtosis are redundant.;[Correlation heatmap]
Variables skewness and curtosis are redundant.;[Correlation heatmap]
Variables variance and curtosis are redundant.;[Correlation heatmap]
Variables entropy and skewness are redundant.;[Correlation heatmap]
Variables curtosis and skewness are redundant.;[Correlation heatmap]
Variables variance and skewness are redundant.;[Correlation heatmap]
Variables entropy and variance are redundant.;[Correlation heatmap]
Variables curtosis and variance are redundant.;[Correlation heatmap]
Variables skewness and variance are redundant.;[Correlation heatmap]
From the correlation analysis alone, it is clear that there are relevant variables.;[Correlation heatmap]
It is clear that variable entropy is one of the five most relevant features.;[Decision tree]
It is clear that variable curtosis is one of the five most relevant features.;[Decision tree]
It is clear that variable skewness is one of the five most relevant features.;[Decision tree]
It is clear that variable variance is one of the five most relevant features.;[Decision tree]
It is clear that variable entropy is one of the four most relevant features.;[Decision tree]
It is clear that variable curtosis is one of the four most relevant features.;[Decision tree]
It is clear that variable skewness is one of the four most relevant features.;[Decision tree]
It is clear that variable variance is one of the four most relevant features.;[Decision tree]
It is clear that variable entropy is one of the three most relevant features.;[Decision tree]
It is clear that variable curtosis is one of the three most relevant features.;[Decision tree]
It is clear that variable skewness is one of the three most relevant features.;[Decision tree]
It is clear that variable variance is one of the three most relevant features.;[Decision tree]
It is clear that variable entropy is one of the two most relevant features.;[Decision tree]
It is clear that variable curtosis is one of the two most relevant features.;[Decision tree]
It is clear that variable skewness is one of the two most relevant features.;[Decision tree]
It is clear that variable variance is one of the two most relevant features.;[Decision tree]
The variable entropy seems to be one of the five most relevant features.;[Decision tree]
The variable curtosis seems to be one of the five most relevant features.;[Decision tree]
The variable skewness seems to be one of the five most relevant features.;[Decision tree]
The variable variance seems to be one of the five most relevant features.;[Decision tree]
The variable entropy seems to be one of the four most relevant features.;[Decision tree]
The variable curtosis seems to be one of the four most relevant features.;[Decision tree]
The variable skewness seems to be one of the four most relevant features.;[Decision tree]
The variable variance seems to be one of the four most relevant features.;[Decision tree]
The variable entropy seems to be one of the three most relevant features.;[Decision tree]
The variable curtosis seems to be one of the three most relevant features.;[Decision tree]
The variable skewness seems to be one of the three most relevant features.;[Decision tree]
The variable variance seems to be one of the three most relevant features.;[Decision tree]
The variable entropy seems to be one of the two most relevant features.;[Decision tree]
The variable curtosis seems to be one of the two most relevant features.;[Decision tree]
The variable skewness seems to be one of the two most relevant features.;[Decision tree]
The variable variance seems to be one of the two most relevant features.;[Decision tree]
The variable entropy discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable curtosis discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable skewness discriminates between the target values, as shown in the decision tree.;[Decision tree]
The variable variance discriminates between the target values, as shown in the decision tree.;[Decision tree]
It is possible to state that entropy is the second most discriminative variable regarding the class.;[Decision tree]
It is possible to state that curtosis is the second most discriminative variable regarding the class.;[Decision tree]
It is possible to state that skewness is the second most discriminative variable regarding the class.;[Decision tree]
It is possible to state that variance is the second most discriminative variable regarding the class.;[Decision tree]
It is possible to state that entropy is the first most discriminative variable regarding the class.;[Decision tree]
It is possible to state that curtosis is the first most discriminative variable regarding the class.;[Decision tree]
It is possible to state that skewness is the first most discriminative variable regarding the class.;[Decision tree]
It is possible to state that variance is the first most discriminative variable regarding the class.;[Decision tree]
Variable entropy is one of the most relevant variables.;[Decision tree]
Variable curtosis is one of the most relevant variables.;[Decision tree]
Variable skewness is one of the most relevant variables.;[Decision tree]
Variable variance is one of the most relevant variables.;[Decision tree]
Variable entropy seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable curtosis seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable skewness seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variable variance seems to be relevant for the majority of mining tasks.;[Decision tree,Correlation heatmap]
Variables curtosis and entropy seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables skewness and entropy seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables variance and entropy seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables entropy and curtosis seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables skewness and curtosis seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables variance and curtosis seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables entropy and skewness seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables curtosis and skewness seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables variance and skewness seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables entropy and variance seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables curtosis and variance seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
Variables skewness and variance seem to be useful for classification tasks.;[Decision tree,Correlation heatmap]
A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;[Boxplot global,Single boxplots]
A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;[Boxplot global,Single boxplots]
Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;[Correlation heatmap]
Balancing this dataset by SMOTE would be riskier than oversampling by replication.;[Scatter-plots]
Balancing this dataset by SMOTE would most probably be preferable over undersampling.;[Nr records x nr variables]
Balancing this dataset would be mandatory to improve the results.;[Class histogram]
Both curtosis and entropy variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both skewness and entropy variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both variance and entropy variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both entropy and curtosis variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both skewness and curtosis variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both variance and curtosis variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both entropy and skewness variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both curtosis and skewness variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both variance and skewness variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both entropy and variance variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both curtosis and variance variables could be used to derive a new variable using a concept hierarchy.;[Description]
Both skewness and variance variables could be used to derive a new variable using a concept hierarchy.;[Description]
The generation of a new feature through the conjunction of variables, would require some domain knowledge.;[Description]
Considering the common semantics for curtosis and entropy variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for skewness and entropy variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for variance and entropy variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for entropy and curtosis variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for skewness and curtosis variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for variance and curtosis variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for entropy and skewness variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for curtosis and skewness variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for variance and skewness variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for entropy and variance variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for curtosis and variance variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for skewness and variance variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[Description,All_Histograms]
Considering the common semantics for entropy variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for curtosis variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for skewness variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
Considering the common semantics for variance variable, dummification would be the most adequate encoding.;[Description,All_Histograms]
The variable entropy can be coded as ordinal without losing information.;[All_Histograms]
The variable curtosis can be coded as ordinal without losing information.;[All_Histograms]
The variable skewness can be coded as ordinal without losing information.;[All_Histograms]
The variable variance can be coded as ordinal without losing information.;[All_Histograms]
Discarding variables curtosis and entropy would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables skewness and entropy would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables variance and entropy would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables entropy and curtosis would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables skewness and curtosis would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables variance and curtosis would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables entropy and skewness would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables curtosis and skewness would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables variance and skewness would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables entropy and variance would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables curtosis and variance would be better than discarding all the records with missing values for those variables.;[Missing values]
Discarding variables skewness and variance would be better than discarding all the records with missing values for those variables.;[Missing values]
Dropping all records with missing values would be better than to drop the variables with missing values.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 40% of the original data.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.;[Missing values]
Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.;[Missing values]
There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;[Missing values]
Dummification is mandatory in this dataset.;[]
Dummifying the variables will improve the mining results.;[]
Feature generation based on both variables curtosis and entropy seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables skewness and entropy seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables variance and entropy seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables entropy and curtosis seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables skewness and curtosis seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables variance and curtosis seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables entropy and skewness seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables curtosis and skewness seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables variance and skewness seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables entropy and variance seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables curtosis and variance seems to be promising.;[Missing values,All_Histograms]
Feature generation based on both variables skewness and variance seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable curtosis wouldn’t be useful, but the use of entropy seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable skewness wouldn’t be useful, but the use of entropy seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable variance wouldn’t be useful, but the use of entropy seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable entropy wouldn’t be useful, but the use of curtosis seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable skewness wouldn’t be useful, but the use of curtosis seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable variance wouldn’t be useful, but the use of curtosis seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable entropy wouldn’t be useful, but the use of skewness seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable curtosis wouldn’t be useful, but the use of skewness seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable variance wouldn’t be useful, but the use of skewness seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable entropy wouldn’t be useful, but the use of variance seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable curtosis wouldn’t be useful, but the use of variance seems to be promising.;[Missing values,All_Histograms]
Feature generation based on the use of variable skewness wouldn’t be useful, but the use of variance seems to be promising.;[Missing values,All_Histograms]
Given the usual semantics of entropy variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of curtosis variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of skewness variable, dummification would have been a better codification.;[All_Histograms]
Given the usual semantics of variance variable, dummification would have been a better codification.;[All_Histograms]
If curtosis and entropy were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If skewness and entropy were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If variance and entropy were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If entropy and curtosis were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If skewness and curtosis were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If variance and curtosis were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If entropy and skewness were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If curtosis and skewness were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If variance and skewness were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If entropy and variance were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If curtosis and variance were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If skewness and variance were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
It is better to drop the variable entropy than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable curtosis than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable skewness than removing all records with missing values.;[Missing values,All_Histograms]
It is better to drop the variable variance than removing all records with missing values.;[Missing values,All_Histograms]
Knowing that C and F are strongly correlated (correlation=1), we can say that removing one of those variables, would not have any impact on the performance of a KNN classifier.;[]
Missing value imputation using the mean value per class improves the quality of discovered patterns.;[]
Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;[Boxplots]
Normalization of this dataset could not have impact on a KNN classifier.;[Boxplot global,Single boxplots]
Normalization of this dataset should have a high impact on naïve Bayes classifier.;[]
Scaling this dataset should lead to a faster training with multi-layer perceptron.;[]
Not knowing the semantics of entropy variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of curtosis variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of skewness variable, dummification could have been a more adequate codification.;[All_Histograms]
Not knowing the semantics of variance variable, dummification could have been a more adequate codification.;[All_Histograms]
Removing the class variable from the training will improve model performance over any non-observed records.;[Description]
Removing the entropy variable from the training will improve model performance over any non-observed records.;[Description]
Removing the curtosis variable from the training will improve model performance over any non-observed records.;[Description]
Removing the skewness variable from the training will improve model performance over any non-observed records.;[Description]
Removing the variance variable from the training will improve model performance over any non-observed records.;[Description]
Removing variable entropy might improve the training of decision trees .;[Correlation heatmap]
Removing variable curtosis might improve the training of decision trees .;[Correlation heatmap]
Removing variable skewness might improve the training of decision trees .;[Correlation heatmap]
Removing variable variance might improve the training of decision trees .;[Correlation heatmap]
Scaling this dataset would be mandatory to improve the results with distance-based methods.;[Boxplots]
The first 4 principal components are enough for explaining half the data variance.;[PCA]
The first 3 principal components are enough for explaining half the data variance.;[PCA]
The first 2 principal components are enough for explaining half the data variance.;[PCA]
There is evidence in favour for sequential backward selection to select variable curtosis previously than variable entropy.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable skewness previously than variable entropy.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable variance previously than variable entropy.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable entropy previously than variable curtosis.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable skewness previously than variable curtosis.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable variance previously than variable curtosis.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable entropy previously than variable skewness.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable curtosis previously than variable skewness.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable variance previously than variable skewness.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable entropy previously than variable variance.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable curtosis previously than variable variance.;[Correlation heatmap]
There is evidence in favour for sequential backward selection to select variable skewness previously than variable variance.;[Correlation heatmap]
Using the first 4 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 3 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 2 principal components would imply an error between 15 and 30%.;[PCA]
Using the first 4 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 3 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 2 principal components would imply an error between 10 and 30%.;[PCA]
Using the first 4 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 3 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 2 principal components would imply an error between 5 and 30%.;[PCA]
Using the first 4 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 3 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 2 principal components would imply an error between 15 and 25%.;[PCA]
Using the first 4 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 3 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 2 principal components would imply an error between 10 and 25%.;[PCA]
Using the first 4 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 3 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 2 principal components would imply an error between 5 and 25%.;[PCA]
Using the first 4 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 3 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 2 principal components would imply an error between 15 and 20%.;[PCA]
Using the first 4 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 3 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 2 principal components would imply an error between 10 and 20%.;[PCA]
Using the first 4 principal components would imply an error between 5 and 20%.;[PCA]
Using the first 3 principal components would imply an error between 5 and 20%.;[PCA]
Using the first 2 principal components would imply an error between 5 and 20%.;[PCA]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 9%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 7%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;[Decision tree]
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;[Decision tree]
According to the decision tree overfitting chart, the tree with 8 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 7 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 6 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 4 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the decision tree overfitting chart, the tree with 3 nodes of depth is in overfitting.;[Overfitting Decision Tree]
According to the charts, KNN and Decision Trees present a similar behaviour.;[Overfitting Decision Tree + Overfitting KNN]
As reported in the tree, the number of False Positive is bigger than the number of False Negatives.;[Decision tree]
As reported in the tree, the number of False Positive is smaller than the number of False Negatives.;[Decision tree]
Decision trees and KNN show similar behaviours.;[Overfitting Decision Tree + Overfitting KNN]
KNN and Decision Trees show a different trend in the majority of hyperparameters tested.;[Overfitting Decision Tree + Overfitting KNN]
KNN is in overfitting for k larger than 8.;[Overfitting KNN]
KNN is in overfitting for k less than 8.;[Overfitting KNN]
KNN is in overfitting for k larger than 7.;[Overfitting KNN]
KNN is in overfitting for k less than 7.;[Overfitting KNN]
KNN is in overfitting for k larger than 6.;[Overfitting KNN]
KNN is in overfitting for k less than 6.;[Overfitting KNN]
KNN is in overfitting for k larger than 5.;[Overfitting KNN]
KNN is in overfitting for k less than 5.;[Overfitting KNN]
KNN is in overfitting for k larger than 4.;[Overfitting KNN]
KNN is in overfitting for k less than 4.;[Overfitting KNN]
KNN is in overfitting for k larger than 3.;[Overfitting KNN]
KNN is in overfitting for k less than 3.;[Overfitting KNN]
KNN is in overfitting for k larger than 2.;[Overfitting KNN]
KNN is in overfitting for k less than 2.;[Overfitting KNN]
KNN with 10 neighbour is in overfitting.;[Overfitting KNN]
KNN with 9 neighbour is in overfitting.;[Overfitting KNN]
KNN with 8 neighbour is in overfitting.;[Overfitting KNN]
KNN with 7 neighbour is in overfitting.;[Overfitting KNN]
KNN with 6 neighbour is in overfitting.;[Overfitting KNN]
KNN with 5 neighbour is in overfitting.;[Overfitting KNN]
KNN with 4 neighbour is in overfitting.;[Overfitting KNN]
KNN with 3 neighbour is in overfitting.;[Overfitting KNN]
KNN with 2 neighbour is in overfitting.;[Overfitting KNN]
KNN with 1 neighbour is in overfitting.;[Overfitting KNN]
KNN with less than 8 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 8 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 7 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 7 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 6 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 6 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 5 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 5 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 4 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 4 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 3 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 3 neighbours is in overfitting.;[Overfitting KNN]
KNN with less than 2 neighbours is in overfitting.;[Overfitting KNN]
KNN with more than 2 neighbours is in overfitting.;[Overfitting KNN]
Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;[Overfitting RF]
Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;[Overfitting RF]
Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;[Overfitting RF]
Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;[Overfitting RF]
The specificity for the presented tree is lower than 74%.;[Decision tree]
The precision for the presented tree is lower than 71%.;[Decision tree]
The recall for the presented tree is lower than 77%.;[Decision tree]
The accuracy for the presented tree is lower than 74%.;[Decision tree]
The specificity for the presented tree is higher than 60%.;[Decision tree]
The precision for the presented tree is higher than 73%.;[Decision tree]
The recall for the presented tree is higher than 87%.;[Decision tree]
The accuracy for the presented tree is higher than 71%.;[Decision tree]
The specificity for the presented tree is lower than 64%.;[Decision tree]
The precision for the presented tree is lower than 74%.;[Decision tree]
The recall for the presented tree is lower than 88%.;[Decision tree]
The accuracy for the presented tree is lower than 76%.;[Decision tree]
The specificity for the presented tree is higher than 66%.;[Decision tree]
The precision for the presented tree is higher than 80%.;[Decision tree]
The recall for the presented tree is higher than 68%.;[Decision tree]
The accuracy for the presented tree is higher than 73%.;[Decision tree]
The specificity for the presented tree is lower than 86%.;[Decision tree]
The precision for the presented tree is lower than 64%.;[Decision tree]
The recall for the presented tree is lower than 85%.;[Decision tree]
The accuracy for the presented tree is lower than 83%.;[Decision tree]
The specificity for the presented tree is higher than 71%.;[Decision tree]
The precision for the presented tree is higher than 78%.;[Decision tree]
The recall for the presented tree is higher than 72%.;[Decision tree]
The accuracy for the presented tree is higher than 67%.;[Decision tree]
The specificity for the presented tree is lower than 81%.;[Decision tree]
The precision for the presented tree is lower than 61%.;[Decision tree]
The recall for the presented tree is lower than 69%.;[Decision tree]
The accuracy for the presented tree is lower than 60%.;[Decision tree]
The specificity for the presented tree is higher than 87%.;[Decision tree]
The precision for the presented tree is higher than 65%.;[Decision tree]
The recall for the presented tree is higher than 84%.;[Decision tree]
The accuracy for the presented tree is higher than 82%.;[Decision tree]
The specificity for the presented tree is lower than 79%.;[Decision tree]
The precision for the presented tree is lower than 62%.;[Decision tree]
The recall for the presented tree is lower than 89%.;[Decision tree]
The accuracy for the presented tree is lower than 90%.;[Decision tree]
The specificity for the presented tree is higher than 70%.;[Decision tree]
The precision for the presented tree is higher than 75%.;[Decision tree]
The recall for the presented tree is higher than 77%.;[Decision tree]
The accuracy for the presented tree is higher than 63%.;[Decision tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.;[Overfitting Decision Tree]
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 10.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 9.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 8.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 7.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 6.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 5.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 4.;[Overfitting Decision Tree]
The decision tree is in overfitting for depths above 3.;[Overfitting Decision Tree]
The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;[Overfitting Accuracy + Recall]
The number of False Negatives reported in the same tree is 17.;[Decision tree]
The number of True Negatives reported in the same tree is 50.;[Decision tree]
The number of False Positives reported in the same tree is 15.;[Decision tree]
The number of True Positives reported in the same tree is 19.;[Decision tree]
The number of False Negatives reported in the same tree is 35.;[Decision tree]
The number of True Negatives reported in the same tree is 33.;[Decision tree]
The number of False Positives reported in the same tree is 14.;[Decision tree]
The number of True Positives reported in the same tree is 43.;[Decision tree]
The number of False Negatives reported in the same tree is 37.;[Decision tree]
The number of True Negatives reported in the same tree is 38.;[Decision tree]
The number of False Positives reported in the same tree is 21.;[Decision tree]
The number of True Positives reported in the same tree is 28.;[Decision tree]
The number of False Negatives reported in the same tree is 27.;[Decision tree]
The number of True Negatives reported in the same tree is 20.;[Decision tree]
The number of False Positives reported in the same tree is 39.;[Decision tree]
The number of True Positives reported in the same tree is 40.;[Decision tree]
The number of False Negatives reported in the same tree is 23.;[Decision tree]
The number of True Negatives reported in the same tree is 45.;[Decision tree]
The number of False Positives reported in the same tree is 29.;[Decision tree]
The number of True Positives reported in the same tree is 48.;[Decision tree]
The number of True Negatives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of False Negatives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of True Negatives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of True Negatives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of True Positives is lower than the number of False Positives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of True Positives is higher than the number of False Positives for the presented tree.;[Decision tree]
The number of False Negatives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of True Negatives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of False Positives is lower than the number of True Positives for the presented tree.;[Decision tree]
The number of False Negatives is higher than the number of True Positives for the presented tree.;[Decision tree]
The number of True Negatives is higher than the number of True Positives for the presented tree.;[Decision tree]
The number of False Positives is higher than the number of True Positives for the presented tree.;[Decision tree]
The precision for the presented tree is lower than its specificity.;[Decision tree]
The recall for the presented tree is lower than its specificity.;[Decision tree]
The accuracy for the presented tree is lower than its specificity.;[Decision tree]
The precision for the presented tree is higher than its specificity.;[Decision tree]
The recall for the presented tree is higher than its specificity.;[Decision tree]
The accuracy for the presented tree is higher than its specificity.;[Decision tree]
The specificity for the presented tree is lower than its precision.;[Decision tree]
The recall for the presented tree is lower than its precision.;[Decision tree]
The accuracy for the presented tree is lower than its precision.;[Decision tree]
The specificity for the presented tree is higher than its precision.;[Decision tree]
The recall for the presented tree is higher than its precision.;[Decision tree]
The accuracy for the presented tree is higher than its precision.;[Decision tree]
The specificity for the presented tree is lower than its recall.;[Decision tree]
The precision for the presented tree is lower than its recall.;[Decision tree]
The accuracy for the presented tree is lower than its recall.;[Decision tree]
The specificity for the presented tree is higher than its recall.;[Decision tree]
The precision for the presented tree is higher than its recall.;[Decision tree]
The accuracy for the presented tree is higher than its recall.;[Decision tree]
The specificity for the presented tree is lower than its accuracy.;[Decision tree]
The precision for the presented tree is lower than its accuracy.;[Decision tree]
The recall for the presented tree is lower than its accuracy.;[Decision tree]
The specificity for the presented tree is higher than its accuracy.;[Decision tree]
The precision for the presented tree is higher than its accuracy.;[Decision tree]
The recall for the presented tree is higher than its accuracy.;[Decision tree]
The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;[Overfitting RF]
We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;[Overfitting Decision Tree]
We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;[Overfitting KNN]
We are able to identify the existence of overfitting for random forest models with more than 82 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 125 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 113 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 88 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for random forest models with more than 132 estimators.;[Overfitting RF]
We are able to identify the existence of overfitting for gradient boosting models with more than 95 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 99 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 117 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 70 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for gradient boosting models with more than 88 estimators.;[Overfitting GB]
We are able to identify the existence of overfitting for MLP models trained longer than 1164 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 1124 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 1448 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 1297 episodes.;[Overfitting MLP]
We are able to identify the existence of overfitting for MLP models trained longer than 668 episodes.;[Overfitting MLP]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, the Decision Tree presented classifies (not A, not B) as 1.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, the Decision Tree presented classifies (A, not B) as 1.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, the Decision Tree presented classifies (not A, B) as 1.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, the Decision Tree presented classifies (A,B) as 1.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, the Decision Tree presented classifies (not A, not B) as 0.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, the Decision Tree presented classifies (A, not B) as 0.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, the Decision Tree presented classifies (not A, B) as 0.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, the Decision Tree presented classifies (A,B) as 0.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, not B) as 1 for any k ≤ 131.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 131.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 131.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 131.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 131.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 131.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 131.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 131.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, not B) as 1 for any k ≤ 179.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 179.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 179.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 179.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 179.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 179.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 179.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 179.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, not B) as 1 for any k ≤ 436.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 436.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 436.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 436.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 436.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 436.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 436.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 436.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, not B) as 1 for any k ≤ 214.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 214.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 214.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 214.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 214.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 214.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 214.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 214.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 1.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that Naive Bayes algorithm classifies (A, not B), as 1.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that Naive Bayes algorithm classifies (not A, B), as 1.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that Naive Bayes algorithm classifies (A,B), as 1.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 0.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that Naive Bayes algorithm classifies (not A, B), as 0.;[Decision tree]
Considering that A=True<=>skewness<=5.16 and B=True<=>curtosis<=0.19, it is possible to state that Naive Bayes algorithm classifies (A,B), as 0.;[Decision tree]
