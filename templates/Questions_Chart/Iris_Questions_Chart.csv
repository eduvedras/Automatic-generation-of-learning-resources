Question;Charts_id
Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;['Iris_nr_records_nr_variables']
Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;['Iris_nr_records_nr_variables']
Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;['Iris_nr_records_nr_variables']
Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;['Iris_nr_records_nr_variables']
We face the curse of dimensionality when training a classifier with this dataset.;['Iris_nr_records_nr_variables']
The intrinsic dimensionality of this dataset is 29.;['Iris_correlation_heatmap']
The intrinsic dimensionality of this dataset is 21.;['Iris_correlation_heatmap']
The intrinsic dimensionality of this dataset is 44.;['Iris_correlation_heatmap']
The intrinsic dimensionality of this dataset is 91.;['Iris_correlation_heatmap']
The intrinsic dimensionality of this dataset is 100.;['Iris_correlation_heatmap']
The figure doesn’t show any missing values for PetalWidthCm, but these may be hidden as some non-pre-identified value.;['Iris_histograms_numeric', 'Iris_mv']
The figure doesn’t show any missing values for PetalLengthCm, but these may be hidden as some non-pre-identified value.;['Iris_histograms_numeric', 'Iris_mv']
The figure doesn’t show any missing values for SepalWidthCm, but these may be hidden as some non-pre-identified value.;['Iris_histograms_numeric', 'Iris_mv']
The figure doesn’t show any missing values for SepalLengthCm, but these may be hidden as some non-pre-identified value.;['Iris_histograms_numeric', 'Iris_mv']
All variables, but the class, should be dealt with as symbolic.;['Iris_scatter-plots', 'Iris_histograms']
All variables, but the class, should be dealt with as date.;['Iris_scatter-plots', 'Iris_histograms']
All variables, but the class, should be dealt with as binary.;['Iris_scatter-plots', 'Iris_histograms']
All variables, but the class, should be dealt with as numeric.;['Iris_scatter-plots', 'Iris_histograms']
The variable PetalWidthCm can be seen as ordinal.;['Iris_scatter-plots', 'Iris_histograms']
The variable PetalLengthCm can be seen as ordinal.;['Iris_scatter-plots', 'Iris_histograms']
The variable SepalWidthCm can be seen as ordinal.;['Iris_scatter-plots', 'Iris_histograms']
The variable SepalLengthCm can be seen as ordinal.;['Iris_scatter-plots', 'Iris_histograms']
The variable PetalWidthCm can be seen as ordinal without losing information.;['Iris_scatter-plots', 'Iris_histograms']
The variable PetalLengthCm can be seen as ordinal without losing information.;['Iris_scatter-plots', 'Iris_histograms']
The variable SepalWidthCm can be seen as ordinal without losing information.;['Iris_scatter-plots', 'Iris_histograms']
The variable SepalLengthCm can be seen as ordinal without losing information.;['Iris_scatter-plots', 'Iris_histograms']
Variable PetalWidthCm is balanced.;['Iris_histograms_numeric', 'Iris_boxplots']
Variable PetalLengthCm is balanced.;['Iris_histograms_numeric', 'Iris_boxplots']
Variable SepalWidthCm is balanced.;['Iris_histograms_numeric', 'Iris_boxplots']
Variable SepalLengthCm is balanced.;['Iris_histograms_numeric', 'Iris_boxplots']
Those boxplots show that the data is not normalized.;['Iris_boxplots']
It is clear that variable PetalLengthCm shows some outliers, but we can’t be sure of the same for variable PetalWidthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalWidthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable SepalLengthCm shows some outliers, but we can’t be sure of the same for variable PetalWidthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable SepalLengthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalWidthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable PetalLengthCm shows some outliers, but we can’t be sure of the same for variable SepalWidthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable SepalLengthCm shows some outliers, but we can’t be sure of the same for variable SepalWidthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable PetalLengthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.;['Iris_boxplots', 'Iris_histograms_numeric']
Outliers seem to be a problem in the dataset.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable PetalWidthCm shows a high number of outlier values.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable PetalLengthCm shows a high number of outlier values.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable SepalWidthCm shows a high number of outlier values.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable SepalLengthCm shows a high number of outlier values.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable PetalWidthCm shows some outlier values.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable PetalLengthCm shows some outlier values.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable SepalWidthCm shows some outlier values.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable SepalLengthCm shows some outlier values.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable PetalWidthCm doesn’t have any outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable PetalLengthCm doesn’t have any outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable SepalWidthCm doesn’t have any outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable SepalLengthCm doesn’t have any outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable PetalWidthCm presents some outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable PetalLengthCm presents some outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable SepalWidthCm presents some outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable SepalLengthCm presents some outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
At least 85 of the variables present outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
At least 75 of the variables present outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
At least 60 of the variables present outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
At least 50 of the variables present outliers.;['Iris_boxplots', 'Iris_histograms_numeric']
The histograms presented show a large number of outliers for most of the numeric variables.;['Iris_boxplots', 'Iris_histograms_numeric']
The boxplots presented show a large number of outliers for most of the numeric variables.;['Iris_boxplots', 'Iris_histograms_numeric']
The existence of outliers is one of the problems to tackle in this dataset.;['Iris_boxplots', 'Iris_histograms_numeric']
Variable Species is a false predictor.;['Iris_description']
Variable PetalWidthCm is a false predictor.;['Iris_description']
Variable PetalLengthCm is a false predictor.;['Iris_description']
Variable SepalWidthCm is a false predictor.;['Iris_description']
Variable SepalLengthCm is a false predictor.;['Iris_description']
One of the variables PetalLengthCm or PetalWidthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables SepalWidthCm or PetalWidthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables SepalLengthCm or PetalWidthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables PetalWidthCm or PetalLengthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables SepalWidthCm or PetalLengthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables SepalLengthCm or PetalLengthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables PetalWidthCm or SepalWidthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables PetalLengthCm or SepalWidthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables SepalLengthCm or SepalWidthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables PetalWidthCm or SepalLengthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables PetalLengthCm or SepalLengthCm can be discarded without losing information.;['Iris_correlation_heatmap']
One of the variables SepalWidthCm or SepalLengthCm can be discarded without losing information.;['Iris_correlation_heatmap']
The variable PetalWidthCm can be discarded without risking losing information.;['Iris_correlation_heatmap']
The variable PetalLengthCm can be discarded without risking losing information.;['Iris_correlation_heatmap']
The variable SepalWidthCm can be discarded without risking losing information.;['Iris_correlation_heatmap']
The variable SepalLengthCm can be discarded without risking losing information.;['Iris_correlation_heatmap']
Variables SepalLengthCm and SepalWidthCm are redundant, but we can’t say the same for the pair PetalLengthCm and PetalWidthCm.;['Iris_correlation_heatmap']
Variables SepalLengthCm and PetalLengthCm are redundant, but we can’t say the same for the pair SepalWidthCm and PetalWidthCm.;['Iris_correlation_heatmap']
Variables SepalWidthCm and PetalWidthCm are redundant, but we can’t say the same for the pair SepalLengthCm and PetalLengthCm.;['Iris_correlation_heatmap']
Variables PetalLengthCm and PetalWidthCm are redundant, but we can’t say the same for the pair SepalLengthCm and SepalWidthCm.;['Iris_correlation_heatmap']
Variables SepalLengthCm and PetalWidthCm are redundant, but we can’t say the same for the pair SepalWidthCm and PetalLengthCm.;['Iris_correlation_heatmap']
Variables SepalWidthCm and PetalLengthCm are redundant, but we can’t say the same for the pair SepalLengthCm and PetalWidthCm.;['Iris_correlation_heatmap']
Variables PetalLengthCm and PetalWidthCm are redundant.;['Iris_correlation_heatmap']
Variables SepalWidthCm and PetalWidthCm are redundant.;['Iris_correlation_heatmap']
Variables SepalLengthCm and PetalWidthCm are redundant.;['Iris_correlation_heatmap']
Variables PetalWidthCm and PetalLengthCm are redundant.;['Iris_correlation_heatmap']
Variables SepalWidthCm and PetalLengthCm are redundant.;['Iris_correlation_heatmap']
Variables SepalLengthCm and PetalLengthCm are redundant.;['Iris_correlation_heatmap']
Variables PetalWidthCm and SepalWidthCm are redundant.;['Iris_correlation_heatmap']
Variables PetalLengthCm and SepalWidthCm are redundant.;['Iris_correlation_heatmap']
Variables SepalLengthCm and SepalWidthCm are redundant.;['Iris_correlation_heatmap']
Variables PetalWidthCm and SepalLengthCm are redundant.;['Iris_correlation_heatmap']
Variables PetalLengthCm and SepalLengthCm are redundant.;['Iris_correlation_heatmap']
Variables SepalWidthCm and SepalLengthCm are redundant.;['Iris_correlation_heatmap']
From the correlation analysis alone, it is clear that there are relevant variables.;['Iris_correlation_heatmap']
It is clear that variable PetalWidthCm is one of the five most relevant features.;['Iris_decision_tree']
It is clear that variable PetalLengthCm is one of the five most relevant features.;['Iris_decision_tree']
It is clear that variable SepalWidthCm is one of the five most relevant features.;['Iris_decision_tree']
It is clear that variable SepalLengthCm is one of the five most relevant features.;['Iris_decision_tree']
It is clear that variable PetalWidthCm is one of the four most relevant features.;['Iris_decision_tree']
It is clear that variable PetalLengthCm is one of the four most relevant features.;['Iris_decision_tree']
It is clear that variable SepalWidthCm is one of the four most relevant features.;['Iris_decision_tree']
It is clear that variable SepalLengthCm is one of the four most relevant features.;['Iris_decision_tree']
It is clear that variable PetalWidthCm is one of the three most relevant features.;['Iris_decision_tree']
It is clear that variable PetalLengthCm is one of the three most relevant features.;['Iris_decision_tree']
It is clear that variable SepalWidthCm is one of the three most relevant features.;['Iris_decision_tree']
It is clear that variable SepalLengthCm is one of the three most relevant features.;['Iris_decision_tree']
It is clear that variable PetalWidthCm is one of the two most relevant features.;['Iris_decision_tree']
It is clear that variable PetalLengthCm is one of the two most relevant features.;['Iris_decision_tree']
It is clear that variable SepalWidthCm is one of the two most relevant features.;['Iris_decision_tree']
It is clear that variable SepalLengthCm is one of the two most relevant features.;['Iris_decision_tree']
The variable PetalWidthCm seems to be one of the five most relevant features.;['Iris_decision_tree']
The variable PetalLengthCm seems to be one of the five most relevant features.;['Iris_decision_tree']
The variable SepalWidthCm seems to be one of the five most relevant features.;['Iris_decision_tree']
The variable SepalLengthCm seems to be one of the five most relevant features.;['Iris_decision_tree']
The variable PetalWidthCm seems to be one of the four most relevant features.;['Iris_decision_tree']
The variable PetalLengthCm seems to be one of the four most relevant features.;['Iris_decision_tree']
The variable SepalWidthCm seems to be one of the four most relevant features.;['Iris_decision_tree']
The variable SepalLengthCm seems to be one of the four most relevant features.;['Iris_decision_tree']
The variable PetalWidthCm seems to be one of the three most relevant features.;['Iris_decision_tree']
The variable PetalLengthCm seems to be one of the three most relevant features.;['Iris_decision_tree']
The variable SepalWidthCm seems to be one of the three most relevant features.;['Iris_decision_tree']
The variable SepalLengthCm seems to be one of the three most relevant features.;['Iris_decision_tree']
The variable PetalWidthCm seems to be one of the two most relevant features.;['Iris_decision_tree']
The variable PetalLengthCm seems to be one of the two most relevant features.;['Iris_decision_tree']
The variable SepalWidthCm seems to be one of the two most relevant features.;['Iris_decision_tree']
The variable SepalLengthCm seems to be one of the two most relevant features.;['Iris_decision_tree']
The variable PetalWidthCm discriminates between the target values, as shown in the decision tree.;['Iris_decision_tree']
The variable PetalLengthCm discriminates between the target values, as shown in the decision tree.;['Iris_decision_tree']
The variable SepalWidthCm discriminates between the target values, as shown in the decision tree.;['Iris_decision_tree']
The variable SepalLengthCm discriminates between the target values, as shown in the decision tree.;['Iris_decision_tree']
It is possible to state that PetalWidthCm is the second most discriminative variable regarding the class.;['Iris_decision_tree']
It is possible to state that PetalLengthCm is the second most discriminative variable regarding the class.;['Iris_decision_tree']
It is possible to state that SepalWidthCm is the second most discriminative variable regarding the class.;['Iris_decision_tree']
It is possible to state that SepalLengthCm is the second most discriminative variable regarding the class.;['Iris_decision_tree']
It is possible to state that PetalWidthCm is the first most discriminative variable regarding the class.;['Iris_decision_tree']
It is possible to state that PetalLengthCm is the first most discriminative variable regarding the class.;['Iris_decision_tree']
It is possible to state that SepalWidthCm is the first most discriminative variable regarding the class.;['Iris_decision_tree']
It is possible to state that SepalLengthCm is the first most discriminative variable regarding the class.;['Iris_decision_tree']
Variable PetalWidthCm is one of the most relevant variables.;['Iris_decision_tree']
Variable PetalLengthCm is one of the most relevant variables.;['Iris_decision_tree']
Variable SepalWidthCm is one of the most relevant variables.;['Iris_decision_tree']
Variable SepalLengthCm is one of the most relevant variables.;['Iris_decision_tree']
Variable PetalWidthCm seems to be relevant for the majority of mining tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variable PetalLengthCm seems to be relevant for the majority of mining tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variable SepalWidthCm seems to be relevant for the majority of mining tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variable SepalLengthCm seems to be relevant for the majority of mining tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables PetalLengthCm and PetalWidthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables SepalWidthCm and PetalWidthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables SepalLengthCm and PetalWidthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables PetalWidthCm and PetalLengthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables SepalWidthCm and PetalLengthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables SepalLengthCm and PetalLengthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables PetalWidthCm and SepalWidthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables PetalLengthCm and SepalWidthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables SepalLengthCm and SepalWidthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables PetalWidthCm and SepalLengthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables PetalLengthCm and SepalLengthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
Variables SepalWidthCm and SepalLengthCm seem to be useful for classification tasks.;['Iris_decision_tree', 'Iris_correlation_heatmap']
A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;['Iris_boxplots']
A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;['Iris_boxplots']
Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;['Iris_correlation_heatmap']
Balancing this dataset by SMOTE would be riskier than oversampling by replication.;['Iris_scatter-plots']
Balancing this dataset by SMOTE would most probably be preferable over undersampling.;['Iris_nr_records_nr_variables']
Balancing this dataset would be mandatory to improve the results.;['Iris_class_histogram']
Both PetalLengthCm and PetalWidthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both SepalWidthCm and PetalWidthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both SepalLengthCm and PetalWidthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both PetalWidthCm and PetalLengthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both SepalWidthCm and PetalLengthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both SepalLengthCm and PetalLengthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both PetalWidthCm and SepalWidthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both PetalLengthCm and SepalWidthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both SepalLengthCm and SepalWidthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both PetalWidthCm and SepalLengthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both PetalLengthCm and SepalLengthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
Both SepalWidthCm and SepalLengthCm variables could be used to derive a new variable using a concept hierarchy.;['Iris_description']
The generation of a new feature through the conjunction of variables, would require some domain knowledge.;['Iris_description']
Considering the common semantics for PetalLengthCm and PetalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for SepalWidthCm and PetalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for SepalLengthCm and PetalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for PetalWidthCm and PetalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for SepalWidthCm and PetalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for SepalLengthCm and PetalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for PetalWidthCm and SepalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for PetalLengthCm and SepalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for SepalLengthCm and SepalWidthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for PetalWidthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for PetalLengthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for SepalWidthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;['Iris_description', 'Iris_histograms']
Considering the common semantics for PetalWidthCm variable, dummification would be the most adequate encoding.;['Iris_description', 'Iris_histograms']
Considering the common semantics for PetalLengthCm variable, dummification would be the most adequate encoding.;['Iris_description', 'Iris_histograms']
Considering the common semantics for SepalWidthCm variable, dummification would be the most adequate encoding.;['Iris_description', 'Iris_histograms']
Considering the common semantics for SepalLengthCm variable, dummification would be the most adequate encoding.;['Iris_description', 'Iris_histograms']
The variable PetalWidthCm can be coded as ordinal without losing information.;['Iris_histograms']
The variable PetalLengthCm can be coded as ordinal without losing information.;['Iris_histograms']
The variable SepalWidthCm can be coded as ordinal without losing information.;['Iris_histograms']
The variable SepalLengthCm can be coded as ordinal without losing information.;['Iris_histograms']
Discarding variables PetalLengthCm and PetalWidthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables SepalWidthCm and PetalWidthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables SepalLengthCm and PetalWidthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables PetalWidthCm and PetalLengthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables SepalWidthCm and PetalLengthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables SepalLengthCm and PetalLengthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables PetalWidthCm and SepalWidthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables PetalLengthCm and SepalWidthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables SepalLengthCm and SepalWidthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables PetalWidthCm and SepalLengthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables PetalLengthCm and SepalLengthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Discarding variables SepalWidthCm and SepalLengthCm would be better than discarding all the records with missing values for those variables.;['Iris_mv']
Dropping all records with missing values would be better than to drop the variables with missing values.;['Iris_mv']
Dropping all rows with missing values can lead to a dataset with less than 40% of the original data.;['Iris_mv']
Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.;['Iris_mv']
Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.;['Iris_mv']
There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;['Iris_mv']
Dummification is mandatory in this dataset.;[]
Dummifying the variables will improve the mining results.;[]
Feature generation based on both variables PetalLengthCm and PetalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables SepalWidthCm and PetalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables SepalLengthCm and PetalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables PetalWidthCm and PetalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables SepalWidthCm and PetalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables SepalLengthCm and PetalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables PetalWidthCm and SepalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables PetalLengthCm and SepalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables SepalLengthCm and SepalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables PetalWidthCm and SepalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables PetalLengthCm and SepalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on both variables SepalWidthCm and SepalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of PetalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable SepalWidthCm wouldn’t be useful, but the use of PetalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable SepalLengthCm wouldn’t be useful, but the use of PetalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable PetalWidthCm wouldn’t be useful, but the use of PetalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable SepalWidthCm wouldn’t be useful, but the use of PetalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable SepalLengthCm wouldn’t be useful, but the use of PetalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable PetalWidthCm wouldn’t be useful, but the use of SepalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of SepalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable SepalLengthCm wouldn’t be useful, but the use of SepalWidthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable PetalWidthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Feature generation based on the use of variable SepalWidthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.;['Iris_mv', 'Iris_histograms']
Given the usual semantics of PetalWidthCm variable, dummification would have been a better codification.;['Iris_histograms']
Given the usual semantics of PetalLengthCm variable, dummification would have been a better codification.;['Iris_histograms']
Given the usual semantics of SepalWidthCm variable, dummification would have been a better codification.;['Iris_histograms']
Given the usual semantics of SepalLengthCm variable, dummification would have been a better codification.;['Iris_histograms']
If PetalLengthCm and PetalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalWidthCm and PetalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalLengthCm and PetalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalWidthCm and PetalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalWidthCm and PetalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalLengthCm and PetalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalWidthCm and SepalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalLengthCm and SepalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalLengthCm and SepalWidthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalWidthCm and SepalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If PetalLengthCm and SepalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
If SepalWidthCm and SepalLengthCm were redundant then selecting just one of them would obviously increase the accuracy of KNN.;[]
It is better to drop the variable PetalWidthCm than removing all records with missing values.;['Iris_mv', 'Iris_histograms']
It is better to drop the variable PetalLengthCm than removing all records with missing values.;['Iris_mv', 'Iris_histograms']
It is better to drop the variable SepalWidthCm than removing all records with missing values.;['Iris_mv', 'Iris_histograms']
It is better to drop the variable SepalLengthCm than removing all records with missing values.;['Iris_mv', 'Iris_histograms']
Knowing that C and F are strongly correlated (correlation=1), we can say that removing one of those variables, would not have any impact on the performance of a KNN classifier.;[]
Missing value imputation using the mean value per class improves the quality of discovered patterns.;[]
Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;['Iris_boxplots']
Normalization of this dataset could not have impact on a KNN classifier.;['Iris_boxplots']
Normalization of this dataset should have a high impact on naïve Bayes classifier.;[]
Scaling this dataset should lead to a faster training with multi-layer perceptron.;[]
Not knowing the semantics of PetalWidthCm variable, dummification could have been a more adequate codification.;['Iris_histograms']
Not knowing the semantics of PetalLengthCm variable, dummification could have been a more adequate codification.;['Iris_histograms']
Not knowing the semantics of SepalWidthCm variable, dummification could have been a more adequate codification.;['Iris_histograms']
Not knowing the semantics of SepalLengthCm variable, dummification could have been a more adequate codification.;['Iris_histograms']
Removing the Species variable from the training will improve model performance over any non-observed records.;['Iris_description']
Removing the PetalWidthCm variable from the training will improve model performance over any non-observed records.;['Iris_description']
Removing the PetalLengthCm variable from the training will improve model performance over any non-observed records.;['Iris_description']
Removing the SepalWidthCm variable from the training will improve model performance over any non-observed records.;['Iris_description']
Removing the SepalLengthCm variable from the training will improve model performance over any non-observed records.;['Iris_description']
Removing variable PetalWidthCm might improve the training of decision trees .;['Iris_correlation_heatmap']
Removing variable PetalLengthCm might improve the training of decision trees .;['Iris_correlation_heatmap']
Removing variable SepalWidthCm might improve the training of decision trees .;['Iris_correlation_heatmap']
Removing variable SepalLengthCm might improve the training of decision trees .;['Iris_correlation_heatmap']
Scaling this dataset would be mandatory to improve the results with distance-based methods.;['Iris_boxplots']
The first 4 principal components are enough for explaining half the data variance.;['Iris_pca']
The first 3 principal components are enough for explaining half the data variance.;['Iris_pca']
The first 2 principal components are enough for explaining half the data variance.;['Iris_pca']
There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable PetalWidthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable SepalWidthCm previously than variable PetalWidthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable SepalLengthCm previously than variable PetalWidthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable PetalWidthCm previously than variable PetalLengthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable SepalWidthCm previously than variable PetalLengthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable SepalLengthCm previously than variable PetalLengthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable PetalWidthCm previously than variable SepalWidthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable SepalWidthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable SepalLengthCm previously than variable SepalWidthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable PetalWidthCm previously than variable SepalLengthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable SepalLengthCm.;['Iris_correlation_heatmap']
There is evidence in favour for sequential backward selection to select variable SepalWidthCm previously than variable SepalLengthCm.;['Iris_correlation_heatmap']
Using the first 4 principal components would imply an error between 15 and 30%.;['Iris_pca']
Using the first 3 principal components would imply an error between 15 and 30%.;['Iris_pca']
Using the first 2 principal components would imply an error between 15 and 30%.;['Iris_pca']
Using the first 4 principal components would imply an error between 10 and 30%.;['Iris_pca']
Using the first 3 principal components would imply an error between 10 and 30%.;['Iris_pca']
Using the first 2 principal components would imply an error between 10 and 30%.;['Iris_pca']
Using the first 4 principal components would imply an error between 5 and 30%.;['Iris_pca']
Using the first 3 principal components would imply an error between 5 and 30%.;['Iris_pca']
Using the first 2 principal components would imply an error between 5 and 30%.;['Iris_pca']
Using the first 4 principal components would imply an error between 15 and 25%.;['Iris_pca']
Using the first 3 principal components would imply an error between 15 and 25%.;['Iris_pca']
Using the first 2 principal components would imply an error between 15 and 25%.;['Iris_pca']
Using the first 4 principal components would imply an error between 10 and 25%.;['Iris_pca']
Using the first 3 principal components would imply an error between 10 and 25%.;['Iris_pca']
Using the first 2 principal components would imply an error between 10 and 25%.;['Iris_pca']
Using the first 4 principal components would imply an error between 5 and 25%.;['Iris_pca']
Using the first 3 principal components would imply an error between 5 and 25%.;['Iris_pca']
Using the first 2 principal components would imply an error between 5 and 25%.;['Iris_pca']
Using the first 4 principal components would imply an error between 15 and 20%.;['Iris_pca']
Using the first 3 principal components would imply an error between 15 and 20%.;['Iris_pca']
Using the first 2 principal components would imply an error between 15 and 20%.;['Iris_pca']
Using the first 4 principal components would imply an error between 10 and 20%.;['Iris_pca']
Using the first 3 principal components would imply an error between 10 and 20%.;['Iris_pca']
Using the first 2 principal components would imply an error between 10 and 20%.;['Iris_pca']
Using the first 4 principal components would imply an error between 5 and 20%.;['Iris_pca']
Using the first 3 principal components would imply an error between 5 and 20%.;['Iris_pca']
Using the first 2 principal components would imply an error between 5 and 20%.;['Iris_pca']
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;['Iris_decision_tree']
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 9%.;['Iris_decision_tree']
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;['Iris_decision_tree']
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 7%.;['Iris_decision_tree']
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;['Iris_decision_tree']
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;['Iris_decision_tree']
According to the decision tree overfitting chart, the tree with 8 nodes of depth is in overfitting.;['Iris_overfitting_decision_tree']
According to the decision tree overfitting chart, the tree with 7 nodes of depth is in overfitting.;['Iris_overfitting_decision_tree']
According to the decision tree overfitting chart, the tree with 6 nodes of depth is in overfitting.;['Iris_overfitting_decision_tree']
According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;['Iris_overfitting_decision_tree']
According to the decision tree overfitting chart, the tree with 4 nodes of depth is in overfitting.;['Iris_overfitting_decision_tree']
According to the decision tree overfitting chart, the tree with 3 nodes of depth is in overfitting.;['Iris_overfitting_decision_tree']
According to the charts, KNN and Decision Trees present a similar behaviour.;['Iris_overfitting_decision_tree', 'Iris_overfitting_knn']
As reported in the tree, the number of False Positive is bigger than the number of False Negatives.;['Iris_decision_tree']
As reported in the tree, the number of False Positive is smaller than the number of False Negatives.;['Iris_decision_tree']
Decision trees and KNN show similar behaviours.;['Iris_overfitting_decision_tree', 'Iris_overfitting_knn']
KNN and Decision Trees show a different trend in the majority of hyperparameters tested.;['Iris_overfitting_decision_tree', 'Iris_overfitting_knn']
KNN is in overfitting for k larger than 8.;['Iris_overfitting_knn']
KNN is in overfitting for k less than 8.;['Iris_overfitting_knn']
KNN is in overfitting for k larger than 7.;['Iris_overfitting_knn']
KNN is in overfitting for k less than 7.;['Iris_overfitting_knn']
KNN is in overfitting for k larger than 6.;['Iris_overfitting_knn']
KNN is in overfitting for k less than 6.;['Iris_overfitting_knn']
KNN is in overfitting for k larger than 5.;['Iris_overfitting_knn']
KNN is in overfitting for k less than 5.;['Iris_overfitting_knn']
KNN is in overfitting for k larger than 4.;['Iris_overfitting_knn']
KNN is in overfitting for k less than 4.;['Iris_overfitting_knn']
KNN is in overfitting for k larger than 3.;['Iris_overfitting_knn']
KNN is in overfitting for k less than 3.;['Iris_overfitting_knn']
KNN is in overfitting for k larger than 2.;['Iris_overfitting_knn']
KNN is in overfitting for k less than 2.;['Iris_overfitting_knn']
KNN with 10 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 9 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 8 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 7 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 6 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 5 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 4 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 3 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 2 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with 1 neighbour is in overfitting.;['Iris_overfitting_knn']
KNN with less than 8 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with more than 8 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with less than 7 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with more than 7 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with less than 6 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with more than 6 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with less than 5 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with more than 5 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with less than 4 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with more than 4 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with less than 3 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with more than 3 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with less than 2 neighbours is in overfitting.;['Iris_overfitting_knn']
KNN with more than 2 neighbours is in overfitting.;['Iris_overfitting_knn']
Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;['Iris_overfitting_rf']
Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;['Iris_overfitting_rf']
Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;['Iris_overfitting_rf']
Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;['Iris_overfitting_rf']
Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;['Iris_overfitting_rf']
Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;['Iris_overfitting_rf']
The specificity for the presented tree is lower than 62%.;['Iris_decision_tree']
The precision for the presented tree is lower than 88%.;['Iris_decision_tree']
The recall for the presented tree is lower than 87%.;['Iris_decision_tree']
The accuracy for the presented tree is lower than 85%.;['Iris_decision_tree']
The specificity for the presented tree is higher than 65%.;['Iris_decision_tree']
The precision for the presented tree is higher than 80%.;['Iris_decision_tree']
The recall for the presented tree is higher than 65%.;['Iris_decision_tree']
The accuracy for the presented tree is higher than 70%.;['Iris_decision_tree']
The specificity for the presented tree is lower than 87%.;['Iris_decision_tree']
The precision for the presented tree is lower than 71%.;['Iris_decision_tree']
The recall for the presented tree is lower than 60%.;['Iris_decision_tree']
The accuracy for the presented tree is lower than 77%.;['Iris_decision_tree']
The specificity for the presented tree is higher than 75%.;['Iris_decision_tree']
The precision for the presented tree is higher than 88%.;['Iris_decision_tree']
The recall for the presented tree is higher than 61%.;['Iris_decision_tree']
The accuracy for the presented tree is higher than 90%.;['Iris_decision_tree']
The specificity for the presented tree is lower than 72%.;['Iris_decision_tree']
The precision for the presented tree is lower than 69%.;['Iris_decision_tree']
The recall for the presented tree is lower than 66%.;['Iris_decision_tree']
The accuracy for the presented tree is lower than 84%.;['Iris_decision_tree']
The specificity for the presented tree is higher than 89%.;['Iris_decision_tree']
The precision for the presented tree is higher than 82%.;['Iris_decision_tree']
The recall for the presented tree is higher than 85%.;['Iris_decision_tree']
The accuracy for the presented tree is higher than 70%.;['Iris_decision_tree']
The specificity for the presented tree is lower than 81%.;['Iris_decision_tree']
The precision for the presented tree is lower than 83%.;['Iris_decision_tree']
The recall for the presented tree is lower than 67%.;['Iris_decision_tree']
The accuracy for the presented tree is lower than 78%.;['Iris_decision_tree']
The specificity for the presented tree is higher than 86%.;['Iris_decision_tree']
The precision for the presented tree is higher than 87%.;['Iris_decision_tree']
The recall for the presented tree is higher than 74%.;['Iris_decision_tree']
The accuracy for the presented tree is higher than 63%.;['Iris_decision_tree']
The specificity for the presented tree is lower than 64%.;['Iris_decision_tree']
The precision for the presented tree is lower than 80%.;['Iris_decision_tree']
The recall for the presented tree is lower than 79%.;['Iris_decision_tree']
The accuracy for the presented tree is lower than 73%.;['Iris_decision_tree']
The specificity for the presented tree is higher than 68%.;['Iris_decision_tree']
The precision for the presented tree is higher than 76%.;['Iris_decision_tree']
The recall for the presented tree is higher than 62%.;['Iris_decision_tree']
The accuracy for the presented tree is higher than 65%.;['Iris_decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.;['Iris_overfitting_decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.;['Iris_overfitting_decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.;['Iris_overfitting_decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.;['Iris_overfitting_decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.;['Iris_overfitting_decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.;['Iris_overfitting_decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.;['Iris_overfitting_decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.;['Iris_overfitting_decision_tree']
The decision tree is in overfitting for depths above 10.;['Iris_overfitting_decision_tree']
The decision tree is in overfitting for depths above 9.;['Iris_overfitting_decision_tree']
The decision tree is in overfitting for depths above 8.;['Iris_overfitting_decision_tree']
The decision tree is in overfitting for depths above 7.;['Iris_overfitting_decision_tree']
The decision tree is in overfitting for depths above 6.;['Iris_overfitting_decision_tree']
The decision tree is in overfitting for depths above 5.;['Iris_overfitting_decision_tree']
The decision tree is in overfitting for depths above 4.;['Iris_overfitting_decision_tree']
The decision tree is in overfitting for depths above 3.;['Iris_overfitting_decision_tree']
The number of False Negatives reported in the same tree is 18.;['Iris_decision_tree']
The number of True Negatives reported in the same tree is 42.;['Iris_decision_tree']
The number of False Positives reported in the same tree is 26.;['Iris_decision_tree']
The number of True Positives reported in the same tree is 32.;['Iris_decision_tree']
The number of False Negatives reported in the same tree is 24.;['Iris_decision_tree']
The number of True Negatives reported in the same tree is 46.;['Iris_decision_tree']
The number of False Positives reported in the same tree is 15.;['Iris_decision_tree']
The number of True Positives reported in the same tree is 47.;['Iris_decision_tree']
The number of False Negatives reported in the same tree is 33.;['Iris_decision_tree']
The number of True Negatives reported in the same tree is 37.;['Iris_decision_tree']
The number of False Positives reported in the same tree is 10.;['Iris_decision_tree']
The number of True Positives reported in the same tree is 43.;['Iris_decision_tree']
The number of False Negatives reported in the same tree is 49.;['Iris_decision_tree']
The number of True Negatives reported in the same tree is 45.;['Iris_decision_tree']
The number of False Positives reported in the same tree is 14.;['Iris_decision_tree']
The number of True Positives reported in the same tree is 21.;['Iris_decision_tree']
The number of False Negatives reported in the same tree is 29.;['Iris_decision_tree']
The number of True Negatives reported in the same tree is 41.;['Iris_decision_tree']
The number of False Positives reported in the same tree is 17.;['Iris_decision_tree']
The number of True Positives reported in the same tree is 27.;['Iris_decision_tree']
The number of True Negatives is lower than the number of False Negatives for the presented tree.;['Iris_decision_tree']
The number of False Positives is lower than the number of False Negatives for the presented tree.;['Iris_decision_tree']
The number of True Positives is lower than the number of False Negatives for the presented tree.;['Iris_decision_tree']
The number of True Negatives is higher than the number of False Negatives for the presented tree.;['Iris_decision_tree']
The number of False Positives is higher than the number of False Negatives for the presented tree.;['Iris_decision_tree']
The number of True Positives is higher than the number of False Negatives for the presented tree.;['Iris_decision_tree']
The number of False Negatives is lower than the number of True Negatives for the presented tree.;['Iris_decision_tree']
The number of False Positives is lower than the number of True Negatives for the presented tree.;['Iris_decision_tree']
The number of True Positives is lower than the number of True Negatives for the presented tree.;['Iris_decision_tree']
The number of False Negatives is higher than the number of True Negatives for the presented tree.;['Iris_decision_tree']
The number of False Positives is higher than the number of True Negatives for the presented tree.;['Iris_decision_tree']
The number of True Positives is higher than the number of True Negatives for the presented tree.;['Iris_decision_tree']
The number of False Negatives is lower than the number of False Positives for the presented tree.;['Iris_decision_tree']
The number of True Negatives is lower than the number of False Positives for the presented tree.;['Iris_decision_tree']
The number of True Positives is lower than the number of False Positives for the presented tree.;['Iris_decision_tree']
The number of False Negatives is higher than the number of False Positives for the presented tree.;['Iris_decision_tree']
The number of True Negatives is higher than the number of False Positives for the presented tree.;['Iris_decision_tree']
The number of True Positives is higher than the number of False Positives for the presented tree.;['Iris_decision_tree']
The number of False Negatives is lower than the number of True Positives for the presented tree.;['Iris_decision_tree']
The number of True Negatives is lower than the number of True Positives for the presented tree.;['Iris_decision_tree']
The number of False Positives is lower than the number of True Positives for the presented tree.;['Iris_decision_tree']
The number of False Negatives is higher than the number of True Positives for the presented tree.;['Iris_decision_tree']
The number of True Negatives is higher than the number of True Positives for the presented tree.;['Iris_decision_tree']
The number of False Positives is higher than the number of True Positives for the presented tree.;['Iris_decision_tree']
The precision for the presented tree is lower than its specificity.;['Iris_decision_tree']
The recall for the presented tree is lower than its specificity.;['Iris_decision_tree']
The accuracy for the presented tree is lower than its specificity.;['Iris_decision_tree']
The precision for the presented tree is higher than its specificity.;['Iris_decision_tree']
The recall for the presented tree is higher than its specificity.;['Iris_decision_tree']
The accuracy for the presented tree is higher than its specificity.;['Iris_decision_tree']
The specificity for the presented tree is lower than its precision.;['Iris_decision_tree']
The recall for the presented tree is lower than its precision.;['Iris_decision_tree']
The accuracy for the presented tree is lower than its precision.;['Iris_decision_tree']
The specificity for the presented tree is higher than its precision.;['Iris_decision_tree']
The recall for the presented tree is higher than its precision.;['Iris_decision_tree']
The accuracy for the presented tree is higher than its precision.;['Iris_decision_tree']
The specificity for the presented tree is lower than its recall.;['Iris_decision_tree']
The precision for the presented tree is lower than its recall.;['Iris_decision_tree']
The accuracy for the presented tree is lower than its recall.;['Iris_decision_tree']
The specificity for the presented tree is higher than its recall.;['Iris_decision_tree']
The precision for the presented tree is higher than its recall.;['Iris_decision_tree']
The accuracy for the presented tree is higher than its recall.;['Iris_decision_tree']
The specificity for the presented tree is lower than its accuracy.;['Iris_decision_tree']
The precision for the presented tree is lower than its accuracy.;['Iris_decision_tree']
The recall for the presented tree is lower than its accuracy.;['Iris_decision_tree']
The specificity for the presented tree is higher than its accuracy.;['Iris_decision_tree']
The precision for the presented tree is higher than its accuracy.;['Iris_decision_tree']
The recall for the presented tree is higher than its accuracy.;['Iris_decision_tree']
The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;['Iris_overfitting_rf']
We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;['Iris_overfitting_decision_tree']
We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;['Iris_overfitting_decision_tree']
We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;['Iris_overfitting_decision_tree']
We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;['Iris_overfitting_decision_tree']
We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;['Iris_overfitting_decision_tree']
We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;['Iris_overfitting_knn']
We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;['Iris_overfitting_knn']
We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;['Iris_overfitting_knn']
We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;['Iris_overfitting_knn']
We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;['Iris_overfitting_knn']
We are able to identify the existence of overfitting for random forest models with more than 54 estimators.;['Iris_overfitting_rf']
We are able to identify the existence of overfitting for random forest models with more than 109 estimators.;['Iris_overfitting_rf']
We are able to identify the existence of overfitting for random forest models with more than 97 estimators.;['Iris_overfitting_rf']
We are able to identify the existence of overfitting for random forest models with more than 90 estimators.;['Iris_overfitting_rf']
We are able to identify the existence of overfitting for random forest models with more than 87 estimators.;['Iris_overfitting_rf']
We are able to identify the existence of overfitting for gradient boosting models with more than 55 estimators.;['Iris_overfitting_gb']
We are able to identify the existence of overfitting for gradient boosting models with more than 98 estimators.;['Iris_overfitting_gb']
We are able to identify the existence of overfitting for gradient boosting models with more than 67 estimators.;['Iris_overfitting_gb']
We are able to identify the existence of overfitting for gradient boosting models with more than 72 estimators.;['Iris_overfitting_gb']
We are able to identify the existence of overfitting for gradient boosting models with more than 148 estimators.;['Iris_overfitting_gb']
We are able to identify the existence of overfitting for MLP models trained longer than 767 episodes.;['Iris_overfitting_mlp']
We are able to identify the existence of overfitting for MLP models trained longer than 541 episodes.;['Iris_overfitting_mlp']
We are able to identify the existence of overfitting for MLP models trained longer than 939 episodes.;['Iris_overfitting_mlp']
We are able to identify the existence of overfitting for MLP models trained longer than 566 episodes.;['Iris_overfitting_mlp']
We are able to identify the existence of overfitting for MLP models trained longer than 598 episodes.;['Iris_overfitting_mlp']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (not A, not B) as Iris-virginica.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (A, not B) as Iris-virginica.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (not A, B) as Iris-virginica.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (A,B) as Iris-virginica.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (not A, not B) as Iris-versicolor.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (A, not B) as Iris-versicolor.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (not A, B) as Iris-versicolor.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (A,B) as Iris-versicolor.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (not A, not B) as Iris-setosa.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (A, not B) as Iris-setosa.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (not A, B) as Iris-setosa.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, the Decision Tree presented classifies (A,B) as Iris-setosa.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-virginica for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-virginica for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-virginica for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-virginica for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-versicolor for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-versicolor for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-versicolor for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-versicolor for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-setosa for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-setosa for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-setosa for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-setosa for any k ≤ 32.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-virginica for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-virginica for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-virginica for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-virginica for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-versicolor for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-versicolor for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-versicolor for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-versicolor for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-setosa for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-setosa for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-setosa for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-setosa for any k ≤ 38.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-virginica for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-virginica for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-virginica for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-virginica for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-versicolor for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-versicolor for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-versicolor for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-versicolor for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, not B) as Iris-setosa for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A, not B) as Iris-setosa for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (not A, B) as Iris-setosa for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that KNN algorithm classifies (A,B) as Iris-setosa for any k ≤ 35.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (not A, not B), as Iris-virginica.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (A, not B), as Iris-virginica.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (not A, B), as Iris-virginica.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (A,B), as Iris-virginica.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (not A, not B), as Iris-versicolor.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (A, not B), as Iris-versicolor.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (not A, B), as Iris-versicolor.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (A,B), as Iris-versicolor.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (not A, not B), as Iris-setosa.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (A, not B), as Iris-setosa.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (not A, B), as Iris-setosa.;['Iris_decision_tree']
Considering that A=True<=>PetalWidthCm<=0.7 and B=True<=>PetalWidthCm<=1.75, it is possible to state that Naive Bayes algorithm classifies (A,B), as Iris-setosa.;['Iris_decision_tree']
