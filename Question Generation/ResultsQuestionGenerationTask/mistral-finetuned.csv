Image,Prediction,References
Titanic_decision_tree.png,"The chart doesn't provide enough information to determine if the condition of the second decision is Parch <= 0.5 or not.","[""It is clear that variable Pclass is one of the five most relevant features.', 'The variable Pclass seems to be one of the four most relevant features.', 'The variable Age discriminates between the target values, as shown in the decision tree.', 'It is possible to state that Age is the first most discriminative variable regarding the class.', 'Variable Pclass is one of the most relevant variables.', 'Variable Age seems to be relevant for the majority of mining tasks.', 'Variables Parch and SibSp seem to be useful for classification tasks.', 'A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.', 'As reported in the tree, the number of False Positive is bigger than the number of False Negatives.', 'The recall for the presented tree is higher than 60%.', 'The number of False Negatives is lower than the number of True Negatives for the presented tree.', 'The number of True Negatives is higher than the number of False Positives for the presented tree.', 'The number of False Negatives is lower than the number of True Negatives for the presented tree.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 0.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 72.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 181.""]"
Titanic_overfitting_mlp.png,"This chart indicates that the accuracy reaches its highest value after 100 iterations.",['We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.']
Titanic_overfitting_gb.png,"An overfitting occurs when the number of estimators is greater than 1024.",['We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.']
Titanic_overfitting_rf.png,"random forests with more estimators always achieve higher accuracy.","[""Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.', 'The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.', 'We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.""]"
Titanic_overfitting_knn.png,"The accuracy increases as the number of neighbors approaches 6.","[""KNN is in overfitting for k less than 17.', 'KNN with 11 neighbour is in overfitting.', 'KNN with more than 7 neighbours is in overfitting.', 'We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.""]"
Titanic_overfitting_decision_tree.png,"""If the decision tree max depth increases, the accuracy of the model will also increase up to a certain point, beyond which the accuracy will start decreasing.""","[""According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.', 'The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.', 'The decision tree is in overfitting for depths above 3.', 'We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.""]"
Titanic_overfitting_dt_acc_rec.png," Increasing the max depth leads to an optimal balance between accuracy and recall.",['The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.']
Titanic_pca.png,"The last principal component explains approximately 3% of the variance.","[""The first 4 principal components are enough for explaining half the data variance.', 'Using the first 4 principal components would imply an error between 10 and 20%.""]"
Titanic_correlation_heatmap.png,"The variables with highest correlation among each other are 'Pclass' and 'Age'.","[""The intrinsic dimensionality of this dataset is 2.', 'One of the variables Fare or Pclass can be discarded without losing information.', 'The variable Pclass can be discarded without risking losing information.', 'Variables Age and Parch are redundant, but we can’t say the same for the pair Fare and Pclass.', 'Variables SibSp and Fare are redundant.', 'From the correlation analysis alone, it is clear that there are relevant variables.', 'Variable Age seems to be relevant for the majority of mining tasks.', 'Variables Parch and Fare seem to be useful for classification tasks.', 'Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.', 'Removing variable Parch might improve the training of decision trees .', 'There is evidence in favour for sequential backward selection to select variable Parch previously than variable Age.""]"
Titanic_boxplots.png," The passengers with low fare values usually had a larger number of siblings and parents aboard.","[""Variable Fare is balanced.', 'Those boxplots show that the data is not normalized.', 'It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Pclass.', 'Outliers seem to be a problem in the dataset.', 'Variable Parch shows some outlier values.', 'Variable Parch doesn’t have any outliers.', 'Variable Parch presents some outliers.', 'At least 60 of the variables present outliers.', 'The boxplots presented show a large number of outliers for most of the numeric variables.', 'The existence of outliers is one of the problems to tackle in this dataset.', 'A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.', 'Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.', 'Normalization of this dataset could not have impact on a KNN classifier.', 'Scaling this dataset would be mandatory to improve the results with distance-based methods.""]"
Titanic_histograms_symbolic.png," Yes, for the given Embarked value, the corresponding Sex bar chart can be consulted.","[""All variables, but the class, should be dealt with as date.', 'The variable Embarked can be seen as ordinal.', 'The variable Embarked can be seen as ordinal without losing information.', 'Considering the common semantics for Sex and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.', 'Considering the common semantics for Embarked variable, dummification would be the most adequate encoding.', 'The variable Embarked can be coded as ordinal without losing information.', 'Feature generation based on variable Sex seems to be promising.', 'Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Embarked seems to be promising.', 'Given the usual semantics of Embarked variable, dummification would have been a better codification.', 'It is better to drop the variable Embarked than removing all records with missing values.', 'Not knowing the semantics of Sex variable, dummification could have been a more adequate codification.""]"
Titanic_mv.png,"There is no missing value in the variable ""Embarked"".","[""Discarding variable Age would be better than discarding all the records with missing values for that variable.', 'Dropping all records with missing values would be better than to drop the variables with missing values.', 'Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.', 'There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.', 'Feature generation based on variable Embarked seems to be promising.', 'It is better to drop the variable Embarked than removing all records with missing values.""]"
Titanic_class_histogram.png,"The passengers in the 2nd class have a higher survival rate than the passengers in the 1st class.",['Balancing this dataset would be mandatory to improve the results.']
Titanic_nr_records_nr_variables.png,"The chart indicates the number of records is greater than the number of variables.","[""Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.', 'We face the curse of dimensionality when training a classifier with this dataset.', 'Balancing this dataset by SMOTE would most probably be preferable over undersampling.""]"
Titanic_histograms_numeric.png,"On average, first class passengers were older than third class passengers.","[""All variables, but the class, should be dealt with as date.', 'The variable Age can be seen as ordinal.', 'The variable Fare can be seen as ordinal without losing information.', 'Variable Age is balanced.', 'It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable Age.', 'Outliers seem to be a problem in the dataset.', 'Variable Parch shows some outlier values.', 'Variable Fare doesn’t have any outliers.', 'Variable Age presents some outliers.', 'At least 60 of the variables present outliers.', 'The boxplots presented show a large number of outliers for most of the numeric variables.', 'The existence of outliers is one of the problems to tackle in this dataset.', 'Considering the common semantics for Fare and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.', 'Considering the common semantics for Fare variable, dummification would be the most adequate encoding.', 'The variable Age can be coded as ordinal without losing information.', 'Feature generation based on variable SibSp seems to be promising.', 'Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Pclass seems to be promising.', 'Given the usual semantics of Age variable, dummification would have been a better codification.', 'It is better to drop the variable SibSp than removing all records with missing values.', 'Not knowing the semantics of Parch variable, dummification could have been a more adequate codification.""]"
