{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489c6b60-368b-47ca-976d-0dd184ad627c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/b6/4d/fbe6d89fde59d8107f0a02816c4ac4542a8f9a85559fdf33c68282affcc1/transformers-4.38.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/ab/28/d4b691840d73126d4c9845f8a22dad033ac872509b6d3a0d93b456eef424/huggingface_hub-0.21.4-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/eb/10/4ccc8eed80f11c082a2883d49d4090aa80c7f65704216a529f490cb089b1/regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/0e/d7/3220a4483d240180d0b9423206cc57a4997fd4b49a8393e5ce9a2f7908a9/tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/50/7f/8f6dd5b16cdc9efb01ea6169037c2b4a1e3b433baae78ab14c0f8f88f012/safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/ad/30/2281c062222dc39328843bd1ddd30ff3005ef8e30b2fd09c4d2792766061/fsspec-2024.2.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Using cached regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
      "Using cached safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.21.4 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.2\n"
     ]
    }
   ],
   "source": [
    "! pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a33916-d7f1-4247-a457-be5b054936ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/95/fc/661a7f06e8b7d48fcbd3f55423b7ff1ac3ce59526f146fda87a1e1788ee4/datasets-2.18.0-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=12.0.0 from https://files.pythonhosted.org/packages/cc/58/086a8c7e8b4c8ee9f505eec227a7641e385c06a2ac9d4bbf18447b4e1ed2/pyarrow-15.0.1-cp38-cp38-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached pyarrow-15.0.1-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/ad/80/8fc9a4d76b259c901f2c85ed10f330a8fb51993a577bddfd53a852595e12/xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached pyarrow-15.0.1-cp38-cp38-manylinux_2_28_x86_64.whl (38.4 MB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, datasets\n",
      "Successfully installed datasets-2.18.0 pyarrow-15.0.1 pyarrow-hotfix-0.6 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e772363c-2277-46b0-a969-c0ca9ea18143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/datasets/load.py:1461: FutureWarning: The repository for eduvedras/Img_Vars contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eduvedras/Img_Vars\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d07558078124f94bb4529eff7400ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2957d09747f1441b857845007d82387a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54dbbc28bcc48c3ba7f1a98c70738e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"eduvedras/Img_Vars\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c783dc17-7c38-4f5d-9985-c7b3ad7bfb27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "MAX_PATCHES = 1024\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"Chart\"], return_tensors=\"pt\", add_special_tokens=True, max_patches=MAX_PATCHES)\n",
    "        \n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        encoding[\"Description\"] = item[\"Description\"]\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4d3dbc-9030-4a9e-8c38-0fcc58229e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n",
    "model_id = \"google/pix2struct-textcaps-base\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cb8585-31e0-466c-a802-fdb985799eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "  new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n",
    "  texts = [item[\"Description\"] for item in batch]\n",
    "  \n",
    "  text_inputs = processor(text=texts, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True, max_length=166)\n",
    "  \n",
    "  new_batch[\"labels\"] = text_inputs.input_ids\n",
    "  \n",
    "  for item in batch:\n",
    "    new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n",
    "    new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n",
    "  \n",
    "  new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n",
    "  new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n",
    "\n",
    "  return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3424b2-57cc-462a-85a4-54dade788f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dd243-1848-48d9-bc56-06dd23295ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/studio-lab-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 27.539587020874023\n",
      "Loss: 26.689891815185547\n",
      "Loss: 28.286235809326172\n",
      "Loss: 21.287620544433594\n",
      "Loss: 24.557132720947266\n",
      "Loss: 21.823354721069336\n",
      "Loss: 25.669546127319336\n",
      "Loss: 24.576026916503906\n",
      "Loss: 22.962800979614258\n",
      "Loss: 25.834272384643555\n",
      "Loss: 25.072450637817383\n",
      "Loss: 23.41463279724121\n",
      "Loss: 22.915016174316406\n",
      "Loss: 22.870162963867188\n",
      "Loss: 23.773395538330078\n",
      "Loss: 23.415298461914062\n",
      "Loss: 21.443838119506836\n",
      "Loss: 21.514514923095703\n",
      "Loss: 21.064828872680664\n",
      "Loss: 21.76187515258789\n",
      "Loss: 19.642311096191406\n",
      "Loss: 19.69300651550293\n",
      "Loss: 19.848854064941406\n",
      "Loss: 20.894939422607422\n",
      "Loss: 19.127105712890625\n",
      "Loss: 19.688093185424805\n",
      "Loss: 16.032325744628906\n",
      "Loss: 16.332115173339844\n",
      "Loss: 17.446090698242188\n",
      "Loss: 16.651466369628906\n",
      "Loss: 17.373544692993164\n",
      "Loss: 17.421518325805664\n",
      "Loss: 17.924711227416992\n",
      "Loss: 15.474569320678711\n",
      "Loss: 17.50925636291504\n",
      "Loss: 16.92034912109375\n",
      "Loss: 15.217510223388672\n",
      "Loss: 17.391155242919922\n",
      "Loss: 16.032602310180664\n",
      "Loss: 17.054828643798828\n",
      "Loss: 15.127188682556152\n",
      "Loss: 15.786541938781738\n",
      "Loss: 16.57895851135254\n",
      "Loss: 14.199146270751953\n",
      "Loss: 14.546807289123535\n",
      "Loss: 15.466270446777344\n",
      "Loss: 16.708070755004883\n",
      "Loss: 14.440120697021484\n",
      "Loss: 12.349329948425293\n",
      "Loss: 12.333602905273438\n",
      "Loss: 14.34322452545166\n",
      "Loss: 14.178559303283691\n",
      "Loss: 14.517732620239258\n",
      "Loss: 14.603584289550781\n",
      "Loss: 12.278228759765625\n",
      "Loss: 13.531683921813965\n",
      "Loss: 13.19288444519043\n",
      "Loss: 13.206032752990723\n",
      "Loss: 12.108960151672363\n",
      "Loss: 12.661832809448242\n",
      "Loss: 12.413836479187012\n",
      "Loss: 12.049790382385254\n",
      "Loss: 12.35106086730957\n",
      "Loss: 11.443551063537598\n",
      "Loss: 10.71764087677002\n",
      "Loss: 10.534449577331543\n",
      "Loss: 10.89797306060791\n",
      "Loss: 10.353367805480957\n",
      "Loss: 10.679619789123535\n",
      "Loss: 9.054676055908203\n",
      "Loss: 9.901594161987305\n",
      "Loss: 9.703415870666504\n",
      "Loss: 10.378316879272461\n",
      "Loss: 8.594993591308594\n",
      "Loss: 10.176307678222656\n",
      "Loss: 7.907256126403809\n",
      "Loss: 8.578110694885254\n",
      "Loss: 9.121563911437988\n",
      "Loss: 7.879021644592285\n",
      "Loss: 7.6215620040893555\n",
      "Loss: 7.703033924102783\n",
      "Loss: 8.259147644042969\n",
      "Loss: 7.451797008514404\n",
      "Loss: 9.87674617767334\n",
      "Loss: 7.427979946136475\n",
      "Loss: 7.332357883453369\n",
      "Loss: 8.25245475769043\n",
      "Loss: 7.049187660217285\n",
      "Loss: 8.129251480102539\n",
      "Loss: 7.420421123504639\n",
      "Loss: 7.0328450202941895\n",
      "Loss: 7.054995059967041\n",
      "Loss: 7.43701696395874\n",
      "Loss: 6.3177947998046875\n",
      "Loss: 6.054254055023193\n",
      "Loss: 6.826630592346191\n",
      "Loss: 6.230525970458984\n",
      "Loss: 6.489139556884766\n",
      "Loss: 6.169029712677002\n",
      "Loss: 5.97221565246582\n",
      "Loss: 6.808906078338623\n",
      "Loss: 6.450864315032959\n",
      "Loss: 6.371256351470947\n",
      "Loss: 6.397097110748291\n",
      "Loss: 6.105777263641357\n",
      "Loss: 6.2969465255737305\n",
      "Loss: 5.9545207023620605\n",
      "Loss: 6.24312162399292\n",
      "Loss: 5.887964725494385\n",
      "Loss: 5.902169704437256\n",
      "Loss: 5.762589454650879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/63 [01:29<1:32:07, 89.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.187178134918213\n",
      "Epoch: 1\n",
      "Loss: 5.479941368103027\n",
      "Loss: 5.531160354614258\n",
      "Loss: 5.536831378936768\n",
      "Loss: 6.252987861633301\n",
      "Loss: 5.694084644317627\n",
      "Loss: 5.377201080322266\n",
      "Loss: 5.2012619972229\n",
      "Loss: 5.518664836883545\n",
      "Loss: 6.120620250701904\n",
      "Loss: 5.592771530151367\n",
      "Loss: 5.926486015319824\n",
      "Loss: 5.619228363037109\n",
      "Loss: 6.0750885009765625\n",
      "Loss: 5.808279037475586\n",
      "Loss: 6.12955904006958\n",
      "Loss: 5.972214221954346\n",
      "Loss: 5.694229602813721\n",
      "Loss: 6.222742080688477\n",
      "Loss: 5.537179470062256\n",
      "Loss: 5.915146350860596\n",
      "Loss: 5.841073513031006\n",
      "Loss: 5.9164204597473145\n",
      "Loss: 5.891717910766602\n",
      "Loss: 4.693178176879883\n",
      "Loss: 5.447741508483887\n",
      "Loss: 5.5254316329956055\n",
      "Loss: 5.878856182098389\n",
      "Loss: 6.030336856842041\n",
      "Loss: 6.035490036010742\n",
      "Loss: 5.628006935119629\n",
      "Loss: 5.575549602508545\n",
      "Loss: 5.188356876373291\n",
      "Loss: 6.102481842041016\n",
      "Loss: 5.745399475097656\n",
      "Loss: 5.154699325561523\n",
      "Loss: 5.904969692230225\n",
      "Loss: 5.5325775146484375\n",
      "Loss: 5.735956192016602\n",
      "Loss: 5.464939117431641\n",
      "Loss: 5.4704132080078125\n",
      "Loss: 5.691005706787109\n",
      "Loss: 4.358926773071289\n",
      "Loss: 5.78913688659668\n",
      "Loss: 5.564294338226318\n",
      "Loss: 5.659938335418701\n",
      "Loss: 5.605105400085449\n",
      "Loss: 5.087378978729248\n",
      "Loss: 5.069167137145996\n",
      "Loss: 5.79927921295166\n",
      "Loss: 5.678962230682373\n",
      "Loss: 5.092253684997559\n",
      "Loss: 5.3829216957092285\n",
      "Loss: 5.941095352172852\n",
      "Loss: 5.27476167678833\n",
      "Loss: 5.851761341094971\n",
      "Loss: 5.444330215454102\n",
      "Loss: 5.782077312469482\n",
      "Loss: 5.597659587860107\n",
      "Loss: 5.115196704864502\n",
      "Loss: 4.655331134796143\n",
      "Loss: 5.259954929351807\n",
      "Loss: 5.793611526489258\n",
      "Loss: 5.747950553894043\n",
      "Loss: 5.8735198974609375\n",
      "Loss: 5.672618389129639\n",
      "Loss: 5.813342094421387\n",
      "Loss: 5.650536060333252\n",
      "Loss: 5.484132766723633\n",
      "Loss: 5.444563865661621\n",
      "Loss: 5.542508602142334\n",
      "Loss: 5.327118873596191\n",
      "Loss: 5.533193111419678\n",
      "Loss: 5.262752532958984\n",
      "Loss: 5.527918338775635\n",
      "Loss: 5.410320281982422\n",
      "Loss: 5.221920967102051\n",
      "Loss: 5.544620513916016\n",
      "Loss: 5.145116806030273\n",
      "Loss: 5.017389297485352\n",
      "Loss: 5.1689276695251465\n",
      "Loss: 4.6258697509765625\n",
      "Loss: 4.916450023651123\n",
      "Loss: 5.653512477874756\n",
      "Loss: 5.306424617767334\n",
      "Loss: 5.621133327484131\n",
      "Loss: 5.743349552154541\n",
      "Loss: 5.381931781768799\n",
      "Loss: 5.307578086853027\n",
      "Loss: 5.170902252197266\n",
      "Loss: 5.344384670257568\n",
      "Loss: 5.172582626342773\n",
      "Loss: 5.307726860046387\n",
      "Loss: 5.395814895629883\n",
      "Loss: 4.832823753356934\n",
      "Loss: 5.675224781036377\n",
      "Loss: 5.639398574829102\n",
      "Loss: 4.568716526031494\n",
      "Loss: 5.2263078689575195\n",
      "Loss: 5.414684772491455\n",
      "Loss: 5.633152484893799\n",
      "Loss: 4.706419944763184\n",
      "Loss: 5.163404941558838\n",
      "Loss: 5.715615272521973\n",
      "Loss: 5.386406421661377\n",
      "Loss: 5.423720836639404\n",
      "Loss: 5.731624126434326\n",
      "Loss: 5.460089206695557\n",
      "Loss: 5.52081823348999\n",
      "Loss: 5.194289207458496\n",
      "Loss: 5.039493560791016\n",
      "Loss: 5.264644145965576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/63 [02:58<1:30:32, 89.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.551211357116699\n",
      "Epoch: 2\n",
      "Loss: 4.679264068603516\n",
      "Loss: 4.5368828773498535\n",
      "Loss: 5.271759986877441\n",
      "Loss: 4.675450801849365\n",
      "Loss: 5.213108539581299\n",
      "Loss: 5.245861530303955\n",
      "Loss: 5.350129127502441\n",
      "Loss: 4.922002792358398\n",
      "Loss: 4.382055282592773\n",
      "Loss: 5.062652587890625\n",
      "Loss: 5.579261302947998\n",
      "Loss: 5.186740398406982\n",
      "Loss: 5.238481044769287\n",
      "Loss: 5.532126426696777\n",
      "Loss: 4.9352827072143555\n",
      "Loss: 4.974579811096191\n",
      "Loss: 4.884280681610107\n",
      "Loss: 5.116837978363037\n",
      "Loss: 4.829580783843994\n",
      "Loss: 4.7991623878479\n",
      "Loss: 5.08726167678833\n",
      "Loss: 5.517478942871094\n",
      "Loss: 5.4141645431518555\n",
      "Loss: 5.1354146003723145\n",
      "Loss: 4.761096477508545\n",
      "Loss: 5.415634632110596\n",
      "Loss: 4.819347858428955\n",
      "Loss: 5.342661380767822\n",
      "Loss: 4.966150283813477\n",
      "Loss: 5.686895847320557\n",
      "Loss: 4.138999938964844\n",
      "Loss: 5.114664554595947\n",
      "Loss: 4.985445499420166\n",
      "Loss: 5.105896472930908\n",
      "Loss: 4.59055233001709\n",
      "Loss: 5.342808246612549\n",
      "Loss: 5.435965538024902\n",
      "Loss: 5.455926418304443\n",
      "Loss: 4.718613624572754\n",
      "Loss: 5.18539571762085\n",
      "Loss: 5.268765926361084\n",
      "Loss: 4.942630290985107\n",
      "Loss: 5.4259419441223145\n",
      "Loss: 5.391870021820068\n",
      "Loss: 5.035126686096191\n",
      "Loss: 4.886444568634033\n",
      "Loss: 5.011537551879883\n",
      "Loss: 4.558062553405762\n",
      "Loss: 4.219923973083496\n",
      "Loss: 4.919896602630615\n",
      "Loss: 5.144975185394287\n",
      "Loss: 4.744562149047852\n",
      "Loss: 4.901182651519775\n",
      "Loss: 4.868409633636475\n",
      "Loss: 5.314174175262451\n",
      "Loss: 4.772330284118652\n",
      "Loss: 5.033313274383545\n",
      "Loss: 4.730654239654541\n",
      "Loss: 4.98441219329834\n",
      "Loss: 4.989905834197998\n",
      "Loss: 4.916121959686279\n",
      "Loss: 4.78338623046875\n",
      "Loss: 4.811148643493652\n",
      "Loss: 4.603215217590332\n",
      "Loss: 4.948119163513184\n",
      "Loss: 5.039722919464111\n",
      "Loss: 5.0360107421875\n",
      "Loss: 5.1100053787231445\n",
      "Loss: 4.775084495544434\n",
      "Loss: 5.146405220031738\n",
      "Loss: 4.292080402374268\n",
      "Loss: 4.579451084136963\n",
      "Loss: 4.8661651611328125\n",
      "Loss: 4.783255577087402\n",
      "Loss: 4.7716779708862305\n",
      "Loss: 4.735195159912109\n",
      "Loss: 5.0311431884765625\n",
      "Loss: 4.353353977203369\n",
      "Loss: 4.834254741668701\n",
      "Loss: 4.991479873657227\n",
      "Loss: 5.0619611740112305\n",
      "Loss: 4.900555610656738\n",
      "Loss: 4.437089443206787\n",
      "Loss: 4.64047908782959\n",
      "Loss: 5.045628070831299\n",
      "Loss: 4.727139472961426\n",
      "Loss: 4.349259376525879\n",
      "Loss: 4.808014869689941\n",
      "Loss: 4.559983730316162\n",
      "Loss: 4.763986110687256\n",
      "Loss: 4.447865009307861\n",
      "Loss: 4.912436008453369\n",
      "Loss: 4.701685905456543\n",
      "Loss: 3.833895444869995\n",
      "Loss: 4.493050575256348\n",
      "Loss: 4.271528720855713\n",
      "Loss: 4.533882141113281\n",
      "Loss: 4.624180793762207\n",
      "Loss: 4.726400852203369\n",
      "Loss: 4.628762245178223\n",
      "Loss: 4.629791259765625\n",
      "Loss: 4.59784460067749\n",
      "Loss: 4.675231456756592\n",
      "Loss: 4.235334873199463\n",
      "Loss: 4.541276931762695\n",
      "Loss: 4.718656539916992\n",
      "Loss: 4.315347671508789\n",
      "Loss: 4.6337761878967285\n",
      "Loss: 4.3730387687683105\n",
      "Loss: 4.344931602478027\n",
      "Loss: 4.467680931091309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3/63 [04:27<1:28:59, 89.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.373330593109131\n",
      "Epoch: 3\n",
      "Loss: 4.277536869049072\n",
      "Loss: 3.8921244144439697\n",
      "Loss: 4.22133207321167\n",
      "Loss: 4.4464826583862305\n",
      "Loss: 4.344461917877197\n",
      "Loss: 4.003015518188477\n",
      "Loss: 4.522649765014648\n",
      "Loss: 3.969212532043457\n",
      "Loss: 3.63124680519104\n",
      "Loss: 4.078798770904541\n",
      "Loss: 4.287940979003906\n",
      "Loss: 3.943053722381592\n",
      "Loss: 4.253369331359863\n",
      "Loss: 4.080347537994385\n",
      "Loss: 3.9770095348358154\n",
      "Loss: 4.055909156799316\n",
      "Loss: 3.902789354324341\n",
      "Loss: 3.921980142593384\n",
      "Loss: 3.917710304260254\n",
      "Loss: 4.416262149810791\n",
      "Loss: 4.051992893218994\n",
      "Loss: 4.143100738525391\n",
      "Loss: 3.812769889831543\n",
      "Loss: 3.8650479316711426\n",
      "Loss: 4.016848087310791\n",
      "Loss: 4.084847927093506\n",
      "Loss: 4.001314640045166\n",
      "Loss: 3.6529290676116943\n",
      "Loss: 3.5467779636383057\n",
      "Loss: 3.707948684692383\n",
      "Loss: 3.5191268920898438\n",
      "Loss: 3.6031055450439453\n",
      "Loss: 3.7803051471710205\n",
      "Loss: 3.770984411239624\n",
      "Loss: 3.7201104164123535\n",
      "Loss: 3.4572572708129883\n",
      "Loss: 3.4347915649414062\n",
      "Loss: 3.8054111003875732\n",
      "Loss: 3.689385414123535\n",
      "Loss: 3.1490676403045654\n",
      "Loss: 3.5447537899017334\n",
      "Loss: 3.533689498901367\n",
      "Loss: 2.910736560821533\n",
      "Loss: 3.535490036010742\n",
      "Loss: 3.584807872772217\n",
      "Loss: 3.401007890701294\n",
      "Loss: 3.3326165676116943\n",
      "Loss: 2.965226173400879\n",
      "Loss: 3.67968487739563\n",
      "Loss: 3.6922736167907715\n",
      "Loss: 2.8363282680511475\n",
      "Loss: 3.4447498321533203\n",
      "Loss: 3.225770950317383\n",
      "Loss: 3.01904559135437\n",
      "Loss: 2.6071877479553223\n",
      "Loss: 3.109363555908203\n",
      "Loss: 3.2761642932891846\n",
      "Loss: 3.2126429080963135\n",
      "Loss: 3.1068828105926514\n",
      "Loss: 3.2828402519226074\n",
      "Loss: 3.066570281982422\n",
      "Loss: 3.032088041305542\n",
      "Loss: 3.128398895263672\n",
      "Loss: 2.7393059730529785\n",
      "Loss: 3.1376845836639404\n",
      "Loss: 2.8301267623901367\n",
      "Loss: 2.942321300506592\n",
      "Loss: 2.8600311279296875\n",
      "Loss: 2.7463128566741943\n",
      "Loss: 2.9244749546051025\n",
      "Loss: 3.0563247203826904\n",
      "Loss: 2.904022693634033\n",
      "Loss: 2.608837366104126\n",
      "Loss: 2.8001182079315186\n",
      "Loss: 2.8185529708862305\n",
      "Loss: 2.7372488975524902\n",
      "Loss: 2.7645628452301025\n",
      "Loss: 2.513184070587158\n",
      "Loss: 2.82309627532959\n",
      "Loss: 2.5829272270202637\n",
      "Loss: 2.43475341796875\n",
      "Loss: 2.4842708110809326\n",
      "Loss: 2.573542356491089\n",
      "Loss: 2.6066322326660156\n",
      "Loss: 2.0539791584014893\n",
      "Loss: 2.3739817142486572\n",
      "Loss: 2.5611507892608643\n",
      "Loss: 2.3596718311309814\n",
      "Loss: 2.455142021179199\n",
      "Loss: 2.507511854171753\n",
      "Loss: 2.5067927837371826\n",
      "Loss: 2.353501558303833\n",
      "Loss: 3.030313730239868\n",
      "Loss: 2.211695432662964\n",
      "Loss: 2.1832098960876465\n",
      "Loss: 2.4726943969726562\n",
      "Loss: 2.523653984069824\n",
      "Loss: 2.2742838859558105\n",
      "Loss: 2.167757034301758\n",
      "Loss: 2.4072728157043457\n",
      "Loss: 2.2944254875183105\n",
      "Loss: 2.3574061393737793\n",
      "Loss: 2.4428088665008545\n",
      "Loss: 2.1911418437957764\n",
      "Loss: 2.197525978088379\n",
      "Loss: 2.244957447052002\n",
      "Loss: 2.7492780685424805\n",
      "Loss: 2.317753553390503\n",
      "Loss: 1.9735233783721924\n",
      "Loss: 2.3722522258758545\n",
      "Loss: 2.5365161895751953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 4/63 [05:55<1:27:28, 88.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9396436214447021\n",
      "Epoch: 4\n",
      "Loss: 2.1513144969940186\n",
      "Loss: 2.06052565574646\n",
      "Loss: 2.2209084033966064\n",
      "Loss: 2.083016872406006\n",
      "Loss: 1.9458822011947632\n",
      "Loss: 2.1360061168670654\n",
      "Loss: 2.111884355545044\n",
      "Loss: 2.085406541824341\n",
      "Loss: 2.279062271118164\n",
      "Loss: 2.058152437210083\n",
      "Loss: 2.097738742828369\n",
      "Loss: 2.4519762992858887\n",
      "Loss: 1.9668079614639282\n",
      "Loss: 2.1880555152893066\n",
      "Loss: 2.065200090408325\n",
      "Loss: 1.9179155826568604\n",
      "Loss: 2.077577590942383\n",
      "Loss: 1.8053303956985474\n",
      "Loss: 1.93804931640625\n",
      "Loss: 1.719712734222412\n",
      "Loss: 1.7581360340118408\n",
      "Loss: 2.1639041900634766\n",
      "Loss: 2.0618979930877686\n",
      "Loss: 1.9116122722625732\n",
      "Loss: 1.8484541177749634\n",
      "Loss: 1.7156542539596558\n",
      "Loss: 1.6777360439300537\n",
      "Loss: 2.081611394882202\n",
      "Loss: 1.7571245431900024\n",
      "Loss: 2.1719348430633545\n",
      "Loss: 1.8331830501556396\n",
      "Loss: 1.6738898754119873\n",
      "Loss: 1.6515130996704102\n",
      "Loss: 1.8371391296386719\n",
      "Loss: 1.7541441917419434\n",
      "Loss: 1.7347838878631592\n",
      "Loss: 1.5622700452804565\n",
      "Loss: 1.6383249759674072\n",
      "Loss: 1.9424854516983032\n",
      "Loss: 1.5758191347122192\n",
      "Loss: 1.7453243732452393\n",
      "Loss: 1.7929887771606445\n",
      "Loss: 1.7348436117172241\n",
      "Loss: 1.6909217834472656\n",
      "Loss: 1.6060669422149658\n",
      "Loss: 1.5697449445724487\n",
      "Loss: 1.5364505052566528\n",
      "Loss: 1.5529887676239014\n",
      "Loss: 1.7979052066802979\n",
      "Loss: 1.5265387296676636\n",
      "Loss: 1.4871221780776978\n",
      "Loss: 1.6216758489608765\n",
      "Loss: 1.299316644668579\n",
      "Loss: 1.8044639825820923\n",
      "Loss: 1.9016445875167847\n",
      "Loss: 1.5800092220306396\n",
      "Loss: 1.6049530506134033\n",
      "Loss: 1.4495362043380737\n",
      "Loss: 1.6976559162139893\n",
      "Loss: 1.5193376541137695\n",
      "Loss: 1.5683808326721191\n",
      "Loss: 1.5554221868515015\n",
      "Loss: 1.3820582628250122\n",
      "Loss: 1.6610311269760132\n",
      "Loss: 1.3665951490402222\n",
      "Loss: 1.2609384059906006\n",
      "Loss: 1.379292368888855\n",
      "Loss: 1.2810791730880737\n",
      "Loss: 1.258697271347046\n",
      "Loss: 1.1691749095916748\n",
      "Loss: 1.4904979467391968\n",
      "Loss: 1.2883933782577515\n",
      "Loss: 1.2575889825820923\n",
      "Loss: 1.3414806127548218\n",
      "Loss: 1.1457457542419434\n",
      "Loss: 1.4430632591247559\n",
      "Loss: 1.1628990173339844\n",
      "Loss: 1.4370930194854736\n",
      "Loss: 1.3068015575408936\n",
      "Loss: 1.2520463466644287\n",
      "Loss: 1.7340284585952759\n",
      "Loss: 1.2601449489593506\n",
      "Loss: 1.2728692293167114\n",
      "Loss: 1.2303906679153442\n",
      "Loss: 1.237620234489441\n",
      "Loss: 1.0973196029663086\n",
      "Loss: 1.2262508869171143\n",
      "Loss: 1.1552696228027344\n",
      "Loss: 1.1611247062683105\n",
      "Loss: 1.1551744937896729\n",
      "Loss: 1.015515923500061\n",
      "Loss: 1.3031424283981323\n",
      "Loss: 1.3709863424301147\n",
      "Loss: 1.2549439668655396\n",
      "Loss: 1.5286051034927368\n",
      "Loss: 1.2412645816802979\n",
      "Loss: 1.1046894788742065\n",
      "Loss: 1.1130619049072266\n",
      "Loss: 1.1491765975952148\n",
      "Loss: 1.2474560737609863\n",
      "Loss: 1.130374789237976\n",
      "Loss: 1.1842318773269653\n",
      "Loss: 1.1807126998901367\n",
      "Loss: 1.114227533340454\n",
      "Loss: 1.0124706029891968\n",
      "Loss: 1.307013988494873\n",
      "Loss: 0.9603386521339417\n",
      "Loss: 1.18497633934021\n",
      "Loss: 1.1595641374588013\n",
      "Loss: 1.1769378185272217\n",
      "Loss: 1.0544415712356567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/63 [07:25<1:26:03, 89.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.109157681465149\n",
      "Epoch: 5\n",
      "Loss: 0.8798984289169312\n",
      "Loss: 1.1360290050506592\n",
      "Loss: 1.0889616012573242\n",
      "Loss: 1.1070435047149658\n",
      "Loss: 1.0743836164474487\n",
      "Loss: 1.0407629013061523\n",
      "Loss: 1.3843659162521362\n",
      "Loss: 0.9104006886482239\n",
      "Loss: 0.9717797040939331\n",
      "Loss: 1.4100209474563599\n",
      "Loss: 1.0529779195785522\n",
      "Loss: 0.9905781745910645\n",
      "Loss: 1.024414300918579\n",
      "Loss: 0.9277926683425903\n",
      "Loss: 1.013804316520691\n",
      "Loss: 0.961068332195282\n",
      "Loss: 0.8244364857673645\n",
      "Loss: 1.003886342048645\n",
      "Loss: 0.9885803461074829\n",
      "Loss: 0.9468539953231812\n",
      "Loss: 0.9787197709083557\n",
      "Loss: 1.1110783815383911\n",
      "Loss: 1.0413451194763184\n",
      "Loss: 1.319549322128296\n",
      "Loss: 0.916081428527832\n",
      "Loss: 0.915226399898529\n",
      "Loss: 0.7650831937789917\n",
      "Loss: 0.8247923254966736\n",
      "Loss: 0.9286952018737793\n",
      "Loss: 0.8463528156280518\n",
      "Loss: 0.9287314414978027\n",
      "Loss: 1.146462082862854\n",
      "Loss: 1.0188491344451904\n",
      "Loss: 1.091239094734192\n",
      "Loss: 0.9499204754829407\n",
      "Loss: 0.9597130417823792\n",
      "Loss: 0.9682174324989319\n",
      "Loss: 0.9036121368408203\n",
      "Loss: 0.9141045212745667\n",
      "Loss: 0.7844658493995667\n",
      "Loss: 0.863086462020874\n",
      "Loss: 0.8021065592765808\n",
      "Loss: 0.816612720489502\n",
      "Loss: 0.9096730947494507\n",
      "Loss: 0.8377424478530884\n",
      "Loss: 0.8520447015762329\n",
      "Loss: 1.0589613914489746\n",
      "Loss: 0.7935653328895569\n",
      "Loss: 1.0972946882247925\n",
      "Loss: 0.8189142346382141\n",
      "Loss: 0.9432260990142822\n",
      "Loss: 0.7975565195083618\n",
      "Loss: 0.8456319570541382\n",
      "Loss: 0.7972186803817749\n",
      "Loss: 0.8826629519462585\n",
      "Loss: 0.7366665601730347\n",
      "Loss: 0.8478007316589355\n",
      "Loss: 0.6907627582550049\n",
      "Loss: 0.8578562140464783\n",
      "Loss: 0.8580759763717651\n",
      "Loss: 1.0981876850128174\n",
      "Loss: 0.7399966716766357\n",
      "Loss: 0.8005001544952393\n",
      "Loss: 0.7892613410949707\n",
      "Loss: 0.7499812245368958\n",
      "Loss: 0.9482824206352234\n",
      "Loss: 0.8114049434661865\n",
      "Loss: 1.3470497131347656\n",
      "Loss: 0.6887724995613098\n",
      "Loss: 0.8706180453300476\n",
      "Loss: 0.688117504119873\n",
      "Loss: 0.7528520226478577\n",
      "Loss: 0.7539708614349365\n",
      "Loss: 1.0406252145767212\n",
      "Loss: 0.7512074708938599\n",
      "Loss: 0.7648029327392578\n",
      "Loss: 0.7354265451431274\n",
      "Loss: 0.6972305774688721\n",
      "Loss: 0.7903433442115784\n",
      "Loss: 0.7829446196556091\n",
      "Loss: 0.6326293349266052\n",
      "Loss: 0.653823971748352\n",
      "Loss: 0.730479896068573\n",
      "Loss: 0.719312310218811\n",
      "Loss: 0.7377358078956604\n",
      "Loss: 0.7871016263961792\n",
      "Loss: 0.6743462085723877\n",
      "Loss: 0.5997150540351868\n",
      "Loss: 0.6699272394180298\n",
      "Loss: 0.775375485420227\n",
      "Loss: 0.728600800037384\n",
      "Loss: 0.5693836808204651\n",
      "Loss: 0.5615281462669373\n",
      "Loss: 0.6804296374320984\n",
      "Loss: 0.7389988303184509\n",
      "Loss: 0.6259004473686218\n",
      "Loss: 0.5912057161331177\n",
      "Loss: 0.664321780204773\n",
      "Loss: 0.6792455315589905\n",
      "Loss: 0.6307359933853149\n",
      "Loss: 0.6256409287452698\n",
      "Loss: 0.6013632416725159\n",
      "Loss: 0.8314938545227051\n",
      "Loss: 0.6419614553451538\n",
      "Loss: 0.7949032187461853\n",
      "Loss: 0.5734679698944092\n",
      "Loss: 0.6096314191818237\n",
      "Loss: 0.6253207325935364\n",
      "Loss: 0.5856308341026306\n",
      "Loss: 0.5886284708976746\n",
      "Loss: 0.6792499423027039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 6/63 [08:54<1:24:36, 89.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0692957639694214\n",
      "Epoch: 6\n",
      "Loss: 0.6163198351860046\n",
      "Loss: 1.056698203086853\n",
      "Loss: 0.5258537530899048\n",
      "Loss: 0.5894086956977844\n",
      "Loss: 0.4596818685531616\n",
      "Loss: 0.5667406320571899\n",
      "Loss: 0.613491952419281\n",
      "Loss: 0.6851949095726013\n",
      "Loss: 0.8322299122810364\n",
      "Loss: 0.5701220631599426\n",
      "Loss: 0.4933702051639557\n",
      "Loss: 0.5177536606788635\n",
      "Loss: 0.7328066229820251\n",
      "Loss: 0.6294350028038025\n",
      "Loss: 0.5126982927322388\n",
      "Loss: 0.5752568244934082\n",
      "Loss: 0.5666760206222534\n",
      "Loss: 0.5819476246833801\n",
      "Loss: 0.6903502345085144\n",
      "Loss: 0.5703465938568115\n",
      "Loss: 0.562425971031189\n",
      "Loss: 0.4657612442970276\n",
      "Loss: 0.5524534583091736\n",
      "Loss: 0.5445088148117065\n",
      "Loss: 0.4781911075115204\n",
      "Loss: 0.4461904764175415\n",
      "Loss: 1.3973644971847534\n",
      "Loss: 0.5081130266189575\n",
      "Loss: 0.8253400921821594\n",
      "Loss: 0.527668297290802\n",
      "Loss: 0.503354012966156\n",
      "Loss: 0.566729724407196\n",
      "Loss: 0.5031244158744812\n",
      "Loss: 0.4935050308704376\n",
      "Loss: 0.4662801921367645\n",
      "Loss: 0.4371390640735626\n",
      "Loss: 0.8068644404411316\n",
      "Loss: 0.5130359530448914\n",
      "Loss: 0.5054579973220825\n",
      "Loss: 0.5105812549591064\n",
      "Loss: 0.473198264837265\n",
      "Loss: 0.4760669469833374\n",
      "Loss: 0.3646748661994934\n",
      "Loss: 0.4534929096698761\n",
      "Loss: 0.5143870115280151\n",
      "Loss: 0.5868433117866516\n",
      "Loss: 0.46667715907096863\n",
      "Loss: 0.49225088953971863\n",
      "Loss: 0.4972206950187683\n",
      "Loss: 0.4827646315097809\n",
      "Loss: 0.48681044578552246\n",
      "Loss: 0.4258415997028351\n",
      "Loss: 0.48184889554977417\n",
      "Loss: 0.4778612554073334\n",
      "Loss: 0.4802069664001465\n",
      "Loss: 0.6629754304885864\n",
      "Loss: 0.5343509912490845\n",
      "Loss: 0.46689870953559875\n",
      "Loss: 0.4422181248664856\n",
      "Loss: 0.45948460698127747\n",
      "Loss: 0.4881891906261444\n",
      "Loss: 0.6283307075500488\n",
      "Loss: 0.8218738436698914\n",
      "Loss: 0.44717881083488464\n",
      "Loss: 0.400477796792984\n",
      "Loss: 0.4828125834465027\n",
      "Loss: 0.4483872950077057\n",
      "Loss: 0.4610641598701477\n",
      "Loss: 0.46572619676589966\n",
      "Loss: 0.769393265247345\n",
      "Loss: 0.4430612623691559\n",
      "Loss: 0.3703829348087311\n",
      "Loss: 0.4624589681625366\n",
      "Loss: 0.4316532611846924\n",
      "Loss: 0.48539507389068604\n",
      "Loss: 0.45703524351119995\n",
      "Loss: 0.4234519302845001\n",
      "Loss: 0.4143586754798889\n",
      "Loss: 0.4135367274284363\n",
      "Loss: 0.48493942618370056\n",
      "Loss: 0.3950260579586029\n",
      "Loss: 0.3533981144428253\n",
      "Loss: 0.40555140376091003\n",
      "Loss: 0.43200454115867615\n",
      "Loss: 0.5145525932312012\n",
      "Loss: 0.45591530203819275\n",
      "Loss: 0.49852684140205383\n",
      "Loss: 0.37484025955200195\n",
      "Loss: 0.7402872443199158\n",
      "Loss: 0.40599873661994934\n",
      "Loss: 0.36960074305534363\n",
      "Loss: 0.39395827054977417\n",
      "Loss: 0.4161818027496338\n",
      "Loss: 0.4126662313938141\n",
      "Loss: 0.38312241435050964\n",
      "Loss: 0.3828883171081543\n",
      "Loss: 0.38537687063217163\n",
      "Loss: 0.3768271803855896\n",
      "Loss: 0.4847891926765442\n",
      "Loss: 0.40191787481307983\n",
      "Loss: 0.3451790511608124\n",
      "Loss: 0.5753669738769531\n",
      "Loss: 0.3477054834365845\n",
      "Loss: 0.43970590829849243\n",
      "Loss: 0.44993048906326294\n",
      "Loss: 0.6366897821426392\n",
      "Loss: 0.3728334605693817\n",
      "Loss: 0.3657899796962738\n",
      "Loss: 0.5719911456108093\n",
      "Loss: 0.3554961085319519\n",
      "Loss: 0.4168917238712311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 7/63 [10:22<1:23:01, 88.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3791632652282715\n",
      "Epoch: 7\n",
      "Loss: 0.39289653301239014\n",
      "Loss: 0.3108016550540924\n",
      "Loss: 0.32220718264579773\n",
      "Loss: 0.41122370958328247\n",
      "Loss: 0.8054331541061401\n",
      "Loss: 0.32736843824386597\n",
      "Loss: 0.3880246579647064\n",
      "Loss: 0.33472177386283875\n",
      "Loss: 0.49226564168930054\n",
      "Loss: 0.4154128134250641\n",
      "Loss: 0.47009962797164917\n",
      "Loss: 0.2996942102909088\n",
      "Loss: 0.3089893162250519\n",
      "Loss: 0.4715573787689209\n",
      "Loss: 0.4352225661277771\n",
      "Loss: 0.30733636021614075\n",
      "Loss: 0.4496020972728729\n",
      "Loss: 0.37786659598350525\n",
      "Loss: 0.3865550458431244\n",
      "Loss: 0.7100712060928345\n",
      "Loss: 0.3184875249862671\n",
      "Loss: 0.3794674575328827\n",
      "Loss: 0.32543209195137024\n",
      "Loss: 0.6013738512992859\n",
      "Loss: 0.33745938539505005\n",
      "Loss: 0.40603411197662354\n",
      "Loss: 0.324680358171463\n",
      "Loss: 0.42459115386009216\n",
      "Loss: 0.273227334022522\n",
      "Loss: 0.30636918544769287\n",
      "Loss: 0.3639298379421234\n",
      "Loss: 0.3052637577056885\n",
      "Loss: 0.36346226930618286\n",
      "Loss: 0.3165992200374603\n",
      "Loss: 0.34473514556884766\n",
      "Loss: 0.3803991377353668\n",
      "Loss: 0.2976353168487549\n",
      "Loss: 0.6389063000679016\n",
      "Loss: 0.6960887312889099\n",
      "Loss: 0.3051958382129669\n",
      "Loss: 0.33556944131851196\n",
      "Loss: 0.4797355532646179\n",
      "Loss: 0.4865092933177948\n",
      "Loss: 0.3735743463039398\n",
      "Loss: 0.3803192377090454\n",
      "Loss: 0.3990011513233185\n",
      "Loss: 0.3027585744857788\n",
      "Loss: 0.33795371651649475\n",
      "Loss: 0.2753443717956543\n",
      "Loss: 0.33690229058265686\n",
      "Loss: 0.2888583540916443\n",
      "Loss: 0.32338738441467285\n",
      "Loss: 0.3207300901412964\n",
      "Loss: 0.3791685998439789\n",
      "Loss: 0.3002522587776184\n",
      "Loss: 0.25578945875167847\n",
      "Loss: 0.2958342432975769\n",
      "Loss: 0.49855837225914\n",
      "Loss: 0.2699400782585144\n",
      "Loss: 0.316855788230896\n",
      "Loss: 0.33069244027137756\n",
      "Loss: 0.30840227007865906\n",
      "Loss: 0.4512898921966553\n",
      "Loss: 0.3732593357563019\n",
      "Loss: 0.304593950510025\n",
      "Loss: 0.3322281539440155\n",
      "Loss: 0.3030119836330414\n",
      "Loss: 0.32143887877464294\n",
      "Loss: 0.44109591841697693\n",
      "Loss: 0.31297940015792847\n",
      "Loss: 0.2559135854244232\n",
      "Loss: 0.586436927318573\n",
      "Loss: 0.24590051174163818\n",
      "Loss: 0.30417901277542114\n",
      "Loss: 0.3166583478450775\n",
      "Loss: 0.2633061707019806\n",
      "Loss: 0.2939751446247101\n",
      "Loss: 0.30014416575431824\n",
      "Loss: 0.2987419366836548\n",
      "Loss: 0.2704644799232483\n",
      "Loss: 0.24004612863063812\n",
      "Loss: 0.3410274088382721\n",
      "Loss: 0.26839637756347656\n",
      "Loss: 0.2654420733451843\n",
      "Loss: 0.27994605898857117\n",
      "Loss: 0.34620365500450134\n",
      "Loss: 0.24220991134643555\n",
      "Loss: 0.29925599694252014\n",
      "Loss: 0.27117371559143066\n",
      "Loss: 0.23958726227283478\n",
      "Loss: 0.26465168595314026\n",
      "Loss: 0.28155747056007385\n",
      "Loss: 0.23399436473846436\n",
      "Loss: 0.2701103091239929\n",
      "Loss: 0.24871088564395905\n",
      "Loss: 0.34277233481407166\n",
      "Loss: 0.27150970697402954\n",
      "Loss: 0.33598965406417847\n",
      "Loss: 0.2438337504863739\n",
      "Loss: 0.6639788150787354\n",
      "Loss: 0.27434447407722473\n",
      "Loss: 0.2491370290517807\n",
      "Loss: 0.2542012631893158\n",
      "Loss: 0.2553788721561432\n",
      "Loss: 0.3307117223739624\n",
      "Loss: 0.2731381058692932\n",
      "Loss: 0.7169020771980286\n",
      "Loss: 0.27208518981933594\n",
      "Loss: 0.25272688269615173\n",
      "Loss: 0.3430265486240387\n",
      "Loss: 0.36644411087036133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 8/63 [11:51<1:21:32, 88.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3355165421962738\n",
      "Epoch: 8\n",
      "Loss: 0.24039702117443085\n",
      "Loss: 0.35490959882736206\n",
      "Loss: 0.22961701452732086\n",
      "Loss: 0.24111083149909973\n",
      "Loss: 0.23507893085479736\n",
      "Loss: 0.30087515711784363\n",
      "Loss: 0.26342713832855225\n",
      "Loss: 0.2547716498374939\n",
      "Loss: 0.692039966583252\n",
      "Loss: 0.5515340566635132\n",
      "Loss: 0.2745557725429535\n",
      "Loss: 0.5501656532287598\n",
      "Loss: 0.21067343652248383\n",
      "Loss: 0.23934079706668854\n",
      "Loss: 0.2709601819515228\n",
      "Loss: 0.6765381097793579\n",
      "Loss: 0.2627113461494446\n",
      "Loss: 0.2767854928970337\n",
      "Loss: 0.487138956785202\n",
      "Loss: 0.2367091327905655\n",
      "Loss: 0.2589357793331146\n",
      "Loss: 0.49892300367355347\n",
      "Loss: 0.2573806345462799\n",
      "Loss: 0.23624470829963684\n",
      "Loss: 0.23792007565498352\n",
      "Loss: 0.33499857783317566\n",
      "Loss: 0.3673306405544281\n",
      "Loss: 0.2344347983598709\n",
      "Loss: 0.2397356778383255\n",
      "Loss: 0.36614149808883667\n",
      "Loss: 0.25180697441101074\n",
      "Loss: 0.41192659735679626\n",
      "Loss: 0.20260857045650482\n",
      "Loss: 0.3444260060787201\n",
      "Loss: 0.2164241075515747\n",
      "Loss: 0.2192898988723755\n",
      "Loss: 0.38724347949028015\n",
      "Loss: 0.1880711168050766\n",
      "Loss: 0.25084999203681946\n",
      "Loss: 0.29476019740104675\n",
      "Loss: 0.199773371219635\n",
      "Loss: 0.3035277724266052\n",
      "Loss: 0.20224040746688843\n",
      "Loss: 0.34989503026008606\n",
      "Loss: 0.2558431029319763\n",
      "Loss: 0.18735216557979584\n",
      "Loss: 0.24883387982845306\n",
      "Loss: 0.5453512072563171\n",
      "Loss: 0.2355530560016632\n",
      "Loss: 0.21957877278327942\n",
      "Loss: 0.19726651906967163\n",
      "Loss: 0.3264505863189697\n",
      "Loss: 0.31071367859840393\n",
      "Loss: 0.25088322162628174\n",
      "Loss: 0.25178268551826477\n",
      "Loss: 0.278936505317688\n",
      "Loss: 0.23310424387454987\n",
      "Loss: 0.27639544010162354\n",
      "Loss: 0.2321762591600418\n",
      "Loss: 0.22740758955478668\n",
      "Loss: 0.1751537024974823\n",
      "Loss: 0.23030756413936615\n",
      "Loss: 0.1914588212966919\n",
      "Loss: 0.23442305624485016\n",
      "Loss: 0.1868434101343155\n",
      "Loss: 0.2484210878610611\n",
      "Loss: 0.23393096029758453\n",
      "Loss: 0.17849475145339966\n",
      "Loss: 0.22436551749706268\n",
      "Loss: 0.1891414076089859\n",
      "Loss: 0.20678800344467163\n",
      "Loss: 0.19495518505573273\n",
      "Loss: 0.17827539145946503\n",
      "Loss: 0.17165690660476685\n",
      "Loss: 0.27015262842178345\n",
      "Loss: 0.2636174261569977\n",
      "Loss: 0.21621274948120117\n",
      "Loss: 0.22585546970367432\n",
      "Loss: 0.18140770494937897\n",
      "Loss: 0.19196492433547974\n",
      "Loss: 0.4247587323188782\n",
      "Loss: 0.17687305808067322\n",
      "Loss: 0.21941062808036804\n",
      "Loss: 0.23169071972370148\n",
      "Loss: 0.5445637702941895\n",
      "Loss: 0.1961345672607422\n",
      "Loss: 0.19138778746128082\n",
      "Loss: 0.18289384245872498\n",
      "Loss: 0.22076818346977234\n",
      "Loss: 0.1983354240655899\n",
      "Loss: 0.289278507232666\n",
      "Loss: 0.22759506106376648\n",
      "Loss: 0.1917089968919754\n",
      "Loss: 0.17412252724170685\n",
      "Loss: 0.17565608024597168\n",
      "Loss: 0.30608588457107544\n",
      "Loss: 0.2040831595659256\n",
      "Loss: 0.19335207343101501\n",
      "Loss: 0.19118373095989227\n",
      "Loss: 0.18818657100200653\n",
      "Loss: 0.2975088953971863\n",
      "Loss: 0.16570833325386047\n",
      "Loss: 0.2141532599925995\n",
      "Loss: 0.1704077273607254\n",
      "Loss: 0.16393277049064636\n",
      "Loss: 0.19559310376644135\n",
      "Loss: 0.19424179196357727\n",
      "Loss: 0.17803014814853668\n",
      "Loss: 0.20203153789043427\n",
      "Loss: 0.1792486310005188\n",
      "Loss: 0.18507389724254608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 9/63 [13:21<1:20:08, 89.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.31035125255584717\n",
      "Epoch: 9\n",
      "Loss: 0.3625514805316925\n",
      "Loss: 0.1965322196483612\n",
      "Loss: 0.17940489947795868\n",
      "Loss: 0.19555829465389252\n",
      "Loss: 0.1517351269721985\n",
      "Loss: 0.18286393582820892\n",
      "Loss: 0.4848465621471405\n",
      "Loss: 0.19376763701438904\n",
      "Loss: 0.22178234159946442\n",
      "Loss: 0.307028204202652\n",
      "Loss: 0.17330577969551086\n",
      "Loss: 0.20995473861694336\n",
      "Loss: 0.24256573617458344\n",
      "Loss: 0.15979336202144623\n",
      "Loss: 0.46755072474479675\n",
      "Loss: 0.24393750727176666\n",
      "Loss: 0.1686614751815796\n",
      "Loss: 0.1819007843732834\n",
      "Loss: 0.22184033691883087\n",
      "Loss: 0.21704800426959991\n",
      "Loss: 0.17091244459152222\n",
      "Loss: 0.15281660854816437\n",
      "Loss: 0.1867828667163849\n",
      "Loss: 0.23720957338809967\n",
      "Loss: 0.1834835708141327\n",
      "Loss: 0.24555464088916779\n",
      "Loss: 0.18651722371578217\n",
      "Loss: 0.17733335494995117\n",
      "Loss: 0.16388225555419922\n",
      "Loss: 0.62741619348526\n",
      "Loss: 0.18293724954128265\n",
      "Loss: 0.26410964131355286\n",
      "Loss: 0.18258444964885712\n",
      "Loss: 0.23896726965904236\n",
      "Loss: 0.16617247462272644\n",
      "Loss: 0.23707914352416992\n",
      "Loss: 0.18537408113479614\n",
      "Loss: 0.17512032389640808\n",
      "Loss: 0.5311597585678101\n",
      "Loss: 0.23133501410484314\n",
      "Loss: 0.1737268567085266\n",
      "Loss: 0.1666891872882843\n",
      "Loss: 0.17893482744693756\n",
      "Loss: 0.1517142951488495\n",
      "Loss: 0.26373136043548584\n",
      "Loss: 0.45925843715667725\n",
      "Loss: 0.17762738466262817\n",
      "Loss: 0.30775386095046997\n",
      "Loss: 0.17900246381759644\n",
      "Loss: 0.1778871864080429\n",
      "Loss: 0.30415672063827515\n",
      "Loss: 0.2610087990760803\n",
      "Loss: 0.24576058983802795\n",
      "Loss: 0.16747908294200897\n",
      "Loss: 0.17272473871707916\n",
      "Loss: 0.14381542801856995\n",
      "Loss: 0.21183212101459503\n",
      "Loss: 0.20965537428855896\n",
      "Loss: 0.16238616406917572\n",
      "Loss: 0.2600703537464142\n",
      "Loss: 0.45662784576416016\n",
      "Loss: 0.1510382890701294\n",
      "Loss: 0.17901185154914856\n",
      "Loss: 0.1509009301662445\n",
      "Loss: 0.2350275218486786\n",
      "Loss: 0.12322567403316498\n",
      "Loss: 0.22362492978572845\n",
      "Loss: 0.17831429839134216\n",
      "Loss: 0.24329054355621338\n",
      "Loss: 0.22782254219055176\n",
      "Loss: 0.14922842383384705\n",
      "Loss: 0.19366954267024994\n",
      "Loss: 0.17245018482208252\n",
      "Loss: 0.4022444784641266\n",
      "Loss: 0.16399690508842468\n",
      "Loss: 0.3057352304458618\n",
      "Loss: 0.18624210357666016\n",
      "Loss: 0.14792782068252563\n",
      "Loss: 0.15670333802700043\n",
      "Loss: 0.15079249441623688\n",
      "Loss: 0.13813522458076477\n",
      "Loss: 0.1870572715997696\n",
      "Loss: 0.1734253317117691\n",
      "Loss: 0.15478338301181793\n",
      "Loss: 0.1651429831981659\n",
      "Loss: 0.16300702095031738\n",
      "Loss: 0.18535614013671875\n",
      "Loss: 0.1806546300649643\n",
      "Loss: 0.1530178338289261\n",
      "Loss: 0.15449680387973785\n",
      "Loss: 0.15112227201461792\n",
      "Loss: 0.1982128620147705\n",
      "Loss: 0.2253503054380417\n",
      "Loss: 0.15326614677906036\n",
      "Loss: 0.16083623468875885\n",
      "Loss: 0.21354743838310242\n",
      "Loss: 0.11556485295295715\n",
      "Loss: 0.14101943373680115\n",
      "Loss: 0.18228872120380402\n",
      "Loss: 0.4426036477088928\n",
      "Loss: 0.17643022537231445\n",
      "Loss: 0.15783749520778656\n",
      "Loss: 0.19225801527500153\n",
      "Loss: 0.1319301277399063\n",
      "Loss: 0.16321897506713867\n",
      "Loss: 0.1502152681350708\n",
      "Loss: 0.1807924062013626\n",
      "Loss: 0.14144770801067352\n",
      "Loss: 0.16511419415473938\n",
      "Loss: 0.1441352516412735\n",
      "Loss: 0.13989922404289246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/63 [14:50<1:18:42, 89.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1811518520116806\n",
      "Epoch: 10\n",
      "Loss: 0.2014041543006897\n",
      "Loss: 0.1715961992740631\n",
      "Loss: 0.18018202483654022\n",
      "Loss: 0.16026653349399567\n",
      "Loss: 0.24227696657180786\n",
      "Loss: 0.17865292727947235\n",
      "Loss: 0.42966607213020325\n",
      "Loss: 0.14309978485107422\n",
      "Loss: 0.4056958556175232\n",
      "Loss: 0.13191305100917816\n",
      "Loss: 0.15690073370933533\n",
      "Loss: 0.13131576776504517\n",
      "Loss: 0.12881457805633545\n",
      "Loss: 0.3104098439216614\n",
      "Loss: 0.1459072381258011\n",
      "Loss: 0.173319473862648\n",
      "Loss: 0.3139130175113678\n",
      "Loss: 0.1485731601715088\n",
      "Loss: 0.16195958852767944\n",
      "Loss: 0.15620683133602142\n",
      "Loss: 0.3707544803619385\n",
      "Loss: 0.17411965131759644\n",
      "Loss: 0.1262727826833725\n",
      "Loss: 0.15645591914653778\n",
      "Loss: 0.11875667423009872\n",
      "Loss: 0.15144461393356323\n",
      "Loss: 0.2431344836950302\n",
      "Loss: 0.15899626910686493\n",
      "Loss: 0.1667502373456955\n",
      "Loss: 0.1384926736354828\n",
      "Loss: 0.20106294751167297\n",
      "Loss: 0.15865592658519745\n",
      "Loss: 0.18579337000846863\n",
      "Loss: 0.15042568743228912\n",
      "Loss: 0.14391234517097473\n",
      "Loss: 0.1509491354227066\n",
      "Loss: 0.1134248897433281\n",
      "Loss: 0.2514694929122925\n",
      "Loss: 0.5077510476112366\n",
      "Loss: 0.13254518806934357\n",
      "Loss: 0.1270209103822708\n",
      "Loss: 0.2789461612701416\n",
      "Loss: 0.10895802825689316\n",
      "Loss: 0.14522451162338257\n",
      "Loss: 0.11201721429824829\n",
      "Loss: 0.14504972100257874\n",
      "Loss: 0.2221360206604004\n",
      "Loss: 0.15651926398277283\n",
      "Loss: 0.13194596767425537\n",
      "Loss: 0.13022586703300476\n",
      "Loss: 0.21627607941627502\n",
      "Loss: 0.21179814636707306\n",
      "Loss: 0.1441739797592163\n",
      "Loss: 0.13387030363082886\n",
      "Loss: 0.19791123270988464\n",
      "Loss: 0.11809969693422318\n",
      "Loss: 0.15127886831760406\n",
      "Loss: 0.16468989849090576\n",
      "Loss: 0.18247419595718384\n",
      "Loss: 0.158388152718544\n",
      "Loss: 0.1768074780702591\n",
      "Loss: 0.16828906536102295\n",
      "Loss: 0.28552576899528503\n",
      "Loss: 0.11902622133493423\n",
      "Loss: 0.4094436466693878\n",
      "Loss: 0.17296771705150604\n",
      "Loss: 0.14045892655849457\n",
      "Loss: 0.11706135421991348\n",
      "Loss: 0.11614231765270233\n",
      "Loss: 0.398598313331604\n",
      "Loss: 0.13737857341766357\n",
      "Loss: 0.444366455078125\n",
      "Loss: 0.1673734039068222\n",
      "Loss: 0.11961998790502548\n",
      "Loss: 0.1656232476234436\n",
      "Loss: 0.1200360655784607\n",
      "Loss: 0.16338561475276947\n",
      "Loss: 0.14699599146842957\n",
      "Loss: 0.12437157332897186\n",
      "Loss: 0.13892070949077606\n",
      "Loss: 0.18539133667945862\n",
      "Loss: 0.10394107550382614\n",
      "Loss: 0.19000355899333954\n",
      "Loss: 0.12635543942451477\n",
      "Loss: 0.14324311912059784\n",
      "Loss: 0.14527679979801178\n",
      "Loss: 0.12762905657291412\n",
      "Loss: 0.12720073759555817\n",
      "Loss: 0.2763063907623291\n",
      "Loss: 0.1277565062046051\n",
      "Loss: 0.3762420415878296\n",
      "Loss: 0.11232174932956696\n",
      "Loss: 0.1281610131263733\n",
      "Loss: 0.147274911403656\n",
      "Loss: 0.1478099524974823\n",
      "Loss: 0.11762148141860962\n",
      "Loss: 0.12772347033023834\n",
      "Loss: 0.14873188734054565\n",
      "Loss: 0.14672227203845978\n",
      "Loss: 0.16978150606155396\n",
      "Loss: 0.13839948177337646\n",
      "Loss: 0.12624821066856384\n",
      "Loss: 0.11357784271240234\n",
      "Loss: 0.2499828040599823\n",
      "Loss: 0.11574826389551163\n",
      "Loss: 0.14028652012348175\n",
      "Loss: 0.2675435245037079\n",
      "Loss: 0.11496531218290329\n",
      "Loss: 0.21834012866020203\n",
      "Loss: 0.15097448229789734\n",
      "Loss: 0.1455199271440506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 11/63 [16:19<1:17:11, 89.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2160968780517578\n",
      "Epoch: 11\n",
      "Loss: 0.41823169589042664\n",
      "Loss: 0.10413677990436554\n",
      "Loss: 0.13342273235321045\n",
      "Loss: 0.09879567474126816\n",
      "Loss: 0.10667966306209564\n",
      "Loss: 0.15552778542041779\n",
      "Loss: 0.29970794916152954\n",
      "Loss: 0.12868960201740265\n",
      "Loss: 0.10300115495920181\n",
      "Loss: 0.16625076532363892\n",
      "Loss: 0.09611436724662781\n",
      "Loss: 0.11864829808473587\n",
      "Loss: 0.09095893055200577\n",
      "Loss: 0.11845418810844421\n",
      "Loss: 0.11517360061407089\n",
      "Loss: 0.1273854523897171\n",
      "Loss: 0.13240209221839905\n",
      "Loss: 0.1056598424911499\n",
      "Loss: 0.1302182525396347\n",
      "Loss: 0.18616724014282227\n",
      "Loss: 0.1237715631723404\n",
      "Loss: 0.1211344376206398\n",
      "Loss: 0.12287987768650055\n",
      "Loss: 0.10186421871185303\n",
      "Loss: 0.09897416830062866\n",
      "Loss: 0.15147297084331512\n",
      "Loss: 0.10578697919845581\n",
      "Loss: 0.09319083392620087\n",
      "Loss: 0.27050596475601196\n",
      "Loss: 0.35676854848861694\n",
      "Loss: 0.11224930733442307\n",
      "Loss: 0.1386169195175171\n",
      "Loss: 0.2098645567893982\n",
      "Loss: 0.10562794655561447\n",
      "Loss: 0.10146182775497437\n",
      "Loss: 0.11853048950433731\n",
      "Loss: 0.12765492498874664\n",
      "Loss: 0.10005134344100952\n",
      "Loss: 0.09939723461866379\n",
      "Loss: 0.2002977877855301\n",
      "Loss: 0.44390302896499634\n",
      "Loss: 0.33078068494796753\n",
      "Loss: 0.09022320806980133\n",
      "Loss: 0.33950698375701904\n",
      "Loss: 0.1294507533311844\n",
      "Loss: 0.20758265256881714\n",
      "Loss: 0.12928302586078644\n",
      "Loss: 0.18415836989879608\n",
      "Loss: 0.09476909041404724\n",
      "Loss: 0.16305740177631378\n",
      "Loss: 0.09913482517004013\n",
      "Loss: 0.13299031555652618\n",
      "Loss: 0.08519256860017776\n",
      "Loss: 0.0961918756365776\n",
      "Loss: 0.10728208720684052\n",
      "Loss: 0.11834421753883362\n",
      "Loss: 0.14275126159191132\n",
      "Loss: 0.09037122130393982\n",
      "Loss: 0.09642343968153\n",
      "Loss: 0.1183348149061203\n",
      "Loss: 0.15035796165466309\n",
      "Loss: 0.18458537757396698\n",
      "Loss: 0.09492778033018112\n",
      "Loss: 0.28938028216362\n",
      "Loss: 0.07608542591333389\n",
      "Loss: 0.09451092034578323\n",
      "Loss: 0.15416669845581055\n",
      "Loss: 0.08347753435373306\n",
      "Loss: 0.0819927453994751\n",
      "Loss: 0.14376255869865417\n",
      "Loss: 0.18269506096839905\n",
      "Loss: 0.06905236840248108\n",
      "Loss: 0.0795784518122673\n",
      "Loss: 0.10663003474473953\n",
      "Loss: 0.09901644289493561\n",
      "Loss: 0.13017216324806213\n",
      "Loss: 0.13053126633167267\n",
      "Loss: 0.10827308893203735\n",
      "Loss: 0.07111487537622452\n",
      "Loss: 0.0851886197924614\n",
      "Loss: 0.07303223013877869\n",
      "Loss: 0.1230136826634407\n",
      "Loss: 0.07762902975082397\n",
      "Loss: 0.10604692995548248\n",
      "Loss: 0.15937261283397675\n",
      "Loss: 0.11622688919305801\n",
      "Loss: 0.07635015994310379\n",
      "Loss: 0.12083757668733597\n",
      "Loss: 0.0889023095369339\n",
      "Loss: 0.2011595517396927\n",
      "Loss: 0.08519960939884186\n",
      "Loss: 0.0952053815126419\n",
      "Loss: 0.09379428625106812\n",
      "Loss: 0.3067810535430908\n",
      "Loss: 0.11981465667486191\n",
      "Loss: 0.10682716965675354\n",
      "Loss: 0.07860548794269562\n",
      "Loss: 0.16207270324230194\n",
      "Loss: 0.28535348176956177\n",
      "Loss: 0.0768435001373291\n",
      "Loss: 0.14332816004753113\n",
      "Loss: 0.17094343900680542\n",
      "Loss: 0.10325761884450912\n",
      "Loss: 0.3591550588607788\n",
      "Loss: 0.12312653660774231\n",
      "Loss: 0.08395276218652725\n",
      "Loss: 0.07814506441354752\n",
      "Loss: 0.16638323664665222\n",
      "Loss: 0.07369339466094971\n",
      "Loss: 0.0856376439332962\n",
      "Loss: 0.13068920373916626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 12/63 [17:48<1:15:46, 89.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.12010519206523895\n",
      "Epoch: 12\n",
      "Loss: 0.09098927676677704\n",
      "Loss: 0.1083153784275055\n",
      "Loss: 0.2775152623653412\n",
      "Loss: 0.08746061474084854\n",
      "Loss: 0.11441391706466675\n",
      "Loss: 0.07189935445785522\n",
      "Loss: 0.11408507078886032\n",
      "Loss: 0.1959492713212967\n",
      "Loss: 0.07290352135896683\n",
      "Loss: 0.10001382231712341\n",
      "Loss: 0.11320361495018005\n",
      "Loss: 0.08648449182510376\n",
      "Loss: 0.07656323164701462\n",
      "Loss: 0.07956434786319733\n",
      "Loss: 0.26787328720092773\n",
      "Loss: 0.16980287432670593\n",
      "Loss: 0.10096278786659241\n",
      "Loss: 0.11933669447898865\n",
      "Loss: 0.0636773407459259\n",
      "Loss: 0.24708519876003265\n",
      "Loss: 0.07390952110290527\n",
      "Loss: 0.0892755389213562\n",
      "Loss: 0.07656573504209518\n",
      "Loss: 0.1886860728263855\n",
      "Loss: 0.21809954941272736\n",
      "Loss: 0.11174539476633072\n",
      "Loss: 0.12355618178844452\n",
      "Loss: 0.08270415663719177\n",
      "Loss: 0.07207980006933212\n",
      "Loss: 0.07554487884044647\n",
      "Loss: 0.2772442698478699\n",
      "Loss: 0.07727699726819992\n",
      "Loss: 0.18815737962722778\n",
      "Loss: 0.07887370884418488\n",
      "Loss: 0.06993196904659271\n",
      "Loss: 0.07107766717672348\n",
      "Loss: 0.12350767105817795\n",
      "Loss: 0.1174008920788765\n",
      "Loss: 0.12005068361759186\n",
      "Loss: 0.1044975221157074\n",
      "Loss: 0.37258440256118774\n",
      "Loss: 0.3196341097354889\n",
      "Loss: 0.0937035009264946\n",
      "Loss: 0.07852040976285934\n",
      "Loss: 0.07061417400836945\n",
      "Loss: 0.0775357186794281\n",
      "Loss: 0.06983617693185806\n",
      "Loss: 0.10547035932540894\n",
      "Loss: 0.11562612652778625\n",
      "Loss: 0.07584311068058014\n",
      "Loss: 0.06964989006519318\n",
      "Loss: 0.07892173528671265\n",
      "Loss: 0.06827522814273834\n",
      "Loss: 0.14011140167713165\n",
      "Loss: 0.32130005955696106\n",
      "Loss: 0.0796414390206337\n",
      "Loss: 0.10624813288450241\n",
      "Loss: 0.08186494559049606\n",
      "Loss: 0.10048364102840424\n",
      "Loss: 0.0697970911860466\n",
      "Loss: 0.09524188190698624\n",
      "Loss: 0.06307947635650635\n",
      "Loss: 0.08530957996845245\n",
      "Loss: 0.2973368167877197\n",
      "Loss: 0.2710692882537842\n",
      "Loss: 0.0720972865819931\n",
      "Loss: 0.1466941237449646\n",
      "Loss: 0.08695702999830246\n",
      "Loss: 0.11796590685844421\n",
      "Loss: 0.07630251348018646\n",
      "Loss: 0.3686126470565796\n",
      "Loss: 0.06102021783590317\n",
      "Loss: 0.12246392667293549\n",
      "Loss: 0.08939255028963089\n",
      "Loss: 0.1448519080877304\n",
      "Loss: 0.0546671561896801\n",
      "Loss: 0.08091586828231812\n",
      "Loss: 0.10155853629112244\n",
      "Loss: 0.1378425806760788\n",
      "Loss: 0.069965660572052\n",
      "Loss: 0.2816660702228546\n",
      "Loss: 0.06962895393371582\n",
      "Loss: 0.06493426859378815\n",
      "Loss: 0.09632337838411331\n",
      "Loss: 0.08372855186462402\n",
      "Loss: 0.1307455599308014\n",
      "Loss: 0.07300001382827759\n",
      "Loss: 0.20850960910320282\n",
      "Loss: 0.06648702919483185\n",
      "Loss: 0.06485719233751297\n",
      "Loss: 0.09457090497016907\n",
      "Loss: 0.07841381430625916\n",
      "Loss: 0.08129482716321945\n",
      "Loss: 0.09830562025308609\n",
      "Loss: 0.15931472182273865\n",
      "Loss: 0.08541636914014816\n",
      "Loss: 0.09035683423280716\n",
      "Loss: 0.08342842012643814\n",
      "Loss: 0.07531862705945969\n",
      "Loss: 0.06550781428813934\n",
      "Loss: 0.1419607698917389\n",
      "Loss: 0.1225120797753334\n",
      "Loss: 0.07274099439382553\n",
      "Loss: 0.09544768929481506\n",
      "Loss: 0.08356177061796188\n",
      "Loss: 0.06826765090227127\n",
      "Loss: 0.0785985067486763\n",
      "Loss: 0.15313704311847687\n",
      "Loss: 0.05279802531003952\n",
      "Loss: 0.08148859441280365\n",
      "Loss: 0.07387416809797287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 13/63 [19:18<1:14:21, 89.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0759609118103981\n",
      "Epoch: 13\n",
      "Loss: 0.08550550788640976\n",
      "Loss: 0.07982579618692398\n",
      "Loss: 0.06762686371803284\n",
      "Loss: 0.22640612721443176\n",
      "Loss: 0.06533106416463852\n",
      "Loss: 0.09643016010522842\n",
      "Loss: 0.0609605610370636\n",
      "Loss: 0.07841724157333374\n",
      "Loss: 0.053375668823719025\n",
      "Loss: 0.07795903086662292\n",
      "Loss: 0.07142611593008041\n",
      "Loss: 0.06441360712051392\n",
      "Loss: 0.06532569229602814\n",
      "Loss: 0.17333966493606567\n",
      "Loss: 0.07457326352596283\n",
      "Loss: 0.05488603189587593\n",
      "Loss: 0.09091924875974655\n",
      "Loss: 0.07269313186407089\n",
      "Loss: 0.06697627902030945\n",
      "Loss: 0.058276306837797165\n",
      "Loss: 0.05561628192663193\n",
      "Loss: 0.07752107828855515\n",
      "Loss: 0.08909568935632706\n",
      "Loss: 0.06330857425928116\n",
      "Loss: 0.09658126533031464\n",
      "Loss: 0.07594313472509384\n",
      "Loss: 0.08623412251472473\n",
      "Loss: 0.06320354342460632\n",
      "Loss: 0.08167897909879684\n",
      "Loss: 0.34195074439048767\n",
      "Loss: 0.15467891097068787\n",
      "Loss: 0.06188606470823288\n",
      "Loss: 0.06462334841489792\n",
      "Loss: 0.17049255967140198\n",
      "Loss: 0.06774569302797318\n",
      "Loss: 0.06291541457176208\n",
      "Loss: 0.04794703796505928\n",
      "Loss: 0.13067956268787384\n",
      "Loss: 0.09702610224485397\n",
      "Loss: 0.07338659465312958\n",
      "Loss: 0.1359798014163971\n",
      "Loss: 0.05852159857749939\n",
      "Loss: 0.06809080392122269\n",
      "Loss: 0.047929394990205765\n",
      "Loss: 0.06346359848976135\n",
      "Loss: 0.17649626731872559\n",
      "Loss: 0.07941082864999771\n",
      "Loss: 0.0575365275144577\n",
      "Loss: 0.06145625561475754\n",
      "Loss: 0.059960875660181046\n",
      "Loss: 0.05922364443540573\n",
      "Loss: 0.05048249289393425\n",
      "Loss: 0.07302817702293396\n",
      "Loss: 0.06621631979942322\n",
      "Loss: 0.126370370388031\n",
      "Loss: 0.25992366671562195\n",
      "Loss: 0.058409180492162704\n",
      "Loss: 0.04585076496005058\n",
      "Loss: 0.10697068274021149\n",
      "Loss: 0.06208258867263794\n",
      "Loss: 0.0817616805434227\n",
      "Loss: 0.3467106521129608\n",
      "Loss: 0.05386529490351677\n",
      "Loss: 0.05876673758029938\n",
      "Loss: 0.07293089479207993\n",
      "Loss: 0.08017769455909729\n",
      "Loss: 0.09945466369390488\n",
      "Loss: 0.07240748405456543\n",
      "Loss: 0.06496626138687134\n",
      "Loss: 0.061224564909935\n",
      "Loss: 0.251385897397995\n",
      "Loss: 0.05176745355129242\n",
      "Loss: 0.10087576508522034\n",
      "Loss: 0.0676644966006279\n",
      "Loss: 0.20191501080989838\n",
      "Loss: 0.13286156952381134\n",
      "Loss: 0.06980129331350327\n",
      "Loss: 0.07279791682958603\n",
      "Loss: 0.14303793013095856\n",
      "Loss: 0.19053202867507935\n",
      "Loss: 0.2591853141784668\n",
      "Loss: 0.05381080508232117\n",
      "Loss: 0.1343415528535843\n",
      "Loss: 0.058117251843214035\n",
      "Loss: 0.20256420969963074\n",
      "Loss: 0.05983039736747742\n",
      "Loss: 0.08900168538093567\n",
      "Loss: 0.10082998871803284\n",
      "Loss: 0.049127548933029175\n",
      "Loss: 0.0491088442504406\n",
      "Loss: 0.060526762157678604\n",
      "Loss: 0.05747661367058754\n",
      "Loss: 0.046686556190252304\n",
      "Loss: 0.05766550824046135\n",
      "Loss: 0.06597891449928284\n",
      "Loss: 0.059776902198791504\n",
      "Loss: 0.0556001253426075\n",
      "Loss: 0.15788987278938293\n",
      "Loss: 0.08131205290555954\n",
      "Loss: 0.06903741508722305\n",
      "Loss: 0.08505407720804214\n",
      "Loss: 0.05118126422166824\n",
      "Loss: 0.05667446181178093\n",
      "Loss: 0.2781740725040436\n",
      "Loss: 0.04618842527270317\n",
      "Loss: 0.05129415914416313\n",
      "Loss: 0.052593398839235306\n",
      "Loss: 0.05542592704296112\n",
      "Loss: 0.12954100966453552\n",
      "Loss: 0.12196208536624908\n",
      "Loss: 0.054761990904808044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 14/63 [20:46<1:12:45, 89.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.043887700885534286\n",
      "Epoch: 14\n",
      "Loss: 0.05382366478443146\n",
      "Loss: 0.05154578760266304\n",
      "Loss: 0.07197180390357971\n",
      "Loss: 0.0648820623755455\n",
      "Loss: 0.058609262108802795\n",
      "Loss: 0.26120176911354065\n",
      "Loss: 0.044728562235832214\n",
      "Loss: 0.07087460905313492\n",
      "Loss: 0.042730316519737244\n",
      "Loss: 0.05906008556485176\n",
      "Loss: 0.05106519162654877\n",
      "Loss: 0.0836055800318718\n",
      "Loss: 0.051763854920864105\n",
      "Loss: 0.08696911484003067\n",
      "Loss: 0.06303498148918152\n",
      "Loss: 0.06944657862186432\n",
      "Loss: 0.2974160313606262\n",
      "Loss: 0.05748070776462555\n",
      "Loss: 0.0476054847240448\n",
      "Loss: 0.04397156834602356\n",
      "Loss: 0.05202365666627884\n",
      "Loss: 0.0722116231918335\n",
      "Loss: 0.043431080877780914\n",
      "Loss: 0.11399096995592117\n",
      "Loss: 0.05806408077478409\n",
      "Loss: 0.05314849317073822\n",
      "Loss: 0.05757887288928032\n",
      "Loss: 0.08517313003540039\n",
      "Loss: 0.04256383329629898\n",
      "Loss: 0.06116801127791405\n",
      "Loss: 0.048707686364650726\n",
      "Loss: 0.0665416494011879\n",
      "Loss: 0.061246465891599655\n",
      "Loss: 0.0776229053735733\n",
      "Loss: 0.050108909606933594\n",
      "Loss: 0.07096952944993973\n",
      "Loss: 0.099798284471035\n",
      "Loss: 0.04987945407629013\n",
      "Loss: 0.0485798716545105\n",
      "Loss: 0.064363032579422\n",
      "Loss: 0.04295043274760246\n",
      "Loss: 0.046982284635305405\n",
      "Loss: 0.05737275257706642\n",
      "Loss: 0.04417732357978821\n",
      "Loss: 0.03719814866781235\n",
      "Loss: 0.0625188872218132\n",
      "Loss: 0.18439845740795135\n",
      "Loss: 0.09542614221572876\n",
      "Loss: 0.2464333027601242\n",
      "Loss: 0.05403457209467888\n",
      "Loss: 0.1025727316737175\n",
      "Loss: 0.11002042889595032\n",
      "Loss: 0.09744561463594437\n",
      "Loss: 0.044356249272823334\n",
      "Loss: 0.04339103028178215\n",
      "Loss: 0.04717264696955681\n",
      "Loss: 0.09061653912067413\n",
      "Loss: 0.0846618115901947\n",
      "Loss: 0.053887948393821716\n",
      "Loss: 0.08731592446565628\n",
      "Loss: 0.05443568527698517\n",
      "Loss: 0.05368839576840401\n",
      "Loss: 0.15852558612823486\n",
      "Loss: 0.05080263689160347\n",
      "Loss: 0.2180275321006775\n",
      "Loss: 0.11883784830570221\n",
      "Loss: 0.10458023846149445\n",
      "Loss: 0.06061200052499771\n",
      "Loss: 0.07590663433074951\n",
      "Loss: 0.09930255264043808\n",
      "Loss: 0.2598835229873657\n",
      "Loss: 0.040415868163108826\n",
      "Loss: 0.08323604613542557\n",
      "Loss: 0.040521539747714996\n",
      "Loss: 0.1401400864124298\n",
      "Loss: 0.04600564390420914\n",
      "Loss: 0.13036102056503296\n",
      "Loss: 0.043924324214458466\n",
      "Loss: 0.05955149605870247\n",
      "Loss: 0.04385102912783623\n",
      "Loss: 0.10126496851444244\n",
      "Loss: 0.1572851538658142\n",
      "Loss: 0.04208843782544136\n",
      "Loss: 0.049135420471429825\n",
      "Loss: 0.05200079828500748\n",
      "Loss: 0.23288977146148682\n",
      "Loss: 0.05788291245698929\n",
      "Loss: 0.055881328880786896\n",
      "Loss: 0.2728203237056732\n",
      "Loss: 0.044216133654117584\n",
      "Loss: 0.045261841267347336\n",
      "Loss: 0.037591852247714996\n",
      "Loss: 0.10422834008932114\n",
      "Loss: 0.04997687786817551\n",
      "Loss: 0.04983025789260864\n",
      "Loss: 0.04605884104967117\n",
      "Loss: 0.0760982558131218\n",
      "Loss: 0.05616248771548271\n",
      "Loss: 0.17356598377227783\n",
      "Loss: 0.05142954736948013\n",
      "Loss: 0.08138525485992432\n",
      "Loss: 0.06475032866001129\n",
      "Loss: 0.042771872133016586\n",
      "Loss: 0.08226530253887177\n",
      "Loss: 0.047096509486436844\n",
      "Loss: 0.0424933023750782\n",
      "Loss: 0.06175152212381363\n",
      "Loss: 0.04207858815789223\n",
      "Loss: 0.04971275106072426\n",
      "Loss: 0.0419965423643589\n",
      "Loss: 0.04387170076370239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 15/63 [22:16<1:11:16, 89.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.049177151173353195\n",
      "Epoch: 15\n",
      "Loss: 0.042813315987586975\n",
      "Loss: 0.21149703860282898\n",
      "Loss: 0.09748803824186325\n",
      "Loss: 0.041847553104162216\n",
      "Loss: 0.04324624687433243\n",
      "Loss: 0.03953317180275917\n",
      "Loss: 0.05399223789572716\n",
      "Loss: 0.03900577500462532\n",
      "Loss: 0.044489167630672455\n",
      "Loss: 0.041547439992427826\n",
      "Loss: 0.057543445378541946\n",
      "Loss: 0.03745061531662941\n",
      "Loss: 0.045445963740348816\n",
      "Loss: 0.050321247428655624\n",
      "Loss: 0.041651759296655655\n",
      "Loss: 0.043829597532749176\n",
      "Loss: 0.04761354997754097\n",
      "Loss: 0.09265678375959396\n",
      "Loss: 0.11942535638809204\n",
      "Loss: 0.11603710800409317\n",
      "Loss: 0.034405872225761414\n",
      "Loss: 0.05469581484794617\n",
      "Loss: 0.0408872626721859\n",
      "Loss: 0.1967276781797409\n",
      "Loss: 0.035475295037031174\n",
      "Loss: 0.06429208815097809\n",
      "Loss: 0.06637879461050034\n",
      "Loss: 0.04388590157032013\n",
      "Loss: 0.04728972539305687\n",
      "Loss: 0.07567077875137329\n",
      "Loss: 0.05961526185274124\n",
      "Loss: 0.03760397434234619\n",
      "Loss: 0.06677739322185516\n",
      "Loss: 0.08759699761867523\n",
      "Loss: 0.04937945306301117\n",
      "Loss: 0.05024762451648712\n",
      "Loss: 0.18835662305355072\n",
      "Loss: 0.042278870940208435\n",
      "Loss: 0.04557059332728386\n",
      "Loss: 0.047322727739810944\n",
      "Loss: 0.04810946434736252\n",
      "Loss: 0.03806766867637634\n",
      "Loss: 0.058030594140291214\n",
      "Loss: 0.04110970348119736\n",
      "Loss: 0.04330085963010788\n",
      "Loss: 0.04540100321173668\n",
      "Loss: 0.04213777929544449\n",
      "Loss: 0.05168208107352257\n",
      "Loss: 0.08348116278648376\n",
      "Loss: 0.10761766880750656\n",
      "Loss: 0.029241589829325676\n",
      "Loss: 0.03191554546356201\n",
      "Loss: 0.08162230253219604\n",
      "Loss: 0.05004851892590523\n",
      "Loss: 0.09613748639822006\n",
      "Loss: 0.04004443809390068\n",
      "Loss: 0.07203260809183121\n",
      "Loss: 0.07771753519773483\n",
      "Loss: 0.04208642244338989\n",
      "Loss: 0.1691092550754547\n",
      "Loss: 0.05247056856751442\n",
      "Loss: 0.20215843617916107\n",
      "Loss: 0.03709685802459717\n",
      "Loss: 0.059807661920785904\n",
      "Loss: 0.04269138351082802\n",
      "Loss: 0.04181625321507454\n",
      "Loss: 0.036863747984170914\n",
      "Loss: 0.04158756881952286\n",
      "Loss: 0.038075316697359085\n",
      "Loss: 0.03834332153201103\n",
      "Loss: 0.11232952028512955\n",
      "Loss: 0.03523848205804825\n",
      "Loss: 0.0342995822429657\n",
      "Loss: 0.050783753395080566\n",
      "Loss: 0.3280969560146332\n",
      "Loss: 0.10916003584861755\n",
      "Loss: 0.19614212214946747\n",
      "Loss: 0.04487217217683792\n",
      "Loss: 0.04667024314403534\n",
      "Loss: 0.03452293202280998\n",
      "Loss: 0.0327703095972538\n",
      "Loss: 0.16255368292331696\n",
      "Loss: 0.09105730056762695\n",
      "Loss: 0.05381811782717705\n",
      "Loss: 0.035565633326768875\n",
      "Loss: 0.03383048251271248\n",
      "Loss: 0.041518595069646835\n",
      "Loss: 0.046807922422885895\n",
      "Loss: 0.05383579805493355\n",
      "Loss: 0.03923305869102478\n",
      "Loss: 0.044286053627729416\n",
      "Loss: 0.07816945761442184\n",
      "Loss: 0.03280803933739662\n",
      "Loss: 0.05488014221191406\n",
      "Loss: 0.04717731475830078\n",
      "Loss: 0.06791966408491135\n",
      "Loss: 0.11986787617206573\n",
      "Loss: 0.04852239042520523\n",
      "Loss: 0.04729630798101425\n",
      "Loss: 0.047436926513910294\n",
      "Loss: 0.045563507825136185\n",
      "Loss: 0.03766392916440964\n",
      "Loss: 0.06405074149370193\n",
      "Loss: 0.14025504887104034\n",
      "Loss: 0.03873483091592789\n",
      "Loss: 0.03583655506372452\n",
      "Loss: 0.04151550680398941\n",
      "Loss: 0.042975932359695435\n",
      "Loss: 0.04720010608434677\n",
      "Loss: 0.03496844321489334\n",
      "Loss: 0.040014248341321945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 16/63 [23:45<1:09:46, 89.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.09411090612411499\n",
      "Epoch: 16\n",
      "Loss: 0.08050099015235901\n",
      "Loss: 0.04046887531876564\n",
      "Loss: 0.20662130415439606\n",
      "Loss: 0.03637652471661568\n",
      "Loss: 0.0331464484333992\n",
      "Loss: 0.09964707493782043\n",
      "Loss: 0.033764008432626724\n",
      "Loss: 0.05071454867720604\n",
      "Loss: 0.03895578160881996\n",
      "Loss: 0.09791375696659088\n",
      "Loss: 0.03515585884451866\n",
      "Loss: 0.2359904944896698\n",
      "Loss: 0.028433429077267647\n",
      "Loss: 0.0486072413623333\n",
      "Loss: 0.034710343927145004\n",
      "Loss: 0.08943869918584824\n",
      "Loss: 0.04304404929280281\n",
      "Loss: 0.04763157293200493\n",
      "Loss: 0.034410543739795685\n",
      "Loss: 0.03597956895828247\n",
      "Loss: 0.1268709897994995\n",
      "Loss: 0.1385161429643631\n",
      "Loss: 0.03185505047440529\n",
      "Loss: 0.05158812925219536\n",
      "Loss: 0.042504630982875824\n",
      "Loss: 0.06828003376722336\n",
      "Loss: 0.03291897848248482\n",
      "Loss: 0.05480523407459259\n",
      "Loss: 0.03767654299736023\n",
      "Loss: 0.040891293436288834\n",
      "Loss: 0.03488513082265854\n",
      "Loss: 0.05524167791008949\n",
      "Loss: 0.04119681566953659\n",
      "Loss: 0.042149487882852554\n",
      "Loss: 0.029780738055706024\n",
      "Loss: 0.06226598098874092\n",
      "Loss: 0.14669479429721832\n",
      "Loss: 0.0438418909907341\n",
      "Loss: 0.22398604452610016\n",
      "Loss: 0.029386164620518684\n",
      "Loss: 0.03959662839770317\n",
      "Loss: 0.05980968475341797\n",
      "Loss: 0.04203597456216812\n",
      "Loss: 0.032858721911907196\n",
      "Loss: 0.0886598750948906\n",
      "Loss: 0.08050892502069473\n",
      "Loss: 0.20748883485794067\n",
      "Loss: 0.03873560577630997\n",
      "Loss: 0.03563663735985756\n",
      "Loss: 0.210501566529274\n",
      "Loss: 0.03513704612851143\n",
      "Loss: 0.03425872325897217\n",
      "Loss: 0.04035321623086929\n",
      "Loss: 0.040806110948324203\n",
      "Loss: 0.04293189197778702\n",
      "Loss: 0.041524916887283325\n",
      "Loss: 0.048496343195438385\n",
      "Loss: 0.03703051432967186\n",
      "Loss: 0.07710155099630356\n",
      "Loss: 0.03558690473437309\n",
      "Loss: 0.06346075236797333\n",
      "Loss: 0.03658372536301613\n",
      "Loss: 0.10864800214767456\n",
      "Loss: 0.041860196739435196\n",
      "Loss: 0.17108792066574097\n",
      "Loss: 0.15518136322498322\n",
      "Loss: 0.04125618189573288\n",
      "Loss: 0.03351864963769913\n",
      "Loss: 0.12751425802707672\n",
      "Loss: 0.041945576667785645\n",
      "Loss: 0.03614983335137367\n",
      "Loss: 0.03278625011444092\n",
      "Loss: 0.06465471535921097\n",
      "Loss: 0.06079261377453804\n",
      "Loss: 0.030934952199459076\n",
      "Loss: 0.07266496121883392\n",
      "Loss: 0.04011046886444092\n",
      "Loss: 0.04275971278548241\n",
      "Loss: 0.03569383919239044\n",
      "Loss: 0.09721463918685913\n",
      "Loss: 0.08071945607662201\n",
      "Loss: 0.04479137808084488\n",
      "Loss: 0.033644042909145355\n",
      "Loss: 0.029170222580432892\n",
      "Loss: 0.04798762872815132\n",
      "Loss: 0.04440303519368172\n",
      "Loss: 0.037973590195178986\n",
      "Loss: 0.03551512956619263\n",
      "Loss: 0.046938419342041016\n",
      "Loss: 0.03422948345541954\n",
      "Loss: 0.032156746834516525\n",
      "Loss: 0.041142772883176804\n",
      "Loss: 0.036603305488824844\n",
      "Loss: 0.0367807000875473\n",
      "Loss: 0.04374642297625542\n",
      "Loss: 0.06126340106129646\n",
      "Loss: 0.03594719618558884\n",
      "Loss: 0.03028038889169693\n",
      "Loss: 0.03296376392245293\n",
      "Loss: 0.026512511074543\n",
      "Loss: 0.03970135375857353\n",
      "Loss: 0.03648952394723892\n",
      "Loss: 0.04417455568909645\n",
      "Loss: 0.03973862901329994\n",
      "Loss: 0.03671295568346977\n",
      "Loss: 0.03039991483092308\n",
      "Loss: 0.04730532690882683\n",
      "Loss: 0.06840312480926514\n",
      "Loss: 0.027826277539134026\n",
      "Loss: 0.0701606348156929\n",
      "Loss: 0.0388132706284523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 17/63 [25:14<1:08:19, 89.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.035665981471538544\n",
      "Epoch: 17\n",
      "Loss: 0.03431306779384613\n",
      "Loss: 0.03095603547990322\n",
      "Loss: 0.029658285900950432\n",
      "Loss: 0.03084459714591503\n",
      "Loss: 0.12682576477527618\n",
      "Loss: 0.03407546877861023\n",
      "Loss: 0.07153700292110443\n",
      "Loss: 0.059825729578733444\n",
      "Loss: 0.21732109785079956\n",
      "Loss: 0.044994793832302094\n",
      "Loss: 0.060882747173309326\n",
      "Loss: 0.03532968834042549\n",
      "Loss: 0.032077856361866\n",
      "Loss: 0.1330711543560028\n",
      "Loss: 0.03876909986138344\n",
      "Loss: 0.044572990387678146\n",
      "Loss: 0.03927292674779892\n",
      "Loss: 0.028057284653186798\n",
      "Loss: 0.07652311772108078\n",
      "Loss: 0.028729375451803207\n",
      "Loss: 0.035546932369470596\n",
      "Loss: 0.02629605308175087\n",
      "Loss: 0.08877298980951309\n",
      "Loss: 0.03525145351886749\n",
      "Loss: 0.09026709944009781\n",
      "Loss: 0.043278373777866364\n",
      "Loss: 0.15362681448459625\n",
      "Loss: 0.06416124105453491\n",
      "Loss: 0.13433097302913666\n",
      "Loss: 0.0395437628030777\n",
      "Loss: 0.039006102830171585\n",
      "Loss: 0.058944810181856155\n",
      "Loss: 0.14593976736068726\n",
      "Loss: 0.023855846375226974\n",
      "Loss: 0.030905527994036674\n",
      "Loss: 0.031059348955750465\n",
      "Loss: 0.040360841900110245\n",
      "Loss: 0.047161031514406204\n",
      "Loss: 0.03575930371880531\n",
      "Loss: 0.034730780869722366\n",
      "Loss: 0.030255621299147606\n",
      "Loss: 0.03434623032808304\n",
      "Loss: 0.04384332522749901\n",
      "Loss: 0.0344579815864563\n",
      "Loss: 0.07760542631149292\n",
      "Loss: 0.03382828086614609\n",
      "Loss: 0.0652441754937172\n",
      "Loss: 0.03431018441915512\n",
      "Loss: 0.028948422521352768\n",
      "Loss: 0.03348338231444359\n",
      "Loss: 0.04035161808133125\n",
      "Loss: 0.03701126575469971\n",
      "Loss: 0.032597292214632034\n",
      "Loss: 0.02695060707628727\n",
      "Loss: 0.04715772718191147\n",
      "Loss: 0.10770855098962784\n",
      "Loss: 0.09595154225826263\n",
      "Loss: 0.026269884780049324\n",
      "Loss: 0.10048900544643402\n",
      "Loss: 0.025348559021949768\n",
      "Loss: 0.0360540896654129\n",
      "Loss: 0.06388302147388458\n",
      "Loss: 0.037429872900247574\n",
      "Loss: 0.026566224172711372\n",
      "Loss: 0.025531401857733727\n",
      "Loss: 0.02919265814125538\n",
      "Loss: 0.03269992023706436\n",
      "Loss: 0.028603777289390564\n",
      "Loss: 0.02707194723188877\n",
      "Loss: 0.036531493067741394\n",
      "Loss: 0.029604047536849976\n",
      "Loss: 0.02737130969762802\n",
      "Loss: 0.035220228135585785\n",
      "Loss: 0.03375833481550217\n",
      "Loss: 0.026569010689854622\n",
      "Loss: 0.034531284123659134\n",
      "Loss: 0.03371664509177208\n",
      "Loss: 0.04252362996339798\n",
      "Loss: 0.0658922791481018\n",
      "Loss: 0.08453556895256042\n",
      "Loss: 0.03476567938923836\n",
      "Loss: 0.15753167867660522\n",
      "Loss: 0.15588589012622833\n",
      "Loss: 0.044249027967453\n",
      "Loss: 0.08530204743146896\n",
      "Loss: 0.06376013904809952\n",
      "Loss: 0.027797222137451172\n",
      "Loss: 0.028904052451252937\n",
      "Loss: 0.02615518495440483\n",
      "Loss: 0.026725778356194496\n",
      "Loss: 0.031156063079833984\n",
      "Loss: 0.03908403962850571\n",
      "Loss: 0.06103014200925827\n",
      "Loss: 0.03454986959695816\n",
      "Loss: 0.10962158441543579\n",
      "Loss: 0.024640681222081184\n",
      "Loss: 0.03543839976191521\n",
      "Loss: 0.03511103615164757\n",
      "Loss: 0.03491924703121185\n",
      "Loss: 0.23911455273628235\n",
      "Loss: 0.02254863828420639\n",
      "Loss: 0.12043162435293198\n",
      "Loss: 0.03547156974673271\n",
      "Loss: 0.041317105293273926\n",
      "Loss: 0.03698062151670456\n",
      "Loss: 0.03059951588511467\n",
      "Loss: 0.03286532685160637\n",
      "Loss: 0.02378939650952816\n",
      "Loss: 0.14648205041885376\n",
      "Loss: 0.06255898624658585\n",
      "Loss: 0.03407149389386177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 18/63 [26:43<1:06:57, 89.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.09657683968544006\n",
      "Epoch: 18\n",
      "Loss: 0.06509613245725632\n",
      "Loss: 0.0261865071952343\n",
      "Loss: 0.02890164963901043\n",
      "Loss: 0.0628759115934372\n",
      "Loss: 0.0295462254434824\n",
      "Loss: 0.0252204779535532\n",
      "Loss: 0.03727271780371666\n",
      "Loss: 0.048563700169324875\n",
      "Loss: 0.024079248309135437\n",
      "Loss: 0.1324971467256546\n",
      "Loss: 0.04226373881101608\n",
      "Loss: 0.1455388218164444\n",
      "Loss: 0.0589170977473259\n",
      "Loss: 0.12263749539852142\n",
      "Loss: 0.024712514132261276\n",
      "Loss: 0.024030759930610657\n",
      "Loss: 0.025240182876586914\n",
      "Loss: 0.026178954169154167\n",
      "Loss: 0.02657831646502018\n",
      "Loss: 0.024205606430768967\n",
      "Loss: 0.05102301761507988\n",
      "Loss: 0.07863415032625198\n",
      "Loss: 0.02656000480055809\n",
      "Loss: 0.03307483717799187\n",
      "Loss: 0.045629583299160004\n",
      "Loss: 0.025102127343416214\n",
      "Loss: 0.1436125934123993\n",
      "Loss: 0.023199142888188362\n",
      "Loss: 0.1157318577170372\n",
      "Loss: 0.024081885814666748\n",
      "Loss: 0.107606902718544\n",
      "Loss: 0.07880760729312897\n",
      "Loss: 0.041190918534994125\n",
      "Loss: 0.0251630786806345\n",
      "Loss: 0.025536207482218742\n",
      "Loss: 0.06284837424755096\n",
      "Loss: 0.027455341070890427\n",
      "Loss: 0.12630575895309448\n",
      "Loss: 0.027357224375009537\n",
      "Loss: 0.20143529772758484\n",
      "Loss: 0.02488008514046669\n",
      "Loss: 0.06033901497721672\n",
      "Loss: 0.024549970403313637\n",
      "Loss: 0.03349892050027847\n",
      "Loss: 0.07047457993030548\n",
      "Loss: 0.07755770534276962\n",
      "Loss: 0.0247209332883358\n",
      "Loss: 0.024017341434955597\n",
      "Loss: 0.02455000951886177\n",
      "Loss: 0.037459228187799454\n",
      "Loss: 0.07135196775197983\n",
      "Loss: 0.03594106808304787\n",
      "Loss: 0.028053872287273407\n",
      "Loss: 0.032765768468379974\n",
      "Loss: 0.027599725872278214\n",
      "Loss: 0.0234144926071167\n",
      "Loss: 0.024497343227267265\n",
      "Loss: 0.0235744621604681\n",
      "Loss: 0.03008655272424221\n",
      "Loss: 0.024134863168001175\n",
      "Loss: 0.02582670748233795\n",
      "Loss: 0.03217790648341179\n",
      "Loss: 0.14329010248184204\n",
      "Loss: 0.03708003833889961\n",
      "Loss: 0.024715449661016464\n",
      "Loss: 0.059425272047519684\n",
      "Loss: 0.034194186329841614\n",
      "Loss: 0.04265490919351578\n",
      "Loss: 0.07746803015470505\n",
      "Loss: 0.03366347774863243\n",
      "Loss: 0.026355870068073273\n",
      "Loss: 0.025028588250279427\n",
      "Loss: 0.04749421030282974\n",
      "Loss: 0.115322045981884\n",
      "Loss: 0.0620979443192482\n",
      "Loss: 0.028172949329018593\n",
      "Loss: 0.06295908242464066\n",
      "Loss: 0.023771118372678757\n",
      "Loss: 0.045600686222314835\n",
      "Loss: 0.025951826944947243\n",
      "Loss: 0.070981964468956\n",
      "Loss: 0.023076046258211136\n",
      "Loss: 0.0262487530708313\n",
      "Loss: 0.02545596845448017\n",
      "Loss: 0.0365646630525589\n",
      "Loss: 0.06756569445133209\n",
      "Loss: 0.030208243057131767\n",
      "Loss: 0.04055660963058472\n",
      "Loss: 0.056189220398664474\n",
      "Loss: 0.0316070057451725\n",
      "Loss: 0.042441174387931824\n",
      "Loss: 0.032073937356472015\n",
      "Loss: 0.025412406772375107\n",
      "Loss: 0.026237333193421364\n",
      "Loss: 0.08134400844573975\n",
      "Loss: 0.08352010697126389\n",
      "Loss: 0.02294507622718811\n",
      "Loss: 0.031725868582725525\n",
      "Loss: 0.028904635459184647\n",
      "Loss: 0.023114653304219246\n",
      "Loss: 0.03507405146956444\n",
      "Loss: 0.03128835931420326\n",
      "Loss: 0.08277604728937149\n",
      "Loss: 0.026319503784179688\n",
      "Loss: 0.03996703028678894\n",
      "Loss: 0.04588872939348221\n",
      "Loss: 0.05858582630753517\n",
      "Loss: 0.0402158759534359\n",
      "Loss: 0.07274375110864639\n",
      "Loss: 0.030757363885641098\n",
      "Loss: 0.029633626341819763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 19/63 [28:13<1:05:27, 89.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04275583475828171\n",
      "Epoch: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b672d77d4046ce8322f5fb83599079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.022867705672979355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['target', 'policy_tenure, age_of_car, age_of_policyholder, air']\n",
      "Loss: 0.022330980747938156\n",
      "Predictions: ['6', 'Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior']\n",
      "Loss: 0.017807120457291603\n",
      "Predictions: ['hours-per-week <= 41.5, capital-loss <= 18', 'booking_status']\n",
      "Loss: 0.023135293275117874\n",
      "Predictions: ['6', 'Age, TB, DB, Alkphos, Sgpt, Sgot, TP']\n",
      "Loss: 0.016200901940464973\n",
      "Predictions: ['Hardness <= 278.29, Chloramines <= 6.', 'type_of_meal_plan, room_type_reserved, required_']\n",
      "Loss: 0.016731170937418938\n",
      "Predictions: ['CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary', 'Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque']\n",
      "Loss: 0.01833643764257431\n",
      "Predictions: ['JoiningYear <= 2017.5, ExperienceInCurrentDomain <= 3', 'texture_mean, perimeter_mean, texture_se, perimeter_se, area_se']\n",
      "Loss: 0.01749548316001892\n",
      "Predictions: ['Geography, Gender, HasCrCard, IsActiveMember', 'Dependents, Property_Area, Gender, Married, Education, Self_Empl']\n",
      "Loss: 0.015375887043774128\n",
      "Predictions: ['Pregnancies, Glucose, BloodPressure, SkinThickness, In', 'area_cluster, segment, model, fuel_type, max_torque, max_']\n",
      "Loss: 0.04646053537726402\n",
      "Predictions: ['Rainfall, WindGustDir, WindDir9am, WindSpeed9am, Pressure', 'Rainfall, WindSpeed9am, Pressure9am, Pressure3pm, Cloud9am']\n",
      "Loss: 0.023851845413446426\n",
      "Predictions: ['sex, hear_left, hear_right', '4']\n",
      "Loss: 0.054728541523218155\n",
      "Predictions: ['Age Ratio', '6']\n",
      "Loss: 0.018257351592183113\n",
      "Predictions: ['quality', 'Age, Height, Weight, FCVC, NCP, CH2O, FAF,']\n",
      "Loss: 0.04676809534430504\n",
      "Predictions: ['Potability', '491, 50, 50']\n",
      "Loss: 0.016047099605202675\n",
      "Predictions: ['Age, pH, Specific Gravity', 'no_of_adults, no_of_children, no_of_weekend']\n",
      "Loss: 0.020555604249238968\n",
      "Predictions: ['Alcohol, Malic acid, Ash, Alcalinity of ash, Total phenol', '6']\n",
      "Loss: 0.017467617988586426\n",
      "Predictions: ['ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount', 'fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free']\n",
      "Loss: 0.015380817465484142\n",
      "Predictions: ['Education, City, Gender, EverBenched', 'age, height, weight, waistline, SBP, BLDS, tot_c']\n",
      "Loss: 0.017392173409461975\n",
      "Predictions: ['Age, Work_Experience, Family_Size', 'Ia, Ib, Ic, Va, Vb, Vc']\n",
      "Loss: 0.01769370399415493\n",
      "Predictions: ['Alkphos <= 211.5, Sgot <= 26.', 'anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem']\n",
      "Loss: 0.01668773591518402\n",
      "Predictions: ['anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem', 'Segmentation']\n",
      "Loss: 0.01775982230901718\n",
      "Predictions: ['RainTomorrow', 'basic_needs <= 3.5, bullying <= 1.5']\n",
      "Loss: 0.025929830968379974\n",
      "Predictions: ['MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO', 'variance, skewness, curtosis, entropy']\n",
      "Loss: 0.016438240185379982\n",
      "Predictions: ['slope <= 1.5, restecg <= 0.5', 'age, fnlwgt, educational-num, capital-gain, capital-loss, hours']\n",
      "Loss: 0.03449609875679016\n",
      "Predictions: ['ra, dec, run, camcol, field, redshift, plate, mjd', 'COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY']\n",
      "Loss: 0.017236828804016113\n",
      "Predictions: ['ReachedOnTime', 'PetalWidthCm <= 0.7, PetalWidthCm <= 1']\n",
      "Loss: 0.017513085156679153\n",
      "Predictions: ['SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet', 'Class']\n",
      "Loss: 0.01643708534538746\n",
      "Predictions: ['Age <= 42.5, NumOfProducts <= 2.5', 'Warehouse_block, Mode_of_Shipment, Product_importance, Gender']\n",
      "Loss: 0.0288486760109663\n",
      "Predictions: ['4', '6']\n",
      "Loss: 0.02423088438808918\n",
      "Predictions: ['BMI <= 29.85, Age <= 27.5', '10']\n",
      "Loss: 0.01594550721347332\n",
      "Predictions: ['age, fnlwgt, educational-num, capital-gain, capital-loss, hours', 'age, cp, trestbps, chol, restecg, thalach, old']\n",
      "Loss: 0.027199136093258858\n",
      "Predictions: ['class', '11']\n",
      "Loss: 0.015563552267849445\n",
      "Predictions: ['Age, TB, DB, Alkphos, Sgpt, Sgot, TP', 'MAJORSKEWNESS <= 74.5, CIRCULARITY <=']\n",
      "Loss: 0.017964381724596024\n",
      "Predictions: ['Age, pH, Specific Gravity', 'Machine_failure']\n",
      "Loss: 0.01652803085744381\n",
      "Predictions: ['ph, Hardness, Chloramines, Sulfate, Conductivity, Trihal', 'variance, skewness, curtosis, entropy']\n",
      "Loss: 0.017174575477838516\n",
      "Predictions: ['LeaveOrNot', 'Ic <= 71.01, Vb <= -0.37']\n",
      "Loss: 0.028973106294870377\n",
      "Predictions: ['Size, Weight, Sweetness, Crunchiness, Juiciness, Ripeness', 'age, height, weight, waistline, SBP, BDDS, tot_c']\n",
      "Loss: 0.02128353714942932\n",
      "Predictions: ['10', 'CAEC, CALC, MTRANS, Gender, family_history_with_over']\n",
      "Loss: 0.027874130755662918\n",
      "Predictions: ['Rotational speed [rpm] <= 1381.5, Torque [', 'ia, ib, ic, Va, Vb, Vc']\n",
      "Loss: 0.015940411016345024\n",
      "Predictions: ['Area, Perimeter, MinorAxisLength, AspectRation, Eccentricity,', 'mental_health_history']\n",
      "Loss: 0.0165301151573658\n",
      "Predictions: ['Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior', 'Quality']\n",
      "Loss: 0.022525658831000328\n",
      "Predictions: ['10', 'lead_time <= 151.5, no_of_special_requests']\n",
      "Loss: 0.014496228657662868\n",
      "Predictions: ['battery_power, fc, int_memory, mobile_wt, n_cores', 'ssc_p, hsc_p, degree_p, etest_p']\n",
      "Loss: 0.07924085110425949\n",
      "Predictions: ['10', 'Age <= 98372.5, Age <= 1955.']\n",
      "Loss: 0.023334460332989693\n",
      "Predictions: ['target', '6']\n",
      "Loss: 0.03544216230511665\n",
      "Predictions: ['Alcohol, Malic acid, Ash, alkalinity of ash, Total phenol', 'anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem']\n",
      "Loss: 0.02450774982571602\n",
      "Predictions: ['10', 'texture_mean, perimeter_mean, texture_se, perimeter_se, area_se']\n",
      "Loss: 0.014245782978832722\n",
      "Predictions: ['Location, WindGustDir, WindDir9am, WindDir3pm, RainToday', 'age, height, weight, waistline, SBP, BLDS, tot_c']\n",
      "Loss: 0.01770896092057228\n",
      "Predictions: ['Age, Work_Experience, Family_Size', 'Class']\n",
      "Loss: 0.020045120269060135\n",
      "Predictions: ['Gender, Departments, Self_Employed, Loan_Amount_Term, Credit', 'Age, Work_Experience, Family_Size']\n",
      "Loss: 0.016227956861257553\n",
      "Predictions: ['texture_mean, perimeter_mean, texture_se, perimeter_se, area_se', 'income']\n",
      "Loss: 0.017409108579158783\n",
      "Predictions: ['Sex', 'USMER, SEX, PATIENT_TYPE']\n",
      "Loss: 0.080207459628582\n",
      "Predictions: ['Box, Perimeter, AbsoluteLength, SuperPulsion, Accelerality, Liquid', 'Pregnancies, Glucose, BloodPressure, InhalinThickness']\n",
      "Loss: 0.022794092074036598\n",
      "Predictions: ['6', 'Rainfall, WindSpeed9am, Pressure9am, Pressure3pm, Cloud9am']\n",
      "Loss: 0.018541445955634117\n",
      "Predictions: ['class', 'status']\n",
      "Loss: 0.016793938353657722\n",
      "Predictions: ['class', 'SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet']\n",
      "Loss: 0.018027866259217262\n",
      "Predictions: ['Color', 'fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free']\n",
      "Loss: 0.04043664038181305\n",
      "Predictions: ['no_of_adults, no_of_children, no_of_weekend', 'FAF <= 2.0, Height <= 1.25']\n",
      "Loss: 0.016317972913384438\n",
      "Predictions: ['Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight', 'DRK_YN']\n",
      "Loss: 0.023724449798464775\n",
      "Predictions: ['10', 'Diagnosis']\n",
      "Loss: 0.016929492354393005\n",
      "Predictions: ['Age, Height, Weight, FCVC, NCP, CH2O, FAF,', 'policy_tenure, age_of_car, age_of_policyholder, air']\n",
      "Loss: 0.014999553561210632\n",
      "Predictions: ['Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque', 'SMK_stat_type_cd <= 1.5, gamma_GTP <=']\n",
      "Loss: 0.015673672780394554\n",
      "Predictions: ['Family_Size <= 2.5, Work_Experience <= 9.5', 'Profession, Spending_Score, Var_1, Gender, Ever_Married,']\n",
      "Loss: 0.014964927919209003\n",
      "Predictions: ['Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior', 'Height <= 0.13, Diameter <= 0.45']\n",
      "Loss: 0.02225266396999359\n",
      "Predictions: ['Total phenols <= 2.36, Proanthocyanins <=', '10']\n",
      "Loss: 0.015135104767978191\n",
      "Predictions: ['age, cp, trestbps, chol, restecg, thalach, old', 'ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount']\n",
      "Loss: 0.025287579745054245\n",
      "Predictions: ['Output', 'Age, TB, DB, Alkphos, Sgot, Sgot, TP,']\n",
      "Loss: 0.014767362736165524\n",
      "Predictions: ['MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO', 'Selector']\n",
      "Loss: 0.024546772241592407\n",
      "Predictions: ['10', 'Color, Transparency, Glucose, Protein, Epithelial Cells, Mu']\n",
      "Loss: 0.015311327762901783\n",
      "Predictions: ['Size, Weight, Sweetness, Crunchiness, Juiciness, Ripeness', 'existing_credits <= 1.5, residence_since <= 3.5']\n",
      "Loss: 0.017317725345492363\n",
      "Predictions: ['10', 'COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY']\n",
      "Loss: 0.01509001199156046\n",
      "Predictions: ['ph, Hardness, Chloramines, Sulfate, Conductivity, Trihal', 'Prior_purchases <= 3.5, Customer_care_calls <= 4.']\n",
      "Loss: 0.016426172107458115\n",
      "Predictions: ['ra, dec, run, camcol, field, redshift, plate, mjd', 'Exited']\n",
      "Loss: 0.03201780095696449\n",
      "Predictions: ['Excir Married, Graduated, Profession, Work Experience, Family Size, Var', 'ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount']\n",
      "Loss: 0.01644952967762947\n",
      "Predictions: ['Outcome', 'duration, credit_amount, installment_commitment, residence_since, age, existing_']\n",
      "Loss: 0.017639020457863808\n",
      "Predictions: ['Ia, Ib, Ic, Va, Vb, Vc', 'NObeyesdad']\n",
      "Loss: 0.015778349712491035\n",
      "Predictions: ['workclass, education, marital-status, occupation, relationship, race, gender', 'CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary']\n",
      "Loss: 0.01577630266547203\n",
      "Predictions: ['Age <= 0.1, pH <= 5.5', 'Juiciness <= -0.3, Crunchiness <= 2.25']\n",
      "Loss: 0.03350440412759781\n",
      "Predictions: ['age, cp, trentban, chol, restecg, trentban, oil', 'Age, pH, Specific Gravity']\n",
      "Loss: 0.02149983122944832\n",
      "Predictions: ['10', 'checking_status, employment, other_parties, other_payment_plans, housing']\n",
      "Loss: 0.022254569455981255\n",
      "Predictions: ['Length, Diameter, Height, Whole weight, Shucked weight, Violata weight,', 'Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque']\n",
      "Loss: 0.028782658278942108\n",
      "Predictions: ['6', '6']\n",
      "Loss: 0.015300758183002472\n",
      "Predictions: ['ssc_p, hsc_p, degree_p, etest_p', 'ra, dec, run, camcol, field, redshift, plate, mjd']\n",
      "Loss: 0.014374850317835808\n",
      "Predictions: ['CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary', 'no_of_adults, no_of_children, no_of_weekend']\n",
      "Loss: 0.015024960972368717\n",
      "Predictions: ['Size, Weight, Sweetness, Crunchiness, Juiciness, Ripeness', 'Alcohol, Malic acid, Ash, Alcalinity of ash, Total phenol']\n",
      "Loss: 0.021924780681729317\n",
      "Predictions: ['11', 'perimeter_mean <= 90.47, texture_worst <= 27.']\n",
      "Loss: 0.019948860630393028\n",
      "Predictions: ['Pregnancies, Glucose, BloodPressure, SkinThickness, In', '10']\n",
      "Loss: 0.01745370402932167\n",
      "Predictions: ['stress_level', 'Species']\n",
      "Loss: 0.027192046865820885\n",
      "Predictions: ['10', 'ph, Hardness, Chloramines, Sulfate, Conductivity, Thiamine']\n",
      "Loss: 0.024049874395132065\n",
      "Predictions: ['duration, credit_amount, installment_commitment, residence_since, age, existing_', '11']\n",
      "Loss: 0.014844086021184921\n",
      "Predictions: ['ssc_p, hsc_p, degree_p, etest_p', 'skewness <= 5.16, curtosis <= 0.1']\n",
      "Loss: 0.021607650443911552\n",
      "Predictions: ['6', 'sex, fbs, exang']\n",
      "Loss: 0.0160446185618639\n",
      "Predictions: ['Rainfall <= 0.1, Pressure3pm <= 1009.6', 'is_claim']\n",
      "Loss: 0.016003228724002838\n",
      "Predictions: ['Gender', 'ssc_p <= 60.09, hsc_p <= ']\n",
      "Loss: 0.023079877719283104\n",
      "Predictions: ['age, fnlwgt, educational-num, capital-gain, capital-loss, hours', '11']\n",
      "Loss: 0.016145091503858566\n",
      "Predictions: ['blue, dual_sim, four_g, three_g, touch_screen, wifi', 'price_range']\n",
      "Loss: 0.014679779298603535\n",
      "Predictions: ['JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain', 'fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free']\n",
      "Loss: 0.013569694943726063\n",
      "Predictions: ['density <= 1.0, chlorides <= 0.08', 'MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO']\n",
      "Loss: 0.016580810770392418\n",
      "Predictions: ['JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain', 'int_memory <= 30.5, mobile_wt <= 91.']\n",
      "Loss: 0.020729102194309235\n",
      "Predictions: ['6', 'displacement <= 1196.5, height <= 1519.0']\n",
      "Loss: 0.01321162935346365\n",
      "Predictions: ['COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY', 'Loan_Status']\n",
      "Loss: 0.024039220064878464\n",
      "Predictions: ['diagnosis', '6']\n",
      "Loss: 0.01872265338897705\n",
      "Predictions: ['battery_power, fc, int_memory, mobile_wt, n_cores', '10']\n",
      "Loss: 0.015136560425162315\n",
      "Predictions: ['SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet', 'variance, skewness, curtosis, entropy']\n",
      "Loss: 0.016437815502285957\n",
      "Predictions: ['CLASSIFICATION', 'Rainfall, WindSpeed9am, Pressure9am, Pressure3pm, Cloud9am']\n",
      "Loss: 0.0142445620149374\n",
      "Predictions: ['policy_tenure, age_of_car, age_of_policyholder, air', 'CARDIOVASCULAR <= 50.0, ASHTMA <= 1']\n",
      "Loss: 0.0137788662686944\n",
      "Predictions: ['Area, Perimeter, MinorAxisLength, AspectRation, Eccentricity,', 'Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight']\n",
      "Loss: 0.014412414282560349\n",
      "Predictions: ['Loan_Amount_Term <= 420.0, ApplicantIncome <=', 'Age, Height, Weight, FCVC, NCP, CH2O, FAF,']\n",
      "Loss: 0.014608977362513542\n",
      "Predictions: ['hsc_s, degree_t, gender, ssc_b, hsc', 'dec <= 22.21, mjd <= 55090']\n",
      "Loss: 0.015034974552690983\n",
      "Predictions: ['duration, credit_amount, installment_commitment, residence_since, age, existing_', 'JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain']\n",
      "Loss: 0.022213123738765717\n",
      "Predictions: ['Type, TWF, HDF, PWF, OSF, RNF', '10']\n",
      "Loss: 0.031973857432603836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 20/63 [31:33<1:27:52, 122.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['battery_power, fc, int_memory, mobile_wt, c_cores', '11']\n",
      "Epoch: 20\n",
      "Loss: 0.016726579517126083\n",
      "Loss: 0.014567984268069267\n",
      "Loss: 0.01514372043311596\n",
      "Loss: 0.015937983989715576\n",
      "Loss: 0.02296629175543785\n",
      "Loss: 0.020796451717615128\n",
      "Loss: 0.02377442643046379\n",
      "Loss: 0.015367826446890831\n",
      "Loss: 0.01427458692342043\n",
      "Loss: 0.013804905116558075\n",
      "Loss: 0.022238127887248993\n",
      "Loss: 0.020231036469340324\n",
      "Loss: 0.015243329107761383\n",
      "Loss: 0.01950065605342388\n",
      "Loss: 0.013607118278741837\n",
      "Loss: 0.014889499172568321\n",
      "Loss: 0.014889230020344257\n",
      "Loss: 0.014414145611226559\n",
      "Loss: 0.020806949585676193\n",
      "Loss: 0.012805821374058723\n",
      "Loss: 0.014361872337758541\n",
      "Loss: 0.014316919259727001\n",
      "Loss: 0.01461538951843977\n",
      "Loss: 0.015136467292904854\n",
      "Loss: 0.027652103453874588\n",
      "Loss: 0.020278016105294228\n",
      "Loss: 0.017867226153612137\n",
      "Loss: 0.027748527005314827\n",
      "Loss: 0.01527013722807169\n",
      "Loss: 0.013967077247798443\n",
      "Loss: 0.024451052770018578\n",
      "Loss: 0.04165222495794296\n",
      "Loss: 0.013582931831479073\n",
      "Loss: 0.02243782952427864\n",
      "Loss: 0.016256248578429222\n",
      "Loss: 0.01997801847755909\n",
      "Loss: 0.0147203728556633\n",
      "Loss: 0.015450551174581051\n",
      "Loss: 0.01635162904858589\n",
      "Loss: 0.014074898324906826\n",
      "Loss: 0.014749416150152683\n",
      "Loss: 0.015212771482765675\n",
      "Loss: 0.014018336310982704\n",
      "Loss: 0.022156711667776108\n",
      "Loss: 0.03397229313850403\n",
      "Loss: 0.021780338138341904\n",
      "Loss: 0.015444613061845303\n",
      "Loss: 0.0220112856477499\n",
      "Loss: 0.014370075426995754\n",
      "Loss: 0.014040029607713223\n",
      "Loss: 0.017582030966877937\n",
      "Loss: 0.04059980437159538\n",
      "Loss: 0.013449878431856632\n",
      "Loss: 0.026328517124056816\n",
      "Loss: 0.013865434564650059\n",
      "Loss: 0.037601638585329056\n",
      "Loss: 0.021687962114810944\n",
      "Loss: 0.014361117035150528\n",
      "Loss: 0.08321771025657654\n",
      "Loss: 0.012735072523355484\n",
      "Loss: 0.015862172469496727\n",
      "Loss: 0.0142012694850564\n",
      "Loss: 0.013043126091361046\n",
      "Loss: 0.01310828048735857\n",
      "Loss: 0.013286661356687546\n",
      "Loss: 0.015053593553602695\n",
      "Loss: 0.014879218302667141\n",
      "Loss: 0.013641301542520523\n",
      "Loss: 0.014977216720581055\n",
      "Loss: 0.014324959367513657\n",
      "Loss: 0.014379856176674366\n",
      "Loss: 0.019320635125041008\n",
      "Loss: 0.01204639207571745\n",
      "Loss: 0.015027236193418503\n",
      "Loss: 0.02095831371843815\n",
      "Loss: 0.014057083055377007\n",
      "Loss: 0.014259576797485352\n",
      "Loss: 0.012848865240812302\n",
      "Loss: 0.019509444013237953\n",
      "Loss: 0.014666165225207806\n",
      "Loss: 0.011195020750164986\n",
      "Loss: 0.014196888543665409\n",
      "Loss: 0.013549881987273693\n",
      "Loss: 0.015178176574409008\n",
      "Loss: 0.01427946425974369\n",
      "Loss: 0.014397883787751198\n",
      "Loss: 0.015167705714702606\n",
      "Loss: 0.01360464096069336\n",
      "Loss: 0.014212441630661488\n",
      "Loss: 0.013962396420538425\n",
      "Loss: 0.014663776382803917\n",
      "Loss: 0.0220636036247015\n",
      "Loss: 0.02029608190059662\n",
      "Loss: 0.014675050042569637\n",
      "Loss: 0.018225761130452156\n",
      "Loss: 0.0267241969704628\n",
      "Loss: 0.01912592723965645\n",
      "Loss: 0.017405275255441666\n",
      "Loss: 0.013687467202544212\n",
      "Loss: 0.019414985552430153\n",
      "Loss: 0.018081342801451683\n",
      "Loss: 0.011756006628274918\n",
      "Loss: 0.016473034396767616\n",
      "Loss: 0.013058156706392765\n",
      "Loss: 0.014307694509625435\n",
      "Loss: 0.017813367769122124\n",
      "Loss: 0.014116907492280006\n",
      "Loss: 0.02109639160335064\n",
      "Loss: 0.01816200092434883\n",
      "Loss: 0.013773974031209946\n",
      "Loss: 0.013257941231131554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 21/63 [32:59<1:18:03, 111.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.010544759221374989\n",
      "Epoch: 21\n",
      "Loss: 0.012933922000229359\n",
      "Loss: 0.014289317652583122\n",
      "Loss: 0.017877958714962006\n",
      "Loss: 0.01214310247451067\n",
      "Loss: 0.0240252073854208\n",
      "Loss: 0.01849282532930374\n",
      "Loss: 0.011927626095712185\n",
      "Loss: 0.01767127215862274\n",
      "Loss: 0.012871881946921349\n",
      "Loss: 0.014298301190137863\n",
      "Loss: 0.014163007028400898\n",
      "Loss: 0.019503453746438026\n",
      "Loss: 0.016749277710914612\n",
      "Loss: 0.013685199432075024\n",
      "Loss: 0.013342169113457203\n",
      "Loss: 0.017116567119956017\n",
      "Loss: 0.012728272937238216\n",
      "Loss: 0.016469212248921394\n",
      "Loss: 0.012472921051084995\n",
      "Loss: 0.01399313285946846\n",
      "Loss: 0.013024680316448212\n",
      "Loss: 0.02975890412926674\n",
      "Loss: 0.023935481905937195\n",
      "Loss: 0.013698413968086243\n",
      "Loss: 0.013485166244208813\n",
      "Loss: 0.015051962807774544\n",
      "Loss: 0.015062959864735603\n",
      "Loss: 0.016426755115389824\n",
      "Loss: 0.011868203990161419\n",
      "Loss: 0.013515565544366837\n",
      "Loss: 0.016595032066106796\n",
      "Loss: 0.013162564486265182\n",
      "Loss: 0.019606400281190872\n",
      "Loss: 0.017141401767730713\n",
      "Loss: 0.014243187382817268\n",
      "Loss: 0.013394562527537346\n",
      "Loss: 0.012521606869995594\n",
      "Loss: 0.020544961094856262\n",
      "Loss: 0.012583665549755096\n",
      "Loss: 0.018965253606438637\n",
      "Loss: 0.029184337705373764\n",
      "Loss: 0.01593754068017006\n",
      "Loss: 0.012875276617705822\n",
      "Loss: 0.013021455146372318\n",
      "Loss: 0.01151465903967619\n",
      "Loss: 0.012760487385094166\n",
      "Loss: 0.01336478441953659\n",
      "Loss: 0.013684609904885292\n",
      "Loss: 0.01111323107033968\n",
      "Loss: 0.01949493959546089\n",
      "Loss: 0.010250319726765156\n",
      "Loss: 0.013643434271216393\n",
      "Loss: 0.01283320039510727\n",
      "Loss: 0.015351785346865654\n",
      "Loss: 0.012831891886889935\n",
      "Loss: 0.013431710191071033\n",
      "Loss: 0.014686539769172668\n",
      "Loss: 0.02382100373506546\n",
      "Loss: 0.013047758489847183\n",
      "Loss: 0.013473974540829659\n",
      "Loss: 0.012222778052091599\n",
      "Loss: 0.012411700561642647\n",
      "Loss: 0.015061812475323677\n",
      "Loss: 0.012348771095275879\n",
      "Loss: 0.015640830621123314\n",
      "Loss: 0.012013264000415802\n",
      "Loss: 0.016013795509934425\n",
      "Loss: 0.012346930801868439\n",
      "Loss: 0.01707249879837036\n",
      "Loss: 0.012145216576755047\n",
      "Loss: 0.012333841063082218\n",
      "Loss: 0.010791853070259094\n",
      "Loss: 0.012865224853157997\n",
      "Loss: 0.01291634701192379\n",
      "Loss: 0.012109371833503246\n",
      "Loss: 0.01173390168696642\n",
      "Loss: 0.012879756279289722\n",
      "Loss: 0.021909533068537712\n",
      "Loss: 0.01168787106871605\n",
      "Loss: 0.016343342140316963\n",
      "Loss: 0.02469172514975071\n",
      "Loss: 0.07034566253423691\n",
      "Loss: 0.010888363234698772\n",
      "Loss: 0.012780308723449707\n",
      "Loss: 0.01687595061957836\n",
      "Loss: 0.011715997010469437\n",
      "Loss: 0.015062496066093445\n",
      "Loss: 0.013334416784346104\n",
      "Loss: 0.011199472472071648\n",
      "Loss: 0.013197226449847221\n",
      "Loss: 0.01219091471284628\n",
      "Loss: 0.02234608121216297\n",
      "Loss: 0.012329808436334133\n",
      "Loss: 0.022219520062208176\n",
      "Loss: 0.020426804199814796\n",
      "Loss: 0.011960438452661037\n",
      "Loss: 0.014292698353528976\n",
      "Loss: 0.016602380201220512\n",
      "Loss: 0.011153683066368103\n",
      "Loss: 0.025475990027189255\n",
      "Loss: 0.01361760776489973\n",
      "Loss: 0.012751891277730465\n",
      "Loss: 0.012184609659016132\n",
      "Loss: 0.015868529677391052\n",
      "Loss: 0.013792283833026886\n",
      "Loss: 0.012200321070849895\n",
      "Loss: 0.021026231348514557\n",
      "Loss: 0.010823092423379421\n",
      "Loss: 0.013085166923701763\n",
      "Loss: 0.01257496327161789\n",
      "Loss: 0.010622050613164902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 22/63 [34:24<1:10:50, 103.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.012304605916142464\n",
      "Epoch: 22\n",
      "Loss: 0.011675694026052952\n",
      "Loss: 0.01575530506670475\n",
      "Loss: 0.010686959140002728\n",
      "Loss: 0.010989716276526451\n",
      "Loss: 0.018314648419618607\n",
      "Loss: 0.01261840108782053\n",
      "Loss: 0.012608552351593971\n",
      "Loss: 0.009941201657056808\n",
      "Loss: 0.011956887319684029\n",
      "Loss: 0.01222082320600748\n",
      "Loss: 0.024214500561356544\n",
      "Loss: 0.021248972043395042\n",
      "Loss: 0.01618848741054535\n",
      "Loss: 0.016615262255072594\n",
      "Loss: 0.016635993495583534\n",
      "Loss: 0.012114805169403553\n",
      "Loss: 0.01232417393475771\n",
      "Loss: 0.011825832538306713\n",
      "Loss: 0.012127271853387356\n",
      "Loss: 0.014012816362082958\n",
      "Loss: 0.01359099242836237\n",
      "Loss: 0.01188026275485754\n",
      "Loss: 0.00933629646897316\n",
      "Loss: 0.012587886303663254\n",
      "Loss: 0.017023641616106033\n",
      "Loss: 0.01318367850035429\n",
      "Loss: 0.011960201896727085\n",
      "Loss: 0.013834696263074875\n",
      "Loss: 0.01647401601076126\n",
      "Loss: 0.011668791063129902\n",
      "Loss: 0.014759212732315063\n",
      "Loss: 0.008981836959719658\n",
      "Loss: 0.01291957963258028\n",
      "Loss: 0.01330595463514328\n",
      "Loss: 0.01090184785425663\n",
      "Loss: 0.010832213796675205\n",
      "Loss: 0.012386512942612171\n",
      "Loss: 0.010025227442383766\n",
      "Loss: 0.011972170323133469\n",
      "Loss: 0.010131039656698704\n",
      "Loss: 0.015882458537817\n",
      "Loss: 0.020071761682629585\n",
      "Loss: 0.011018343269824982\n",
      "Loss: 0.01185114961117506\n",
      "Loss: 0.01240273006260395\n",
      "Loss: 0.017856331542134285\n",
      "Loss: 0.016930406913161278\n",
      "Loss: 0.012030254118144512\n",
      "Loss: 0.015756677836179733\n",
      "Loss: 0.01152801513671875\n",
      "Loss: 0.011157135479152203\n",
      "Loss: 0.011674102395772934\n",
      "Loss: 0.011846721172332764\n",
      "Loss: 0.010344686917960644\n",
      "Loss: 0.014706159010529518\n",
      "Loss: 0.013191095553338528\n",
      "Loss: 0.013375484384596348\n",
      "Loss: 0.014334220439195633\n",
      "Loss: 0.011746999807655811\n",
      "Loss: 0.012381932698190212\n",
      "Loss: 0.009557869285345078\n",
      "Loss: 0.00939165148884058\n",
      "Loss: 0.012561818584799767\n",
      "Loss: 0.02519223839044571\n",
      "Loss: 0.012265772558748722\n",
      "Loss: 0.010748352855443954\n",
      "Loss: 0.013072162866592407\n",
      "Loss: 0.012062414549291134\n",
      "Loss: 0.014325650408864021\n",
      "Loss: 0.012095729820430279\n",
      "Loss: 0.011852830648422241\n",
      "Loss: 0.011571493931114674\n",
      "Loss: 0.012479651719331741\n",
      "Loss: 0.012359071522951126\n",
      "Loss: 0.012519369833171368\n",
      "Loss: 0.012964126653969288\n",
      "Loss: 0.011508648283779621\n",
      "Loss: 0.012438509613275528\n",
      "Loss: 0.010660807602107525\n",
      "Loss: 0.030464351177215576\n",
      "Loss: 0.012056374922394753\n",
      "Loss: 0.01279305387288332\n",
      "Loss: 0.01097486075013876\n",
      "Loss: 0.011831454932689667\n",
      "Loss: 0.0128026083111763\n",
      "Loss: 0.012572509236633778\n",
      "Loss: 0.011054621078073978\n",
      "Loss: 0.011458514258265495\n",
      "Loss: 0.013931727968156338\n",
      "Loss: 0.012727923691272736\n",
      "Loss: 0.011243809945881367\n",
      "Loss: 0.01289201620966196\n",
      "Loss: 0.011740989051759243\n",
      "Loss: 0.011021916754543781\n",
      "Loss: 0.012985973618924618\n",
      "Loss: 0.01537784468382597\n",
      "Loss: 0.011828151531517506\n",
      "Loss: 0.011201220564544201\n",
      "Loss: 0.011209897696971893\n",
      "Loss: 0.01201711967587471\n",
      "Loss: 0.01176474429666996\n",
      "Loss: 0.011823010630905628\n",
      "Loss: 0.011903535574674606\n",
      "Loss: 0.012388328090310097\n",
      "Loss: 0.011896699666976929\n",
      "Loss: 0.019661977887153625\n",
      "Loss: 0.011876355856657028\n",
      "Loss: 0.013414316810667515\n",
      "Loss: 0.01166226714849472\n",
      "Loss: 0.02070373296737671\n",
      "Loss: 0.011075797490775585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 23/63 [35:50<1:05:29, 98.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.010475674644112587\n",
      "Epoch: 23\n",
      "Loss: 0.011235596612095833\n",
      "Loss: 0.011255687102675438\n",
      "Loss: 0.01030066329985857\n",
      "Loss: 0.011999950744211674\n",
      "Loss: 0.011673837900161743\n",
      "Loss: 0.012568755075335503\n",
      "Loss: 0.010907446965575218\n",
      "Loss: 0.011598395183682442\n",
      "Loss: 0.011596974916756153\n",
      "Loss: 0.009826552122831345\n",
      "Loss: 0.009656420908868313\n",
      "Loss: 0.011382048018276691\n",
      "Loss: 0.01115452777594328\n",
      "Loss: 0.013503321446478367\n",
      "Loss: 0.009631187655031681\n",
      "Loss: 0.012381953187286854\n",
      "Loss: 0.010401295498013496\n",
      "Loss: 0.013074034824967384\n",
      "Loss: 0.017975201830267906\n",
      "Loss: 0.010894070379436016\n",
      "Loss: 0.013139578513801098\n",
      "Loss: 0.010780628770589828\n",
      "Loss: 0.010843677446246147\n",
      "Loss: 0.009724347852170467\n",
      "Loss: 0.011687250807881355\n",
      "Loss: 0.010509192943572998\n",
      "Loss: 0.013587973080575466\n",
      "Loss: 0.011739282868802547\n",
      "Loss: 0.012335238978266716\n",
      "Loss: 0.018925394862890244\n",
      "Loss: 0.01217783335596323\n",
      "Loss: 0.012257766909897327\n",
      "Loss: 0.014911921694874763\n",
      "Loss: 0.010438417084515095\n",
      "Loss: 0.013490865007042885\n",
      "Loss: 0.021178951486945152\n",
      "Loss: 0.011495610699057579\n",
      "Loss: 0.01258804276585579\n",
      "Loss: 0.01310355681926012\n",
      "Loss: 0.009959502145648003\n",
      "Loss: 0.01594804786145687\n",
      "Loss: 0.017290689051151276\n",
      "Loss: 0.010409356094896793\n",
      "Loss: 0.01240722369402647\n",
      "Loss: 0.011615835130214691\n",
      "Loss: 0.011168442666530609\n",
      "Loss: 0.020918183028697968\n",
      "Loss: 0.01211470179259777\n",
      "Loss: 0.010219419375061989\n",
      "Loss: 0.011207420378923416\n",
      "Loss: 0.010772300884127617\n",
      "Loss: 0.010791241191327572\n",
      "Loss: 0.01122260745614767\n",
      "Loss: 0.009962298907339573\n",
      "Loss: 0.010981221683323383\n",
      "Loss: 0.01127918902784586\n",
      "Loss: 0.010164887644350529\n",
      "Loss: 0.010186104103922844\n",
      "Loss: 0.01348288357257843\n",
      "Loss: 0.010622251778841019\n",
      "Loss: 0.012343814596533775\n",
      "Loss: 0.010728922672569752\n",
      "Loss: 0.010979725979268551\n",
      "Loss: 0.010229037143290043\n",
      "Loss: 0.010963402688503265\n",
      "Loss: 0.010567563585937023\n",
      "Loss: 0.011012749746441841\n",
      "Loss: 0.009080210700631142\n",
      "Loss: 0.009191534481942654\n",
      "Loss: 0.013147640973329544\n",
      "Loss: 0.009645861573517323\n",
      "Loss: 0.010585934855043888\n",
      "Loss: 0.013611470349133015\n",
      "Loss: 0.010799962095916271\n",
      "Loss: 0.01123750302940607\n",
      "Loss: 0.009073927067220211\n",
      "Loss: 0.008159209042787552\n",
      "Loss: 0.010397437028586864\n",
      "Loss: 0.012776084244251251\n",
      "Loss: 0.01062722783535719\n",
      "Loss: 0.013734590262174606\n",
      "Loss: 0.015176062472164631\n",
      "Loss: 0.01029602624475956\n",
      "Loss: 0.010057087987661362\n",
      "Loss: 0.010225053876638412\n",
      "Loss: 0.010346152819693089\n",
      "Loss: 0.010953791439533234\n",
      "Loss: 0.0106230853125453\n",
      "Loss: 0.010239404626190662\n",
      "Loss: 0.009057420305907726\n",
      "Loss: 0.008904033340513706\n",
      "Loss: 0.012463469058275223\n",
      "Loss: 0.01116701029241085\n",
      "Loss: 0.011808959767222404\n",
      "Loss: 0.010158202610909939\n",
      "Loss: 0.010990401729941368\n",
      "Loss: 0.02355431579053402\n",
      "Loss: 0.010265609249472618\n",
      "Loss: 0.009384865872561932\n",
      "Loss: 0.011387544684112072\n",
      "Loss: 0.017639659345149994\n",
      "Loss: 0.014253324829041958\n",
      "Loss: 0.007260645739734173\n",
      "Loss: 0.010153200477361679\n",
      "Loss: 0.011770674027502537\n",
      "Loss: 0.011315596289932728\n",
      "Loss: 0.009359483607113361\n",
      "Loss: 0.010370377451181412\n",
      "Loss: 0.011371651664376259\n",
      "Loss: 0.012052612379193306\n",
      "Loss: 0.010048559866845608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 24/63 [37:15<1:01:25, 94.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.014917582273483276\n",
      "Epoch: 24\n",
      "Loss: 0.010216531343758106\n",
      "Loss: 0.011169223114848137\n",
      "Loss: 0.011192013509571552\n",
      "Loss: 0.011244421824812889\n",
      "Loss: 0.010625232011079788\n",
      "Loss: 0.010920804925262928\n",
      "Loss: 0.010127156972885132\n",
      "Loss: 0.010523969307541847\n",
      "Loss: 0.009884838946163654\n",
      "Loss: 0.013334195129573345\n",
      "Loss: 0.012808251194655895\n",
      "Loss: 0.008744887076318264\n",
      "Loss: 0.016154345124959946\n",
      "Loss: 0.010817770846188068\n",
      "Loss: 0.013718986883759499\n",
      "Loss: 0.00866665132343769\n",
      "Loss: 0.009923785924911499\n",
      "Loss: 0.010559472255408764\n",
      "Loss: 0.01056554727256298\n",
      "Loss: 0.01486397348344326\n",
      "Loss: 0.011139613576233387\n",
      "Loss: 0.009957963600754738\n",
      "Loss: 0.010145989246666431\n",
      "Loss: 0.010236595757305622\n",
      "Loss: 0.019547807052731514\n",
      "Loss: 0.009194396436214447\n",
      "Loss: 0.009324640035629272\n",
      "Loss: 0.011448390781879425\n",
      "Loss: 0.015682922676205635\n",
      "Loss: 0.011965402401983738\n",
      "Loss: 0.01743633858859539\n",
      "Loss: 0.008568404242396355\n",
      "Loss: 0.00971038918942213\n",
      "Loss: 0.010312594473361969\n",
      "Loss: 0.009634140878915787\n",
      "Loss: 0.00917069986462593\n",
      "Loss: 0.010685012675821781\n",
      "Loss: 0.013246111571788788\n",
      "Loss: 0.009260931983590126\n",
      "Loss: 0.010014534927904606\n",
      "Loss: 0.012735930271446705\n",
      "Loss: 0.010777641087770462\n",
      "Loss: 0.009699760936200619\n",
      "Loss: 0.010310737416148186\n",
      "Loss: 0.010584808886051178\n",
      "Loss: 0.009480132721364498\n",
      "Loss: 0.009910755790770054\n",
      "Loss: 0.010050235316157341\n",
      "Loss: 0.012486842460930347\n",
      "Loss: 0.012284157797694206\n",
      "Loss: 0.009732563979923725\n",
      "Loss: 0.009601260535418987\n",
      "Loss: 0.009644703008234501\n",
      "Loss: 0.01452567521482706\n",
      "Loss: 0.009056178852915764\n",
      "Loss: 0.011053349822759628\n",
      "Loss: 0.008423351682722569\n",
      "Loss: 0.009354128502309322\n",
      "Loss: 0.010220104828476906\n",
      "Loss: 0.009854499250650406\n",
      "Loss: 0.00916350819170475\n",
      "Loss: 0.011984901502728462\n",
      "Loss: 0.009422004222869873\n",
      "Loss: 0.016076015308499336\n",
      "Loss: 0.012541728094220161\n",
      "Loss: 0.009288964793086052\n",
      "Loss: 0.008334743790328503\n",
      "Loss: 0.009463244117796421\n",
      "Loss: 0.01052408292889595\n",
      "Loss: 0.010110939852893353\n",
      "Loss: 0.009330607019364834\n",
      "Loss: 0.01038698572665453\n",
      "Loss: 0.010630656965076923\n",
      "Loss: 0.010278216563165188\n",
      "Loss: 0.009520061314105988\n",
      "Loss: 0.009299790486693382\n",
      "Loss: 0.012744066305458546\n",
      "Loss: 0.009866276755928993\n",
      "Loss: 0.009422930888831615\n",
      "Loss: 0.011253037489950657\n",
      "Loss: 0.00801155623048544\n",
      "Loss: 0.00844514835625887\n",
      "Loss: 0.010593285784125328\n",
      "Loss: 0.009499595500528812\n",
      "Loss: 0.009485495276749134\n",
      "Loss: 0.007146412506699562\n",
      "Loss: 0.009759186767041683\n",
      "Loss: 0.0092831552028656\n",
      "Loss: 0.011457523331046104\n",
      "Loss: 0.010737022385001183\n",
      "Loss: 0.009355793707072735\n",
      "Loss: 0.009090520441532135\n",
      "Loss: 0.009616333059966564\n",
      "Loss: 0.010541776195168495\n",
      "Loss: 0.00893185380846262\n",
      "Loss: 0.00997899193316698\n",
      "Loss: 0.009775249287486076\n",
      "Loss: 0.009142095223069191\n",
      "Loss: 0.010516395792365074\n",
      "Loss: 0.008607575669884682\n",
      "Loss: 0.00944651011377573\n",
      "Loss: 0.009540891274809837\n",
      "Loss: 0.010754159651696682\n",
      "Loss: 0.00923801027238369\n",
      "Loss: 0.01034645363688469\n",
      "Loss: 0.00960418302565813\n",
      "Loss: 0.009508217684924603\n",
      "Loss: 0.010140127502381802\n",
      "Loss: 0.012249347753822803\n",
      "Loss: 0.009697280824184418\n",
      "Loss: 0.009655346162617207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 25/63 [38:41<58:10, 91.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00936050433665514\n",
      "Epoch: 25\n",
      "Loss: 0.00968113262206316\n",
      "Loss: 0.009965338744223118\n",
      "Loss: 0.010787833482027054\n",
      "Loss: 0.00945060234516859\n",
      "Loss: 0.009260338731110096\n",
      "Loss: 0.00925490166991949\n",
      "Loss: 0.010045756585896015\n",
      "Loss: 0.009971190243959427\n",
      "Loss: 0.008675946854054928\n",
      "Loss: 0.008798268623650074\n",
      "Loss: 0.008892934769392014\n",
      "Loss: 0.013923732563853264\n",
      "Loss: 0.009893560782074928\n",
      "Loss: 0.010122109204530716\n",
      "Loss: 0.009211488999426365\n",
      "Loss: 0.010033370926976204\n",
      "Loss: 0.009614838287234306\n",
      "Loss: 0.00989964883774519\n",
      "Loss: 0.00961341056972742\n",
      "Loss: 0.009944130666553974\n",
      "Loss: 0.009106501936912537\n",
      "Loss: 0.010110103525221348\n",
      "Loss: 0.008589699864387512\n",
      "Loss: 0.00964292511343956\n",
      "Loss: 0.011491619981825352\n",
      "Loss: 0.008618544787168503\n",
      "Loss: 0.012220974080264568\n",
      "Loss: 0.009414482861757278\n",
      "Loss: 0.011007792316377163\n",
      "Loss: 0.00962700042873621\n",
      "Loss: 0.009352574124932289\n",
      "Loss: 0.009617770090699196\n",
      "Loss: 0.010215425863862038\n",
      "Loss: 0.009812915697693825\n",
      "Loss: 0.007137613836675882\n",
      "Loss: 0.009386077523231506\n",
      "Loss: 0.008912978693842888\n",
      "Loss: 0.009479428641498089\n",
      "Loss: 0.01008013915270567\n",
      "Loss: 0.009695791639387608\n",
      "Loss: 0.008639170788228512\n",
      "Loss: 0.009922252967953682\n",
      "Loss: 0.010180753655731678\n",
      "Loss: 0.00899855513125658\n",
      "Loss: 0.009749951772391796\n",
      "Loss: 0.009012551046907902\n",
      "Loss: 0.009481322951614857\n",
      "Loss: 0.008611206896603107\n",
      "Loss: 0.009811854921281338\n",
      "Loss: 0.007652509491890669\n",
      "Loss: 0.008773554116487503\n",
      "Loss: 0.009559710510075092\n",
      "Loss: 0.009471521712839603\n",
      "Loss: 0.010006695985794067\n",
      "Loss: 0.008353492245078087\n",
      "Loss: 0.008335600607097149\n",
      "Loss: 0.009397001937031746\n",
      "Loss: 0.009779333136975765\n",
      "Loss: 0.009689994156360626\n",
      "Loss: 0.00783785805106163\n",
      "Loss: 0.008913603611290455\n",
      "Loss: 0.009345710277557373\n",
      "Loss: 0.009356320835649967\n",
      "Loss: 0.009346050210297108\n",
      "Loss: 0.00859680213034153\n",
      "Loss: 0.008072142489254475\n",
      "Loss: 0.008671887218952179\n",
      "Loss: 0.009393536485731602\n",
      "Loss: 0.007710973732173443\n",
      "Loss: 0.009979801252484322\n",
      "Loss: 0.00973503477871418\n",
      "Loss: 0.009369703009724617\n",
      "Loss: 0.00881105475127697\n",
      "Loss: 0.007602259982377291\n",
      "Loss: 0.008227254264056683\n",
      "Loss: 0.008838342502713203\n",
      "Loss: 0.008772879838943481\n",
      "Loss: 0.009367316961288452\n",
      "Loss: 0.009757878258824348\n",
      "Loss: 0.009407704696059227\n",
      "Loss: 0.008273609913885593\n",
      "Loss: 0.010614646598696709\n",
      "Loss: 0.009555894881486893\n",
      "Loss: 0.008940582163631916\n",
      "Loss: 0.011382916942238808\n",
      "Loss: 0.01137948501855135\n",
      "Loss: 0.01104172132909298\n",
      "Loss: 0.008611325174570084\n",
      "Loss: 0.009103250689804554\n",
      "Loss: 0.012218729592859745\n",
      "Loss: 0.007880481891334057\n",
      "Loss: 0.009277834556996822\n",
      "Loss: 0.010694541037082672\n",
      "Loss: 0.00924603920429945\n",
      "Loss: 0.009165367111563683\n",
      "Loss: 0.017108850181102753\n",
      "Loss: 0.009129026904702187\n",
      "Loss: 0.008774219080805779\n",
      "Loss: 0.009160426445305347\n",
      "Loss: 0.009554453194141388\n",
      "Loss: 0.008328255265951157\n",
      "Loss: 0.007563977502286434\n",
      "Loss: 0.007646501995623112\n",
      "Loss: 0.008570387959480286\n",
      "Loss: 0.009184293448925018\n",
      "Loss: 0.008615901693701744\n",
      "Loss: 0.0092070447281003\n",
      "Loss: 0.010046401061117649\n",
      "Loss: 0.009170540608465672\n",
      "Loss: 0.008883076719939709\n",
      "Loss: 0.009062109515070915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 26/63 [40:07<55:27, 89.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.006456015165895224\n",
      "Epoch: 26\n",
      "Loss: 0.00771787203848362\n",
      "Loss: 0.00919456034898758\n",
      "Loss: 0.009034445509314537\n",
      "Loss: 0.008992987684905529\n",
      "Loss: 0.008858797140419483\n",
      "Loss: 0.007080839481204748\n",
      "Loss: 0.009594768285751343\n",
      "Loss: 0.008493414148688316\n",
      "Loss: 0.00963324960321188\n",
      "Loss: 0.008409364148974419\n",
      "Loss: 0.008295811712741852\n",
      "Loss: 0.008123808540403843\n",
      "Loss: 0.009472853504121304\n",
      "Loss: 0.00864416640251875\n",
      "Loss: 0.008806318044662476\n",
      "Loss: 0.012583238072693348\n",
      "Loss: 0.008632329292595387\n",
      "Loss: 0.009198576211929321\n",
      "Loss: 0.008495410904288292\n",
      "Loss: 0.008234280161559582\n",
      "Loss: 0.00910932756960392\n",
      "Loss: 0.009376684203743935\n",
      "Loss: 0.00846046768128872\n",
      "Loss: 0.007498413790017366\n",
      "Loss: 0.00938011147081852\n",
      "Loss: 0.008723724633455276\n",
      "Loss: 0.00842649769037962\n",
      "Loss: 0.009310130961239338\n",
      "Loss: 0.009576916694641113\n",
      "Loss: 0.008685935288667679\n",
      "Loss: 0.009538047015666962\n",
      "Loss: 0.009515933692455292\n",
      "Loss: 0.008379563689231873\n",
      "Loss: 0.009031821973621845\n",
      "Loss: 0.009478557854890823\n",
      "Loss: 0.00848268624395132\n",
      "Loss: 0.007326659746468067\n",
      "Loss: 0.009472817182540894\n",
      "Loss: 0.008558050729334354\n",
      "Loss: 0.007489899639040232\n",
      "Loss: 0.009098515845835209\n",
      "Loss: 0.008967800997197628\n",
      "Loss: 0.007930509746074677\n",
      "Loss: 0.008469040505588055\n",
      "Loss: 0.008838573470711708\n",
      "Loss: 0.007418742403388023\n",
      "Loss: 0.009924291633069515\n",
      "Loss: 0.007862425409257412\n",
      "Loss: 0.0064443377777934074\n",
      "Loss: 0.007004354149103165\n",
      "Loss: 0.007726462557911873\n",
      "Loss: 0.008412541821599007\n",
      "Loss: 0.008781865239143372\n",
      "Loss: 0.011132537387311459\n",
      "Loss: 0.009042292833328247\n",
      "Loss: 0.008710427209734917\n",
      "Loss: 0.00811771396547556\n",
      "Loss: 0.008818316273391247\n",
      "Loss: 0.008172865025699139\n",
      "Loss: 0.008185802027583122\n",
      "Loss: 0.0075881630182266235\n",
      "Loss: 0.007833992131054401\n",
      "Loss: 0.0071157305501401424\n",
      "Loss: 0.009323027916252613\n",
      "Loss: 0.00923814345151186\n",
      "Loss: 0.00919533334672451\n",
      "Loss: 0.008763919584453106\n",
      "Loss: 0.008553490974009037\n",
      "Loss: 0.008373498916625977\n",
      "Loss: 0.009003756567835808\n",
      "Loss: 0.00835313182324171\n",
      "Loss: 0.007944859564304352\n",
      "Loss: 0.008454861119389534\n",
      "Loss: 0.0084352632984519\n",
      "Loss: 0.008606864139437675\n",
      "Loss: 0.008300457149744034\n",
      "Loss: 0.009262748062610626\n",
      "Loss: 0.009198647923767567\n",
      "Loss: 0.0074782646261155605\n",
      "Loss: 0.008277883753180504\n",
      "Loss: 0.008356750011444092\n",
      "Loss: 0.008855202235281467\n",
      "Loss: 0.008647614158689976\n",
      "Loss: 0.007692015264183283\n",
      "Loss: 0.00863699335604906\n",
      "Loss: 0.008782154880464077\n",
      "Loss: 0.00789509154856205\n",
      "Loss: 0.008379897102713585\n",
      "Loss: 0.009454715996980667\n",
      "Loss: 0.008256876841187477\n",
      "Loss: 0.007869177497923374\n",
      "Loss: 0.007556776516139507\n",
      "Loss: 0.007800815161317587\n",
      "Loss: 0.00804050825536251\n",
      "Loss: 0.008305988274514675\n",
      "Loss: 0.008435823954641819\n",
      "Loss: 0.015361558645963669\n",
      "Loss: 0.008472125045955181\n",
      "Loss: 0.00866176001727581\n",
      "Loss: 0.014071528799831867\n",
      "Loss: 0.0074006132781505585\n",
      "Loss: 0.00782029703259468\n",
      "Loss: 0.008761592209339142\n",
      "Loss: 0.006948921829462051\n",
      "Loss: 0.007600478362292051\n",
      "Loss: 0.007882407866418362\n",
      "Loss: 0.008479543030261993\n",
      "Loss: 0.0070758238434791565\n",
      "Loss: 0.00823749229311943\n",
      "Loss: 0.008639735169708729\n",
      "Loss: 0.009096206165850163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 27/63 [41:32<53:12, 88.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00817764550447464\n",
      "Epoch: 27\n",
      "Loss: 0.007251617964357138\n",
      "Loss: 0.007820193655788898\n",
      "Loss: 0.008079388178884983\n",
      "Loss: 0.00838869996368885\n",
      "Loss: 0.006699361838400364\n",
      "Loss: 0.006851675920188427\n",
      "Loss: 0.0212764460593462\n",
      "Loss: 0.011832929216325283\n",
      "Loss: 0.00848584994673729\n",
      "Loss: 0.008486880920827389\n",
      "Loss: 0.00841282494366169\n",
      "Loss: 0.006502780597656965\n",
      "Loss: 0.00829140655696392\n",
      "Loss: 0.00785523559898138\n",
      "Loss: 0.008213866502046585\n",
      "Loss: 0.007907251827418804\n",
      "Loss: 0.007757880724966526\n",
      "Loss: 0.007349708583205938\n",
      "Loss: 0.008041925728321075\n",
      "Loss: 0.016098851338028908\n",
      "Loss: 0.008422844111919403\n",
      "Loss: 0.008059841580688953\n",
      "Loss: 0.00795795675367117\n",
      "Loss: 0.007655344903469086\n",
      "Loss: 0.00805723387748003\n",
      "Loss: 0.008694566786289215\n",
      "Loss: 0.013826556503772736\n",
      "Loss: 0.02829604223370552\n",
      "Loss: 0.0077113257721066475\n",
      "Loss: 0.007984855212271214\n",
      "Loss: 0.009906578809022903\n",
      "Loss: 0.00788040179759264\n",
      "Loss: 0.008205515332520008\n",
      "Loss: 0.007027512416243553\n",
      "Loss: 0.0074461461044847965\n",
      "Loss: 0.008280559442937374\n",
      "Loss: 0.02330649271607399\n",
      "Loss: 0.007783392909914255\n",
      "Loss: 0.007675327360630035\n",
      "Loss: 0.019505711272358894\n",
      "Loss: 0.008122444152832031\n",
      "Loss: 0.006708227563649416\n",
      "Loss: 0.008679729886353016\n",
      "Loss: 0.008935153484344482\n",
      "Loss: 0.007608076091855764\n",
      "Loss: 0.008049983531236649\n",
      "Loss: 0.007860003039240837\n",
      "Loss: 0.00806189700961113\n",
      "Loss: 0.007610856089740992\n",
      "Loss: 0.017323270440101624\n",
      "Loss: 0.007599939592182636\n",
      "Loss: 0.008036782033741474\n",
      "Loss: 0.008268453180789948\n",
      "Loss: 0.008533725515007973\n",
      "Loss: 0.00719355046749115\n",
      "Loss: 0.00914161279797554\n",
      "Loss: 0.008436014875769615\n",
      "Loss: 0.00875193253159523\n",
      "Loss: 0.006800265982747078\n",
      "Loss: 0.00821060873568058\n",
      "Loss: 0.00866143498569727\n",
      "Loss: 0.008029293268918991\n",
      "Loss: 0.007353556342422962\n",
      "Loss: 0.007602032274007797\n",
      "Loss: 0.006670740898698568\n",
      "Loss: 0.00837651640176773\n",
      "Loss: 0.007468003313988447\n",
      "Loss: 0.008146259933710098\n",
      "Loss: 0.008175509981811047\n",
      "Loss: 0.008303288370370865\n",
      "Loss: 0.007537560071796179\n",
      "Loss: 0.007666006218641996\n",
      "Loss: 0.008204635232686996\n",
      "Loss: 0.008202730678021908\n",
      "Loss: 0.010625898838043213\n",
      "Loss: 0.00744481710717082\n",
      "Loss: 0.0078703248873353\n",
      "Loss: 0.007793124299496412\n",
      "Loss: 0.008435617201030254\n",
      "Loss: 0.006692070513963699\n",
      "Loss: 0.008353727869689465\n",
      "Loss: 0.008092893287539482\n",
      "Loss: 0.00861784815788269\n",
      "Loss: 0.007223302964121103\n",
      "Loss: 0.007857236079871655\n",
      "Loss: 0.007874039001762867\n",
      "Loss: 0.007982902228832245\n",
      "Loss: 0.007841006852686405\n",
      "Loss: 0.007621948141604662\n",
      "Loss: 0.0074821654707193375\n",
      "Loss: 0.006974713411182165\n",
      "Loss: 0.007186050061136484\n",
      "Loss: 0.006833985447883606\n",
      "Loss: 0.006269726902246475\n",
      "Loss: 0.007315726950764656\n",
      "Loss: 0.019330184906721115\n",
      "Loss: 0.009118226356804371\n",
      "Loss: 0.007215925958007574\n",
      "Loss: 0.007886986248195171\n",
      "Loss: 0.007022337056696415\n",
      "Loss: 0.0071215699426829815\n",
      "Loss: 0.007447817362844944\n",
      "Loss: 0.007277154829353094\n",
      "Loss: 0.007409385871142149\n",
      "Loss: 0.011168437078595161\n",
      "Loss: 0.00773771433159709\n",
      "Loss: 0.008226580917835236\n",
      "Loss: 0.007481803186237812\n",
      "Loss: 0.007691460195928812\n",
      "Loss: 0.007493200711905956\n",
      "Loss: 0.007692996878176928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 28/63 [42:58<51:13, 87.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.006458258721977472\n",
      "Epoch: 28\n",
      "Loss: 0.007698914501816034\n",
      "Loss: 0.007183724083006382\n",
      "Loss: 0.015115615911781788\n",
      "Loss: 0.007835297845304012\n",
      "Loss: 0.007663632743060589\n",
      "Loss: 0.007427308242768049\n",
      "Loss: 0.00605485076084733\n",
      "Loss: 0.007669435814023018\n",
      "Loss: 0.007225049659609795\n",
      "Loss: 0.007383392658084631\n",
      "Loss: 0.008464592508971691\n",
      "Loss: 0.007930323481559753\n",
      "Loss: 0.0076526678167283535\n",
      "Loss: 0.00708941463381052\n",
      "Loss: 0.010721779428422451\n",
      "Loss: 0.0067578568123281\n",
      "Loss: 0.0071682981215417385\n",
      "Loss: 0.006885087117552757\n",
      "Loss: 0.007405794691294432\n",
      "Loss: 0.007269041147083044\n",
      "Loss: 0.0073219602927565575\n",
      "Loss: 0.014553883112967014\n",
      "Loss: 0.008118507452309132\n",
      "Loss: 0.0072322264313697815\n",
      "Loss: 0.007377543952316046\n",
      "Loss: 0.007536599412560463\n",
      "Loss: 0.008207208476960659\n",
      "Loss: 0.007397528737783432\n",
      "Loss: 0.007775142788887024\n",
      "Loss: 0.006946940906345844\n",
      "Loss: 0.006865917704999447\n",
      "Loss: 0.013496818020939827\n",
      "Loss: 0.00761115038767457\n",
      "Loss: 0.008361849933862686\n",
      "Loss: 0.00702469190582633\n",
      "Loss: 0.005731956101953983\n",
      "Loss: 0.014415190555155277\n",
      "Loss: 0.00701515655964613\n",
      "Loss: 0.007564638741314411\n",
      "Loss: 0.007629279047250748\n",
      "Loss: 0.0075930519960820675\n",
      "Loss: 0.007977807894349098\n",
      "Loss: 0.007483370136469603\n",
      "Loss: 0.00934533216059208\n",
      "Loss: 0.006213143467903137\n",
      "Loss: 0.007664322853088379\n",
      "Loss: 0.007554447278380394\n",
      "Loss: 0.006707675289362669\n",
      "Loss: 0.006341641303151846\n",
      "Loss: 0.006995165254920721\n",
      "Loss: 0.007362488657236099\n",
      "Loss: 0.007419286761432886\n",
      "Loss: 0.006538569927215576\n",
      "Loss: 0.006989700254052877\n",
      "Loss: 0.00678322184830904\n",
      "Loss: 0.007741621229797602\n",
      "Loss: 0.007614098954945803\n",
      "Loss: 0.006279223132878542\n",
      "Loss: 0.007219837512820959\n",
      "Loss: 0.007677960675209761\n",
      "Loss: 0.006903863046318293\n",
      "Loss: 0.00730736693367362\n",
      "Loss: 0.007723088841885328\n",
      "Loss: 0.006142748519778252\n",
      "Loss: 0.006907640490680933\n",
      "Loss: 0.007007914129644632\n",
      "Loss: 0.007137331645935774\n",
      "Loss: 0.008139165118336678\n",
      "Loss: 0.006449966691434383\n",
      "Loss: 0.007408584468066692\n",
      "Loss: 0.006117973476648331\n",
      "Loss: 0.008023442700505257\n",
      "Loss: 0.006584438029676676\n",
      "Loss: 0.007921132259070873\n",
      "Loss: 0.007545863278210163\n",
      "Loss: 0.0074333189986646175\n",
      "Loss: 0.007564379367977381\n",
      "Loss: 0.006428698543459177\n",
      "Loss: 0.006841079797595739\n",
      "Loss: 0.006889548152685165\n",
      "Loss: 0.007966887205839157\n",
      "Loss: 0.008021741174161434\n",
      "Loss: 0.006932440213859081\n",
      "Loss: 0.006933456286787987\n",
      "Loss: 0.006698895711451769\n",
      "Loss: 0.007287926506251097\n",
      "Loss: 0.007416801992803812\n",
      "Loss: 0.007687032222747803\n",
      "Loss: 0.007501085754483938\n",
      "Loss: 0.006977159064263105\n",
      "Loss: 0.00674763647839427\n",
      "Loss: 0.00775782810524106\n",
      "Loss: 0.007380407769232988\n",
      "Loss: 0.007274974603205919\n",
      "Loss: 0.006421398837119341\n",
      "Loss: 0.007381425704807043\n",
      "Loss: 0.006420767400413752\n",
      "Loss: 0.007103199604898691\n",
      "Loss: 0.006707800552248955\n",
      "Loss: 0.006910826079547405\n",
      "Loss: 0.006387060042470694\n",
      "Loss: 0.007746460847556591\n",
      "Loss: 0.006026946473866701\n",
      "Loss: 0.006514379754662514\n",
      "Loss: 0.007647151593118906\n",
      "Loss: 0.006754780188202858\n",
      "Loss: 0.007509282324463129\n",
      "Loss: 0.007789333816617727\n",
      "Loss: 0.007401426322758198\n",
      "Loss: 0.0058666374534368515\n",
      "Loss: 0.00708515802398324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 29/63 [44:24<49:22, 87.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.006318011786788702\n",
      "Epoch: 29\n",
      "Loss: 0.0070776487700641155\n",
      "Loss: 0.007109703030437231\n",
      "Loss: 0.007641270291060209\n",
      "Loss: 0.006223284173756838\n",
      "Loss: 0.006663958076387644\n",
      "Loss: 0.006724626291543245\n",
      "Loss: 0.006731684785336256\n",
      "Loss: 0.007237154990434647\n",
      "Loss: 0.007382374722510576\n",
      "Loss: 0.006325055845081806\n",
      "Loss: 0.00660356180742383\n",
      "Loss: 0.006969001609832048\n",
      "Loss: 0.005225404165685177\n",
      "Loss: 0.007438957691192627\n",
      "Loss: 0.006574815139174461\n",
      "Loss: 0.007565037813037634\n",
      "Loss: 0.006666542496532202\n",
      "Loss: 0.007044127676635981\n",
      "Loss: 0.005941921845078468\n",
      "Loss: 0.005977316293865442\n",
      "Loss: 0.006247031968086958\n",
      "Loss: 0.008048536255955696\n",
      "Loss: 0.007139904890209436\n",
      "Loss: 0.007723111659288406\n",
      "Loss: 0.011487809009850025\n",
      "Loss: 0.006952547002583742\n",
      "Loss: 0.0075705344788730145\n",
      "Loss: 0.006480419542640448\n",
      "Loss: 0.007092130370438099\n",
      "Loss: 0.006866213865578175\n",
      "Loss: 0.007124896626919508\n",
      "Loss: 0.006845351308584213\n",
      "Loss: 0.006295286118984222\n",
      "Loss: 0.007016111630946398\n",
      "Loss: 0.006584244314581156\n",
      "Loss: 0.007068056613206863\n",
      "Loss: 0.007541072554886341\n",
      "Loss: 0.00605938583612442\n",
      "Loss: 0.0066712950356304646\n",
      "Loss: 0.00628917058929801\n",
      "Loss: 0.00708027696236968\n",
      "Loss: 0.00673041632398963\n",
      "Loss: 0.007210560608655214\n",
      "Loss: 0.006852423772215843\n",
      "Loss: 0.006301825400441885\n",
      "Loss: 0.007392827421426773\n",
      "Loss: 0.0074251070618629456\n",
      "Loss: 0.007259156554937363\n",
      "Loss: 0.006705508101731539\n",
      "Loss: 0.007338298484683037\n",
      "Loss: 0.00646041939035058\n",
      "Loss: 0.00751798041164875\n",
      "Loss: 0.006236663553863764\n",
      "Loss: 0.006768617779016495\n",
      "Loss: 0.006716251373291016\n",
      "Loss: 0.007375191431492567\n",
      "Loss: 0.0065468004904687405\n",
      "Loss: 0.006677044555544853\n",
      "Loss: 0.006372186820954084\n",
      "Loss: 0.005614082328975201\n",
      "Loss: 0.007097685243934393\n",
      "Loss: 0.0059810117818415165\n",
      "Loss: 0.006646683905273676\n",
      "Loss: 0.006797289475798607\n",
      "Loss: 0.006840016692876816\n",
      "Loss: 0.006947820540517569\n",
      "Loss: 0.006749974563717842\n",
      "Loss: 0.00684065418317914\n",
      "Loss: 0.006537416484206915\n",
      "Loss: 0.006385869346559048\n",
      "Loss: 0.006236968096345663\n",
      "Loss: 0.006477798335254192\n",
      "Loss: 0.006591998506337404\n",
      "Loss: 0.005997559521347284\n",
      "Loss: 0.006459459662437439\n",
      "Loss: 0.006884148810058832\n",
      "Loss: 0.006594125181436539\n",
      "Loss: 0.006632925011217594\n",
      "Loss: 0.006791448220610619\n",
      "Loss: 0.00680532306432724\n",
      "Loss: 0.006143185775727034\n",
      "Loss: 0.006362877786159515\n",
      "Loss: 0.006818223278969526\n",
      "Loss: 0.00605003722012043\n",
      "Loss: 0.006983274593949318\n",
      "Loss: 0.005633375141769648\n",
      "Loss: 0.006019629072397947\n",
      "Loss: 0.006010778713971376\n",
      "Loss: 0.006309241987764835\n",
      "Loss: 0.005724391434341669\n",
      "Loss: 0.007931121625006199\n",
      "Loss: 0.006655077449977398\n",
      "Loss: 0.0063850972801446915\n",
      "Loss: 0.0068359351716935635\n",
      "Loss: 0.0067464085295796394\n",
      "Loss: 0.005213463678956032\n",
      "Loss: 0.00694326451048255\n",
      "Loss: 0.005528067238628864\n",
      "Loss: 0.006153889466077089\n",
      "Loss: 0.005968389101326466\n",
      "Loss: 0.0068379975855350494\n",
      "Loss: 0.0051551321521401405\n",
      "Loss: 0.006451332941651344\n",
      "Loss: 0.006774268113076687\n",
      "Loss: 0.006311462726444006\n",
      "Loss: 0.0070644100196659565\n",
      "Loss: 0.006371947005391121\n",
      "Loss: 0.007388190366327763\n",
      "Loss: 0.005889484658837318\n",
      "Loss: 0.006708130706101656\n",
      "Loss: 0.006749296095222235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 30/63 [45:49<47:38, 86.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.006257879547774792\n",
      "Epoch: 30\n",
      "Loss: 0.006161104422062635\n",
      "Loss: 0.006585787516087294\n",
      "Loss: 0.006932975258678198\n",
      "Loss: 0.006781218107789755\n",
      "Loss: 0.006654016207903624\n",
      "Loss: 0.006965383887290955\n",
      "Loss: 0.005969387013465166\n",
      "Loss: 0.006672907155007124\n",
      "Loss: 0.006859948858618736\n",
      "Loss: 0.00659094238653779\n",
      "Loss: 0.006513675209134817\n",
      "Loss: 0.006364794913679361\n",
      "Loss: 0.006433587521314621\n",
      "Loss: 0.005487928632646799\n",
      "Loss: 0.006434476003050804\n",
      "Loss: 0.005971217527985573\n",
      "Loss: 0.006902693305164576\n",
      "Loss: 0.006626693531870842\n",
      "Loss: 0.0053655835799872875\n",
      "Loss: 0.0069892932660877705\n",
      "Loss: 0.006167692597955465\n",
      "Loss: 0.0064923204481601715\n",
      "Loss: 0.006799769587814808\n",
      "Loss: 0.0063521647825837135\n",
      "Loss: 0.006560088135302067\n",
      "Loss: 0.006436538882553577\n",
      "Loss: 0.00615245895460248\n",
      "Loss: 0.005937974434345961\n",
      "Loss: 0.005952381994575262\n",
      "Loss: 0.006386440712958574\n",
      "Loss: 0.00583430752158165\n",
      "Loss: 0.006473721470683813\n",
      "Loss: 0.0059707327745854855\n",
      "Loss: 0.006611888762563467\n",
      "Loss: 0.006376245524734259\n",
      "Loss: 0.006383906584233046\n",
      "Loss: 0.00650345254689455\n",
      "Loss: 0.005395234562456608\n",
      "Loss: 0.006244609132409096\n",
      "Loss: 0.005984200630337\n",
      "Loss: 0.004965875763446093\n",
      "Loss: 0.007106197997927666\n",
      "Loss: 0.006065132562071085\n",
      "Loss: 0.006637637969106436\n",
      "Loss: 0.006428524851799011\n",
      "Loss: 0.006194186396896839\n",
      "Loss: 0.0059225670993328094\n",
      "Loss: 0.010919696651399136\n",
      "Loss: 0.006580444052815437\n",
      "Loss: 0.006288088392466307\n",
      "Loss: 0.005293157882988453\n",
      "Loss: 0.0066084847785532475\n",
      "Loss: 0.006408716551959515\n",
      "Loss: 0.00524086644873023\n",
      "Loss: 0.006437450181692839\n",
      "Loss: 0.006658267695456743\n",
      "Loss: 0.005741533357650042\n",
      "Loss: 0.006367289926856756\n",
      "Loss: 0.005568335764110088\n",
      "Loss: 0.00656382879242301\n",
      "Loss: 0.006545055191963911\n",
      "Loss: 0.005436521489173174\n",
      "Loss: 0.005339829251170158\n",
      "Loss: 0.006760008167475462\n",
      "Loss: 0.006303461268544197\n",
      "Loss: 0.0059740906581282616\n",
      "Loss: 0.00596377719193697\n",
      "Loss: 0.005794689059257507\n",
      "Loss: 0.006882096640765667\n",
      "Loss: 0.005624423269182444\n",
      "Loss: 0.006125258281826973\n",
      "Loss: 0.006513508502393961\n",
      "Loss: 0.005546756088733673\n",
      "Loss: 0.006085596978664398\n",
      "Loss: 0.006205231882631779\n",
      "Loss: 0.006326090078800917\n",
      "Loss: 0.005396554246544838\n",
      "Loss: 0.005975819658488035\n",
      "Loss: 0.006256292574107647\n",
      "Loss: 0.006176056805998087\n",
      "Loss: 0.005645384080708027\n",
      "Loss: 0.006743452046066523\n",
      "Loss: 0.005612906999886036\n",
      "Loss: 0.006205931305885315\n",
      "Loss: 0.005866470746695995\n",
      "Loss: 0.006062530446797609\n",
      "Loss: 0.006602486129850149\n",
      "Loss: 0.005486918147653341\n",
      "Loss: 0.006651976145803928\n",
      "Loss: 0.005951703991740942\n",
      "Loss: 0.006335231941193342\n",
      "Loss: 0.005694246850907803\n",
      "Loss: 0.005946019198745489\n",
      "Loss: 0.005862085148692131\n",
      "Loss: 0.0057546948082745075\n",
      "Loss: 0.005837281234562397\n",
      "Loss: 0.006184723693877459\n",
      "Loss: 0.006376232020556927\n",
      "Loss: 0.005804894026368856\n",
      "Loss: 0.0064072380773723125\n",
      "Loss: 0.004866401199251413\n",
      "Loss: 0.005576536059379578\n",
      "Loss: 0.006565703544765711\n",
      "Loss: 0.006075217854231596\n",
      "Loss: 0.0061722612008452415\n",
      "Loss: 0.006082287058234215\n",
      "Loss: 0.005334429908543825\n",
      "Loss: 0.006036284379661083\n",
      "Loss: 0.006323120556771755\n",
      "Loss: 0.0063462830148637295\n",
      "Loss: 0.006060120649635792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 31/63 [47:15<46:02, 86.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.005865608341991901\n",
      "Epoch: 31\n",
      "Loss: 0.006037905812263489\n",
      "Loss: 0.006566691678017378\n",
      "Loss: 0.005299899727106094\n",
      "Loss: 0.006088039372116327\n",
      "Loss: 0.005554744508117437\n",
      "Loss: 0.0064477757550776005\n",
      "Loss: 0.0056808567605912685\n",
      "Loss: 0.006152366288006306\n",
      "Loss: 0.005971482954919338\n",
      "Loss: 0.005778508726507425\n",
      "Loss: 0.006152379792183638\n",
      "Loss: 0.005551105365157127\n",
      "Loss: 0.005718766711652279\n",
      "Loss: 0.006188340950757265\n",
      "Loss: 0.005808570887893438\n",
      "Loss: 0.008369475603103638\n",
      "Loss: 0.005646025761961937\n",
      "Loss: 0.0058468119241297245\n",
      "Loss: 0.006037701386958361\n",
      "Loss: 0.005744151305407286\n",
      "Loss: 0.004694350529462099\n",
      "Loss: 0.006017557345330715\n",
      "Loss: 0.005438590422272682\n",
      "Loss: 0.0055629052221775055\n",
      "Loss: 0.005560409743338823\n",
      "Loss: 0.005957856308668852\n",
      "Loss: 0.005406071897596121\n",
      "Loss: 0.005859205033630133\n",
      "Loss: 0.006332445424050093\n",
      "Loss: 0.0059820194728672504\n",
      "Loss: 0.00465124286711216\n",
      "Loss: 0.005801708437502384\n",
      "Loss: 0.00604415824636817\n",
      "Loss: 0.005871156230568886\n",
      "Loss: 0.005994892213493586\n",
      "Loss: 0.005313395988196135\n",
      "Loss: 0.005647327750921249\n",
      "Loss: 0.006119104567915201\n",
      "Loss: 0.005961151327937841\n",
      "Loss: 0.006304796785116196\n",
      "Loss: 0.005804677959531546\n",
      "Loss: 0.0055112033151090145\n",
      "Loss: 0.0056643676944077015\n",
      "Loss: 0.0058136070147156715\n",
      "Loss: 0.0062247104942798615\n",
      "Loss: 0.005879115778952837\n",
      "Loss: 0.0053269830532372\n",
      "Loss: 0.006441230420023203\n",
      "Loss: 0.005184924229979515\n",
      "Loss: 0.005918045528233051\n",
      "Loss: 0.004383080638945103\n",
      "Loss: 0.00613037496805191\n",
      "Loss: 0.005400596186518669\n",
      "Loss: 0.005514658987522125\n",
      "Loss: 0.005913489032536745\n",
      "Loss: 0.006338570266962051\n",
      "Loss: 0.006096002645790577\n",
      "Loss: 0.006014017388224602\n",
      "Loss: 0.00525770103558898\n",
      "Loss: 0.005844347644597292\n",
      "Loss: 0.00610384251922369\n",
      "Loss: 0.005327660124748945\n",
      "Loss: 0.005249344743788242\n",
      "Loss: 0.006203311029821634\n",
      "Loss: 0.005443362053483725\n",
      "Loss: 0.0062866793014109135\n",
      "Loss: 0.005500049330294132\n",
      "Loss: 0.0060181571170687675\n",
      "Loss: 0.006388229783624411\n",
      "Loss: 0.006216950248926878\n",
      "Loss: 0.005823747720569372\n",
      "Loss: 0.005758615676313639\n",
      "Loss: 0.005425554234534502\n",
      "Loss: 0.006277097854763269\n",
      "Loss: 0.006060539744794369\n",
      "Loss: 0.005948886275291443\n",
      "Loss: 0.005874902009963989\n",
      "Loss: 0.004745687823742628\n",
      "Loss: 0.005307519808411598\n",
      "Loss: 0.004524389747530222\n",
      "Loss: 0.005655957385897636\n",
      "Loss: 0.005935535300523043\n",
      "Loss: 0.00562891224399209\n",
      "Loss: 0.005906077567487955\n",
      "Loss: 0.0053657959215343\n",
      "Loss: 0.004875432699918747\n",
      "Loss: 0.005651374347507954\n",
      "Loss: 0.005588863044977188\n",
      "Loss: 0.00541435182094574\n",
      "Loss: 0.00557479914277792\n",
      "Loss: 0.00544312596321106\n",
      "Loss: 0.005438577849417925\n",
      "Loss: 0.005600549280643463\n",
      "Loss: 0.005596152041107416\n",
      "Loss: 0.004789004568010569\n",
      "Loss: 0.006064839195460081\n",
      "Loss: 0.005876865703612566\n",
      "Loss: 0.005899995099753141\n",
      "Loss: 0.005473041441291571\n",
      "Loss: 0.0061724805273115635\n",
      "Loss: 0.005550617352128029\n",
      "Loss: 0.00567292096093297\n",
      "Loss: 0.005634597968310118\n",
      "Loss: 0.005888823885470629\n",
      "Loss: 0.005225706845521927\n",
      "Loss: 0.0057288650423288345\n",
      "Loss: 0.005400605034083128\n",
      "Loss: 0.005737085826694965\n",
      "Loss: 0.005604240577667952\n",
      "Loss: 0.005627356935292482\n",
      "Loss: 0.0061563123017549515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 32/63 [48:40<44:28, 86.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.005249525420367718\n",
      "Epoch: 32\n",
      "Loss: 0.006049301940947771\n",
      "Loss: 0.005649462342262268\n",
      "Loss: 0.006059496663510799\n",
      "Loss: 0.0052449083887040615\n",
      "Loss: 0.00561458058655262\n",
      "Loss: 0.005918534938246012\n",
      "Loss: 0.005511686205863953\n",
      "Loss: 0.005609686486423016\n",
      "Loss: 0.005612222012132406\n",
      "Loss: 0.005954418331384659\n",
      "Loss: 0.005819662474095821\n",
      "Loss: 0.005546451546251774\n",
      "Loss: 0.005512612406164408\n",
      "Loss: 0.005284226965159178\n",
      "Loss: 0.005692420061677694\n",
      "Loss: 0.004909357521682978\n",
      "Loss: 0.005892680026590824\n",
      "Loss: 0.005359381902962923\n",
      "Loss: 0.005974230822175741\n",
      "Loss: 0.005399603862315416\n",
      "Loss: 0.005630930420011282\n",
      "Loss: 0.006080709397792816\n",
      "Loss: 0.0049216486513614655\n",
      "Loss: 0.004881235305219889\n",
      "Loss: 0.00540673965588212\n",
      "Loss: 0.005421598441898823\n",
      "Loss: 0.005654620938003063\n",
      "Loss: 0.006042445078492165\n",
      "Loss: 0.005549079272896051\n",
      "Loss: 0.006183442659676075\n",
      "Loss: 0.004987906664609909\n",
      "Loss: 0.005054367706179619\n",
      "Loss: 0.005451478064060211\n",
      "Loss: 0.005559901241213083\n",
      "Loss: 0.005776521749794483\n",
      "Loss: 0.005570224951952696\n",
      "Loss: 0.005253682844340801\n",
      "Loss: 0.005299718119204044\n",
      "Loss: 0.00532820587977767\n",
      "Loss: 0.005278692115098238\n",
      "Loss: 0.005494052544236183\n",
      "Loss: 0.006820742040872574\n",
      "Loss: 0.005024767480790615\n",
      "Loss: 0.0052387770265340805\n",
      "Loss: 0.005646657198667526\n",
      "Loss: 0.004996880888938904\n",
      "Loss: 0.0049352627247571945\n",
      "Loss: 0.005219404119998217\n",
      "Loss: 0.004851276520639658\n",
      "Loss: 0.005702598486095667\n",
      "Loss: 0.005831199698150158\n",
      "Loss: 0.0036901289131492376\n",
      "Loss: 0.004464743193238974\n",
      "Loss: 0.004653013776987791\n",
      "Loss: 0.005217512603849173\n",
      "Loss: 0.0055846902541816235\n",
      "Loss: 0.005592657718807459\n",
      "Loss: 0.005417052656412125\n",
      "Loss: 0.00555899553000927\n",
      "Loss: 0.004984760191291571\n",
      "Loss: 0.005544856656342745\n",
      "Loss: 0.005463591311126947\n",
      "Loss: 0.004971200600266457\n",
      "Loss: 0.004847342148423195\n",
      "Loss: 0.005147421732544899\n",
      "Loss: 0.004963776096701622\n",
      "Loss: 0.005243796389549971\n",
      "Loss: 0.0050642057321965694\n",
      "Loss: 0.00503452867269516\n",
      "Loss: 0.0055219801142811775\n",
      "Loss: 0.004978112410753965\n",
      "Loss: 0.005502910353243351\n",
      "Loss: 0.0054916623048484325\n",
      "Loss: 0.005331219173967838\n",
      "Loss: 0.004876845050603151\n",
      "Loss: 0.005110905971378088\n",
      "Loss: 0.005689897574484348\n",
      "Loss: 0.005095715168863535\n",
      "Loss: 0.005216010846197605\n",
      "Loss: 0.005728095769882202\n",
      "Loss: 0.005379220936447382\n",
      "Loss: 0.005755256395787001\n",
      "Loss: 0.004791244398802519\n",
      "Loss: 0.005040111485868692\n",
      "Loss: 0.005270265508443117\n",
      "Loss: 0.0058409543707966805\n",
      "Loss: 0.004800692200660706\n",
      "Loss: 0.005811990238726139\n",
      "Loss: 0.004904518835246563\n",
      "Loss: 0.0049413046799600124\n",
      "Loss: 0.004875057376921177\n",
      "Loss: 0.005593559239059687\n",
      "Loss: 0.005161188542842865\n",
      "Loss: 0.0051982165314257145\n",
      "Loss: 0.004559773486107588\n",
      "Loss: 0.005518848076462746\n",
      "Loss: 0.005268978886306286\n",
      "Loss: 0.00531725911423564\n",
      "Loss: 0.005097552668303251\n",
      "Loss: 0.00422873068600893\n",
      "Loss: 0.005313607398420572\n",
      "Loss: 0.004779507871717215\n",
      "Loss: 0.005290633533149958\n",
      "Loss: 0.005471918731927872\n",
      "Loss: 0.005362548399716616\n",
      "Loss: 0.004837104119360447\n",
      "Loss: 0.004991253837943077\n",
      "Loss: 0.0047743059694767\n",
      "Loss: 0.004660982172936201\n",
      "Loss: 0.005334081593900919\n",
      "Loss: 0.00569513812661171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 33/63 [50:06<42:56, 85.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.005379064474254847\n",
      "Epoch: 33\n",
      "Loss: 0.00571809709072113\n",
      "Loss: 0.005141153000295162\n",
      "Loss: 0.005026572849601507\n",
      "Loss: 0.005135276820510626\n",
      "Loss: 0.0051688808016479015\n",
      "Loss: 0.0045751468278467655\n",
      "Loss: 0.005006077233701944\n",
      "Loss: 0.005736897699534893\n",
      "Loss: 0.005023053381592035\n",
      "Loss: 0.004066750407218933\n",
      "Loss: 0.004322430118918419\n",
      "Loss: 0.005280329380184412\n",
      "Loss: 0.004994073882699013\n",
      "Loss: 0.005321219097822905\n",
      "Loss: 0.004729646723717451\n",
      "Loss: 0.005133177153766155\n",
      "Loss: 0.005247040186077356\n",
      "Loss: 0.0042411694303154945\n",
      "Loss: 0.004849196411669254\n",
      "Loss: 0.005437008570879698\n",
      "Loss: 0.004357051104307175\n",
      "Loss: 0.004985864274203777\n",
      "Loss: 0.005041327327489853\n",
      "Loss: 0.005584836937487125\n",
      "Loss: 0.0056697167456150055\n",
      "Loss: 0.005051381420344114\n",
      "Loss: 0.005355942528694868\n",
      "Loss: 0.004691125825047493\n",
      "Loss: 0.004338724538683891\n",
      "Loss: 0.004628604743629694\n",
      "Loss: 0.005422299727797508\n",
      "Loss: 0.0052449870854616165\n",
      "Loss: 0.0046610478311777115\n",
      "Loss: 0.004702609032392502\n",
      "Loss: 0.004929112270474434\n",
      "Loss: 0.005274072755128145\n",
      "Loss: 0.005095845088362694\n",
      "Loss: 0.004798725247383118\n",
      "Loss: 0.004827743861824274\n",
      "Loss: 0.005454967264086008\n",
      "Loss: 0.00471898727118969\n",
      "Loss: 0.004593807738274336\n",
      "Loss: 0.004585965070873499\n",
      "Loss: 0.004678519908338785\n",
      "Loss: 0.004877510480582714\n",
      "Loss: 0.004961212165653706\n",
      "Loss: 0.0050165592692792416\n",
      "Loss: 0.004874357488006353\n",
      "Loss: 0.005247261840850115\n",
      "Loss: 0.005417626816779375\n",
      "Loss: 0.004515078384429216\n",
      "Loss: 0.0050177257508039474\n",
      "Loss: 0.005749735049903393\n",
      "Loss: 0.005257959011942148\n",
      "Loss: 0.0048855082131922245\n",
      "Loss: 0.004141510464251041\n",
      "Loss: 0.005079667083919048\n",
      "Loss: 0.004602814093232155\n",
      "Loss: 0.004779409617185593\n",
      "Loss: 0.004702149890363216\n",
      "Loss: 0.005173646379262209\n",
      "Loss: 0.005144991911947727\n",
      "Loss: 0.0049973283894360065\n",
      "Loss: 0.0044370912946760654\n",
      "Loss: 0.004147261381149292\n",
      "Loss: 0.004834847059100866\n",
      "Loss: 0.005015627481043339\n",
      "Loss: 0.0050780498422682285\n",
      "Loss: 0.004812578205019236\n",
      "Loss: 0.005150420591235161\n",
      "Loss: 0.003989854361861944\n",
      "Loss: 0.004547950346022844\n",
      "Loss: 0.005287833046168089\n",
      "Loss: 0.004809166770428419\n",
      "Loss: 0.005159568507224321\n",
      "Loss: 0.005086786579340696\n",
      "Loss: 0.005493949167430401\n",
      "Loss: 0.005220375955104828\n",
      "Loss: 0.004802769515663385\n",
      "Loss: 0.004970209207385778\n",
      "Loss: 0.004703889600932598\n",
      "Loss: 0.004936201963573694\n",
      "Loss: 0.005303381010890007\n",
      "Loss: 0.004628689028322697\n",
      "Loss: 0.0045733461156487465\n",
      "Loss: 0.0051345727406442165\n",
      "Loss: 0.004992717877030373\n",
      "Loss: 0.005097603425383568\n",
      "Loss: 0.005047195125371218\n",
      "Loss: 0.005144388414919376\n",
      "Loss: 0.0050947219133377075\n",
      "Loss: 0.0051914993673563\n",
      "Loss: 0.00472436985000968\n",
      "Loss: 0.004700080957263708\n",
      "Loss: 0.00475404504686594\n",
      "Loss: 0.005250954534858465\n",
      "Loss: 0.005029685329645872\n",
      "Loss: 0.0051008351147174835\n",
      "Loss: 0.005152514670044184\n",
      "Loss: 0.004939649254083633\n",
      "Loss: 0.0052312761545181274\n",
      "Loss: 0.005263522733002901\n",
      "Loss: 0.00479608541354537\n",
      "Loss: 0.004893096629530191\n",
      "Loss: 0.004607030656188726\n",
      "Loss: 0.00495891273021698\n",
      "Loss: 0.005039700306952\n",
      "Loss: 0.004311625380069017\n",
      "Loss: 0.005141290370374918\n",
      "Loss: 0.004516440909355879\n",
      "Loss: 0.004647128749638796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 34/63 [51:31<41:28, 85.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00465767178684473\n",
      "Epoch: 34\n",
      "Loss: 0.004958061501383781\n",
      "Loss: 0.004674084950238466\n",
      "Loss: 0.004581011366099119\n",
      "Loss: 0.003306870348751545\n",
      "Loss: 0.004849646240472794\n",
      "Loss: 0.004800271242856979\n",
      "Loss: 0.004323248751461506\n",
      "Loss: 0.005322399083524942\n",
      "Loss: 0.004622918087989092\n",
      "Loss: 0.004738560877740383\n",
      "Loss: 0.004345727618783712\n",
      "Loss: 0.00499760964885354\n",
      "Loss: 0.005044226069003344\n",
      "Loss: 0.004893516656011343\n",
      "Loss: 0.004716850817203522\n",
      "Loss: 0.003977048210799694\n",
      "Loss: 0.005256818141788244\n",
      "Loss: 0.003832160960882902\n",
      "Loss: 0.005279924720525742\n",
      "Loss: 0.004866400267928839\n",
      "Loss: 0.004765526857227087\n",
      "Loss: 0.005147870164364576\n",
      "Loss: 0.005001673474907875\n",
      "Loss: 0.004832589067518711\n",
      "Loss: 0.00466322572901845\n",
      "Loss: 0.004523868672549725\n",
      "Loss: 0.004816890694200993\n",
      "Loss: 0.004310412798076868\n",
      "Loss: 0.004269538447260857\n",
      "Loss: 0.005032999906688929\n",
      "Loss: 0.004573204554617405\n",
      "Loss: 0.004696910735219717\n",
      "Loss: 0.004236482549458742\n",
      "Loss: 0.004640684463083744\n",
      "Loss: 0.005204983055591583\n",
      "Loss: 0.004743373021483421\n",
      "Loss: 0.004873496014624834\n",
      "Loss: 0.005194797646254301\n",
      "Loss: 0.00458918372169137\n",
      "Loss: 0.005109162535518408\n",
      "Loss: 0.004526891279965639\n",
      "Loss: 0.004474518820643425\n",
      "Loss: 0.004739823285490274\n",
      "Loss: 0.004771498963236809\n",
      "Loss: 0.003964823670685291\n",
      "Loss: 0.005071931052953005\n",
      "Loss: 0.0045083919540047646\n",
      "Loss: 0.004812912549823523\n",
      "Loss: 0.004473496228456497\n",
      "Loss: 0.004778494127094746\n",
      "Loss: 0.004717695992439985\n",
      "Loss: 0.004936117213219404\n",
      "Loss: 0.004724589176476002\n",
      "Loss: 0.0047802370972931385\n",
      "Loss: 0.004555927123874426\n",
      "Loss: 0.004033870529383421\n",
      "Loss: 0.004726696293801069\n",
      "Loss: 0.004516450222581625\n",
      "Loss: 0.00421992177143693\n",
      "Loss: 0.004247451201081276\n",
      "Loss: 0.004905284382402897\n",
      "Loss: 0.004359513986855745\n",
      "Loss: 0.003970721270889044\n",
      "Loss: 0.004169337917119265\n",
      "Loss: 0.004005141090601683\n",
      "Loss: 0.004381824284791946\n",
      "Loss: 0.005045609083026648\n",
      "Loss: 0.004379613790661097\n",
      "Loss: 0.004580677021294832\n",
      "Loss: 0.004560127388685942\n",
      "Loss: 0.004459873773157597\n",
      "Loss: 0.003951233811676502\n",
      "Loss: 0.004774227738380432\n",
      "Loss: 0.004797523375600576\n",
      "Loss: 0.004728252999484539\n",
      "Loss: 0.004889169242233038\n",
      "Loss: 0.004445882514119148\n",
      "Loss: 0.004706654231995344\n",
      "Loss: 0.004258892964571714\n",
      "Loss: 0.0047550811432302\n",
      "Loss: 0.004760255105793476\n",
      "Loss: 0.005059285555034876\n",
      "Loss: 0.004259829875081778\n",
      "Loss: 0.004853927530348301\n",
      "Loss: 0.004321685526520014\n",
      "Loss: 0.004743475001305342\n",
      "Loss: 0.004641969688236713\n",
      "Loss: 0.004613334778696299\n",
      "Loss: 0.0050409575924277306\n",
      "Loss: 0.004439413547515869\n",
      "Loss: 0.004917127545922995\n",
      "Loss: 0.00441314373165369\n",
      "Loss: 0.004548769909888506\n",
      "Loss: 0.004793897736817598\n",
      "Loss: 0.004262915812432766\n",
      "Loss: 0.004684373736381531\n",
      "Loss: 0.0035352036356925964\n",
      "Loss: 0.004124870989471674\n",
      "Loss: 0.004216124303638935\n",
      "Loss: 0.005063211545348167\n",
      "Loss: 0.004930227063596249\n",
      "Loss: 0.0038752311374992132\n",
      "Loss: 0.004587528761476278\n",
      "Loss: 0.004439238924533129\n",
      "Loss: 0.004192422144114971\n",
      "Loss: 0.004659805912524462\n",
      "Loss: 0.004395602270960808\n",
      "Loss: 0.00499901594594121\n",
      "Loss: 0.004570646211504936\n",
      "Loss: 0.0038219934795051813\n",
      "Loss: 0.0047446247190237045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 35/63 [52:57<39:58, 85.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.004499682690948248\n",
      "Epoch: 35\n",
      "Loss: 0.004260554909706116\n",
      "Loss: 0.005047881510108709\n",
      "Loss: 0.004894553683698177\n",
      "Loss: 0.004205360077321529\n",
      "Loss: 0.004585633985698223\n",
      "Loss: 0.004442140460014343\n",
      "Loss: 0.0043786694295704365\n",
      "Loss: 0.004329943098127842\n",
      "Loss: 0.004919562488794327\n",
      "Loss: 0.004537166561931372\n",
      "Loss: 0.004733712412416935\n",
      "Loss: 0.004618390463292599\n",
      "Loss: 0.004076754674315453\n",
      "Loss: 0.004036524798721075\n",
      "Loss: 0.004079767968505621\n",
      "Loss: 0.004131869878619909\n",
      "Loss: 0.00406057620421052\n",
      "Loss: 0.004889525938779116\n",
      "Loss: 0.00451731076464057\n",
      "Loss: 0.00491584837436676\n",
      "Loss: 0.003651977051049471\n",
      "Loss: 0.004473655018955469\n",
      "Loss: 0.004529544152319431\n",
      "Loss: 0.004384113010019064\n",
      "Loss: 0.0044244397431612015\n",
      "Loss: 0.004250401630997658\n",
      "Loss: 0.004214770160615444\n",
      "Loss: 0.004210531711578369\n",
      "Loss: 0.004566296935081482\n",
      "Loss: 0.0042779576033353806\n",
      "Loss: 0.004512956365942955\n",
      "Loss: 0.004039762541651726\n",
      "Loss: 0.004360150080174208\n",
      "Loss: 0.004402537830173969\n",
      "Loss: 0.004133526235818863\n",
      "Loss: 0.0049371966160833836\n",
      "Loss: 0.0044659776613116264\n",
      "Loss: 0.004083080682903528\n",
      "Loss: 0.004570827819406986\n",
      "Loss: 0.0048155211843550205\n",
      "Loss: 0.00431879423558712\n",
      "Loss: 0.004227190278470516\n",
      "Loss: 0.00431996863335371\n",
      "Loss: 0.00377678289078176\n",
      "Loss: 0.00419100746512413\n",
      "Loss: 0.004360011778771877\n",
      "Loss: 0.004497884307056665\n",
      "Loss: 0.004407016560435295\n",
      "Loss: 0.004307402763515711\n",
      "Loss: 0.004454341251403093\n",
      "Loss: 0.0038612675853073597\n",
      "Loss: 0.004421057179570198\n",
      "Loss: 0.0044748177751898766\n",
      "Loss: 0.0047339084558188915\n",
      "Loss: 0.004057582002133131\n",
      "Loss: 0.0039041507989168167\n",
      "Loss: 0.00396078173071146\n",
      "Loss: 0.004776810761541128\n",
      "Loss: 0.004439014010131359\n",
      "Loss: 0.004313527140766382\n",
      "Loss: 0.0034285990986973047\n",
      "Loss: 0.004173801746219397\n",
      "Loss: 0.0034570274874567986\n",
      "Loss: 0.004051359370350838\n",
      "Loss: 0.004241323098540306\n",
      "Loss: 0.004717937204986811\n",
      "Loss: 0.004006609320640564\n",
      "Loss: 0.004634140525013208\n",
      "Loss: 0.0031534640584141016\n",
      "Loss: 0.0038588352035731077\n",
      "Loss: 0.004617433063685894\n",
      "Loss: 0.00437964778393507\n",
      "Loss: 0.004382941871881485\n",
      "Loss: 0.004435860086232424\n",
      "Loss: 0.004492594860494137\n",
      "Loss: 0.004437361378222704\n",
      "Loss: 0.004204867873340845\n",
      "Loss: 0.004167894832789898\n",
      "Loss: 0.0043899063020944595\n",
      "Loss: 0.004648481961339712\n",
      "Loss: 0.0041130706667900085\n",
      "Loss: 0.0041251033544540405\n",
      "Loss: 0.004634662065654993\n",
      "Loss: 0.004092903342097998\n",
      "Loss: 0.004232961684465408\n",
      "Loss: 0.0036374868359416723\n",
      "Loss: 0.004132424481213093\n",
      "Loss: 0.004207922611385584\n",
      "Loss: 0.004285529721528292\n",
      "Loss: 0.0041639311239123344\n",
      "Loss: 0.004151018802076578\n",
      "Loss: 0.003935492131859064\n",
      "Loss: 0.004363087471574545\n",
      "Loss: 0.003967832773923874\n",
      "Loss: 0.004077386576682329\n",
      "Loss: 0.004586155526340008\n",
      "Loss: 0.004102372098714113\n",
      "Loss: 0.004660372156649828\n",
      "Loss: 0.004354120697826147\n",
      "Loss: 0.004260181449353695\n",
      "Loss: 0.003991445992141962\n",
      "Loss: 0.003936497028917074\n",
      "Loss: 0.004043084569275379\n",
      "Loss: 0.004076563753187656\n",
      "Loss: 0.003966440446674824\n",
      "Loss: 0.004289553966373205\n",
      "Loss: 0.004296229220926762\n",
      "Loss: 0.004523187410086393\n",
      "Loss: 0.004230352584272623\n",
      "Loss: 0.004023751709610224\n",
      "Loss: 0.0038581225089728832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 36/63 [54:22<38:32, 85.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00410237442702055\n",
      "Epoch: 36\n",
      "Loss: 0.00452096201479435\n",
      "Loss: 0.0034597250632941723\n",
      "Loss: 0.003825114807114005\n",
      "Loss: 0.003963356837630272\n",
      "Loss: 0.004310947377234697\n",
      "Loss: 0.004219316877424717\n",
      "Loss: 0.003633408574387431\n",
      "Loss: 0.004503904841840267\n",
      "Loss: 0.004061429761350155\n",
      "Loss: 0.00446238974109292\n",
      "Loss: 0.0035591530613601208\n",
      "Loss: 0.003351020161062479\n",
      "Loss: 0.004251095000654459\n",
      "Loss: 0.004259487614035606\n",
      "Loss: 0.004046222195029259\n",
      "Loss: 0.00418640673160553\n",
      "Loss: 0.004506194498389959\n",
      "Loss: 0.00407941360026598\n",
      "Loss: 0.004390431102365255\n",
      "Loss: 0.0038542815018445253\n",
      "Loss: 0.0038092806935310364\n",
      "Loss: 0.0043180217035114765\n",
      "Loss: 0.003654640167951584\n",
      "Loss: 0.004448933061212301\n",
      "Loss: 0.004504098556935787\n",
      "Loss: 0.0039970665238797665\n",
      "Loss: 0.004427514038980007\n",
      "Loss: 0.004592739511281252\n",
      "Loss: 0.0041933790780603886\n",
      "Loss: 0.0036123222671449184\n",
      "Loss: 0.004181307274848223\n",
      "Loss: 0.00380883296020329\n",
      "Loss: 0.00421229237690568\n",
      "Loss: 0.004082527942955494\n",
      "Loss: 0.004129314795136452\n",
      "Loss: 0.003483388340100646\n",
      "Loss: 0.004141321871429682\n",
      "Loss: 0.0037001664750277996\n",
      "Loss: 0.0037944004870951176\n",
      "Loss: 0.004201320465654135\n",
      "Loss: 0.004260473884642124\n",
      "Loss: 0.00404766108840704\n",
      "Loss: 0.004108623135834932\n",
      "Loss: 0.004280728753656149\n",
      "Loss: 0.004344949033111334\n",
      "Loss: 0.004044987726956606\n",
      "Loss: 0.0038524032570421696\n",
      "Loss: 0.0038447610568255186\n",
      "Loss: 0.0035418602637946606\n",
      "Loss: 0.0035725035704672337\n",
      "Loss: 0.0037167230620980263\n",
      "Loss: 0.003917349968105555\n",
      "Loss: 0.0037310856860131025\n",
      "Loss: 0.004109373316168785\n",
      "Loss: 0.003943152260035276\n",
      "Loss: 0.0034206039272248745\n",
      "Loss: 0.00375807611271739\n",
      "Loss: 0.004162333905696869\n",
      "Loss: 0.004007164854556322\n",
      "Loss: 0.0038581020198762417\n",
      "Loss: 0.0039057957474142313\n",
      "Loss: 0.004192047752439976\n",
      "Loss: 0.004263771697878838\n",
      "Loss: 0.003999999724328518\n",
      "Loss: 0.003917239606380463\n",
      "Loss: 0.003994116559624672\n",
      "Loss: 0.004352096002548933\n",
      "Loss: 0.00409817835316062\n",
      "Loss: 0.0041041066870093346\n",
      "Loss: 0.0040906756184995174\n",
      "Loss: 0.0043254755437374115\n",
      "Loss: 0.004365226719528437\n",
      "Loss: 0.0036365713458508253\n",
      "Loss: 0.004449804779142141\n",
      "Loss: 0.0030410457402467728\n",
      "Loss: 0.003494104603305459\n",
      "Loss: 0.003707633586600423\n",
      "Loss: 0.0044202376157045364\n",
      "Loss: 0.003860135329887271\n",
      "Loss: 0.003092898288741708\n",
      "Loss: 0.004236551932990551\n",
      "Loss: 0.003753177123144269\n",
      "Loss: 0.004091392736881971\n",
      "Loss: 0.004121475387364626\n",
      "Loss: 0.004296812228858471\n",
      "Loss: 0.003624004079028964\n",
      "Loss: 0.004198864568024874\n",
      "Loss: 0.004443360026925802\n",
      "Loss: 0.0042053647339344025\n",
      "Loss: 0.0041093844920396805\n",
      "Loss: 0.004107533022761345\n",
      "Loss: 0.0039270780980587006\n",
      "Loss: 0.003607084508985281\n",
      "Loss: 0.004005596973001957\n",
      "Loss: 0.0040337215177714825\n",
      "Loss: 0.003912667743861675\n",
      "Loss: 0.004147552419453859\n",
      "Loss: 0.0041345395147800446\n",
      "Loss: 0.004040284547954798\n",
      "Loss: 0.004064416512846947\n",
      "Loss: 0.003877114737406373\n",
      "Loss: 0.003668310819193721\n",
      "Loss: 0.003752458607777953\n",
      "Loss: 0.004098905716091394\n",
      "Loss: 0.003947515971958637\n",
      "Loss: 0.0041358149610459805\n",
      "Loss: 0.004217174369841814\n",
      "Loss: 0.003343795193359256\n",
      "Loss: 0.0035715752746909857\n",
      "Loss: 0.003693535691127181\n",
      "Loss: 0.003676558146253228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 37/63 [55:48<37:07, 85.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.004054788034409285\n",
      "Epoch: 37\n",
      "Loss: 0.003783607389777899\n",
      "Loss: 0.0043219467625021935\n",
      "Loss: 0.0039448970928788185\n",
      "Loss: 0.00403546541929245\n",
      "Loss: 0.00388548756018281\n",
      "Loss: 0.003538481192663312\n",
      "Loss: 0.004178229719400406\n",
      "Loss: 0.0036887347232550383\n",
      "Loss: 0.003763400949537754\n",
      "Loss: 0.003669707803055644\n",
      "Loss: 0.0037542302161455154\n",
      "Loss: 0.0037650016602128744\n",
      "Loss: 0.004197491332888603\n",
      "Loss: 0.0038907057605683804\n",
      "Loss: 0.0041457610204815865\n",
      "Loss: 0.0033116743434220552\n",
      "Loss: 0.003944652155041695\n",
      "Loss: 0.003930964972823858\n",
      "Loss: 0.0038465627003461123\n",
      "Loss: 0.0037139744963496923\n",
      "Loss: 0.0037903764750808477\n",
      "Loss: 0.0038678618147969246\n",
      "Loss: 0.0036427790764719248\n",
      "Loss: 0.003947990480810404\n",
      "Loss: 0.0038861373905092478\n",
      "Loss: 0.0033709537237882614\n",
      "Loss: 0.0038023781962692738\n",
      "Loss: 0.00435030460357666\n",
      "Loss: 0.003934130072593689\n",
      "Loss: 0.003871927037835121\n",
      "Loss: 0.00411118334159255\n",
      "Loss: 0.004356606397777796\n",
      "Loss: 0.004028238356113434\n",
      "Loss: 0.003936095163226128\n",
      "Loss: 0.00383955892175436\n",
      "Loss: 0.003769902279600501\n",
      "Loss: 0.00403642887249589\n",
      "Loss: 0.00376408314332366\n",
      "Loss: 0.0038781026378273964\n",
      "Loss: 0.0036709916312247515\n",
      "Loss: 0.003719865810126066\n",
      "Loss: 0.003910355735570192\n",
      "Loss: 0.0037200849037617445\n",
      "Loss: 0.00382575998082757\n",
      "Loss: 0.0035735969431698322\n",
      "Loss: 0.0036021575797349215\n",
      "Loss: 0.0030510961078107357\n",
      "Loss: 0.003508743131533265\n",
      "Loss: 0.003880030009895563\n",
      "Loss: 0.0032994146458804607\n",
      "Loss: 0.0037595084868371487\n",
      "Loss: 0.0033599792513996363\n",
      "Loss: 0.003734414931386709\n",
      "Loss: 0.0037329508922994137\n",
      "Loss: 0.0033918446861207485\n",
      "Loss: 0.0037954626604914665\n",
      "Loss: 0.004065909888595343\n",
      "Loss: 0.0038966687861829996\n",
      "Loss: 0.0036332744639366865\n",
      "Loss: 0.004085619933903217\n",
      "Loss: 0.003922055941075087\n",
      "Loss: 0.0035142574924975634\n",
      "Loss: 0.003942015580832958\n",
      "Loss: 0.003766728797927499\n",
      "Loss: 0.003629824845120311\n",
      "Loss: 0.00386166968382895\n",
      "Loss: 0.0037788369227200747\n",
      "Loss: 0.003331082873046398\n",
      "Loss: 0.0037255994975566864\n",
      "Loss: 0.0036882665008306503\n",
      "Loss: 0.003851421643048525\n",
      "Loss: 0.003611969295889139\n",
      "Loss: 0.003933482337743044\n",
      "Loss: 0.004006409551948309\n",
      "Loss: 0.004116395488381386\n",
      "Loss: 0.0031679922249168158\n",
      "Loss: 0.003539825789630413\n",
      "Loss: 0.003799572354182601\n",
      "Loss: 0.0033726526889950037\n",
      "Loss: 0.003892621025443077\n",
      "Loss: 0.003837341908365488\n",
      "Loss: 0.003975249361246824\n",
      "Loss: 0.0032479949295520782\n",
      "Loss: 0.0031765541061758995\n",
      "Loss: 0.0038267243653535843\n",
      "Loss: 0.0033870558254420757\n",
      "Loss: 0.0037142825312912464\n",
      "Loss: 0.0031628720462322235\n",
      "Loss: 0.0038446069229394197\n",
      "Loss: 0.003820669138804078\n",
      "Loss: 0.003967911936342716\n",
      "Loss: 0.003487147856503725\n",
      "Loss: 0.0034200905356556177\n",
      "Loss: 0.003007761435583234\n",
      "Loss: 0.003630139632150531\n",
      "Loss: 0.003151671728119254\n",
      "Loss: 0.003403177484869957\n",
      "Loss: 0.004099716432392597\n",
      "Loss: 0.003341975621879101\n",
      "Loss: 0.0037854393012821674\n",
      "Loss: 0.0031078278552740812\n",
      "Loss: 0.004009576980024576\n",
      "Loss: 0.003714620601385832\n",
      "Loss: 0.0034828796051442623\n",
      "Loss: 0.0036083783488720655\n",
      "Loss: 0.0037349483463913202\n",
      "Loss: 0.0036218969617038965\n",
      "Loss: 0.003551381640136242\n",
      "Loss: 0.0033623953349888325\n",
      "Loss: 0.0038912699092179537\n",
      "Loss: 0.003564073471352458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 38/63 [57:13<35:40, 85.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0036500399000942707\n",
      "Epoch: 38\n",
      "Loss: 0.0032974667847156525\n",
      "Loss: 0.00351149495691061\n",
      "Loss: 0.003576422343030572\n",
      "Loss: 0.003192534204572439\n",
      "Loss: 0.0032173548825085163\n",
      "Loss: 0.003323058830574155\n",
      "Loss: 0.003554008435457945\n",
      "Loss: 0.0032807718962430954\n",
      "Loss: 0.003897932358086109\n",
      "Loss: 0.003982601221650839\n",
      "Loss: 0.0033284074161201715\n",
      "Loss: 0.0034381404984742403\n",
      "Loss: 0.003555101342499256\n",
      "Loss: 0.0032632576767355204\n",
      "Loss: 0.003636819077655673\n",
      "Loss: 0.0037445526104420424\n",
      "Loss: 0.003384140320122242\n",
      "Loss: 0.003591779386624694\n",
      "Loss: 0.003694473300129175\n",
      "Loss: 0.0035894305910915136\n",
      "Loss: 0.003627473721280694\n",
      "Loss: 0.004080089274793863\n",
      "Loss: 0.003670085221529007\n",
      "Loss: 0.0035417319741100073\n",
      "Loss: 0.00361427990719676\n",
      "Loss: 0.003790815593674779\n",
      "Loss: 0.00385589268989861\n",
      "Loss: 0.003979514818638563\n",
      "Loss: 0.0038763885386288166\n",
      "Loss: 0.003944682888686657\n",
      "Loss: 0.003714875550940633\n",
      "Loss: 0.0034090434201061726\n",
      "Loss: 0.003932778723537922\n",
      "Loss: 0.0038157415110617876\n",
      "Loss: 0.0037060086615383625\n",
      "Loss: 0.003507383167743683\n",
      "Loss: 0.0029121777042746544\n",
      "Loss: 0.0031817290000617504\n",
      "Loss: 0.0037075963336974382\n",
      "Loss: 0.0033580295275896788\n",
      "Loss: 0.003377219196408987\n",
      "Loss: 0.003756845137104392\n",
      "Loss: 0.0034947069361805916\n",
      "Loss: 0.0033634512219578028\n",
      "Loss: 0.0036506818141788244\n",
      "Loss: 0.003357835114002228\n",
      "Loss: 0.0034630773589015007\n",
      "Loss: 0.0036795581690967083\n",
      "Loss: 0.00351788685657084\n",
      "Loss: 0.0036101918667554855\n",
      "Loss: 0.003560128388926387\n",
      "Loss: 0.003680154914036393\n",
      "Loss: 0.003382320050150156\n",
      "Loss: 0.0032089317683130503\n",
      "Loss: 0.0034268107265233994\n",
      "Loss: 0.003338883863762021\n",
      "Loss: 0.00368671421892941\n",
      "Loss: 0.0038049002178013325\n",
      "Loss: 0.0036127204075455666\n",
      "Loss: 0.0034961437340825796\n",
      "Loss: 0.0030142951291054487\n",
      "Loss: 0.0036471975035965443\n",
      "Loss: 0.0038980289828032255\n",
      "Loss: 0.0033256744500249624\n",
      "Loss: 0.0034349681809544563\n",
      "Loss: 0.0035565164871513844\n",
      "Loss: 0.003387911943718791\n",
      "Loss: 0.0035072967875748873\n",
      "Loss: 0.003515043994411826\n",
      "Loss: 0.0034456730354577303\n",
      "Loss: 0.003611901542171836\n",
      "Loss: 0.0035221928264945745\n",
      "Loss: 0.0032213754020631313\n",
      "Loss: 0.0033044733572751284\n",
      "Loss: 0.002965476829558611\n",
      "Loss: 0.0033690761774778366\n",
      "Loss: 0.0036812631879001856\n",
      "Loss: 0.0029041687957942486\n",
      "Loss: 0.003258603624999523\n",
      "Loss: 0.002988590160384774\n",
      "Loss: 0.0036545819602906704\n",
      "Loss: 0.0031531229615211487\n",
      "Loss: 0.003165258327499032\n",
      "Loss: 0.0035118917003273964\n",
      "Loss: 0.003625753801316023\n",
      "Loss: 0.0035503965336829424\n",
      "Loss: 0.0036524427123367786\n",
      "Loss: 0.0037674361374229193\n",
      "Loss: 0.0036511553917080164\n",
      "Loss: 0.00358743779361248\n",
      "Loss: 0.003294596914201975\n",
      "Loss: 0.00313783367164433\n",
      "Loss: 0.002405635081231594\n",
      "Loss: 0.0033774026669561863\n",
      "Loss: 0.003587250364944339\n",
      "Loss: 0.003514564596116543\n",
      "Loss: 0.0033227158710360527\n",
      "Loss: 0.0034512316342443228\n",
      "Loss: 0.0030980566516518593\n",
      "Loss: 0.00372859719209373\n",
      "Loss: 0.003276689676567912\n",
      "Loss: 0.003372451988980174\n",
      "Loss: 0.0033712233416736126\n",
      "Loss: 0.0032248729839920998\n",
      "Loss: 0.0035161159466952085\n",
      "Loss: 0.0032462356612086296\n",
      "Loss: 0.0036204084753990173\n",
      "Loss: 0.00328269531019032\n",
      "Loss: 0.0031907067168504\n",
      "Loss: 0.0036285181995481253\n",
      "Loss: 0.003320809220895171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 39/63 [58:39<34:14, 85.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0035262166056782007\n",
      "Epoch: 39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94019331c5e4c5fbe4b5808f13d2882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0030344182159751654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['Size, Weight, Sweetness, Crunchiness, Juiciness, Ripeness', 'age, height, weight, waistline, SBP, BLDS, tot_c']\n",
      "Loss: 0.003158820327371359\n",
      "Predictions: ['Rainfall <= 0.1, Pressure3pm <= 1009.6', 'policy_tenure, age_of_car, age_of_policyholder, air']\n",
      "Loss: 0.0033468373585492373\n",
      "Predictions: ['type_of_meal_plan, room_type_reserved, required_', '4']\n",
      "Loss: 0.0037618172354996204\n",
      "Predictions: ['target', 'LeaveOrNot']\n",
      "Loss: 0.0031849753577262163\n",
      "Predictions: ['Alcohol, Malic acid, Ash, Alcalinity of ash, Total phenol', 'Species']\n",
      "Loss: 0.0032042828388512135\n",
      "Predictions: ['Ia, Ib, Ic, Va, Vb, Vc', 'Alcohol, Malic acid, Ash, Alcalinity of ash, Total phenol']\n",
      "Loss: 0.0032445318065583706\n",
      "Predictions: ['ssc_p, hsc_p, degree_p, etest_p', 'displacement <= 1196.5, height <= 1519.0']\n",
      "Loss: 0.00378926913253963\n",
      "Predictions: ['9', 'diagnosis']\n",
      "Loss: 0.0037462664768099785\n",
      "Predictions: ['booking_status', 'class']\n",
      "Loss: 0.003209817921742797\n",
      "Predictions: ['age, cp, trestbps, chol, restecg, thalach, old', 'ph, Hardness, Chloramines, Sulfate, Conductivity, Trihal']\n",
      "Loss: 0.0028482277411967516\n",
      "Predictions: ['policy_tenure, age_of_car, age_of_policyholder, air', 'age, height, weight, waistline, SBP, BLDS, tot_c']\n",
      "Loss: 0.0035219364799559116\n",
      "Predictions: ['Area <= 39172.5, AspectRation <= 1', 'variance, skewness, curtosis, entropy']\n",
      "Loss: 0.0035105645656585693\n",
      "Predictions: ['is_claim', 'blue, dual_sim, four_g, three_g, touch_screen, wifi']\n",
      "Loss: 0.0032523730769753456\n",
      "Predictions: ['Alkphos <= 211.5, Sgot <= 26.', 'perimeter_mean <= 90.47, texture_worst <= 27.']\n",
      "Loss: 0.0032636174000799656\n",
      "Predictions: ['duration, credit_amount, installment_commitment, residence_since, age, existing_', 'PetalWidthCm <= 0.7, PetalWidthCm <= 1']\n",
      "Loss: 0.0032819476909935474\n",
      "Predictions: ['BMI <= 29.85, Age <= 27.5', 'ph, Hardness, Chloramines, Sulfate, Conductivity, Trihal']\n",
      "Loss: 0.003480561077594757\n",
      "Predictions: ['duration, credit_amount, installment_commitment, residence_since, age, existing_', 'Selector']\n",
      "Loss: 0.0034966443199664354\n",
      "Predictions: ['Height <= 0.13, Diameter <= 0.45', '7']\n",
      "Loss: 0.0032279915176331997\n",
      "Predictions: ['Age, Work_Experience, Family_Size', 'Pregnancies, Glucose, BloodPressure, SkinThickness, In']\n",
      "Loss: 0.003503406886011362\n",
      "Predictions: ['class', 'Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight']\n",
      "Loss: 0.0034070145338773727\n",
      "Predictions: ['fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free', 'RainTomorrow']\n",
      "Loss: 0.0033823882695287466\n",
      "Predictions: ['Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque', '10']\n",
      "Loss: 0.003342796117067337\n",
      "Predictions: ['5', 'checking_status, employment, other_parties, other_payment_plans, housing']\n",
      "Loss: 0.0028606995474547148\n",
      "Predictions: ['COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY', '4']\n",
      "Loss: 0.0034360806457698345\n",
      "Predictions: ['price_range', 'lead_time <= 151.5, no_of_special_requests']\n",
      "Loss: 0.0035160116385668516\n",
      "Predictions: ['9', 'Ia, Ib, Ic, Va, Vb, Vc']\n",
      "Loss: 0.0036903435830026865\n",
      "Predictions: ['8', 'income']\n",
      "Loss: 0.0031733352225273848\n",
      "Predictions: ['Age, pH, Specific Gravity', 'battery_power, fc, int_memory, mobile_wt, n_cores']\n",
      "Loss: 0.003029680112376809\n",
      "Predictions: ['Pregnancies, Glucose, BloodPressure, SkinThickness, In', 'Rotational speed [rpm] <= 1381.5, Torque [']\n",
      "Loss: 0.0033821703400462866\n",
      "Predictions: ['Age, Work_Experience, Family_Size', 'Age, Height, Weight, FCVC, NCP, CH2O, FAF,']\n",
      "Loss: 0.0034116539172828197\n",
      "Predictions: ['Class', 'Age, TB, DB, Alkphos, Sgpt, Sgot, TP']\n",
      "Loss: 0.0031156723853200674\n",
      "Predictions: ['hours-per-week <= 41.5, capital-loss <= 18', 'Pregnancies, Glucose, BloodPressure, SkinThickness, In']\n",
      "Loss: 0.0027717554476112127\n",
      "Predictions: ['Rainfall, WindGustDir, WindDir9am, WindDir3pm, Wind', 'no_of_adults, no_of_children, no_of_weekend']\n",
      "Loss: 0.0036886478774249554\n",
      "Predictions: ['quality', 'Exited']\n",
      "Loss: 0.003373892977833748\n",
      "Predictions: ['Total phenols <= 2.36, Proanthocyanins <=', '8']\n",
      "Loss: 0.003228998277336359\n",
      "Predictions: ['Geography, Gender, HasCrCard, IsActiveMember', 'policy_tenure, age_of_car, age_of_policyholder, air']\n",
      "Loss: 0.0030248290859162807\n",
      "Predictions: ['ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount', 'Area, Perimeter, MinorAxisLength, AspectRation, Eccentricity,']\n",
      "Loss: 0.003379193367436528\n",
      "Predictions: ['Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight', 'DRK_YN']\n",
      "Loss: 0.0032924916595220566\n",
      "Predictions: ['SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet', 'USMER, SEX, PATIENT_TYPE']\n",
      "Loss: 0.0031064071226865053\n",
      "Predictions: ['anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem', 'Ever_Married, Graduated, Profession, Work_Experience, Family']\n",
      "Loss: 0.003363397903740406\n",
      "Predictions: ['7', 'Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight']\n",
      "Loss: 0.0031573011074215174\n",
      "Predictions: ['Rainfall, WindSpeed9am, Pressure9am, Pressure3pm, Cloud9am', 'age, cp, trestbps, chol, restecg, thalach, old']\n",
      "Loss: 0.0033017918467521667\n",
      "Predictions: ['Output', 'Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior']\n",
      "Loss: 0.003216598415747285\n",
      "Predictions: ['battery_power, fc, int_memory, mobile_wt, n_cores', '4']\n",
      "Loss: 0.003426868235692382\n",
      "Predictions: ['12', 'skewness <= 5.16, curtosis <= 0.1']\n",
      "Loss: 0.0034039125312119722\n",
      "Predictions: ['ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount', '7']\n",
      "Loss: 0.0030790288001298904\n",
      "Predictions: ['3', 'age, height, weight, waistline, SBP, BLDS, tot_c']\n",
      "Loss: 0.0033783812541514635\n",
      "Predictions: ['10', 'age, fnlwgt, educational-num, capital-gain, capital-loss, hours']\n",
      "Loss: 0.0027679584454745054\n",
      "Predictions: ['no_of_adults, no_of_children, no_of_weekend', 'ph, Hardness, Chloramines, Sulfate, Conductivity, Trihal']\n",
      "Loss: 0.0033535482361912727\n",
      "Predictions: ['Age <= 0.1, pH <= 5.5', 'density <= 1.0, chlorides <= 0.08']\n",
      "Loss: 0.0034060082398355007\n",
      "Predictions: ['Quality', 'age, cp, trestbps, chol, restecg, thalach, old']\n",
      "Loss: 0.0030439007095992565\n",
      "Predictions: ['Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior', 'CARDIOVASCULAR <= 50.0, ASHTMA <= 1']\n",
      "Loss: 0.0037666335701942444\n",
      "Predictions: ['AG_Ratio', 'NObeyesdad']\n",
      "Loss: 0.003270818619057536\n",
      "Predictions: ['slope <= 1.5, restecg <= 0.5', 'CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary']\n",
      "Loss: 0.0033750494476407766\n",
      "Predictions: ['Type, TWF, HDF, PWF, OSF, RNF', 'Age, Work_Experience, Family_Size']\n",
      "Loss: 0.002714699599891901\n",
      "Predictions: ['no_of_adults, no_of_children, no_of_weekend', 'Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque']\n",
      "Loss: 0.003331646090373397\n",
      "Predictions: ['JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain', 'variance, skewness, curtosis, entropy']\n",
      "Loss: 0.003386084223166108\n",
      "Predictions: ['CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary', 'ReachedOnTime']\n",
      "Loss: 0.0026429062709212303\n",
      "Predictions: ['SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet', 'COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY']\n",
      "Loss: 0.0031830507796257734\n",
      "Predictions: ['Area, Perimeter, MinorAxisLength, AspectRation, Eccentricity,', 'ph, Sulfate, Trihalomethanes']\n",
      "Loss: 0.003418576903641224\n",
      "Predictions: ['Potability', 'ra, dec, run, camcol, field, redshift, plate, mjd']\n",
      "Loss: 0.0033877890091389418\n",
      "Predictions: ['Rainfall, WindSpeed9am, Pressure9am, Pressure3pm, Cloud9am', 'status']\n",
      "Loss: 0.003182532498613\n",
      "Predictions: ['Hardness <= 278.29, Chloramines <= 6.', 'Age, Height, Weight, FCVC, NCP, CH2O, FAF,']\n",
      "Loss: 0.002630196511745453\n",
      "Predictions: ['Alcohol, Malic acid, Ash, Alcalinity of ash, Total phenol', 'texture_mean, perimeter_mean, texture_se, perimeter_se, area_se']\n",
      "Loss: 0.003530194517225027\n",
      "Predictions: ['Loan_Status', '6']\n",
      "Loss: 0.003021895419806242\n",
      "Predictions: ['Age, Height, Weight, FCVC, NCP, CH2O, FAF,', 'area_cluster, segment, model, fuel_type, max_torque, max_']\n",
      "Loss: 0.003362421877682209\n",
      "Predictions: ['sex, fbs, exang', 'int_memory <= 30.5, mobile_wt <= 91.']\n",
      "Loss: 0.003343690652400255\n",
      "Predictions: ['5', 'duration, credit_amount, installment_commitment, residence_since, age, existing_']\n",
      "Loss: 0.003050939179956913\n",
      "Predictions: ['fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free', 'Profession, Spending_Score, Var_1, Gender, Ever_Married,']\n",
      "Loss: 0.003166625974699855\n",
      "Predictions: ['basic_needs <= 3.5, bullying <= 1.5', 'ssc_p <= 60.09, hsc_p <= ']\n",
      "Loss: 0.0030861198902130127\n",
      "Predictions: ['Loan_Amount_Term <= 420.0, ApplicantIncome <=', 'dec <= 22.21, mjd <= 55090']\n",
      "Loss: 0.002867084462195635\n",
      "Predictions: ['battery_power, fc, int_memory, mobile_wt, n_cores', 'ssc_p, hsc_p, degree_p, etest_p']\n",
      "Loss: 0.0033143579494208097\n",
      "Predictions: ['Dependents, Property_Area, Gender, Married, Education, Self_Empl', '11']\n",
      "Loss: 0.003375026863068342\n",
      "Predictions: ['Prior_purchases <= 3.5, Customer_care_calls <= 4.', '11']\n",
      "Loss: 0.003472119104117155\n",
      "Predictions: ['sex, hear_left, hear_right', 'CLASSIFICATION']\n",
      "Loss: 0.003453172743320465\n",
      "Predictions: ['9', 'Education, City, Gender, EverBenched']\n",
      "Loss: 0.0031138805206865072\n",
      "Predictions: ['Warehouse_block, Mode_of_Shipment, Product_importance, Gender', 'Age, TB, DB, Alkphos, Sgpt, Sgot, TP']\n",
      "Loss: 0.002978644799441099\n",
      "Predictions: ['anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem', 'anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem']\n",
      "Loss: 0.0034000941086560488\n",
      "Predictions: ['12', 'JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain']\n",
      "Loss: 0.002947053872048855\n",
      "Predictions: ['ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount', 'texture_mean, perimeter_mean, texture_se, perimeter_se, area_se']\n",
      "Loss: 0.002947010099887848\n",
      "Predictions: ['Machine_failure', 'COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY']\n",
      "Loss: 0.00331119610927999\n",
      "Predictions: ['workclass, education, marital-status, occupation, relationship, race, gender', 'Ia, Ib, Ic, Va, Vb, Vc']\n",
      "Loss: 0.0035753189586102962\n",
      "Predictions: ['8', 'Class']\n",
      "Loss: 0.0036229670513421297\n",
      "Predictions: ['target', 'Sex']\n",
      "Loss: 0.0031726558227092028\n",
      "Predictions: ['existing_credits <= 1.5, residence_since <= 3.5', 'Ic <= 71.01, Vb <= -0.37']\n",
      "Loss: 0.003089840756729245\n",
      "Predictions: ['age, fnlwgt, educational-num, capital-gain, capital-loss, hours', 'Juiciness <= -0.3, Crunchiness <= 2.25']\n",
      "Loss: 0.0036490217316895723\n",
      "Predictions: ['Gender', 'Diagnosis']\n",
      "Loss: 0.003333626314997673\n",
      "Predictions: ['ra, dec, run, camcol, field, redshift, plate, mjd', '6']\n",
      "Loss: 0.003255427349358797\n",
      "Predictions: ['stress_level', 'Rainfall, WindSpeed9am, Pressure9am, Pressure3pm, Cloud9am']\n",
      "Loss: 0.0030576724093407393\n",
      "Predictions: ['CAEC, CALC, MTRANS, Gender, family_history_with_over', 'ra, dec, run, camcol, field, redshift, plate, mjd']\n",
      "Loss: 0.0032819753978401423\n",
      "Predictions: ['Age, pH, Specific Gravity', 'ssc_p, hsc_p, degree_p, etest_p']\n",
      "Loss: 0.003595984075218439\n",
      "Predictions: ['Color', 'Segmentation']\n",
      "Loss: 0.0031993547454476357\n",
      "Predictions: ['variance, skewness, curtosis, entropy', 'Area, Perimeter, MinorAxisLength, AspectRation, Eccentricity,']\n",
      "Loss: 0.0032135327346622944\n",
      "Predictions: ['mental_health_history', 'Color, Transparency, Glucose, Protein, Epithelial Cells, Mu']\n",
      "Loss: 0.003157357219606638\n",
      "Predictions: ['4', 'fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free']\n",
      "Loss: 0.0032935573253780603\n",
      "Predictions: ['6', 'Family_Size <= 2.5, Work_Experience <= 9.5']\n",
      "Loss: 0.003224519081413746\n",
      "Predictions: ['Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque', '3']\n",
      "Loss: 0.003504217369481921\n",
      "Predictions: ['Outcome', '9']\n",
      "Loss: 0.0032513851765543222\n",
      "Predictions: ['Size, Weight, Sweetness, Crunchiness, Juiciness, Ripeness', 'FAF <= 2.0, Height <= 1.72']\n",
      "Loss: 0.003245066851377487\n",
      "Predictions: ['Age, TB, DB, Alkphos, Sgpt, Sgot, TP', '12']\n",
      "Loss: 0.003288108156993985\n",
      "Predictions: ['JoiningYear <= 2017.5, ExperienceInCurrentDomain <= 3', '6']\n",
      "Loss: 0.0034438828006386757\n",
      "Predictions: ['8', 'Age, pH, Specific Gravity']\n",
      "Loss: 0.003083784831687808\n",
      "Predictions: ['MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO', '11']\n",
      "Loss: 0.0029634847305715084\n",
      "Predictions: ['texture_mean, perimeter_mean, texture_se, perimeter_se, area_se', 'SMK_stat_type_cd <= 1.5, gamma_GTP <=']\n",
      "Loss: 0.0031377370469272137\n",
      "Predictions: ['JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain', 'MAJORSKEWNESS <= 74.5, CIRCULARITY <=']\n",
      "Loss: 0.002964410465210676\n",
      "Predictions: ['Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior', 'Age <= 42.5, NumOfProducts <= 2.5']\n",
      "Loss: 0.0032777138985693455\n",
      "Predictions: ['class', 'SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet']\n",
      "Loss: 0.0030942638404667377\n",
      "Predictions: ['hsc_s, degree_t, gender, ssc_b, hsc', 'Location, WindGustDir, WindDir9am, WindDir3pm, RainToday']\n",
      "Loss: 0.0028914662543684244\n",
      "Predictions: ['10', 'MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO']\n",
      "Loss: 0.002700998680666089\n",
      "Predictions: ['MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO', 'age, fnlwgt, educational-num, capital-gain, capital-loss, hours']\n",
      "Loss: 0.0030978857539594173\n",
      "Predictions: ['Size, Weight, Sweetness, Crunchiness, Juiciness, Ripeness', 'Gender, Dependents, Self_Employed, Loan_Amount_Term']\n",
      "Loss: 0.0032756004948168993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 40/63 [1:01:56<45:40, 119.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary', '6']\n",
      "Epoch: 40\n",
      "Loss: 0.003134220140054822\n",
      "Loss: 0.0029600956477224827\n",
      "Loss: 0.0032532336190342903\n",
      "Loss: 0.0034208521246910095\n",
      "Loss: 0.0028610387817025185\n",
      "Loss: 0.003380131209269166\n",
      "Loss: 0.00297995051369071\n",
      "Loss: 0.0032149634789675474\n",
      "Loss: 0.003330143168568611\n",
      "Loss: 0.0028897575102746487\n",
      "Loss: 0.00288497656583786\n",
      "Loss: 0.002896709367632866\n",
      "Loss: 0.003044418292120099\n",
      "Loss: 0.0032686113845556974\n",
      "Loss: 0.0027064871974289417\n",
      "Loss: 0.0028515246231108904\n",
      "Loss: 0.002868722192943096\n",
      "Loss: 0.0029276672285050154\n",
      "Loss: 0.0034514504950493574\n",
      "Loss: 0.002768876263871789\n",
      "Loss: 0.003123590722680092\n",
      "Loss: 0.003490069881081581\n",
      "Loss: 0.0029975883662700653\n",
      "Loss: 0.0032074416521936655\n",
      "Loss: 0.002955137984827161\n",
      "Loss: 0.0028799776919186115\n",
      "Loss: 0.0032361799385398626\n",
      "Loss: 0.003122959053143859\n",
      "Loss: 0.0030441959388554096\n",
      "Loss: 0.0030884072184562683\n",
      "Loss: 0.003106198739260435\n",
      "Loss: 0.003171400399878621\n",
      "Loss: 0.0032460452057421207\n",
      "Loss: 0.003081690752878785\n",
      "Loss: 0.003190763061866164\n",
      "Loss: 0.0029757735319435596\n",
      "Loss: 0.0029453723691403866\n",
      "Loss: 0.0031441517639905214\n",
      "Loss: 0.0032117273658514023\n",
      "Loss: 0.003011167049407959\n",
      "Loss: 0.0025812459643930197\n",
      "Loss: 0.0031029272358864546\n",
      "Loss: 0.003173956647515297\n",
      "Loss: 0.00312540284357965\n",
      "Loss: 0.0028875062707811594\n",
      "Loss: 0.00310915638692677\n",
      "Loss: 0.002836421597748995\n",
      "Loss: 0.0028478882741183043\n",
      "Loss: 0.003088719444349408\n",
      "Loss: 0.0029522271361202\n",
      "Loss: 0.0027840244583785534\n",
      "Loss: 0.002932801842689514\n",
      "Loss: 0.0030301338993012905\n",
      "Loss: 0.003420377615839243\n",
      "Loss: 0.003179787890985608\n",
      "Loss: 0.0026052973698824644\n",
      "Loss: 0.0024213448632508516\n",
      "Loss: 0.002899833722040057\n",
      "Loss: 0.0031958972103893757\n",
      "Loss: 0.0031976341269910336\n",
      "Loss: 0.003329211613163352\n",
      "Loss: 0.0029571421910077333\n",
      "Loss: 0.0033807663712650537\n",
      "Loss: 0.0032334299758076668\n",
      "Loss: 0.0032586599700152874\n",
      "Loss: 0.003107207827270031\n",
      "Loss: 0.003400562098249793\n",
      "Loss: 0.0029105572029948235\n",
      "Loss: 0.0031118185725063086\n",
      "Loss: 0.0030901122372597456\n",
      "Loss: 0.003010049695149064\n",
      "Loss: 0.0029017243068665266\n",
      "Loss: 0.0029343166388571262\n",
      "Loss: 0.0028604171238839626\n",
      "Loss: 0.0029819768387824297\n",
      "Loss: 0.00323368888348341\n",
      "Loss: 0.0030781652312725782\n",
      "Loss: 0.0032429276034235954\n",
      "Loss: 0.00333687593229115\n",
      "Loss: 0.0032530571334064007\n",
      "Loss: 0.0030806520953774452\n",
      "Loss: 0.0030833787750452757\n",
      "Loss: 0.003276949981227517\n",
      "Loss: 0.0027541315648704767\n",
      "Loss: 0.00322927488014102\n",
      "Loss: 0.002946648746728897\n",
      "Loss: 0.00275518000125885\n",
      "Loss: 0.003121053334325552\n",
      "Loss: 0.003085510106757283\n",
      "Loss: 0.0031524375081062317\n",
      "Loss: 0.0030955844558775425\n",
      "Loss: 0.002973349066451192\n",
      "Loss: 0.002957466756924987\n",
      "Loss: 0.0025059296749532223\n",
      "Loss: 0.0032905538100749254\n",
      "Loss: 0.003020017873495817\n",
      "Loss: 0.0032007996924221516\n",
      "Loss: 0.0029247256461530924\n",
      "Loss: 0.002684056293219328\n",
      "Loss: 0.0027795126661658287\n",
      "Loss: 0.00323845143429935\n",
      "Loss: 0.0032348090317100286\n",
      "Loss: 0.0029064344707876444\n",
      "Loss: 0.0028730486519634724\n",
      "Loss: 0.0026963003911077976\n",
      "Loss: 0.0025462605990469456\n",
      "Loss: 0.0030908684711903334\n",
      "Loss: 0.0025912970304489136\n",
      "Loss: 0.0031580692157149315\n",
      "Loss: 0.0030024098232388496\n",
      "Loss: 0.0032197521068155766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 41/63 [1:03:22<39:58, 109.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0030963255558162928\n",
      "Epoch: 41\n",
      "Loss: 0.0027130660600960255\n",
      "Loss: 0.0030162339098751545\n",
      "Loss: 0.002780304988846183\n",
      "Loss: 0.0031728222966194153\n",
      "Loss: 0.0029861298389732838\n",
      "Loss: 0.0030530570074915886\n",
      "Loss: 0.00301523064263165\n",
      "Loss: 0.0030720001086592674\n",
      "Loss: 0.0029714973643422127\n",
      "Loss: 0.002552452264353633\n",
      "Loss: 0.002809441415593028\n",
      "Loss: 0.0030383069533854723\n",
      "Loss: 0.0029474925249814987\n",
      "Loss: 0.0028293058276176453\n",
      "Loss: 0.002834363142028451\n",
      "Loss: 0.0027356743812561035\n",
      "Loss: 0.0030838842503726482\n",
      "Loss: 0.003005396807566285\n",
      "Loss: 0.003035629400983453\n",
      "Loss: 0.003225969150662422\n",
      "Loss: 0.00279014790430665\n",
      "Loss: 0.0029651157092303038\n",
      "Loss: 0.0030270118732005358\n",
      "Loss: 0.0029076600912958384\n",
      "Loss: 0.0027466751635074615\n",
      "Loss: 0.002700029406696558\n",
      "Loss: 0.002783016534522176\n",
      "Loss: 0.0028415382839739323\n",
      "Loss: 0.003024932462722063\n",
      "Loss: 0.0026260160375386477\n",
      "Loss: 0.002954753814265132\n",
      "Loss: 0.003027982311323285\n",
      "Loss: 0.002995785791426897\n",
      "Loss: 0.0029630139470100403\n",
      "Loss: 0.0028933724388480186\n",
      "Loss: 0.0027672869618982077\n",
      "Loss: 0.0025609813164919615\n",
      "Loss: 0.003019473282620311\n",
      "Loss: 0.002954356372356415\n",
      "Loss: 0.003197756130248308\n",
      "Loss: 0.0028049072716385126\n",
      "Loss: 0.0024438800755888224\n",
      "Loss: 0.002631331793963909\n",
      "Loss: 0.0028258601669222116\n",
      "Loss: 0.003194474382326007\n",
      "Loss: 0.002902963664382696\n",
      "Loss: 0.0027102031745016575\n",
      "Loss: 0.0031290086917579174\n",
      "Loss: 0.0029176592361181974\n",
      "Loss: 0.0031649263110011816\n",
      "Loss: 0.0027933791279792786\n",
      "Loss: 0.0030835752841085196\n",
      "Loss: 0.002891482785344124\n",
      "Loss: 0.0027955349069088697\n",
      "Loss: 0.002954208990558982\n",
      "Loss: 0.0027313849423080683\n",
      "Loss: 0.0025940213818103075\n",
      "Loss: 0.00305672874674201\n",
      "Loss: 0.003186935093253851\n",
      "Loss: 0.0026388901751488447\n",
      "Loss: 0.0030285425018519163\n",
      "Loss: 0.002783802105113864\n",
      "Loss: 0.0024791155010461807\n",
      "Loss: 0.003090978367254138\n",
      "Loss: 0.0028221383690834045\n",
      "Loss: 0.0029931156896054745\n",
      "Loss: 0.0026991653721779585\n",
      "Loss: 0.002562879351899028\n",
      "Loss: 0.002783776493743062\n",
      "Loss: 0.0030479952692985535\n",
      "Loss: 0.0031977188773453236\n",
      "Loss: 0.0027943227905780077\n",
      "Loss: 0.002815390471369028\n",
      "Loss: 0.002949336776509881\n",
      "Loss: 0.002648926107212901\n",
      "Loss: 0.0025015186984091997\n",
      "Loss: 0.0029287871439009905\n",
      "Loss: 0.0027266519609838724\n",
      "Loss: 0.0030976615380495787\n",
      "Loss: 0.0024214768782258034\n",
      "Loss: 0.0027564417105168104\n",
      "Loss: 0.0023851334117352962\n",
      "Loss: 0.002776132896542549\n",
      "Loss: 0.002571582095697522\n",
      "Loss: 0.0025748421903699636\n",
      "Loss: 0.002954184776172042\n",
      "Loss: 0.003183459397405386\n",
      "Loss: 0.0028876352589577436\n",
      "Loss: 0.0027350105810910463\n",
      "Loss: 0.0028600837104022503\n",
      "Loss: 0.0028231842443346977\n",
      "Loss: 0.0027714050374925137\n",
      "Loss: 0.0030370131134986877\n",
      "Loss: 0.0029796853195875883\n",
      "Loss: 0.0027602214831858873\n",
      "Loss: 0.002401531906798482\n",
      "Loss: 0.0030879888217896223\n",
      "Loss: 0.002589161042124033\n",
      "Loss: 0.0024617554154247046\n",
      "Loss: 0.002366237808018923\n",
      "Loss: 0.0030333094764500856\n",
      "Loss: 0.0025854844134300947\n",
      "Loss: 0.002680129138752818\n",
      "Loss: 0.0025869181845337152\n",
      "Loss: 0.0027080841828137636\n",
      "Loss: 0.0027799177914857864\n",
      "Loss: 0.0030506234616041183\n",
      "Loss: 0.002706467639654875\n",
      "Loss: 0.003003066638484597\n",
      "Loss: 0.0026545817963778973\n",
      "Loss: 0.0029671103693544865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 42/63 [1:04:47<35:42, 102.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.002750261453911662\n",
      "Epoch: 42\n",
      "Loss: 0.002870946889743209\n",
      "Loss: 0.00288976333104074\n",
      "Loss: 0.0028228473383933306\n",
      "Loss: 0.0026726890355348587\n",
      "Loss: 0.0020446409471333027\n",
      "Loss: 0.0030093395616859198\n",
      "Loss: 0.002904091263189912\n",
      "Loss: 0.0029904223047196865\n",
      "Loss: 0.002799167763441801\n",
      "Loss: 0.0028541446663439274\n",
      "Loss: 0.0025611361488699913\n",
      "Loss: 0.0027620247565209866\n",
      "Loss: 0.002795666456222534\n",
      "Loss: 0.002979086246341467\n",
      "Loss: 0.0022940521594136953\n",
      "Loss: 0.0026583943981677294\n",
      "Loss: 0.0026247284840792418\n",
      "Loss: 0.003040153533220291\n",
      "Loss: 0.0027554493863135576\n",
      "Loss: 0.0026330614928156137\n",
      "Loss: 0.002903502434492111\n",
      "Loss: 0.0025719976983964443\n",
      "Loss: 0.003032377688214183\n",
      "Loss: 0.0023561406414955854\n",
      "Loss: 0.0025726568419486284\n",
      "Loss: 0.0026011131703853607\n",
      "Loss: 0.0023654208052903414\n",
      "Loss: 0.0027942340821027756\n",
      "Loss: 0.0028358346316963434\n",
      "Loss: 0.0029854720924049616\n",
      "Loss: 0.0026666915509849787\n",
      "Loss: 0.002817166270688176\n",
      "Loss: 0.002767622936517\n",
      "Loss: 0.003026926424354315\n",
      "Loss: 0.0027932641096413136\n",
      "Loss: 0.0028393748216331005\n",
      "Loss: 0.0028697813395410776\n",
      "Loss: 0.002703605219721794\n",
      "Loss: 0.0029299373272806406\n",
      "Loss: 0.0028265120927244425\n",
      "Loss: 0.002826995449140668\n",
      "Loss: 0.0023620508145540953\n",
      "Loss: 0.00277585256844759\n",
      "Loss: 0.0025682486593723297\n",
      "Loss: 0.00236393790692091\n",
      "Loss: 0.0028574583120644093\n",
      "Loss: 0.0024883782025426626\n",
      "Loss: 0.0025550262071192265\n",
      "Loss: 0.0024288285057991743\n",
      "Loss: 0.0026492238976061344\n",
      "Loss: 0.0025138899218291044\n",
      "Loss: 0.0025779816787689924\n",
      "Loss: 0.0029189158231019974\n",
      "Loss: 0.0027589669916778803\n",
      "Loss: 0.002396373078227043\n",
      "Loss: 0.0024957668501883745\n",
      "Loss: 0.002715914975851774\n",
      "Loss: 0.002720297547057271\n",
      "Loss: 0.0027281325310468674\n",
      "Loss: 0.0025733932852745056\n",
      "Loss: 0.0025289510376751423\n",
      "Loss: 0.0026196897961199284\n",
      "Loss: 0.002519889734685421\n",
      "Loss: 0.002777259098365903\n",
      "Loss: 0.0026571855414658785\n",
      "Loss: 0.00272265262901783\n",
      "Loss: 0.0027654366567730904\n",
      "Loss: 0.0026929115410894156\n",
      "Loss: 0.0026045851409435272\n",
      "Loss: 0.002787037519738078\n",
      "Loss: 0.0025336164981126785\n",
      "Loss: 0.0025521120987832546\n",
      "Loss: 0.0026807391550391912\n",
      "Loss: 0.002469204366207123\n",
      "Loss: 0.0026977809611707926\n",
      "Loss: 0.0026229911018162966\n",
      "Loss: 0.002537692664191127\n",
      "Loss: 0.0027979882434010506\n",
      "Loss: 0.0029247079510241747\n",
      "Loss: 0.002705351682379842\n",
      "Loss: 0.0029147169552743435\n",
      "Loss: 0.002723074285313487\n",
      "Loss: 0.0027579315938055515\n",
      "Loss: 0.002565294038504362\n",
      "Loss: 0.0022805477492511272\n",
      "Loss: 0.0025414451956748962\n",
      "Loss: 0.0027135529089719057\n",
      "Loss: 0.002887946320697665\n",
      "Loss: 0.0026656356640160084\n",
      "Loss: 0.0026545929722487926\n",
      "Loss: 0.0025620076339691877\n",
      "Loss: 0.0027455694507807493\n",
      "Loss: 0.00268826587125659\n",
      "Loss: 0.0023414131719619036\n",
      "Loss: 0.002405827632173896\n",
      "Loss: 0.002747287740930915\n",
      "Loss: 0.002678479766473174\n",
      "Loss: 0.002718926640227437\n",
      "Loss: 0.002192022278904915\n",
      "Loss: 0.002397565171122551\n",
      "Loss: 0.0026831291615962982\n",
      "Loss: 0.00268361484631896\n",
      "Loss: 0.0025022567715495825\n",
      "Loss: 0.0029127744492143393\n",
      "Loss: 0.002491721883416176\n",
      "Loss: 0.0022317355033010244\n",
      "Loss: 0.002370971953496337\n",
      "Loss: 0.0026132117491215467\n",
      "Loss: 0.0025994949974119663\n",
      "Loss: 0.002561494940891862\n",
      "Loss: 0.002595609286800027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 43/63 [1:06:13<32:20, 97.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0024130004458129406\n",
      "Epoch: 43\n",
      "Loss: 0.0025946067180484533\n",
      "Loss: 0.0027790076564997435\n",
      "Loss: 0.002410446759313345\n",
      "Loss: 0.002331551630049944\n",
      "Loss: 0.0028419054578989744\n",
      "Loss: 0.002517838031053543\n",
      "Loss: 0.002451593754813075\n",
      "Loss: 0.0024457869585603476\n",
      "Loss: 0.0026661695446819067\n",
      "Loss: 0.0027651332784444094\n",
      "Loss: 0.0021414158400148153\n",
      "Loss: 0.0023958163801580667\n",
      "Loss: 0.0027949190698564053\n",
      "Loss: 0.0023596729151904583\n",
      "Loss: 0.002658367855474353\n",
      "Loss: 0.002744290279224515\n",
      "Loss: 0.002826736308634281\n",
      "Loss: 0.0025598458014428616\n",
      "Loss: 0.0027258298359811306\n",
      "Loss: 0.0026017397176474333\n",
      "Loss: 0.0028397610876709223\n",
      "Loss: 0.0026568518951535225\n",
      "Loss: 0.0024960115551948547\n",
      "Loss: 0.002626798814162612\n",
      "Loss: 0.002369613852351904\n",
      "Loss: 0.002199734328314662\n",
      "Loss: 0.002541096182540059\n",
      "Loss: 0.002672373317182064\n",
      "Loss: 0.002626783214509487\n",
      "Loss: 0.002170385792851448\n",
      "Loss: 0.002648215275257826\n",
      "Loss: 0.0026982962153851986\n",
      "Loss: 0.002516146982088685\n",
      "Loss: 0.002312706084921956\n",
      "Loss: 0.0025995236355811357\n",
      "Loss: 0.0027086157351732254\n",
      "Loss: 0.002648403402417898\n",
      "Loss: 0.002640732331201434\n",
      "Loss: 0.0024862189311534166\n",
      "Loss: 0.002420727862045169\n",
      "Loss: 0.0023280237801373005\n",
      "Loss: 0.0025886904913932085\n",
      "Loss: 0.0022571170702576637\n",
      "Loss: 0.0027753659524023533\n",
      "Loss: 0.0026955564972013235\n",
      "Loss: 0.0026289643719792366\n",
      "Loss: 0.0025343038141727448\n",
      "Loss: 0.0020811452995985746\n",
      "Loss: 0.0026011979207396507\n",
      "Loss: 0.0025924788787961006\n",
      "Loss: 0.002826749114319682\n",
      "Loss: 0.0024921803269535303\n",
      "Loss: 0.0025941221974790096\n",
      "Loss: 0.0025273102801293135\n",
      "Loss: 0.0023651730734854937\n",
      "Loss: 0.0025276215746998787\n",
      "Loss: 0.0023107370361685753\n",
      "Loss: 0.002402192447334528\n",
      "Loss: 0.0026134641375392675\n",
      "Loss: 0.0025787095073610544\n",
      "Loss: 0.0025659105740487576\n",
      "Loss: 0.002567809307947755\n",
      "Loss: 0.0020387612748891115\n",
      "Loss: 0.002566365525126457\n",
      "Loss: 0.0024721280205994844\n",
      "Loss: 0.002768137725070119\n",
      "Loss: 0.0024575036950409412\n",
      "Loss: 0.002598414197564125\n",
      "Loss: 0.0024343181867152452\n",
      "Loss: 0.002143437508493662\n",
      "Loss: 0.002595348982140422\n",
      "Loss: 0.002567881252616644\n",
      "Loss: 0.0023991733323782682\n",
      "Loss: 0.002577484119683504\n",
      "Loss: 0.002474298933520913\n",
      "Loss: 0.0024631901178508997\n",
      "Loss: 0.00202649412676692\n",
      "Loss: 0.0026878821663558483\n",
      "Loss: 0.002210737904533744\n",
      "Loss: 0.0023701642639935017\n",
      "Loss: 0.0023482413962483406\n",
      "Loss: 0.0022336014080792665\n",
      "Loss: 0.0025328875053673983\n",
      "Loss: 0.002375904703512788\n",
      "Loss: 0.0025911806151270866\n",
      "Loss: 0.0023067989386618137\n",
      "Loss: 0.002359747188165784\n",
      "Loss: 0.0026854565367102623\n",
      "Loss: 0.0027126718778163195\n",
      "Loss: 0.0025788533966988325\n",
      "Loss: 0.0024168360978364944\n",
      "Loss: 0.0025181358214467764\n",
      "Loss: 0.0022830558009445667\n",
      "Loss: 0.0025125211104750633\n",
      "Loss: 0.0025797439739108086\n",
      "Loss: 0.0019735079258680344\n",
      "Loss: 0.0023376699537038803\n",
      "Loss: 0.002504934323951602\n",
      "Loss: 0.0024655654560774565\n",
      "Loss: 0.002594840480014682\n",
      "Loss: 0.002493108855560422\n",
      "Loss: 0.0021855777595192194\n",
      "Loss: 0.0023862062953412533\n",
      "Loss: 0.0025451781693845987\n",
      "Loss: 0.0025081157218664885\n",
      "Loss: 0.0026761689223349094\n",
      "Loss: 0.0023790381383150816\n",
      "Loss: 0.0020598764531314373\n",
      "Loss: 0.0022181158419698477\n",
      "Loss: 0.002694125985726714\n",
      "Loss: 0.0024668530095368624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 44/63 [1:07:39<29:39, 93.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.002262251218780875\n",
      "Epoch: 44\n",
      "Loss: 0.002356934593990445\n",
      "Loss: 0.002457838738337159\n",
      "Loss: 0.002303378889337182\n",
      "Loss: 0.0025520806666463614\n",
      "Loss: 0.0023864072281867266\n",
      "Loss: 0.0025836650747805834\n",
      "Loss: 0.002440684475004673\n",
      "Loss: 0.0022224343847483397\n",
      "Loss: 0.002550886943936348\n",
      "Loss: 0.0022490278352051973\n",
      "Loss: 0.002389873843640089\n",
      "Loss: 0.0023270174860954285\n",
      "Loss: 0.002694555092602968\n",
      "Loss: 0.0024863698054105043\n",
      "Loss: 0.002575238700956106\n",
      "Loss: 0.0023190234787762165\n",
      "Loss: 0.002216327702626586\n",
      "Loss: 0.0023581429850310087\n",
      "Loss: 0.002387173706665635\n",
      "Loss: 0.002557267900556326\n",
      "Loss: 0.0026390186976641417\n",
      "Loss: 0.002316373400390148\n",
      "Loss: 0.0020743918139487505\n",
      "Loss: 0.002472373889759183\n",
      "Loss: 0.0021472976077347994\n",
      "Loss: 0.0024483499582856894\n",
      "Loss: 0.0024806251749396324\n",
      "Loss: 0.002037718892097473\n",
      "Loss: 0.0025306050665676594\n",
      "Loss: 0.002458595670759678\n",
      "Loss: 0.0025838545989245176\n",
      "Loss: 0.002453017048537731\n",
      "Loss: 0.0025047981180250645\n",
      "Loss: 0.0020475885830819607\n",
      "Loss: 0.002378060482442379\n",
      "Loss: 0.0021861607674509287\n",
      "Loss: 0.0025112126022577286\n",
      "Loss: 0.0022382077295333147\n",
      "Loss: 0.0022624493576586246\n",
      "Loss: 0.0023544866126030684\n",
      "Loss: 0.0022738853003829718\n",
      "Loss: 0.002411534311249852\n",
      "Loss: 0.002288644900545478\n",
      "Loss: 0.002430656924843788\n",
      "Loss: 0.0020052199251949787\n",
      "Loss: 0.002106841653585434\n",
      "Loss: 0.0022270330227911472\n",
      "Loss: 0.002420713659375906\n",
      "Loss: 0.0019693791400641203\n",
      "Loss: 0.002481157658621669\n",
      "Loss: 0.0023295108694583178\n",
      "Loss: 0.0021371403709053993\n",
      "Loss: 0.0020525366999208927\n",
      "Loss: 0.0022537820041179657\n",
      "Loss: 0.0023612179793417454\n",
      "Loss: 0.0023575094528496265\n",
      "Loss: 0.0024489492643624544\n",
      "Loss: 0.002298336010426283\n",
      "Loss: 0.0023590456694364548\n",
      "Loss: 0.002589454408735037\n",
      "Loss: 0.0023005513940006495\n",
      "Loss: 0.0026395057793706656\n",
      "Loss: 0.002375947777181864\n",
      "Loss: 0.0020837944466620684\n",
      "Loss: 0.002332379575818777\n",
      "Loss: 0.0021925731562078\n",
      "Loss: 0.002192325657233596\n",
      "Loss: 0.002304052235558629\n",
      "Loss: 0.002563315909355879\n",
      "Loss: 0.0025364926550537348\n",
      "Loss: 0.002319011837244034\n",
      "Loss: 0.002406901214271784\n",
      "Loss: 0.0021948928479105234\n",
      "Loss: 0.0021902061998844147\n",
      "Loss: 0.002359112724661827\n",
      "Loss: 0.0023896831553429365\n",
      "Loss: 0.0022541179787367582\n",
      "Loss: 0.0022661646362394094\n",
      "Loss: 0.0024857984390109777\n",
      "Loss: 0.0025624928530305624\n",
      "Loss: 0.0023570600897073746\n",
      "Loss: 0.002267251256853342\n",
      "Loss: 0.0021316155325621367\n",
      "Loss: 0.002509713638573885\n",
      "Loss: 0.0022387460339814425\n",
      "Loss: 0.0021456468384712934\n",
      "Loss: 0.0023524865973740816\n",
      "Loss: 0.0023797934409230947\n",
      "Loss: 0.002341271610930562\n",
      "Loss: 0.0024971815291792154\n",
      "Loss: 0.002207064302638173\n",
      "Loss: 0.002057693898677826\n",
      "Loss: 0.002309728180989623\n",
      "Loss: 0.002269883407279849\n",
      "Loss: 0.002135676331818104\n",
      "Loss: 0.0025809956714510918\n",
      "Loss: 0.00253658345900476\n",
      "Loss: 0.0024268561974167824\n",
      "Loss: 0.0020653177052736282\n",
      "Loss: 0.0023617534898221493\n",
      "Loss: 0.0020946511067450047\n",
      "Loss: 0.002286379225552082\n",
      "Loss: 0.0023145140148699284\n",
      "Loss: 0.002148555824533105\n",
      "Loss: 0.002340863225981593\n",
      "Loss: 0.002304882276803255\n",
      "Loss: 0.0025084316730499268\n",
      "Loss: 0.0023422008380293846\n",
      "Loss: 0.00215521058999002\n",
      "Loss: 0.002421664074063301\n",
      "Loss: 0.0023606407921761274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 45/63 [1:09:04<27:20, 91.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0022091837599873543\n",
      "Epoch: 45\n",
      "Loss: 0.00178037048317492\n",
      "Loss: 0.0022754697129130363\n",
      "Loss: 0.0023573392536491156\n",
      "Loss: 0.002251227153465152\n",
      "Loss: 0.0023585036396980286\n",
      "Loss: 0.0022649276070296764\n",
      "Loss: 0.002200588583946228\n",
      "Loss: 0.0022371290251612663\n",
      "Loss: 0.002168184844776988\n",
      "Loss: 0.0021146878134459257\n",
      "Loss: 0.002219855086877942\n",
      "Loss: 0.0019387024221941829\n",
      "Loss: 0.0022367320489138365\n",
      "Loss: 0.002541587222367525\n",
      "Loss: 0.0020477420184761286\n",
      "Loss: 0.002305550966411829\n",
      "Loss: 0.0021632302086800337\n",
      "Loss: 0.001823263126425445\n",
      "Loss: 0.0019387949723750353\n",
      "Loss: 0.002053725766018033\n",
      "Loss: 0.002204244490712881\n",
      "Loss: 0.0021679161582142115\n",
      "Loss: 0.0022843051701784134\n",
      "Loss: 0.002174930414184928\n",
      "Loss: 0.0022691963240504265\n",
      "Loss: 0.002316124737262726\n",
      "Loss: 0.002316951286047697\n",
      "Loss: 0.002113103400915861\n",
      "Loss: 0.0024740509688854218\n",
      "Loss: 0.002096636686474085\n",
      "Loss: 0.002303066663444042\n",
      "Loss: 0.0023469955194741488\n",
      "Loss: 0.0022472208365797997\n",
      "Loss: 0.0024311363231390715\n",
      "Loss: 0.0024232587311416864\n",
      "Loss: 0.002260704291984439\n",
      "Loss: 0.002129825297743082\n",
      "Loss: 0.002415559021756053\n",
      "Loss: 0.0021512906532734632\n",
      "Loss: 0.002187346573919058\n",
      "Loss: 0.001988573931157589\n",
      "Loss: 0.0024003542494028807\n",
      "Loss: 0.0022008721716701984\n",
      "Loss: 0.0020247388165444136\n",
      "Loss: 0.002426265040412545\n",
      "Loss: 0.002436430659145117\n",
      "Loss: 0.002457540715113282\n",
      "Loss: 0.0022256295196712017\n",
      "Loss: 0.0023638128768652678\n",
      "Loss: 0.0022591054439544678\n",
      "Loss: 0.002260883105918765\n",
      "Loss: 0.00196327967569232\n",
      "Loss: 0.00212977547198534\n",
      "Loss: 0.002171918749809265\n",
      "Loss: 0.0019851122051477432\n",
      "Loss: 0.0023322587367147207\n",
      "Loss: 0.0024191781412810087\n",
      "Loss: 0.002302150707691908\n",
      "Loss: 0.002170829102396965\n",
      "Loss: 0.002171364612877369\n",
      "Loss: 0.0021891403011977673\n",
      "Loss: 0.002312656491994858\n",
      "Loss: 0.00228885211981833\n",
      "Loss: 0.0020202903542667627\n",
      "Loss: 0.0023064210545271635\n",
      "Loss: 0.0021912911906838417\n",
      "Loss: 0.002255854196846485\n",
      "Loss: 0.0021052993834018707\n",
      "Loss: 0.0021533379331231117\n",
      "Loss: 0.0021099946461617947\n",
      "Loss: 0.0021639661863446236\n",
      "Loss: 0.002436856273561716\n",
      "Loss: 0.002318313578143716\n",
      "Loss: 0.002389597473666072\n",
      "Loss: 0.0022015469148755074\n",
      "Loss: 0.0021336928475648165\n",
      "Loss: 0.0018503941828384995\n",
      "Loss: 0.002279342385008931\n",
      "Loss: 0.0021993801929056644\n",
      "Loss: 0.0017058032099157572\n",
      "Loss: 0.002260118257254362\n",
      "Loss: 0.0020911782048642635\n",
      "Loss: 0.0019983998499810696\n",
      "Loss: 0.0020870522130280733\n",
      "Loss: 0.0019506975077092648\n",
      "Loss: 0.0021971147507429123\n",
      "Loss: 0.002214790554717183\n",
      "Loss: 0.002303965389728546\n",
      "Loss: 0.002063144464045763\n",
      "Loss: 0.001983439549803734\n",
      "Loss: 0.0022532253060489893\n",
      "Loss: 0.002216202672570944\n",
      "Loss: 0.0022576407063752413\n",
      "Loss: 0.002391048474237323\n",
      "Loss: 0.0022803207393735647\n",
      "Loss: 0.0021383080165833235\n",
      "Loss: 0.0020169394556432962\n",
      "Loss: 0.0023506563156843185\n",
      "Loss: 0.002201525494456291\n",
      "Loss: 0.0019736341200768948\n",
      "Loss: 0.002194588538259268\n",
      "Loss: 0.002213099505752325\n",
      "Loss: 0.002162141725420952\n",
      "Loss: 0.0018871836364269257\n",
      "Loss: 0.002227841643616557\n",
      "Loss: 0.0021743541583418846\n",
      "Loss: 0.0022696880623698235\n",
      "Loss: 0.0022277310490608215\n",
      "Loss: 0.0020428006537258625\n",
      "Loss: 0.002089696004986763\n",
      "Loss: 0.0021345072891563177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 46/63 [1:10:29<25:20, 89.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017951090121641755\n",
      "Epoch: 46\n",
      "Loss: 0.0019032800337299705\n",
      "Loss: 0.002141628647223115\n",
      "Loss: 0.002180325798690319\n",
      "Loss: 0.002179025672376156\n",
      "Loss: 0.002163222525268793\n",
      "Loss: 0.0019507057731971145\n",
      "Loss: 0.00225626677274704\n",
      "Loss: 0.001832045498304069\n",
      "Loss: 0.0019988976418972015\n",
      "Loss: 0.002343331929296255\n",
      "Loss: 0.0021819202229380608\n",
      "Loss: 0.0020248752553015947\n",
      "Loss: 0.0022304831072688103\n",
      "Loss: 0.001960293622687459\n",
      "Loss: 0.002162453718483448\n",
      "Loss: 0.00197236449457705\n",
      "Loss: 0.0023659055586904287\n",
      "Loss: 0.0019060152117162943\n",
      "Loss: 0.0021886772010475397\n",
      "Loss: 0.002326119691133499\n",
      "Loss: 0.002218905370682478\n",
      "Loss: 0.0022382307797670364\n",
      "Loss: 0.0022850586101412773\n",
      "Loss: 0.0019953607115894556\n",
      "Loss: 0.002151894150301814\n",
      "Loss: 0.0020734546706080437\n",
      "Loss: 0.0019972077570855618\n",
      "Loss: 0.00232964800670743\n",
      "Loss: 0.0020519031677395105\n",
      "Loss: 0.002140127122402191\n",
      "Loss: 0.0021691734436899424\n",
      "Loss: 0.0017303655622527003\n",
      "Loss: 0.0020612345542758703\n",
      "Loss: 0.002210068516433239\n",
      "Loss: 0.001958903856575489\n",
      "Loss: 0.0018736671190708876\n",
      "Loss: 0.0020433133468031883\n",
      "Loss: 0.0020384755916893482\n",
      "Loss: 0.002113472204655409\n",
      "Loss: 0.0023022887762635946\n",
      "Loss: 0.0018337613437324762\n",
      "Loss: 0.002087336266413331\n",
      "Loss: 0.0022663359995931387\n",
      "Loss: 0.001964944414794445\n",
      "Loss: 0.00209804717451334\n",
      "Loss: 0.0019111470319330692\n",
      "Loss: 0.002053330885246396\n",
      "Loss: 0.0021730265580117702\n",
      "Loss: 0.0022530443966388702\n",
      "Loss: 0.002106674946844578\n",
      "Loss: 0.0019217143999412656\n",
      "Loss: 0.002019480336457491\n",
      "Loss: 0.0022426533978432417\n",
      "Loss: 0.002033866010606289\n",
      "Loss: 0.001969786360859871\n",
      "Loss: 0.0018815306248143315\n",
      "Loss: 0.0021288644056767225\n",
      "Loss: 0.0017224407056346536\n",
      "Loss: 0.0018530788365751505\n",
      "Loss: 0.0021151856053620577\n",
      "Loss: 0.0019503020448610187\n",
      "Loss: 0.001960186753422022\n",
      "Loss: 0.0017266779905185103\n",
      "Loss: 0.0019626356661319733\n",
      "Loss: 0.002097440417855978\n",
      "Loss: 0.002140784403309226\n",
      "Loss: 0.002198048634454608\n",
      "Loss: 0.0020956767257303\n",
      "Loss: 0.0018480687867850065\n",
      "Loss: 0.0016383797628805041\n",
      "Loss: 0.0020975866355001926\n",
      "Loss: 0.002105616731569171\n",
      "Loss: 0.0021261987276375294\n",
      "Loss: 0.002288966439664364\n",
      "Loss: 0.002308901399374008\n",
      "Loss: 0.002114249859005213\n",
      "Loss: 0.002108201617375016\n",
      "Loss: 0.0020982776768505573\n",
      "Loss: 0.0021377932280302048\n",
      "Loss: 0.0020965873263776302\n",
      "Loss: 0.0022305159363895655\n",
      "Loss: 0.0020400865469127893\n",
      "Loss: 0.002086889697238803\n",
      "Loss: 0.0020731939002871513\n",
      "Loss: 0.0020110434852540493\n",
      "Loss: 0.0019813072867691517\n",
      "Loss: 0.002067960100248456\n",
      "Loss: 0.001965275965631008\n",
      "Loss: 0.0020375533495098352\n",
      "Loss: 0.0017423155950382352\n",
      "Loss: 0.0022011813707649708\n",
      "Loss: 0.0019695921801030636\n",
      "Loss: 0.0019887438975274563\n",
      "Loss: 0.0020886033307760954\n",
      "Loss: 0.0019219659734517336\n",
      "Loss: 0.001999323023483157\n",
      "Loss: 0.0022368610370904207\n",
      "Loss: 0.0021067510824650526\n",
      "Loss: 0.002093411283567548\n",
      "Loss: 0.0019136269111186266\n",
      "Loss: 0.0022307473700493574\n",
      "Loss: 0.001815341878682375\n",
      "Loss: 0.0017369426786899567\n",
      "Loss: 0.0020857700146734715\n",
      "Loss: 0.001497977296821773\n",
      "Loss: 0.002075926633551717\n",
      "Loss: 0.0021022739820182323\n",
      "Loss: 0.0019781910814344883\n",
      "Loss: 0.0019389173248782754\n",
      "Loss: 0.0020687624346464872\n",
      "Loss: 0.0019731989596039057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 47/63 [1:11:55<23:32, 88.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.001997959101572633\n",
      "Epoch: 47\n",
      "Loss: 0.002084780251607299\n",
      "Loss: 0.0020151236094534397\n",
      "Loss: 0.001945317373611033\n",
      "Loss: 0.001884819008409977\n",
      "Loss: 0.0020769527181982994\n",
      "Loss: 0.0019756199326366186\n",
      "Loss: 0.0021988353691995144\n",
      "Loss: 0.0018902404699474573\n",
      "Loss: 0.001937913242727518\n",
      "Loss: 0.0020804668311029673\n",
      "Loss: 0.001968121388927102\n",
      "Loss: 0.0018646272365003824\n",
      "Loss: 0.0021124284248799086\n",
      "Loss: 0.002053864998742938\n",
      "Loss: 0.0019289868650957942\n",
      "Loss: 0.002003687433898449\n",
      "Loss: 0.0020268652588129044\n",
      "Loss: 0.0020394267048686743\n",
      "Loss: 0.002071373164653778\n",
      "Loss: 0.002035047160461545\n",
      "Loss: 0.0017817512853071094\n",
      "Loss: 0.002047460526227951\n",
      "Loss: 0.0017936663934960961\n",
      "Loss: 0.0018867715261876583\n",
      "Loss: 0.0018131467513740063\n",
      "Loss: 0.0018591380212455988\n",
      "Loss: 0.0017635984113439918\n",
      "Loss: 0.0019837766885757446\n",
      "Loss: 0.0019338545389473438\n",
      "Loss: 0.0019381028832867742\n",
      "Loss: 0.0020315321162343025\n",
      "Loss: 0.0016054123407229781\n",
      "Loss: 0.0020983926951885223\n",
      "Loss: 0.002134559443220496\n",
      "Loss: 0.001944828312844038\n",
      "Loss: 0.0017061547841876745\n",
      "Loss: 0.0019135040929540992\n",
      "Loss: 0.0017952272901311517\n",
      "Loss: 0.0021233526058495045\n",
      "Loss: 0.002028804738074541\n",
      "Loss: 0.0021187898237258196\n",
      "Loss: 0.0016815158305689692\n",
      "Loss: 0.0018901936709880829\n",
      "Loss: 0.00202374579384923\n",
      "Loss: 0.0018802682170644403\n",
      "Loss: 0.0018628700636327267\n",
      "Loss: 0.0021196394227445126\n",
      "Loss: 0.0020796549506485462\n",
      "Loss: 0.001953971339389682\n",
      "Loss: 0.0020143568981438875\n",
      "Loss: 0.001940842834301293\n",
      "Loss: 0.002027583308517933\n",
      "Loss: 0.0018194506410509348\n",
      "Loss: 0.0018587245140224695\n",
      "Loss: 0.001989545300602913\n",
      "Loss: 0.0018031093059107661\n",
      "Loss: 0.0017841889057308435\n",
      "Loss: 0.001552117639221251\n",
      "Loss: 0.0016444894718006253\n",
      "Loss: 0.002164179226383567\n",
      "Loss: 0.0019178404472768307\n",
      "Loss: 0.0021164091303944588\n",
      "Loss: 0.002032171469181776\n",
      "Loss: 0.0020722716581076384\n",
      "Loss: 0.002074000658467412\n",
      "Loss: 0.001983553171157837\n",
      "Loss: 0.0020354597363620996\n",
      "Loss: 0.0020776004530489445\n",
      "Loss: 0.0016609940212219954\n",
      "Loss: 0.0019147193524986506\n",
      "Loss: 0.0019980077631771564\n",
      "Loss: 0.0019178447546437383\n",
      "Loss: 0.0018320218659937382\n",
      "Loss: 0.0018493052339181304\n",
      "Loss: 0.001961826579645276\n",
      "Loss: 0.001968734199181199\n",
      "Loss: 0.001753002405166626\n",
      "Loss: 0.001975907012820244\n",
      "Loss: 0.001837316551245749\n",
      "Loss: 0.0019993584137409925\n",
      "Loss: 0.0020158488769084215\n",
      "Loss: 0.0020260419696569443\n",
      "Loss: 0.0019415164133533835\n",
      "Loss: 0.0018669134005904198\n",
      "Loss: 0.0021092866081744432\n",
      "Loss: 0.0019884202629327774\n",
      "Loss: 0.001827474101446569\n",
      "Loss: 0.0020669924560934305\n",
      "Loss: 0.0017640917794778943\n",
      "Loss: 0.001833824091590941\n",
      "Loss: 0.0019356281263753772\n",
      "Loss: 0.0018745100824162364\n",
      "Loss: 0.0021187635138630867\n",
      "Loss: 0.0018445098539814353\n",
      "Loss: 0.0015937529969960451\n",
      "Loss: 0.0018254326423630118\n",
      "Loss: 0.001823935192078352\n",
      "Loss: 0.0015904309693723917\n",
      "Loss: 0.0020096206571906805\n",
      "Loss: 0.002081798855215311\n",
      "Loss: 0.0019578903447836637\n",
      "Loss: 0.001870922278612852\n",
      "Loss: 0.0018849160987883806\n",
      "Loss: 0.0021233651787042618\n",
      "Loss: 0.001964341616258025\n",
      "Loss: 0.0019459353061392903\n",
      "Loss: 0.001733732526190579\n",
      "Loss: 0.001857857801951468\n",
      "Loss: 0.0018609956605359912\n",
      "Loss: 0.0016484131338074803\n",
      "Loss: 0.0015596243320032954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 48/63 [1:13:20<21:50, 87.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.001880966010503471\n",
      "Epoch: 48\n",
      "Loss: 0.001529333763755858\n",
      "Loss: 0.0020251136738806963\n",
      "Loss: 0.001946199918165803\n",
      "Loss: 0.0019060794729739428\n",
      "Loss: 0.002059479244053364\n",
      "Loss: 0.0019425494829192758\n",
      "Loss: 0.001750874798744917\n",
      "Loss: 0.0016698200488463044\n",
      "Loss: 0.001951479585841298\n",
      "Loss: 0.0020327549427747726\n",
      "Loss: 0.0019427365623414516\n",
      "Loss: 0.0018992744153365493\n",
      "Loss: 0.001756473327986896\n",
      "Loss: 0.0019271980272606015\n",
      "Loss: 0.0020208244677633047\n",
      "Loss: 0.0018678379710763693\n",
      "Loss: 0.0018415047088637948\n",
      "Loss: 0.0015608933754265308\n",
      "Loss: 0.001956280553713441\n",
      "Loss: 0.0019400957971811295\n",
      "Loss: 0.0020653060637414455\n",
      "Loss: 0.0016153567703440785\n",
      "Loss: 0.0017937369411811233\n",
      "Loss: 0.0015862868167459965\n",
      "Loss: 0.0018158485181629658\n",
      "Loss: 0.001878822222352028\n",
      "Loss: 0.0015876442193984985\n",
      "Loss: 0.0020706146024167538\n",
      "Loss: 0.0016935262829065323\n",
      "Loss: 0.0018690539291128516\n",
      "Loss: 0.001668029697611928\n",
      "Loss: 0.0017708748346194625\n",
      "Loss: 0.00188346893992275\n",
      "Loss: 0.0018460892606526613\n",
      "Loss: 0.0018830838380381465\n",
      "Loss: 0.001904259086586535\n",
      "Loss: 0.0019395265262573957\n",
      "Loss: 0.0016003225464373827\n",
      "Loss: 0.0015367894666269422\n",
      "Loss: 0.0019949625711888075\n",
      "Loss: 0.0019216189393773675\n",
      "Loss: 0.0019054965814575553\n",
      "Loss: 0.0018759644590318203\n",
      "Loss: 0.001923931878991425\n",
      "Loss: 0.0017723407363519073\n",
      "Loss: 0.0016879091272130609\n",
      "Loss: 0.0017022533575072885\n",
      "Loss: 0.0017146749887615442\n",
      "Loss: 0.0015687982086092234\n",
      "Loss: 0.001765399589203298\n",
      "Loss: 0.0018698738422244787\n",
      "Loss: 0.001825630315579474\n",
      "Loss: 0.0018390490440651774\n",
      "Loss: 0.0019854684360325336\n",
      "Loss: 0.001704377238638699\n",
      "Loss: 0.001749362563714385\n",
      "Loss: 0.001872027525678277\n",
      "Loss: 0.0017970340559259057\n",
      "Loss: 0.0018130546668544412\n",
      "Loss: 0.001762374653480947\n",
      "Loss: 0.0017222979804500937\n",
      "Loss: 0.0018554842099547386\n",
      "Loss: 0.0017944216961041093\n",
      "Loss: 0.0016146384878084064\n",
      "Loss: 0.001967664808034897\n",
      "Loss: 0.0018387585878372192\n",
      "Loss: 0.0017729633254930377\n",
      "Loss: 0.0016420503379777074\n",
      "Loss: 0.001930287224240601\n",
      "Loss: 0.001921948161907494\n",
      "Loss: 0.0017525221919640899\n",
      "Loss: 0.0016197168733924627\n",
      "Loss: 0.0017656005220487714\n",
      "Loss: 0.0017051524482667446\n",
      "Loss: 0.001871781307272613\n",
      "Loss: 0.0018533046822994947\n",
      "Loss: 0.001819354365579784\n",
      "Loss: 0.0018744294065982103\n",
      "Loss: 0.0017537011299282312\n",
      "Loss: 0.0018611311679705977\n",
      "Loss: 0.0017597569385543466\n",
      "Loss: 0.0016451464034616947\n",
      "Loss: 0.0019066978711634874\n",
      "Loss: 0.0019472758285701275\n",
      "Loss: 0.0017324186628684402\n",
      "Loss: 0.0016780595760792494\n",
      "Loss: 0.0017261315369978547\n",
      "Loss: 0.0018999684834852815\n",
      "Loss: 0.0018453323282301426\n",
      "Loss: 0.0017900281818583608\n",
      "Loss: 0.001696513150818646\n",
      "Loss: 0.0016838477458804846\n",
      "Loss: 0.0017512957565486431\n",
      "Loss: 0.001910236431285739\n",
      "Loss: 0.0017412328161299229\n",
      "Loss: 0.0017306834924966097\n",
      "Loss: 0.0019812125246971846\n",
      "Loss: 0.0015791330952197313\n",
      "Loss: 0.0019399556331336498\n",
      "Loss: 0.001507433014921844\n",
      "Loss: 0.0018415264785289764\n",
      "Loss: 0.001888923579826951\n",
      "Loss: 0.0016923199873417616\n",
      "Loss: 0.0018178594764322042\n",
      "Loss: 0.0017304293578490615\n",
      "Loss: 0.001820376142859459\n",
      "Loss: 0.0017944712890312076\n",
      "Loss: 0.0019170871237292886\n",
      "Loss: 0.0017088038148358464\n",
      "Loss: 0.0017930757021531463\n",
      "Loss: 0.0018230570713058114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 49/63 [1:14:45<20:13, 86.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.001882544718682766\n",
      "Epoch: 49\n",
      "Loss: 0.001687808777205646\n",
      "Loss: 0.001945238676853478\n",
      "Loss: 0.00179202388972044\n",
      "Loss: 0.0017013837350532413\n",
      "Loss: 0.001689736731350422\n",
      "Loss: 0.001580726122483611\n",
      "Loss: 0.0017048839945346117\n",
      "Loss: 0.0017337137833237648\n",
      "Loss: 0.0017771932762116194\n",
      "Loss: 0.00143233104608953\n",
      "Loss: 0.0017803022637963295\n",
      "Loss: 0.0016509807901456952\n",
      "Loss: 0.0016267044702544808\n",
      "Loss: 0.0016624291893094778\n",
      "Loss: 0.0018721312517300248\n",
      "Loss: 0.0016183463158085942\n",
      "Loss: 0.0016061706701293588\n",
      "Loss: 0.0015930513618513942\n",
      "Loss: 0.0018113900441676378\n",
      "Loss: 0.0017019741935655475\n",
      "Loss: 0.001769138965755701\n",
      "Loss: 0.001811312627978623\n",
      "Loss: 0.0019458469469100237\n",
      "Loss: 0.0016298705013468862\n",
      "Loss: 0.0016961584333330393\n",
      "Loss: 0.0018299894873052835\n",
      "Loss: 0.001755744800902903\n",
      "Loss: 0.0013934350572526455\n",
      "Loss: 0.001571422559209168\n",
      "Loss: 0.0018041707808151841\n",
      "Loss: 0.0018372188787907362\n",
      "Loss: 0.0017566491151228547\n",
      "Loss: 0.0017222000751644373\n",
      "Loss: 0.0016286448808386922\n",
      "Loss: 0.0017813817830756307\n",
      "Loss: 0.0017796804895624518\n",
      "Loss: 0.0017857790226116776\n",
      "Loss: 0.0017626202898100019\n",
      "Loss: 0.001702618319541216\n",
      "Loss: 0.0016780032310634851\n",
      "Loss: 0.0017495851498097181\n",
      "Loss: 0.0017460170201957226\n",
      "Loss: 0.0017812715377658606\n",
      "Loss: 0.0016840430907905102\n",
      "Loss: 0.0017976445378735662\n",
      "Loss: 0.0015471583465114236\n",
      "Loss: 0.001618820708245039\n",
      "Loss: 0.0017412479501217604\n",
      "Loss: 0.001616697758436203\n",
      "Loss: 0.0017650654772296548\n",
      "Loss: 0.0018968579825013876\n",
      "Loss: 0.0016865191282704473\n",
      "Loss: 0.0017481426475569606\n",
      "Loss: 0.0018179025501012802\n",
      "Loss: 0.0017295320285484195\n",
      "Loss: 0.0015554676065221429\n",
      "Loss: 0.0014342685462906957\n",
      "Loss: 0.0017982638673856854\n",
      "Loss: 0.0018514577532187104\n",
      "Loss: 0.001775228651240468\n",
      "Loss: 0.001662957714870572\n",
      "Loss: 0.0017326421802863479\n",
      "Loss: 0.0017178523121401668\n",
      "Loss: 0.0018042994197458029\n",
      "Loss: 0.001426275004632771\n",
      "Loss: 0.001673426479101181\n",
      "Loss: 0.0016974130412563682\n",
      "Loss: 0.0017773694125935435\n",
      "Loss: 0.0016893213614821434\n",
      "Loss: 0.0016194379422813654\n",
      "Loss: 0.0017493460327386856\n",
      "Loss: 0.001592376152984798\n",
      "Loss: 0.0016334261745214462\n",
      "Loss: 0.0017697965959087014\n",
      "Loss: 0.0016275037778541446\n",
      "Loss: 0.0017346719978377223\n",
      "Loss: 0.00174564344342798\n",
      "Loss: 0.00160559406504035\n",
      "Loss: 0.0015669240383431315\n",
      "Loss: 0.0015973764238879085\n",
      "Loss: 0.0018476387485861778\n",
      "Loss: 0.001774905831553042\n",
      "Loss: 0.0014806217513978481\n",
      "Loss: 0.0015371345216408372\n",
      "Loss: 0.0016649194294586778\n",
      "Loss: 0.0017817888874560595\n",
      "Loss: 0.0017221317393705249\n",
      "Loss: 0.0016857422888278961\n",
      "Loss: 0.0017341130878776312\n",
      "Loss: 0.0014023506082594395\n",
      "Loss: 0.001701219822280109\n",
      "Loss: 0.0014574191300198436\n",
      "Loss: 0.0016326365293934941\n",
      "Loss: 0.0018533187685534358\n",
      "Loss: 0.0018164512002840638\n",
      "Loss: 0.0016901688650250435\n",
      "Loss: 0.0016854389104992151\n",
      "Loss: 0.0017527298768982291\n",
      "Loss: 0.0016435752622783184\n",
      "Loss: 0.0015584438806399703\n",
      "Loss: 0.0017084989231079817\n",
      "Loss: 0.001751667819917202\n",
      "Loss: 0.0016970002325251698\n",
      "Loss: 0.0016409638337790966\n",
      "Loss: 0.0015373333590105176\n",
      "Loss: 0.0017713470151647925\n",
      "Loss: 0.001825243467465043\n",
      "Loss: 0.001692738849669695\n",
      "Loss: 0.0016555368201807141\n",
      "Loss: 0.0017487796721979976\n",
      "Loss: 0.0018276814371347427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 50/63 [1:16:11<18:41, 86.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.001616071560420096\n",
      "Epoch: 50\n",
      "Loss: 0.0018281679367646575\n",
      "Loss: 0.001741070649586618\n",
      "Loss: 0.0016926986863836646\n",
      "Loss: 0.0018541790777817369\n",
      "Loss: 0.0016792290844023228\n",
      "Loss: 0.00154738815035671\n",
      "Loss: 0.0016855226131156087\n",
      "Loss: 0.0016956008039414883\n",
      "Loss: 0.0017095999792218208\n",
      "Loss: 0.0013604959240183234\n",
      "Loss: 0.0017226474592462182\n",
      "Loss: 0.0015051253139972687\n",
      "Loss: 0.0017936620861291885\n",
      "Loss: 0.0016996616031974554\n",
      "Loss: 0.001531106885522604\n",
      "Loss: 0.0018059496069326997\n",
      "Loss: 0.0017777744214981794\n",
      "Loss: 0.0014777507167309523\n",
      "Loss: 0.001693065627478063\n",
      "Loss: 0.0015471335500478745\n",
      "Loss: 0.001662367838434875\n",
      "Loss: 0.0017725554062053561\n",
      "Loss: 0.0016678966348990798\n",
      "Loss: 0.0015019861748442054\n",
      "Loss: 0.0016606850549578667\n",
      "Loss: 0.0016239030519500375\n",
      "Loss: 0.0015972766559571028\n",
      "Loss: 0.0014215137343853712\n",
      "Loss: 0.001530689769424498\n",
      "Loss: 0.001439905259758234\n",
      "Loss: 0.0014998936094343662\n",
      "Loss: 0.0017738458700478077\n",
      "Loss: 0.0017003018874675035\n",
      "Loss: 0.001730566960759461\n",
      "Loss: 0.00169098237529397\n",
      "Loss: 0.0017068689921870828\n",
      "Loss: 0.0014560710405930877\n",
      "Loss: 0.0016315861139446497\n",
      "Loss: 0.0017099138349294662\n",
      "Loss: 0.001774897100403905\n",
      "Loss: 0.0015907913912087679\n",
      "Loss: 0.0016881157644093037\n",
      "Loss: 0.0016862488118931651\n",
      "Loss: 0.0014942247653380036\n",
      "Loss: 0.0016652746126055717\n",
      "Loss: 0.001455577090382576\n",
      "Loss: 0.0016572497552260756\n",
      "Loss: 0.0016745527973398566\n",
      "Loss: 0.0016851647524163127\n",
      "Loss: 0.001668713055551052\n",
      "Loss: 0.0016207079170271754\n",
      "Loss: 0.0014340217458084226\n",
      "Loss: 0.0016990031581372023\n",
      "Loss: 0.0015489151701331139\n",
      "Loss: 0.0016193519113585353\n",
      "Loss: 0.0015438636764883995\n",
      "Loss: 0.0015146351652219892\n",
      "Loss: 0.0016268055187538266\n",
      "Loss: 0.001576068694703281\n",
      "Loss: 0.001562400022521615\n",
      "Loss: 0.0014653584221377969\n",
      "Loss: 0.0013537871418520808\n",
      "Loss: 0.0017347289249300957\n",
      "Loss: 0.0015302733518183231\n",
      "Loss: 0.0016615943750366569\n",
      "Loss: 0.0017526381416246295\n",
      "Loss: 0.001653004321269691\n",
      "Loss: 0.0016456590965390205\n",
      "Loss: 0.001444381196051836\n",
      "Loss: 0.0013234515208750963\n",
      "Loss: 0.0015922297025099397\n",
      "Loss: 0.001606182660907507\n",
      "Loss: 0.0013981136726215482\n",
      "Loss: 0.0016193516785278916\n",
      "Loss: 0.001379570341669023\n",
      "Loss: 0.0015528482617810369\n",
      "Loss: 0.001715557067655027\n",
      "Loss: 0.0015935659175738692\n",
      "Loss: 0.0016288850456476212\n",
      "Loss: 0.0013686636229977012\n",
      "Loss: 0.0016102857189252973\n",
      "Loss: 0.001538111362606287\n",
      "Loss: 0.001738163409754634\n",
      "Loss: 0.0013656876981258392\n",
      "Loss: 0.001634209998883307\n",
      "Loss: 0.0015705523546785116\n",
      "Loss: 0.0015614794101566076\n",
      "Loss: 0.0015961849130690098\n",
      "Loss: 0.0017084508435800672\n",
      "Loss: 0.00156404881272465\n",
      "Loss: 0.0014886087737977505\n",
      "Loss: 0.001522728125564754\n",
      "Loss: 0.0015756699722260237\n",
      "Loss: 0.001241195946931839\n",
      "Loss: 0.0016328446799889207\n",
      "Loss: 0.0015729987062513828\n",
      "Loss: 0.001476094126701355\n",
      "Loss: 0.0015293537871912122\n",
      "Loss: 0.001538098556920886\n",
      "Loss: 0.0016039771726354957\n",
      "Loss: 0.001611432177014649\n",
      "Loss: 0.0016774052055552602\n",
      "Loss: 0.0012641632929444313\n",
      "Loss: 0.0016594135668128729\n",
      "Loss: 0.0014939397806301713\n",
      "Loss: 0.001712198369204998\n",
      "Loss: 0.0015265952097252011\n",
      "Loss: 0.001632198109291494\n",
      "Loss: 0.00165013805963099\n",
      "Loss: 0.0016368833603337407\n",
      "Loss: 0.0013555199839174747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 51/63 [1:17:36<17:12, 86.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0014533937210217118\n",
      "Epoch: 51\n",
      "Loss: 0.0015976883005350828\n",
      "Loss: 0.00154268485493958\n",
      "Loss: 0.0015139958122745156\n",
      "Loss: 0.001413704245351255\n",
      "Loss: 0.00134184118360281\n",
      "Loss: 0.0016001276671886444\n",
      "Loss: 0.001696596504189074\n",
      "Loss: 0.0017212566453963518\n",
      "Loss: 0.001635116059333086\n",
      "Loss: 0.0016746503533795476\n",
      "Loss: 0.0013513480080291629\n",
      "Loss: 0.0016189980087801814\n",
      "Loss: 0.001449623960070312\n",
      "Loss: 0.0016532636946067214\n",
      "Loss: 0.0015044156461954117\n",
      "Loss: 0.0017543259309604764\n",
      "Loss: 0.0015850006602704525\n",
      "Loss: 0.0015631761634722352\n",
      "Loss: 0.0016986157279461622\n",
      "Loss: 0.0015451678773388267\n",
      "Loss: 0.0015956166898831725\n",
      "Loss: 0.0015476667322218418\n",
      "Loss: 0.0013178772060200572\n",
      "Loss: 0.0015119019662961364\n",
      "Loss: 0.0015421974239870906\n",
      "Loss: 0.001560452627018094\n",
      "Loss: 0.0014339282643049955\n",
      "Loss: 0.0015693886671215296\n",
      "Loss: 0.0014153787633404136\n",
      "Loss: 0.0015431685606017709\n",
      "Loss: 0.0016470872797071934\n",
      "Loss: 0.0016636442160233855\n",
      "Loss: 0.0014893112238496542\n",
      "Loss: 0.0015827995957806706\n",
      "Loss: 0.0014661038294434547\n",
      "Loss: 0.0016172550385817885\n",
      "Loss: 0.001489172806032002\n",
      "Loss: 0.001648310455493629\n",
      "Loss: 0.0015374177601188421\n",
      "Loss: 0.001129601150751114\n",
      "Loss: 0.0014996835961937904\n",
      "Loss: 0.0015085310442373157\n",
      "Loss: 0.0012218862539157271\n",
      "Loss: 0.0014888710575178266\n",
      "Loss: 0.0015893682138994336\n",
      "Loss: 0.0014124125009402633\n",
      "Loss: 0.00127960997633636\n",
      "Loss: 0.0014249375090003014\n",
      "Loss: 0.0015026573091745377\n",
      "Loss: 0.0016711991047486663\n",
      "Loss: 0.001361720496788621\n",
      "Loss: 0.0015399925177916884\n",
      "Loss: 0.0014579999260604382\n",
      "Loss: 0.0015244646929204464\n",
      "Loss: 0.0013762122252956033\n",
      "Loss: 0.0014391196891665459\n",
      "Loss: 0.0014377983752638102\n",
      "Loss: 0.001529335044324398\n",
      "Loss: 0.0014199961442500353\n",
      "Loss: 0.0016684916336089373\n",
      "Loss: 0.0015264523681253195\n",
      "Loss: 0.001403510570526123\n",
      "Loss: 0.0013837611768394709\n",
      "Loss: 0.001454508281312883\n",
      "Loss: 0.0014016223140060902\n",
      "Loss: 0.0014884575502946973\n",
      "Loss: 0.0014593461528420448\n",
      "Loss: 0.0013351677916944027\n",
      "Loss: 0.0015811178600415587\n",
      "Loss: 0.001598165719769895\n",
      "Loss: 0.001508458168245852\n",
      "Loss: 0.0012634508311748505\n",
      "Loss: 0.0015760035021230578\n",
      "Loss: 0.0012827400350943208\n",
      "Loss: 0.0016011807601898909\n",
      "Loss: 0.0014711907133460045\n",
      "Loss: 0.001548050087876618\n",
      "Loss: 0.0015411892673000693\n",
      "Loss: 0.0013689730549231172\n",
      "Loss: 0.001386616611853242\n",
      "Loss: 0.0014352670405060053\n",
      "Loss: 0.0015182638308033347\n",
      "Loss: 0.0015545050846412778\n",
      "Loss: 0.001527633285149932\n",
      "Loss: 0.0013902747305110097\n",
      "Loss: 0.0014948986936360598\n",
      "Loss: 0.0014377295738086104\n",
      "Loss: 0.0015311046736314893\n",
      "Loss: 0.0015430267667397857\n",
      "Loss: 0.0014487763401120901\n",
      "Loss: 0.0013879433972761035\n",
      "Loss: 0.0014638995053246617\n",
      "Loss: 0.0014862430980429053\n",
      "Loss: 0.0014015502529218793\n",
      "Loss: 0.0015953915426507592\n",
      "Loss: 0.001525018597021699\n",
      "Loss: 0.0014024947304278612\n",
      "Loss: 0.0014038186054676771\n",
      "Loss: 0.0013484935043379664\n",
      "Loss: 0.0014973179204389453\n",
      "Loss: 0.001519576646387577\n",
      "Loss: 0.0015812884084880352\n",
      "Loss: 0.001506522879935801\n",
      "Loss: 0.0015941467136144638\n",
      "Loss: 0.0015673451125621796\n",
      "Loss: 0.0015094497939571738\n",
      "Loss: 0.0014692299300804734\n",
      "Loss: 0.0015474054962396622\n",
      "Loss: 0.0014852372696623206\n",
      "Loss: 0.0012619317276403308\n",
      "Loss: 0.0016394804697483778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 52/63 [1:19:02<15:44, 85.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0014528816100209951\n",
      "Epoch: 52\n",
      "Loss: 0.0013400495517998934\n",
      "Loss: 0.00148570176679641\n",
      "Loss: 0.0014494594652205706\n",
      "Loss: 0.001474010874517262\n",
      "Loss: 0.0012661906657740474\n",
      "Loss: 0.0015883976593613625\n",
      "Loss: 0.0012544600758701563\n",
      "Loss: 0.0014456731732934713\n",
      "Loss: 0.0014140369603410363\n",
      "Loss: 0.0013423928758129478\n",
      "Loss: 0.0013052914291620255\n",
      "Loss: 0.001468235976062715\n",
      "Loss: 0.0014541780110448599\n",
      "Loss: 0.0014680165331810713\n",
      "Loss: 0.0013205019058659673\n",
      "Loss: 0.0012082824250683188\n",
      "Loss: 0.0013295928947627544\n",
      "Loss: 0.0015332381008192897\n",
      "Loss: 0.0014686607755720615\n",
      "Loss: 0.001424223417416215\n",
      "Loss: 0.0014422307722270489\n",
      "Loss: 0.001251580542884767\n",
      "Loss: 0.001501428079791367\n",
      "Loss: 0.0014096625382080674\n",
      "Loss: 0.0012838930124416947\n",
      "Loss: 0.0015804675640538335\n",
      "Loss: 0.0013928742846474051\n",
      "Loss: 0.001525316620245576\n",
      "Loss: 0.0015412485226988792\n",
      "Loss: 0.0014384905807673931\n",
      "Loss: 0.0015639166813343763\n",
      "Loss: 0.0014341988135129213\n",
      "Loss: 0.001389063079841435\n",
      "Loss: 0.0013630080502480268\n",
      "Loss: 0.0013757441192865372\n",
      "Loss: 0.0014798580668866634\n",
      "Loss: 0.0014770177658647299\n",
      "Loss: 0.0014362592482939363\n",
      "Loss: 0.0013915669405832887\n",
      "Loss: 0.0014499019598588347\n",
      "Loss: 0.0013099712086841464\n",
      "Loss: 0.0015243047382682562\n",
      "Loss: 0.0015561176696792245\n",
      "Loss: 0.0012101471656933427\n",
      "Loss: 0.0015834540827199817\n",
      "Loss: 0.0015088581712916493\n",
      "Loss: 0.0014470822643488646\n",
      "Loss: 0.0011834630277007818\n",
      "Loss: 0.001387274358421564\n",
      "Loss: 0.0014981471467763186\n",
      "Loss: 0.0014657776337116957\n",
      "Loss: 0.0013189823366701603\n",
      "Loss: 0.001298826769925654\n",
      "Loss: 0.0013007859233766794\n",
      "Loss: 0.001481253537349403\n",
      "Loss: 0.0015164146898314357\n",
      "Loss: 0.001493303687311709\n",
      "Loss: 0.0014742857310920954\n",
      "Loss: 0.001436401973478496\n",
      "Loss: 0.0014008065918460488\n",
      "Loss: 0.0013380785239860415\n",
      "Loss: 0.0015533540863543749\n",
      "Loss: 0.0013148610014468431\n",
      "Loss: 0.0012856663670390844\n",
      "Loss: 0.0013092353474348783\n",
      "Loss: 0.0014562503201887012\n",
      "Loss: 0.0012875235406681895\n",
      "Loss: 0.0014550541527569294\n",
      "Loss: 0.0013068333500996232\n",
      "Loss: 0.0013483301736414433\n",
      "Loss: 0.0012716350611299276\n",
      "Loss: 0.0014197417767718434\n",
      "Loss: 0.001464323839172721\n",
      "Loss: 0.0013410039246082306\n",
      "Loss: 0.001443394459784031\n",
      "Loss: 0.001441070344299078\n",
      "Loss: 0.0013912381837144494\n",
      "Loss: 0.0013446197845041752\n",
      "Loss: 0.0015256779734045267\n",
      "Loss: 0.0015154288848862052\n",
      "Loss: 0.0014818788040429354\n",
      "Loss: 0.001424400950782001\n",
      "Loss: 0.0013657562667503953\n",
      "Loss: 0.0014587764162570238\n",
      "Loss: 0.0014540506526827812\n",
      "Loss: 0.001359452842734754\n",
      "Loss: 0.001461943262256682\n",
      "Loss: 0.001470141694881022\n",
      "Loss: 0.0014022310497239232\n",
      "Loss: 0.001366526004858315\n",
      "Loss: 0.0013415698194876313\n",
      "Loss: 0.0014152070507407188\n",
      "Loss: 0.0014037895016372204\n",
      "Loss: 0.0014531068736687303\n",
      "Loss: 0.0015476730186492205\n",
      "Loss: 0.0014474538620561361\n",
      "Loss: 0.0014472946058958769\n",
      "Loss: 0.0012712925672531128\n",
      "Loss: 0.001498861936852336\n",
      "Loss: 0.0014058322412893176\n",
      "Loss: 0.0012178458273410797\n",
      "Loss: 0.0014130165800452232\n",
      "Loss: 0.001402963069267571\n",
      "Loss: 0.0012745640706270933\n",
      "Loss: 0.0011827116832137108\n",
      "Loss: 0.0013693474465981126\n",
      "Loss: 0.0013621526304632425\n",
      "Loss: 0.0014405477559193969\n",
      "Loss: 0.0013688631588593125\n",
      "Loss: 0.001406038529239595\n",
      "Loss: 0.0014858557842671871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 53/63 [1:20:27<14:17, 85.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0013156505301594734\n",
      "Epoch: 53\n",
      "Loss: 0.0012829862534999847\n",
      "Loss: 0.0012993874261155725\n",
      "Loss: 0.001352876191958785\n",
      "Loss: 0.0013226254377514124\n",
      "Loss: 0.0013787061907351017\n",
      "Loss: 0.0014839336508885026\n",
      "Loss: 0.0013224524445831776\n",
      "Loss: 0.0014441683888435364\n",
      "Loss: 0.0014057529624551535\n",
      "Loss: 0.0014420064399018884\n",
      "Loss: 0.0013337160926312208\n",
      "Loss: 0.0014128352049738169\n",
      "Loss: 0.001317979651503265\n",
      "Loss: 0.001395639032125473\n",
      "Loss: 0.001358911395072937\n",
      "Loss: 0.0014196254778653383\n",
      "Loss: 0.0014290949329733849\n",
      "Loss: 0.0012470330111682415\n",
      "Loss: 0.0013024642830714583\n",
      "Loss: 0.0013572630705311894\n",
      "Loss: 0.0014275735011324286\n",
      "Loss: 0.0011928996536880732\n",
      "Loss: 0.0015138837043195963\n",
      "Loss: 0.0014717521844431758\n",
      "Loss: 0.0014659318840131164\n",
      "Loss: 0.001418353640474379\n",
      "Loss: 0.0013781917514279485\n",
      "Loss: 0.0012381753185763955\n",
      "Loss: 0.0013980206567794085\n",
      "Loss: 0.0013844856293871999\n",
      "Loss: 0.001367479795590043\n",
      "Loss: 0.0011419326765462756\n",
      "Loss: 0.0013618895318359137\n",
      "Loss: 0.0013424665667116642\n",
      "Loss: 0.0014010254526510835\n",
      "Loss: 0.001203067833557725\n",
      "Loss: 0.001389321289025247\n",
      "Loss: 0.0011829965515062213\n",
      "Loss: 0.0014023205731064081\n",
      "Loss: 0.0013359630247578025\n",
      "Loss: 0.0014505918370559812\n",
      "Loss: 0.0013927372638136148\n",
      "Loss: 0.0014020754024386406\n",
      "Loss: 0.001217000768519938\n",
      "Loss: 0.0013151121092960238\n",
      "Loss: 0.0014004935510456562\n",
      "Loss: 0.0014488577144220471\n",
      "Loss: 0.00121100596152246\n",
      "Loss: 0.0012835043016821146\n",
      "Loss: 0.0013400709722191095\n",
      "Loss: 0.0013527468545362353\n",
      "Loss: 0.0013602146646007895\n",
      "Loss: 0.001349752419628203\n",
      "Loss: 0.0013869550311937928\n",
      "Loss: 0.0014153567608445883\n",
      "Loss: 0.0013070040149614215\n",
      "Loss: 0.0013006130466237664\n",
      "Loss: 0.0013698786497116089\n",
      "Loss: 0.0012963047483935952\n",
      "Loss: 0.0014809638960286975\n",
      "Loss: 0.0014046283904463053\n",
      "Loss: 0.001248020213097334\n",
      "Loss: 0.0012911721132695675\n",
      "Loss: 0.0013972739689052105\n",
      "Loss: 0.0013189885066822171\n",
      "Loss: 0.0012958712177351117\n",
      "Loss: 0.0010754053946584463\n",
      "Loss: 0.0013583976542577147\n",
      "Loss: 0.001280470285564661\n",
      "Loss: 0.0013377052964642644\n",
      "Loss: 0.0013575892662629485\n",
      "Loss: 0.0013971212320029736\n",
      "Loss: 0.0012685599504038692\n",
      "Loss: 0.0013024361105635762\n",
      "Loss: 0.0013666209997609258\n",
      "Loss: 0.0013480663765221834\n",
      "Loss: 0.001286703278310597\n",
      "Loss: 0.0013811498647555709\n",
      "Loss: 0.0012530098902061582\n",
      "Loss: 0.0011539158876985312\n",
      "Loss: 0.0014394094469025731\n",
      "Loss: 0.0012603305513039231\n",
      "Loss: 0.001359744230285287\n",
      "Loss: 0.0011885104468092322\n",
      "Loss: 0.0013161092065274715\n",
      "Loss: 0.0012543767224997282\n",
      "Loss: 0.0012069949880242348\n",
      "Loss: 0.0013255028752610087\n",
      "Loss: 0.0013111515436321497\n",
      "Loss: 0.0011459427187219262\n",
      "Loss: 0.0012825267622247338\n",
      "Loss: 0.0012212747242301702\n",
      "Loss: 0.0014511721674352884\n",
      "Loss: 0.0013208917807787657\n",
      "Loss: 0.0013445108197629452\n",
      "Loss: 0.0011234619887545705\n",
      "Loss: 0.0012076471466571093\n",
      "Loss: 0.0013881124323233962\n",
      "Loss: 0.0011948435567319393\n",
      "Loss: 0.0012245854595676064\n",
      "Loss: 0.0012615679297596216\n",
      "Loss: 0.0014190608635544777\n",
      "Loss: 0.001321399467997253\n",
      "Loss: 0.0011890212772414088\n",
      "Loss: 0.0013267509639263153\n",
      "Loss: 0.0012170770205557346\n",
      "Loss: 0.0013260376872494817\n",
      "Loss: 0.0010954821482300758\n",
      "Loss: 0.0011377166956663132\n",
      "Loss: 0.0012811549240723252\n",
      "Loss: 0.0012087017530575395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 54/63 [1:21:53<12:51, 85.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0013269329210743308\n",
      "Epoch: 54\n",
      "Loss: 0.0013231763150542974\n",
      "Loss: 0.001428461167961359\n",
      "Loss: 0.0011898938100785017\n",
      "Loss: 0.00124759238678962\n",
      "Loss: 0.0013947467086836696\n",
      "Loss: 0.0013722621370106936\n",
      "Loss: 0.0013104722602292895\n",
      "Loss: 0.0013946836115792394\n",
      "Loss: 0.0010467335814610124\n",
      "Loss: 0.0012416639365255833\n",
      "Loss: 0.0013096117181703448\n",
      "Loss: 0.0012853308580815792\n",
      "Loss: 0.0013437472516670823\n",
      "Loss: 0.0012227693805471063\n",
      "Loss: 0.001315180561505258\n",
      "Loss: 0.0012390905758365989\n",
      "Loss: 0.0010643688729032874\n",
      "Loss: 0.0012556883739307523\n",
      "Loss: 0.0012945076450705528\n",
      "Loss: 0.0011928848689422011\n",
      "Loss: 0.001321909367106855\n",
      "Loss: 0.001277259667403996\n",
      "Loss: 0.0012023844756186008\n",
      "Loss: 0.0013240156695246696\n",
      "Loss: 0.0013196344953030348\n",
      "Loss: 0.0012423272710293531\n",
      "Loss: 0.0011846345150843263\n",
      "Loss: 0.0012853745138272643\n",
      "Loss: 0.0012158110039308667\n",
      "Loss: 0.001294985762797296\n",
      "Loss: 0.0013491121353581548\n",
      "Loss: 0.001203425694257021\n",
      "Loss: 0.0011458004591986537\n",
      "Loss: 0.0012378121027722955\n",
      "Loss: 0.0013982988893985748\n",
      "Loss: 0.0012439253041520715\n",
      "Loss: 0.0013008096721023321\n",
      "Loss: 0.0011091845808550715\n",
      "Loss: 0.0012870337814092636\n",
      "Loss: 0.0011582134757190943\n",
      "Loss: 0.0011370399734005332\n",
      "Loss: 0.0012221344513818622\n",
      "Loss: 0.001341021852567792\n",
      "Loss: 0.0010825460776686668\n",
      "Loss: 0.0011850916780531406\n",
      "Loss: 0.0013708466431125998\n",
      "Loss: 0.0012655858881771564\n",
      "Loss: 0.00118370377458632\n",
      "Loss: 0.0012506007915362716\n",
      "Loss: 0.0012674471363425255\n",
      "Loss: 0.0011241058818995953\n",
      "Loss: 0.001201281207613647\n",
      "Loss: 0.0013937180629000068\n",
      "Loss: 0.0012949284864589572\n",
      "Loss: 0.0010589399607852101\n",
      "Loss: 0.0012463897000998259\n",
      "Loss: 0.0011634838301688433\n",
      "Loss: 0.0013137304922565818\n",
      "Loss: 0.0012540448224171996\n",
      "Loss: 0.001098579028621316\n",
      "Loss: 0.0012916610576212406\n",
      "Loss: 0.0012884971220046282\n",
      "Loss: 0.0011909202439710498\n",
      "Loss: 0.001239686505869031\n",
      "Loss: 0.001346913748420775\n",
      "Loss: 0.0012514443369582295\n",
      "Loss: 0.0012932720128446817\n",
      "Loss: 0.0011653868714347482\n",
      "Loss: 0.0010938100749626756\n",
      "Loss: 0.0013501184294000268\n",
      "Loss: 0.0013619981473311782\n",
      "Loss: 0.0012201833305880427\n",
      "Loss: 0.0012683963868767023\n",
      "Loss: 0.0013648610329255462\n",
      "Loss: 0.001307931961491704\n",
      "Loss: 0.0012830976629629731\n",
      "Loss: 0.0011962869903072715\n",
      "Loss: 0.0009684122051112354\n",
      "Loss: 0.0011347192339599133\n",
      "Loss: 0.0012448141351342201\n",
      "Loss: 0.0013434920692816377\n",
      "Loss: 0.0013094014720991254\n",
      "Loss: 0.001339870155788958\n",
      "Loss: 0.001248056418262422\n",
      "Loss: 0.0012734082993119955\n",
      "Loss: 0.0013582509709522128\n",
      "Loss: 0.0011624280596151948\n",
      "Loss: 0.001245578983798623\n",
      "Loss: 0.001177889876998961\n",
      "Loss: 0.0012243152596056461\n",
      "Loss: 0.001330415136180818\n",
      "Loss: 0.001047221478074789\n",
      "Loss: 0.0012581299524754286\n",
      "Loss: 0.0012078700819984078\n",
      "Loss: 0.0012665941612794995\n",
      "Loss: 0.0012613466242328286\n",
      "Loss: 0.0011896080104634166\n",
      "Loss: 0.0011341870995238423\n",
      "Loss: 0.0011428574798628688\n",
      "Loss: 0.0012454501120373607\n",
      "Loss: 0.0012215025490149856\n",
      "Loss: 0.0013184761628508568\n",
      "Loss: 0.001282200450077653\n",
      "Loss: 0.0013576301280409098\n",
      "Loss: 0.0011578801786527038\n",
      "Loss: 0.0012511926470324397\n",
      "Loss: 0.0009879723656922579\n",
      "Loss: 0.0012657662155106664\n",
      "Loss: 0.0012934880796819925\n",
      "Loss: 0.001188696944154799\n",
      "Loss: 0.001108738360926509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 55/63 [1:23:18<11:25, 85.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0012274058535695076\n",
      "Epoch: 55\n",
      "Loss: 0.001006453880108893\n",
      "Loss: 0.0011851016897708178\n",
      "Loss: 0.0011752450373023748\n",
      "Loss: 0.0011004109401255846\n",
      "Loss: 0.0013442524941638112\n",
      "Loss: 0.0011226097121834755\n",
      "Loss: 0.0011266997316852212\n",
      "Loss: 0.0013111053267493844\n",
      "Loss: 0.001178555190563202\n",
      "Loss: 0.0011470927856862545\n",
      "Loss: 0.0011365305399522185\n",
      "Loss: 0.0012207460822537541\n",
      "Loss: 0.001183437299914658\n",
      "Loss: 0.001227229367941618\n",
      "Loss: 0.0011861063539981842\n",
      "Loss: 0.001031186431646347\n",
      "Loss: 0.001219411613419652\n",
      "Loss: 0.0012760593090206385\n",
      "Loss: 0.0013323192251846194\n",
      "Loss: 0.0012134644202888012\n",
      "Loss: 0.0012392536737024784\n",
      "Loss: 0.0012032095110043883\n",
      "Loss: 0.0011421340750530362\n",
      "Loss: 0.0011256657307967544\n",
      "Loss: 0.001094519393518567\n",
      "Loss: 0.001162765664048493\n",
      "Loss: 0.001103949616663158\n",
      "Loss: 0.0013046819949522614\n",
      "Loss: 0.0012434768723323941\n",
      "Loss: 0.0010429262183606625\n",
      "Loss: 0.0013231589691713452\n",
      "Loss: 0.001200455124489963\n",
      "Loss: 0.0011796306353062391\n",
      "Loss: 0.0010979046346619725\n",
      "Loss: 0.0011532425414770842\n",
      "Loss: 0.0013076257891952991\n",
      "Loss: 0.00122792215552181\n",
      "Loss: 0.0009546456858515739\n",
      "Loss: 0.001130610704421997\n",
      "Loss: 0.001116433646529913\n",
      "Loss: 0.001205490087158978\n",
      "Loss: 0.001080361078493297\n",
      "Loss: 0.0011366955004632473\n",
      "Loss: 0.0010982316453009844\n",
      "Loss: 0.0010491759749129415\n",
      "Loss: 0.0012177027529105544\n",
      "Loss: 0.0011879070661962032\n",
      "Loss: 0.001281503471545875\n",
      "Loss: 0.0011936314404010773\n",
      "Loss: 0.0012139012105762959\n",
      "Loss: 0.0012356434017419815\n",
      "Loss: 0.0012010030914098024\n",
      "Loss: 0.0011342852376401424\n",
      "Loss: 0.0011227559298276901\n",
      "Loss: 0.0011299974285066128\n",
      "Loss: 0.0012264776742085814\n",
      "Loss: 0.0011810129508376122\n",
      "Loss: 0.0012185624800622463\n",
      "Loss: 0.0012109297094866633\n",
      "Loss: 0.0011127907782793045\n",
      "Loss: 0.001215692493133247\n",
      "Loss: 0.0012388083850964904\n",
      "Loss: 0.0013031228445470333\n",
      "Loss: 0.0012054810067638755\n",
      "Loss: 0.001255515613593161\n",
      "Loss: 0.0010081172222271562\n",
      "Loss: 0.0012083654291927814\n",
      "Loss: 0.001226891065016389\n",
      "Loss: 0.0009875770192593336\n",
      "Loss: 0.0011709796963259578\n",
      "Loss: 0.001082562142983079\n",
      "Loss: 0.0011380795622244477\n",
      "Loss: 0.0011500428663566709\n",
      "Loss: 0.0012978045269846916\n",
      "Loss: 0.0011840243823826313\n",
      "Loss: 0.001178171020001173\n",
      "Loss: 0.0012013684026896954\n",
      "Loss: 0.0011363518424332142\n",
      "Loss: 0.0011876015923917294\n",
      "Loss: 0.0012042031157761812\n",
      "Loss: 0.0010064563248306513\n",
      "Loss: 0.0012238516937941313\n",
      "Loss: 0.0011426734272390604\n",
      "Loss: 0.001116215600632131\n",
      "Loss: 0.0011507158633321524\n",
      "Loss: 0.0011464711278676987\n",
      "Loss: 0.0011941255070269108\n",
      "Loss: 0.0010380868334323168\n",
      "Loss: 0.0011509408941492438\n",
      "Loss: 0.0010559369111433625\n",
      "Loss: 0.0011398342903703451\n",
      "Loss: 0.0008583235903643072\n",
      "Loss: 0.0010781007586047053\n",
      "Loss: 0.0012410719646140933\n",
      "Loss: 0.001211252762004733\n",
      "Loss: 0.00110082165338099\n",
      "Loss: 0.0011054066708311439\n",
      "Loss: 0.0011727820383384824\n",
      "Loss: 0.001258278964087367\n",
      "Loss: 0.0011277507292106748\n",
      "Loss: 0.0011381009826436639\n",
      "Loss: 0.0011963981669396162\n",
      "Loss: 0.0012804219732061028\n",
      "Loss: 0.001180805265903473\n",
      "Loss: 0.0012733062030747533\n",
      "Loss: 0.001178021659143269\n",
      "Loss: 0.0012028879718855023\n",
      "Loss: 0.001118015730753541\n",
      "Loss: 0.0011913696071133018\n",
      "Loss: 0.0012543359771370888\n",
      "Loss: 0.0012612378923222423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 56/63 [1:24:44<09:59, 85.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011073737405240536\n",
      "Epoch: 56\n",
      "Loss: 0.0012165082152932882\n",
      "Loss: 0.0010855153668671846\n",
      "Loss: 0.00099743134342134\n",
      "Loss: 0.001286194659769535\n",
      "Loss: 0.0011351094581186771\n",
      "Loss: 0.0011097113601863384\n",
      "Loss: 0.0009552296251058578\n",
      "Loss: 0.0010013958672061563\n",
      "Loss: 0.0011983220465481281\n",
      "Loss: 0.0011102774878963828\n",
      "Loss: 0.0010889391414821148\n",
      "Loss: 0.0009458023705519736\n",
      "Loss: 0.0011067399755120277\n",
      "Loss: 0.0011704099597409368\n",
      "Loss: 0.0009682178497314453\n",
      "Loss: 0.0011017995420843363\n",
      "Loss: 0.0011981567367911339\n",
      "Loss: 0.0010967175476253033\n",
      "Loss: 0.0011946003651246428\n",
      "Loss: 0.001071619102731347\n",
      "Loss: 0.00109821034129709\n",
      "Loss: 0.0010214264038950205\n",
      "Loss: 0.0011324853403493762\n",
      "Loss: 0.0009778958046808839\n",
      "Loss: 0.001235254923813045\n",
      "Loss: 0.0010938800405710936\n",
      "Loss: 0.000896905898116529\n",
      "Loss: 0.0011531495256349444\n",
      "Loss: 0.0011561494320631027\n",
      "Loss: 0.0011813470628112555\n",
      "Loss: 0.001067000674083829\n",
      "Loss: 0.0010274030501022935\n",
      "Loss: 0.0011881471145898104\n",
      "Loss: 0.001159094856120646\n",
      "Loss: 0.0012317921500653028\n",
      "Loss: 0.0012086039641872048\n",
      "Loss: 0.001065872493200004\n",
      "Loss: 0.0010713062947615981\n",
      "Loss: 0.001071196049451828\n",
      "Loss: 0.0011495616054162383\n",
      "Loss: 0.001054056454449892\n",
      "Loss: 0.0012133640702813864\n",
      "Loss: 0.0010861329501494765\n",
      "Loss: 0.0011332753347232938\n",
      "Loss: 0.0010712026851251721\n",
      "Loss: 0.0011608324712142348\n",
      "Loss: 0.001158044091425836\n",
      "Loss: 0.0011844550026580691\n",
      "Loss: 0.0010196790099143982\n",
      "Loss: 0.0011523402063176036\n",
      "Loss: 0.0010955362813547254\n",
      "Loss: 0.0011381516233086586\n",
      "Loss: 0.0012288677971810102\n",
      "Loss: 0.0010398757876828313\n",
      "Loss: 0.0011431726161390543\n",
      "Loss: 0.0011857416247949004\n",
      "Loss: 0.0010535238543525338\n",
      "Loss: 0.001150663709267974\n",
      "Loss: 0.0011548689799383283\n",
      "Loss: 0.0011999810812994838\n",
      "Loss: 0.001080580404959619\n",
      "Loss: 0.0009456747211515903\n",
      "Loss: 0.0009532468393445015\n",
      "Loss: 0.0011397076305001974\n",
      "Loss: 0.0011919926619157195\n",
      "Loss: 0.0011608528438955545\n",
      "Loss: 0.001232600654475391\n",
      "Loss: 0.0010380346793681383\n",
      "Loss: 0.0011295900912955403\n",
      "Loss: 0.0011436394415795803\n",
      "Loss: 0.0010211976477876306\n",
      "Loss: 0.0010190460598096251\n",
      "Loss: 0.001099294051527977\n",
      "Loss: 0.0010840597096830606\n",
      "Loss: 0.0011648056097328663\n",
      "Loss: 0.0011145026655867696\n",
      "Loss: 0.0011253056582063437\n",
      "Loss: 0.0010380648309364915\n",
      "Loss: 0.0010826500365510583\n",
      "Loss: 0.0010502049699425697\n",
      "Loss: 0.0010448150569573045\n",
      "Loss: 0.0011229055235162377\n",
      "Loss: 0.0011112437350675464\n",
      "Loss: 0.0011358123738318682\n",
      "Loss: 0.0011214101687073708\n",
      "Loss: 0.0011235527927055955\n",
      "Loss: 0.0009952445980161428\n",
      "Loss: 0.0010655800579115748\n",
      "Loss: 0.0010257726535201073\n",
      "Loss: 0.0010605060961097479\n",
      "Loss: 0.0011884099803864956\n",
      "Loss: 0.001051285769790411\n",
      "Loss: 0.001105758361518383\n",
      "Loss: 0.0009239162318408489\n",
      "Loss: 0.001049702288582921\n",
      "Loss: 0.0010314040118828416\n",
      "Loss: 0.0010340340668335557\n",
      "Loss: 0.0010658892570063472\n",
      "Loss: 0.001118993735872209\n",
      "Loss: 0.0011406614212319255\n",
      "Loss: 0.0010626119328662753\n",
      "Loss: 0.0011462218826636672\n",
      "Loss: 0.0010530238505452871\n",
      "Loss: 0.001051548053510487\n",
      "Loss: 0.0011066808365285397\n",
      "Loss: 0.001095332088880241\n",
      "Loss: 0.0011565672466531396\n",
      "Loss: 0.0011120685376226902\n",
      "Loss: 0.0010621597757562995\n",
      "Loss: 0.0011891050962731242\n",
      "Loss: 0.001022643526084721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 57/63 [1:26:09<08:33, 85.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009600478806532919\n",
      "Epoch: 57\n",
      "Loss: 0.0010888285469263792\n",
      "Loss: 0.001195119577459991\n",
      "Loss: 0.0009960165480151772\n",
      "Loss: 0.0009680780931375921\n",
      "Loss: 0.0010678068501874804\n",
      "Loss: 0.0008739614277146757\n",
      "Loss: 0.0011202017776668072\n",
      "Loss: 0.0010574377374723554\n",
      "Loss: 0.0010949367424473166\n",
      "Loss: 0.0011378871276974678\n",
      "Loss: 0.0009767359588295221\n",
      "Loss: 0.0011399402283132076\n",
      "Loss: 0.0010487833060324192\n",
      "Loss: 0.0011514942161738873\n",
      "Loss: 0.001024928642436862\n",
      "Loss: 0.0010779156582430005\n",
      "Loss: 0.0010552045423537493\n",
      "Loss: 0.0010535992914810777\n",
      "Loss: 0.0010536281624808908\n",
      "Loss: 0.001066586934030056\n",
      "Loss: 0.0010300292633473873\n",
      "Loss: 0.0010632224148139358\n",
      "Loss: 0.0011775734601542354\n",
      "Loss: 0.0009097461588680744\n",
      "Loss: 0.001177318743430078\n",
      "Loss: 0.0011546113528311253\n",
      "Loss: 0.0010927289258688688\n",
      "Loss: 0.0009473805548623204\n",
      "Loss: 0.0010613329941406846\n",
      "Loss: 0.001129350857809186\n",
      "Loss: 0.0009561305632814765\n",
      "Loss: 0.0009767513256520033\n",
      "Loss: 0.0009700950467959046\n",
      "Loss: 0.0011171525111421943\n",
      "Loss: 0.0010803760960698128\n",
      "Loss: 0.0009718725923448801\n",
      "Loss: 0.0010395030258223414\n",
      "Loss: 0.0009985308861359954\n",
      "Loss: 0.0010477995965629816\n",
      "Loss: 0.001027601887471974\n",
      "Loss: 0.000960304809268564\n",
      "Loss: 0.0010244855657219887\n",
      "Loss: 0.0010659941472113132\n",
      "Loss: 0.0010968309361487627\n",
      "Loss: 0.0010844162898138165\n",
      "Loss: 0.0009989533573389053\n",
      "Loss: 0.0010531225707381964\n",
      "Loss: 0.001070488360710442\n",
      "Loss: 0.0010457555763423443\n",
      "Loss: 0.0010026708478108048\n",
      "Loss: 0.0009749040473252535\n",
      "Loss: 0.0011281095212325454\n",
      "Loss: 0.0010212382767349482\n",
      "Loss: 0.0010664715664461255\n",
      "Loss: 0.0010034580482169986\n",
      "Loss: 0.0009283431572839618\n",
      "Loss: 0.0010009981924667954\n",
      "Loss: 0.00106055848300457\n",
      "Loss: 0.001044655917212367\n",
      "Loss: 0.0011306152446195483\n",
      "Loss: 0.0011395505862310529\n",
      "Loss: 0.0010061986977234483\n",
      "Loss: 0.0009485481423325837\n",
      "Loss: 0.001045179320499301\n",
      "Loss: 0.0009503972250968218\n",
      "Loss: 0.0010405273642390966\n",
      "Loss: 0.0008701183833181858\n",
      "Loss: 0.0010901391506195068\n",
      "Loss: 0.0010996753117069602\n",
      "Loss: 0.001143644331023097\n",
      "Loss: 0.0009930956875905395\n",
      "Loss: 0.0010896425228565931\n",
      "Loss: 0.00107960298191756\n",
      "Loss: 0.001040099305100739\n",
      "Loss: 0.001118880813010037\n",
      "Loss: 0.0010219523683190346\n",
      "Loss: 0.0010090722935274243\n",
      "Loss: 0.0011026815045624971\n",
      "Loss: 0.0010466385865584016\n",
      "Loss: 0.0010425073560327291\n",
      "Loss: 0.0010269368067383766\n",
      "Loss: 0.0009466951014474034\n",
      "Loss: 0.0010539916111156344\n",
      "Loss: 0.0010533566819503903\n",
      "Loss: 0.001042713993228972\n",
      "Loss: 0.0009534423588775098\n",
      "Loss: 0.0009708435973152518\n",
      "Loss: 0.0011335851158946753\n",
      "Loss: 0.0010531452717259526\n",
      "Loss: 0.0009550391114316881\n",
      "Loss: 0.0008779572090134025\n",
      "Loss: 0.0009968036320060492\n",
      "Loss: 0.0011323821963742375\n",
      "Loss: 0.0008922630222514272\n",
      "Loss: 0.0010065067326650023\n",
      "Loss: 0.000982979778200388\n",
      "Loss: 0.0009948315564543009\n",
      "Loss: 0.001003755140118301\n",
      "Loss: 0.0010301584843546152\n",
      "Loss: 0.0009789150208234787\n",
      "Loss: 0.0010825387435033917\n",
      "Loss: 0.0011119171977043152\n",
      "Loss: 0.0010003511561080813\n",
      "Loss: 0.0008550462080165744\n",
      "Loss: 0.0008874178165569901\n",
      "Loss: 0.0010748690692707896\n",
      "Loss: 0.0009374964865855873\n",
      "Loss: 0.0009827290195971727\n",
      "Loss: 0.0010559598449617624\n",
      "Loss: 0.0009361594566144049\n",
      "Loss: 0.0010588597506284714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 58/63 [1:27:35<07:07, 85.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010019035544246435\n",
      "Epoch: 58\n",
      "Loss: 0.0009808264439925551\n",
      "Loss: 0.0011039425153285265\n",
      "Loss: 0.0009487993083894253\n",
      "Loss: 0.0009266136912629008\n",
      "Loss: 0.0010951373260468245\n",
      "Loss: 0.0010543456301093102\n",
      "Loss: 0.0009506710339337587\n",
      "Loss: 0.0009210401331074536\n",
      "Loss: 0.0009770297911018133\n",
      "Loss: 0.0009971398394554853\n",
      "Loss: 0.0010749922366812825\n",
      "Loss: 0.0011080713011324406\n",
      "Loss: 0.0010287391487509012\n",
      "Loss: 0.0010418163146823645\n",
      "Loss: 0.0010111165465787053\n",
      "Loss: 0.001016624504700303\n",
      "Loss: 0.0009977075969800353\n",
      "Loss: 0.0009280393715016544\n",
      "Loss: 0.0009642688673920929\n",
      "Loss: 0.0009644830715842545\n",
      "Loss: 0.000935150368604809\n",
      "Loss: 0.0010489091509953141\n",
      "Loss: 0.0009606730891391635\n",
      "Loss: 0.0010546749690547585\n",
      "Loss: 0.0009442292503081262\n",
      "Loss: 0.0008806585101410747\n",
      "Loss: 0.0007611958426423371\n",
      "Loss: 0.0010753024835139513\n",
      "Loss: 0.001037128153257072\n",
      "Loss: 0.0010590574238449335\n",
      "Loss: 0.0010196733055636287\n",
      "Loss: 0.000957630923949182\n",
      "Loss: 0.0010465941159054637\n",
      "Loss: 0.0010880926856771111\n",
      "Loss: 0.0010708215413615108\n",
      "Loss: 0.0009168325341306627\n",
      "Loss: 0.001074113417416811\n",
      "Loss: 0.0009622409706935287\n",
      "Loss: 0.0010298258857801557\n",
      "Loss: 0.0009415366221219301\n",
      "Loss: 0.0010409604292362928\n",
      "Loss: 0.0010872649727389216\n",
      "Loss: 0.0010363900801166892\n",
      "Loss: 0.0008987009641714394\n",
      "Loss: 0.0009809968760237098\n",
      "Loss: 0.000973935064394027\n",
      "Loss: 0.0009880904108285904\n",
      "Loss: 0.0010103407548740506\n",
      "Loss: 0.0009853591909632087\n",
      "Loss: 0.0010999629739671946\n",
      "Loss: 0.0010655696969479322\n",
      "Loss: 0.001001785509288311\n",
      "Loss: 0.0009423833689652383\n",
      "Loss: 0.0010014557046815753\n",
      "Loss: 0.0010148705914616585\n",
      "Loss: 0.0009139832691289485\n",
      "Loss: 0.0010128398425877094\n",
      "Loss: 0.0010063410736620426\n",
      "Loss: 0.0009197975159622729\n",
      "Loss: 0.0009311940521001816\n",
      "Loss: 0.0009926679776981473\n",
      "Loss: 0.0010621086694300175\n",
      "Loss: 0.0009781615808606148\n",
      "Loss: 0.0010613473132252693\n",
      "Loss: 0.0009557466837577522\n",
      "Loss: 0.0009894141694530845\n",
      "Loss: 0.001082568778656423\n",
      "Loss: 0.0008969462942332029\n",
      "Loss: 0.0008828042191453278\n",
      "Loss: 0.0008455187198705971\n",
      "Loss: 0.0010530143044888973\n",
      "Loss: 0.0010223359568044543\n",
      "Loss: 0.0010337460553273559\n",
      "Loss: 0.0009463726310059428\n",
      "Loss: 0.0009372035274282098\n",
      "Loss: 0.0010020978515967727\n",
      "Loss: 0.0009451581281609833\n",
      "Loss: 0.000995136215351522\n",
      "Loss: 0.0008676002616994083\n",
      "Loss: 0.0009696894558146596\n",
      "Loss: 0.0010185276623815298\n",
      "Loss: 0.000795942556578666\n",
      "Loss: 0.0009383212891407311\n",
      "Loss: 0.000995812239125371\n",
      "Loss: 0.0009978601010516286\n",
      "Loss: 0.0010607177391648293\n",
      "Loss: 0.0008321702480316162\n",
      "Loss: 0.0009170924895443022\n",
      "Loss: 0.0010305589530616999\n",
      "Loss: 0.0010458617471158504\n",
      "Loss: 0.001053093932569027\n",
      "Loss: 0.0009022217709571123\n",
      "Loss: 0.0009170809062197804\n",
      "Loss: 0.0008282900089398026\n",
      "Loss: 0.0009249267168343067\n",
      "Loss: 0.0008779566851444542\n",
      "Loss: 0.0008885873830877244\n",
      "Loss: 0.001002898090519011\n",
      "Loss: 0.0007954593747854233\n",
      "Loss: 0.0009582304046489298\n",
      "Loss: 0.0007732227095402777\n",
      "Loss: 0.0009797124657779932\n",
      "Loss: 0.0007574540213681757\n",
      "Loss: 0.0008344343514181674\n",
      "Loss: 0.001049875165335834\n",
      "Loss: 0.0010099316714331508\n",
      "Loss: 0.0008222546894103289\n",
      "Loss: 0.0009911918314173818\n",
      "Loss: 0.0009418943081982434\n",
      "Loss: 0.0008759446791373193\n",
      "Loss: 0.0009020017459988594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 59/63 [1:29:00<05:42, 85.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009265269036404788\n",
      "Epoch: 59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d47143827044fbfa1dac10689a2fbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009978885063901544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['Gender', 'hours-per-week <= 41.5, capital-loss <= 18']\n",
      "Loss: 0.000896371784619987\n",
      "Predictions: ['ssc_p, hsc_p, degree_p, etest_p', 'Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque']\n",
      "Loss: 0.001033353852108121\n",
      "Predictions: ['3', 'ReachedOnTime']\n",
      "Loss: 0.0010253289947286248\n",
      "Predictions: ['4', '8']\n",
      "Loss: 0.000823836016934365\n",
      "Predictions: ['age, height, weight, waistline, SBP, BLDS, tot_c', 'SMK_stat_type_cd <= 1.5, gamma_GTP <=']\n",
      "Loss: 0.0009022348094731569\n",
      "Predictions: ['Pregnancies, Glucose, BloodPressure, SkinThickness, In', 'basic_needs <= 3.5, bullying <= 1.5']\n",
      "Loss: 0.0009474579128436744\n",
      "Predictions: ['JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain', 'skewness <= 5.16, curtosis <= 0.1']\n",
      "Loss: 0.0008678978192619979\n",
      "Predictions: ['Alcohol, Malic acid, Ash, Alcalinity of ash, Total phenol', '6']\n",
      "Loss: 0.0009705916163511574\n",
      "Predictions: ['age, fnlwgt, educational-num, capital-gain, capital-loss, hours', '7']\n",
      "Loss: 0.0009290113812312484\n",
      "Predictions: ['Age, pH, Specific Gravity', 'Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior']\n",
      "Loss: 0.0009692309540696442\n",
      "Predictions: ['9', 'ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount']\n",
      "Loss: 0.0007904084050096571\n",
      "Predictions: ['MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO', 'CAEC, CALC, MTRANS, Gender, family_history_with_over']\n",
      "Loss: 0.0009343214915134013\n",
      "Predictions: ['type_of_meal_plan, room_type_reserved, required_', '8']\n",
      "Loss: 0.0010096291080117226\n",
      "Predictions: ['class', 'Age, Work_Experience, Family_Size']\n",
      "Loss: 0.0009387246100232005\n",
      "Predictions: ['Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior', 'Class']\n",
      "Loss: 0.000967616040725261\n",
      "Predictions: ['Prior_purchases <= 3.5, Customer_care_calls <= 4.', '6']\n",
      "Loss: 0.0009387832251377404\n",
      "Predictions: ['ph, Hardness, Chloramines, Sulfate, Conductivity, Trihal', 'Education, City, Gender, EverBenched']\n",
      "Loss: 0.0009091536630876362\n",
      "Predictions: ['SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet', 'dec <= 22.21, mjd <= 55090']\n",
      "Loss: 0.0009690253064036369\n",
      "Predictions: ['ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount', 'is_claim']\n",
      "Loss: 0.0008414026815444231\n",
      "Predictions: ['Age, Height, Weight, FCVC, NCP, CH2O, FAF,', 'battery_power, fc, int_memory, mobile_wt, n_cores']\n",
      "Loss: 0.000915361160878092\n",
      "Predictions: ['Area, Perimeter, MinorAxisLength, AspectRation, Eccentricity,', '12']\n",
      "Loss: 0.0009287213324569166\n",
      "Predictions: ['6', 'policy_tenure, age_of_car, age_of_policyholder, air']\n",
      "Loss: 0.001028680824674666\n",
      "Predictions: ['booking_status', 'Machine_failure']\n",
      "Loss: 0.000978537485934794\n",
      "Predictions: ['Species', 'Age, TB, DB, Alkphos, Sgpt, Sgot, TP']\n",
      "Loss: 0.0009626562823541462\n",
      "Predictions: ['Size, Weight, Sweetness, Crunchiness, Juiciness, Ripeness', '10']\n",
      "Loss: 0.0008943052380345762\n",
      "Predictions: ['age, height, weight, waistline, SBP, BLDS, tot_c', '6']\n",
      "Loss: 0.0009538994636386633\n",
      "Predictions: ['hsc_s, degree_t, gender, ssc_b, hsc', '4']\n",
      "Loss: 0.0008965579909272492\n",
      "Predictions: ['Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight', 'fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free']\n",
      "Loss: 0.0009877164848148823\n",
      "Predictions: ['11', 'variance, skewness, curtosis, entropy']\n",
      "Loss: 0.0008634443511255085\n",
      "Predictions: ['area_cluster, segment, model, fuel_type, max_torque, max_', 'Dependents, Property_Area, Gender, Married, Education, Self_Empl']\n",
      "Loss: 0.0008027513395063579\n",
      "Predictions: ['COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY', 'int_memory <= 30.5, mobile_wt <= 91.']\n",
      "Loss: 0.0009607349638827145\n",
      "Predictions: ['ssc_p, hsc_p, degree_p, etest_p', 'RainTomorrow']\n",
      "Loss: 0.0009661470539867878\n",
      "Predictions: ['workclass, education, marital-status, occupation, relationship, race, gender', 'Area <= 39172.5, AspectRation <= 1']\n",
      "Loss: 0.001023742719553411\n",
      "Predictions: ['mental_health_history', '7']\n",
      "Loss: 0.0009054546244442463\n",
      "Predictions: ['Rainfall, WindGustDir, WindDir9am, WindDir3pm, Wind', 'Ia, Ib, Ic, Va, Vb, Vc']\n",
      "Loss: 0.00097951153293252\n",
      "Predictions: ['target', 'Size, Weight, Sweetness, Crunchiness, Juiciness, Ripeness']\n",
      "Loss: 0.0009442137670703232\n",
      "Predictions: ['ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount', 'Geography, Gender, HasCrCard, IsActiveMember']\n",
      "Loss: 0.0010236995294690132\n",
      "Predictions: ['class', 'Segmentation']\n",
      "Loss: 0.0009664352983236313\n",
      "Predictions: ['3', 'Gender, Dependents, Self_Employed, Loan_Amount_Term']\n",
      "Loss: 0.0009382932912558317\n",
      "Predictions: ['sex, hear_left, hear_right', 'Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight']\n",
      "Loss: 0.0008884540875442326\n",
      "Predictions: ['Loan_Amount_Term <= 420.0, ApplicantIncome <=', 'Age <= 42.5, NumOfProducts <= 2.5']\n",
      "Loss: 0.0009196958271786571\n",
      "Predictions: ['perimeter_mean <= 90.47, texture_worst <= 27.', 'slope <= 1.5, restecg <= 0.5']\n",
      "Loss: 0.000954481482040137\n",
      "Predictions: ['duration, credit_amount, installment_commitment, residence_since, age, existing_', 'Loan_Status']\n",
      "Loss: 0.0009351953631266952\n",
      "Predictions: ['Age, Work_Experience, Family_Size', 'duration, credit_amount, installment_commitment, residence_since, age, existing_']\n",
      "Loss: 0.0009039723663590848\n",
      "Predictions: ['ph, Hardness, Chloramines, Sulfate, Conductivity, Trihal', 'density <= 1.0, chlorides <= 0.08']\n",
      "Loss: 0.000985712860710919\n",
      "Predictions: ['Diagnosis', 'Warehouse_block, Mode_of_Shipment, Product_importance, Gender']\n",
      "Loss: 0.0009248701971955597\n",
      "Predictions: ['age, fnlwgt, educational-num, capital-gain, capital-loss, hours', 'Ia, Ib, Ic, Va, Vb, Vc']\n",
      "Loss: 0.0006806652527302504\n",
      "Predictions: ['COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY', 'battery_power, fc, int_memory, mobile_wt, n_cores']\n",
      "Loss: 0.0009417632827535272\n",
      "Predictions: ['5', 'Total phenols <= 2.36, Proanthocyanins <=']\n",
      "Loss: 0.0009155810112133622\n",
      "Predictions: ['4', 'policy_tenure, age_of_car, age_of_policyholder, air']\n",
      "Loss: 0.0008697836310602725\n",
      "Predictions: ['Age, Work_Experience, Family_Size', 'MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO']\n",
      "Loss: 0.0009298592922277749\n",
      "Predictions: ['Age <= 0.1, pH <= 5.5', 'CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary']\n",
      "Loss: 0.0008778603514656425\n",
      "Predictions: ['PetalWidthCm <= 0.7, PetalWidthCm <= 1', 'policy_tenure, age_of_car, age_of_policyholder, air']\n",
      "Loss: 0.0009088407387025654\n",
      "Predictions: ['ra, dec, run, camcol, field, redshift, plate, mjd', 'age, cp, trestbps, chol, restecg, thalach, old']\n",
      "Loss: 0.0009090866078622639\n",
      "Predictions: ['Area, Perimeter, MinorAxisLength, AspectRation, Eccentricity,', 'Selector']\n",
      "Loss: 0.0008929267060011625\n",
      "Predictions: ['CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary', 'MAJORSKEWNESS <= 74.5, CIRCULARITY <=']\n",
      "Loss: 0.0008521821582689881\n",
      "Predictions: ['Age, TB, DB, Alkphos, Sgpt, Sgot, TP', 'Rotational speed [rpm] <= 1381.5, Torque [']\n",
      "Loss: 0.0009665226098150015\n",
      "Predictions: ['income', 'duration, credit_amount, installment_commitment, residence_since, age, existing_']\n",
      "Loss: 0.0009635895257815719\n",
      "Predictions: ['SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet', 'AG_Ratio']\n",
      "Loss: 0.0010169720044359565\n",
      "Predictions: ['Outcome', '9']\n",
      "Loss: 0.0009779897518455982\n",
      "Predictions: ['class', 'JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain']\n",
      "Loss: 0.0009658707422204316\n",
      "Predictions: ['status', 'ssc_p, hsc_p, degree_p, etest_p']\n",
      "Loss: 0.0008717154269106686\n",
      "Predictions: ['Area, Perimeter, MinorAxisLength, AspectRation, Eccentricity,', 'Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight']\n",
      "Loss: 0.000765402102842927\n",
      "Predictions: ['Rainfall, WindSpeed9am, Pressure9am, Pressure3pm, Cloud9am', 'no_of_adults, no_of_children, no_of_weekend']\n",
      "Loss: 0.0008896813960745931\n",
      "Predictions: ['ssc_p <= 60.09, hsc_p <= ', 'Ic <= 71.01, Vb <= -0.37']\n",
      "Loss: 0.0010231237392872572\n",
      "Predictions: ['quality', 'Output']\n",
      "Loss: 0.0008549943449907005\n",
      "Predictions: ['JoiningYear, PaymentTier, Age, ExperienceInCurrentDomain', 'Alcohol, Malic acid, Ash, Alcalinity of ash, Total phenol']\n",
      "Loss: 0.000791302474681288\n",
      "Predictions: ['sex, fbs, exang', 'COMPACTNESS, CIRCULARITY, DISTANCE CIRCULARITY']\n",
      "Loss: 0.000851420103572309\n",
      "Predictions: ['12', 'age, height, weight, waistline, SBP, BLDS, tot_c']\n",
      "Loss: 0.0008905988652259111\n",
      "Predictions: ['CARDIOVASCULAR <= 50.0, ASHTMA <= 1', 'Hardness <= 278.29, Chloramines <= 6.']\n",
      "Loss: 0.000892275245860219\n",
      "Predictions: ['Potability', 'texture_mean, perimeter_mean, texture_se, perimeter_se, area_se']\n",
      "Loss: 0.0009515798301436007\n",
      "Predictions: ['price_range', 'ra, dec, run, camcol, field, redshift, plate, mjd']\n",
      "Loss: 0.0009431688813492656\n",
      "Predictions: ['11', 'Alkphos <= 211.5, Sgot <= 26.']\n",
      "Loss: 0.0009417959372512996\n",
      "Predictions: ['10', 'Juiciness <= -0.3, Crunchiness <= 2.25']\n",
      "Loss: 0.000848445575684309\n",
      "Predictions: ['anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem', 'lead_time <= 151.5, no_of_special_requests']\n",
      "Loss: 0.0008790933061391115\n",
      "Predictions: ['Age, TB, DB, Alkphos, Sgpt, Sgot, TP', 'SepalLengthCm, SepalWidthCm, PetalLengthCm, Pet']\n",
      "Loss: 0.0009474075632169843\n",
      "Predictions: ['9', 'blue, dual_sim, four_g, three_g, touch_screen, wifi']\n",
      "Loss: 0.0009863137966021895\n",
      "Predictions: ['5', 'ph, Sulfate, Trihalomethanes']\n",
      "Loss: 0.0009029249777086079\n",
      "Predictions: ['11', 'anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem']\n",
      "Loss: 0.0009379956172779202\n",
      "Predictions: ['stress_level', 'age, cp, trestbps, chol, restecg, thalach, old']\n",
      "Loss: 0.0007840280304662883\n",
      "Predictions: ['JoiningYear <= 2017.5, ExperienceInCurrentDomain <= 3', 'MEDICAL_UNIT, PNEUMONIA, AGE, PREGNANT, CO']\n",
      "Loss: 0.0008364010136574507\n",
      "Predictions: ['Customer_care_calls, Customer_rating, Cost_of_the_Product, Prior', 'ph, Hardness, Chloramines, Sulfate, Conductivity, Trihal']\n",
      "Loss: 0.0009157415479421616\n",
      "Predictions: ['7', 'Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque']\n",
      "Loss: 0.0007853057468309999\n",
      "Predictions: ['Alcohol, Malic acid, Ash, Alcalinity of ash, Total phenol', 'CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary']\n",
      "Loss: 0.0008595514227636158\n",
      "Predictions: ['checking_status, employment, other_parties, other_payment_plans, housing', 'Profession, Spending_Score, Var_1, Gender, Ever_Married,']\n",
      "Loss: 0.000934105773922056\n",
      "Predictions: ['4', 'displacement <= 1196.5, height <= 1519.0']\n",
      "Loss: 0.0008041050750762224\n",
      "Predictions: ['variance, skewness, curtosis, entropy', 'no_of_adults, no_of_children, no_of_weekend']\n",
      "Loss: 0.0009448918863199651\n",
      "Predictions: ['Age, pH, Specific Gravity', 'BMI <= 29.85, Age <= 27.5']\n",
      "Loss: 0.0008963996660895646\n",
      "Predictions: ['8', 'fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free']\n",
      "Loss: 0.0009510840172879398\n",
      "Predictions: ['Ever_Married, Graduated, Profession, Work_Experience, Family', 'diagnosis']\n",
      "Loss: 0.0009367686579935253\n",
      "Predictions: ['Sex', 'age, cp, trestbps, chol, restecg, thalach, old']\n",
      "Loss: 0.0008535064407624304\n",
      "Predictions: ['Rainfall, WindSpeed9am, Pressure9am, Pressure3pm, Cloud9am', 'anxiety_level, self_esteem, depression, headache, sleep_quality, breathing_problem']\n",
      "Loss: 0.0009577989694662392\n",
      "Predictions: ['USMER, SEX, PATIENT_TYPE', '12']\n",
      "Loss: 0.0009123862255364656\n",
      "Predictions: ['Type, TWF, HDF, PWF, OSF, RNF', 'Location, WindGustDir, WindDir9am, WindDir3pm, RainToday']\n",
      "Loss: 0.0008695753058418632\n",
      "Predictions: ['Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque', 'ra, dec, run, camcol, field, redshift, plate, mjd']\n",
      "Loss: 0.0009326626895926893\n",
      "Predictions: ['NObeyesdad', 'Pregnancies, Glucose, BloodPressure, SkinThickness, In']\n",
      "Loss: 0.0008989406633190811\n",
      "Predictions: ['texture_mean, perimeter_mean, texture_se, perimeter_se, area_se', 'CLASSIFICATION']\n",
      "Loss: 0.0009401660645380616\n",
      "Predictions: ['Family_Size <= 2.5, Work_Experience <= 9.5', 'LeaveOrNot']\n",
      "Loss: 0.0008923540008254349\n",
      "Predictions: ['battery_power, fc, int_memory, mobile_wt, n_cores', 'target']\n",
      "Loss: 0.0009516568388789892\n",
      "Predictions: ['6', 'variance, skewness, curtosis, entropy']\n",
      "Loss: 0.0008445450221188366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 59/63 [1:32:03<06:14, 93.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     40\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 42\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflattened_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, processor\u001b[38;5;241m.\u001b[39mbatch_decode(predictions, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2408\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:1733\u001b[0m, in \u001b[0;36mPix2StructForConditionalGeneration.forward\u001b[0;34m(self, flattened_patches, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, labels, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1730\u001b[0m     decoder_attention_mask[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1733\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:1446\u001b[0m, in \u001b[0;36mPix2StructTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, labels, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1442\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer)\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m-> 1446\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_extended_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/modeling_utils.py:957\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device, dtype)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03mMakes broadcastable attention and causal masks so that future and masked tokens are ignored.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;124;03m    `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 957\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (attention_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_decoder):\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;66;03m# show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\u001b[39;00m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/modeling_utils.py:887\u001b[0m, in \u001b[0;36mModuleUtilsMixin.dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    884\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_parameter_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/transformers/modeling_utils.py:243\u001b[0m, in \u001b[0;36mget_parameter_dtype\u001b[0;34m(parameter)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03mReturns the first found floating dtype in parameters if there is one, otherwise returns the last dtype it found.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m last_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m parameter\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    244\u001b[0m     last_dtype \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;66;03m# Adding fix for https://github.com/pytorch/xla/issues/4152\u001b[39;00m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;66;03m# Fixes issue where the model code passes a value that is out of range for XLA_USE_BF16=1\u001b[39;00m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# and XLA_DOWNCAST_BF16=1 so the conversion would cast it to -inf\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# NOTE: `is_torch_tpu_available()` is checked last as it induces a graph break in torch dynamo\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/torch/nn/modules/module.py:2081\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   2061\u001b[0m \n\u001b[1;32m   2062\u001b[0m \u001b[38;5;124;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2079\u001b[0m \n\u001b[1;32m   2080\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2081\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_parameters(recurse\u001b[38;5;241m=\u001b[39mrecurse):\n\u001b[1;32m   2082\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m param\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/torch/nn/modules/module.py:2084\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_parameters(recurse\u001b[38;5;241m=\u001b[39mrecurse):\n\u001b[1;32m   2082\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m param\n\u001b[0;32m-> 2084\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_parameters\u001b[39m(\n\u001b[1;32m   2085\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2086\u001b[0m         prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2087\u001b[0m         recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2088\u001b[0m         remove_duplicate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2089\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Parameter]]:\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;124;03m    name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \n\u001b[1;32m   2111\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_named_members(\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m module: module\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   2114\u001b[0m         prefix\u001b[38;5;241m=\u001b[39mprefix, recurse\u001b[38;5;241m=\u001b[39mrecurse, remove_duplicate\u001b[38;5;241m=\u001b[39mremove_duplicate)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import huggingface_hub\n",
    "from tqdm import tqdm\n",
    "\n",
    "huggingface_hub.login('',True,False,False)\n",
    "model_name = model_id.split(\"/\")[1]\n",
    "\n",
    "EPOCHS = 63\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        model.push_to_hub(f\"{model_name}-desc-vars-final\")\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        labels = batch.pop(\"labels\").to(device)\n",
    "        flattened_patches = batch.pop(\"flattened_patches\").to(device)\n",
    "        attention_mask = batch.pop(\"attention_mask\").to(device)\n",
    "\n",
    "        outputs = model(flattened_patches=flattened_patches,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            model.eval()\n",
    "\n",
    "            predictions = model.generate(flattened_patches=flattened_patches, attention_mask=attention_mask)        \n",
    "            print(\"Predictions:\", processor.batch_decode(predictions, skip_special_tokens=True))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
