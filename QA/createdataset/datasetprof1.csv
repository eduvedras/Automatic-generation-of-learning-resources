Id;Chart;Question;Answer
0;ObesityDataSet_decision_tree.png;The variable FAF discriminates between the target values, as shown in the decision tree.;T
1;ObesityDataSet_decision_tree.png;Variable Height is one of the most relevant variables.;T
2;ObesityDataSet_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
3;ObesityDataSet_decision_tree.png;The variable FAF seems to be one of the two most relevant features.;T
4;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that Naive Bayes algorithm classifies (not A, B), as Overweight_Level_I.;F
5;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], the Decision Tree presented classifies (A, not B) as Obesity_Type_III.;F
6;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that KNN algorithm classifies (A, not B) as Insufficient_Weight for any k ≤ 160.;F
7;ObesityDataSet_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
8;ObesityDataSet_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;F
9;ObesityDataSet_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;T
10;ObesityDataSet_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
11;ObesityDataSet_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
12;ObesityDataSet_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
13;ObesityDataSet_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;F
14;ObesityDataSet_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
15;ObesityDataSet_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;F
16;ObesityDataSet_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.;F
17;ObesityDataSet_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.;F
18;ObesityDataSet_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;F
19;ObesityDataSet_pca.png;The first 7 principal components are enough for explaining half the data variance.;T
20;ObesityDataSet_pca.png;Using the first 7 principal components would imply an error between 15 and 20%.;F
21;ObesityDataSet_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.;F
22;ObesityDataSet_correlation_heatmap.png;One of the variables Age or Height can be discarded without losing information.;F
23;ObesityDataSet_correlation_heatmap.png;The variable Weight can be discarded without risking losing information.;F
24;ObesityDataSet_correlation_heatmap.png;Variables NCP and TUE are redundant, but we can’t say the same for the pair Weight and Height.;F
25;ObesityDataSet_correlation_heatmap.png;Variables FAF and TUE are redundant.;F
26;ObesityDataSet_correlation_heatmap.png;Considering that the target variable is TUE we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
27;ObesityDataSet_correlation_heatmap.png;Considering that the target variable is TUE we can say that variable Height seems to be relevant for the majority of mining tasks.;F
28;ObesityDataSet_correlation_heatmap.png;Considering that the target variable is TUE we can say that variables FAF and Height seem to be useful for classification tasks.;F
29;ObesityDataSet_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
30;ObesityDataSet_correlation_heatmap.png;Removing variable CH2O might improve the training of decision trees .;F
31;ObesityDataSet_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable Height.;F
32;ObesityDataSet_boxplots.png;Variable CH2O is balanced.;T
33;ObesityDataSet_boxplots.png;Those boxplots show that the data is not normalized.;T
34;ObesityDataSet_boxplots.png;It is clear that variable FCVC shows some outliers, but we can’t be sure of the same for variable TUE.;F
35;ObesityDataSet_boxplots.png;Outliers seem to be a problem in the dataset.;F
36;ObesityDataSet_boxplots.png;Variable FAF shows some outlier values.;F
37;ObesityDataSet_boxplots.png;Variable NCP doesn’t have any outliers.;T
38;ObesityDataSet_boxplots.png;Variable Height presents some outliers.;T
39;ObesityDataSet_boxplots.png;At least 75% of the variables present outliers.;F
40;ObesityDataSet_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
41;ObesityDataSet_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;F
42;ObesityDataSet_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
43;ObesityDataSet_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;T
44;ObesityDataSet_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
45;ObesityDataSet_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
46;ObesityDataSet_histograms_symbolic.png;All variables should be dealt with as numeric.;F
47;ObesityDataSet_histograms_symbolic.png;The variable SMOKE can be seen as ordinal.;T
48;ObesityDataSet_histograms_symbolic.png;The variable FAVC can be seen as ordinal without losing information.;T
49;ObesityDataSet_histograms_symbolic.png;Considering the common semantics for FAVC and CAEC variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
50;ObesityDataSet_histograms_symbolic.png;Considering the common semantics for family_history_with_overweight variable, dummification would be the most adequate encoding.;F
51;ObesityDataSet_histograms_symbolic.png;The variable MTRANS can be coded as ordinal without losing information.;F
52;ObesityDataSet_histograms_symbolic.png;Feature generation based on variable family_history_with_overweight seems to be promising.;F
53;ObesityDataSet_histograms_symbolic.png;Feature generation based on the use of variable SCC wouldn’t be useful, but the use of CAEC seems to be promising.;F
54;ObesityDataSet_histograms_symbolic.png;Given the usual semantics of family_history_with_overweight variable, dummification would have been a better codification.;F
55;ObesityDataSet_histograms_symbolic.png;Not knowing the semantics of family_history_with_overweight variable, dummification could have been a more adequate codification.;F
56;ObesityDataSet_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
57;ObesityDataSet_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
58;ObesityDataSet_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
59;ObesityDataSet_histograms_numeric.png;All variables should be dealt with as symbolic.;F
60;ObesityDataSet_histograms_numeric.png;The variable Height can be seen as ordinal.;F
61;ObesityDataSet_histograms_numeric.png;The variable NCP can be seen as ordinal without losing information.;F
62;ObesityDataSet_histograms_numeric.png;Variable FAF is balanced.;F
63;ObesityDataSet_histograms_numeric.png;It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable CH2O.;F
64;ObesityDataSet_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
65;ObesityDataSet_histograms_numeric.png;Variable Height shows a high number of outlier values.;F
66;ObesityDataSet_histograms_numeric.png;Variable TUE doesn’t have any outliers.;T
67;ObesityDataSet_histograms_numeric.png;Variable FCVC presents some outliers.;F
68;ObesityDataSet_histograms_numeric.png;At least 60% of the variables present outliers.;F
69;ObesityDataSet_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
70;ObesityDataSet_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
71;ObesityDataSet_histograms_numeric.png;Considering the common semantics for Weight and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
72;ObesityDataSet_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.;F
73;ObesityDataSet_histograms_numeric.png;The variable Weight can be coded as ordinal without losing information.;F
74;ObesityDataSet_histograms_numeric.png;Feature generation based on variable TUE seems to be promising.;F
75;ObesityDataSet_histograms_numeric.png;Feature generation based on the use of variable Weight wouldn’t be useful, but the use of Age seems to be promising.;F
76;ObesityDataSet_histograms_numeric.png;Not knowing the semantics of CH2O variable, dummification could have been a more adequate codification.;F
77;customer_segmentation_decision_tree.png;The variable Family_Size discriminates between the target values, as shown in the decision tree.;F
78;customer_segmentation_decision_tree.png;Variable Work_Experience is one of the most relevant variables.;T
79;customer_segmentation_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;F
80;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A,B) as B for any k ≤ 11.;F
81;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A, not B) as C for any k ≤ 723.;F
82;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (not A, B) as B for any k ≤ 524.;F
83;customer_segmentation_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
84;customer_segmentation_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;T
85;customer_segmentation_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;F
86;customer_segmentation_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
87;customer_segmentation_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
88;customer_segmentation_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
89;customer_segmentation_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;T
90;customer_segmentation_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
91;customer_segmentation_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;T
92;customer_segmentation_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;T
93;customer_segmentation_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.;T
94;customer_segmentation_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;T
95;customer_segmentation_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
96;customer_segmentation_pca.png;Using the first 2 principal components would imply an error between 10 and 20%.;F
97;customer_segmentation_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
98;customer_segmentation_correlation_heatmap.png;One of the variables Age or Family_Size can be discarded without losing information.;F
99;customer_segmentation_correlation_heatmap.png;The variable Age can be discarded without risking losing information.;F
100;customer_segmentation_correlation_heatmap.png;Considering that the target variable is Family_Size we can say that variables Age and Work_Experience seem to be useful for classification tasks.;F
101;customer_segmentation_correlation_heatmap.png;Variables Age and Work_Experience are redundant.;F
102;customer_segmentation_correlation_heatmap.png;Considering that the target variable is Family_Size we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
103;customer_segmentation_correlation_heatmap.png;Considering that the target variable is Work_Experience we can say that variable Family_Size seems to be relevant for the majority of mining tasks.;F
104;customer_segmentation_correlation_heatmap.png;Considering that the target variable is Family_Size we can say that variables Family_Size and Work_Experience seem to be useful for classification tasks.;F
105;customer_segmentation_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
106;customer_segmentation_correlation_heatmap.png;Removing variable Work_Experience might improve the training of decision trees .;F
107;customer_segmentation_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable Family_Size.;F
108;customer_segmentation_boxplots.png;Variable Family_Size is balanced.;F
109;customer_segmentation_boxplots.png;Those boxplots show that the data is not normalized.;T
110;customer_segmentation_boxplots.png;It is clear that variable Work_Experience shows some outliers, but we can’t be sure of the same for variable Family_Size.;F
111;customer_segmentation_boxplots.png;Outliers seem to be a problem in the dataset.;F
112;customer_segmentation_boxplots.png;Variable Work_Experience shows a high number of outlier values.;F
113;customer_segmentation_boxplots.png;Variable Work_Experience doesn’t have any outliers.;T
114;customer_segmentation_boxplots.png;Variable Age presents some outliers.;F
115;customer_segmentation_boxplots.png;At least 50% of the variables present outliers.;F
116;customer_segmentation_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
117;customer_segmentation_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;F
118;customer_segmentation_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
119;customer_segmentation_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;T
120;customer_segmentation_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
121;customer_segmentation_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
122;customer_segmentation_histograms_symbolic.png;All variables should be dealt with as numeric.;F
123;customer_segmentation_histograms_symbolic.png;The variable Gender can be seen as ordinal.;T
124;customer_segmentation_histograms_symbolic.png;The variable Ever_Married can be seen as ordinal without losing information.;T
125;customer_segmentation_histograms_symbolic.png;Considering the common semantics for Var_1 and Profession variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
126;customer_segmentation_histograms_symbolic.png;Considering the common semantics for Profession variable, dummification would be the most adequate encoding.;F
127;customer_segmentation_histograms_symbolic.png;The variable Graduated can be coded as ordinal without losing information.;T
128;customer_segmentation_histograms_symbolic.png;Feature generation based on variable Gender seems to be promising.;F
129;customer_segmentation_histograms_symbolic.png;Given the usual semantics of Profession variable, dummification would have been a better codification.;F
130;customer_segmentation_histograms_symbolic.png;Not knowing the semantics of Spending_Score variable, dummification could have been a more adequate codification.;T
131;customer_segmentation_mv.png;Considering that the dataset has 800 records, discarding variable Var_1 would be better than discarding all the records with missing values for that variable.;F
132;customer_segmentation_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.;F
133;customer_segmentation_mv.png;Considering that the dataset has 600 records, dropping all rows with missing values can lead to a dataset with less than 30% of the original data.;T
134;customer_segmentation_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;T
135;customer_segmentation_mv.png;Feature generation based on variable Var_1 seems to be promising.;F
136;customer_segmentation_mv.png;It is better to drop the variable Family_Size than removing all records with missing values.;F
137;customer_segmentation_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
138;customer_segmentation_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
139;customer_segmentation_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
140;customer_segmentation_histograms_numeric.png;All variables should be dealt with as symbolic.;F
141;customer_segmentation_histograms_numeric.png;The variable Family_Size can be seen as ordinal.;T
142;customer_segmentation_histograms_numeric.png;The variable Age can be seen as ordinal without losing information.;F
143;customer_segmentation_histograms_numeric.png;Variable Family_Size is balanced.;F
144;customer_segmentation_histograms_numeric.png;It is clear that variable Work_Experience shows some outliers, but we can’t be sure of the same for variable Age.;F
145;customer_segmentation_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
146;customer_segmentation_histograms_numeric.png;Variable Age shows some outlier values.;F
147;customer_segmentation_histograms_numeric.png;Variable Family_Size doesn’t have any outliers.;T
148;customer_segmentation_histograms_numeric.png;Variable Work_Experience presents some outliers.;F
149;customer_segmentation_histograms_numeric.png;At least 75% of the variables present outliers.;F
150;customer_segmentation_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
151;customer_segmentation_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
152;customer_segmentation_histograms_numeric.png;Considering the common semantics for Family_Size and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
153;customer_segmentation_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.;F
154;customer_segmentation_histograms_numeric.png;The variable Age can be coded as ordinal without losing information.;F
155;customer_segmentation_histograms_numeric.png;Feature generation based on variable Work_Experience seems to be promising.;T
156;customer_segmentation_histograms_numeric.png;Feature generation based on the use of variable Work_Experience wouldn’t be useful, but the use of Age seems to be promising.;F
157;customer_segmentation_histograms_numeric.png;Given the usual semantics of Age variable, dummification would have been a better codification.;F
158;customer_segmentation_histograms_numeric.png;Not knowing the semantics of Family_Size variable, dummification could have been a more adequate codification.;F
159;urinalysis_tests_decision_tree.png;The variable Age discriminates between the target values, as shown in the decision tree.;F
160;urinalysis_tests_decision_tree.png;Variable Age is one of the most relevant variables.;T
161;urinalysis_tests_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T
162;urinalysis_tests_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider POSITIVE as the positive class and NEGATIVE as the negative class.;F
163;urinalysis_tests_decision_tree.png;The specificity for the presented tree is higher than 60%, consider POSITIVE as the positive class and NEGATIVE as the negative class.;T
164;urinalysis_tests_decision_tree.png;The number of True Positives reported in the same tree is 10, consider POSITIVE as the positive class and NEGATIVE as the negative class.;F
165;urinalysis_tests_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree, consider POSITIVE as the positive class and NEGATIVE as the negative class.;F
166;urinalysis_tests_decision_tree.png;The recall for the presented tree is lower than its specificity, consider POSITIVE as the positive class and NEGATIVE as the negative class.;T
167;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as NEGATIVE.;T
168;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as POSITIVE.;F
169;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], it is possible to state that KNN algorithm classifies (not A, B) as NEGATIVE for any k ≤ 763.;F
170;urinalysis_tests_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
171;urinalysis_tests_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;T
172;urinalysis_tests_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;F
173;urinalysis_tests_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
174;urinalysis_tests_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
175;urinalysis_tests_overfitting_knn.png;KNN is in overfitting for k larger than 5.;F
176;urinalysis_tests_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;F
177;urinalysis_tests_overfitting_knn.png;KNN with less than 17 neighbours is in overfitting.;F
178;urinalysis_tests_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
179;urinalysis_tests_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;T
180;urinalysis_tests_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.;T
181;urinalysis_tests_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;T
182;urinalysis_tests_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
183;urinalysis_tests_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
184;urinalysis_tests_pca.png;Using the first 2 principal components would imply an error between 5 and 20%.;F
185;urinalysis_tests_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
186;urinalysis_tests_correlation_heatmap.png;One of the variables pH or Age can be discarded without losing information.;F
187;urinalysis_tests_correlation_heatmap.png;The variable Age can be discarded without risking losing information.;F
188;urinalysis_tests_correlation_heatmap.png;Considering that the target variable is pH we can say that variables Specific Gravity and Age seem to be useful for classification tasks.;F
189;urinalysis_tests_correlation_heatmap.png;Variables Age and pH are redundant.;F
190;urinalysis_tests_correlation_heatmap.png;Considering that the target variable is Specific Gravity we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
191;urinalysis_tests_correlation_heatmap.png;Considering that the target variable is pH we can say that variable Specific Gravity seems to be relevant for the majority of mining tasks.;F
192;urinalysis_tests_correlation_heatmap.png;Considering that the target variable is Age we can say that variables Specific Gravity and pH seem to be useful for classification tasks.;F
193;urinalysis_tests_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
194;urinalysis_tests_correlation_heatmap.png;Removing variable pH might improve the training of decision trees .;F
195;urinalysis_tests_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable pH.;F
196;urinalysis_tests_boxplots.png;Variable pH is balanced.;F
197;urinalysis_tests_boxplots.png;Those boxplots show that the data is not normalized.;T
198;urinalysis_tests_boxplots.png;It is clear that variable Specific Gravity shows some outliers, but we can’t be sure of the same for variable Age.;F
199;urinalysis_tests_boxplots.png;Outliers seem to be a problem in the dataset.;T
200;urinalysis_tests_boxplots.png;Variable Specific Gravity shows a high number of outlier values.;F
201;urinalysis_tests_boxplots.png;Variable Age doesn’t have any outliers.;F
202;urinalysis_tests_boxplots.png;Variable Age presents some outliers.;T
203;urinalysis_tests_boxplots.png;At least 60% of the variables present outliers.;T
204;urinalysis_tests_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
205;urinalysis_tests_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
206;urinalysis_tests_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
207;urinalysis_tests_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
208;urinalysis_tests_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
209;urinalysis_tests_histograms_symbolic.png;All variables should be dealt with as symbolic.;T
210;urinalysis_tests_histograms_symbolic.png;The variable Gender can be seen as ordinal.;T
211;urinalysis_tests_histograms_symbolic.png;The variable Mucous Threads can be seen as ordinal without losing information.;T
212;urinalysis_tests_histograms_symbolic.png;Considering the common semantics for Epithelial Cells and Color variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
213;urinalysis_tests_histograms_symbolic.png;Considering the common semantics for Amorphous Urates variable, dummification would be the most adequate encoding.;F
214;urinalysis_tests_histograms_symbolic.png;The variable Color can be coded as ordinal without losing information.;T
215;urinalysis_tests_histograms_symbolic.png;Feature generation based on variable Amorphous Urates seems to be promising.;F
216;urinalysis_tests_histograms_symbolic.png;Feature generation based on the use of variable Protein wouldn’t be useful, but the use of Color seems to be promising.;F
217;urinalysis_tests_histograms_symbolic.png;Given the usual semantics of Bacteria variable, dummification would have been a better codification.;F
218;urinalysis_tests_histograms_symbolic.png;Not knowing the semantics of Epithelial Cells variable, dummification could have been a more adequate codification.;F
219;urinalysis_tests_mv.png;Discarding variable Color would be better than discarding all the records with missing values for that variable.;F
220;urinalysis_tests_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.;T
221;urinalysis_tests_mv.png;Considering that the dataset has 310 records, dropping all rows with missing values can lead to a dataset with less than 30% of the original data.;F
222;urinalysis_tests_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;F
223;urinalysis_tests_mv.png;Feature generation based on variable Color seems to be promising.;F
224;urinalysis_tests_mv.png;It is better to drop the variable Color than removing all records with missing values.;F
225;urinalysis_tests_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
226;urinalysis_tests_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
227;urinalysis_tests_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
228;urinalysis_tests_histograms_numeric.png;All variables should be dealt with as binary.;F
229;urinalysis_tests_histograms_numeric.png;The variable Specific Gravity can be seen as ordinal.;T
230;urinalysis_tests_histograms_numeric.png;The variable Specific Gravity can be seen as ordinal without losing information.;T
231;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity is balanced.;F
232;urinalysis_tests_histograms_numeric.png;It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable pH.;F
233;urinalysis_tests_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
234;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity shows a high number of outlier values.;F
235;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity doesn’t have any outliers.;F
236;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity presents some outliers.;F
237;urinalysis_tests_histograms_numeric.png;At least 50% of the variables present outliers.;F
238;urinalysis_tests_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
239;urinalysis_tests_histograms_numeric.png;Considering the common semantics for Age and pH variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
240;urinalysis_tests_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.;F
241;urinalysis_tests_histograms_numeric.png;The variable pH can be coded as ordinal without losing information.;T
242;urinalysis_tests_histograms_numeric.png;Feature generation based on variable Age seems to be promising.;T
243;urinalysis_tests_histograms_numeric.png;Feature generation based on the use of variable Age wouldn’t be useful, but the use of pH seems to be promising.;F
244;urinalysis_tests_histograms_numeric.png;Given the usual semantics of Specific Gravity variable, dummification would have been a better codification.;F
245;urinalysis_tests_histograms_numeric.png;Not knowing the semantics of Age variable, dummification could have been a more adequate codification.;F
246;detect_dataset_decision_tree.png;The variable Ic discriminates between the target values, as shown in the decision tree.;T
247;detect_dataset_decision_tree.png;Variable Vb is one of the most relevant variables.;T
248;detect_dataset_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
249;detect_dataset_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;T
250;detect_dataset_decision_tree.png;The precision for the presented tree is higher than 75%, consider 1 as the positive class and 0 as the negative class.;T
251;detect_dataset_decision_tree.png;The number of False Negatives is lower than the number of True Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
252;detect_dataset_decision_tree.png;The number of True Positives is lower than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
253;detect_dataset_decision_tree.png;The number of False Negatives reported in the same tree is 50, consider 1 as the positive class and 0 as the negative class.;F
254;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 3.;F
255;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], the Decision Tree presented classifies (A, not B) as 0.;T
256;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], the Decision Tree presented classifies (A,B) as 0.;F
257;detect_dataset_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
258;detect_dataset_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;F
259;detect_dataset_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;F
260;detect_dataset_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
261;detect_dataset_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
262;detect_dataset_overfitting_knn.png;KNN is in overfitting for k less than 17.;F
263;detect_dataset_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;F
264;detect_dataset_overfitting_knn.png;KNN with less than 17 neighbours is in overfitting.;F
265;detect_dataset_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;F
266;detect_dataset_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;F
267;detect_dataset_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.;F
268;detect_dataset_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;F
269;detect_dataset_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
270;detect_dataset_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
271;detect_dataset_pca.png;Using the first 3 principal components would imply an error between 10 and 20%.;F
272;detect_dataset_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.;F
273;detect_dataset_correlation_heatmap.png;One of the variables Vc or Va can be discarded without losing information.;F
274;detect_dataset_correlation_heatmap.png;The variable Ic can be discarded without risking losing information.;F
275;detect_dataset_correlation_heatmap.png;Variables Ia and Ic are redundant, but we can’t say the same for the pair Vc and Vb.;F
276;detect_dataset_correlation_heatmap.png;Variables Ib and Vc are redundant.;F
277;detect_dataset_correlation_heatmap.png;Considering that the target variable is Vc we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
278;detect_dataset_correlation_heatmap.png;Considering that the target variable is Vc we can say that variable Vb seems to be relevant for the majority of mining tasks.;F
279;detect_dataset_correlation_heatmap.png;Considering that the target variable is Vc we can say that variables Ib and Ic seem to be useful for classification tasks.;F
280;detect_dataset_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
281;detect_dataset_correlation_heatmap.png;Removing variable Ic might improve the training of decision trees .;F
282;detect_dataset_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Ic previously than variable Va.;F
283;detect_dataset_boxplots.png;Variable Vb is balanced.;T
284;detect_dataset_boxplots.png;Those boxplots show that the data is not normalized.;T
285;detect_dataset_boxplots.png;It is clear that variable Vb shows some outliers, but we can’t be sure of the same for variable Va.;F
286;detect_dataset_boxplots.png;Outliers seem to be a problem in the dataset.;F
287;detect_dataset_boxplots.png;Variable Vb shows some outlier values.;F
288;detect_dataset_boxplots.png;Variable Vb doesn’t have any outliers.;T
289;detect_dataset_boxplots.png;Variable Ia presents some outliers.;F
290;detect_dataset_boxplots.png;At least 75% of the variables present outliers.;F
291;detect_dataset_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
292;detect_dataset_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;F
293;detect_dataset_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
294;detect_dataset_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
295;detect_dataset_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
296;detect_dataset_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
297;detect_dataset_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;F
298;detect_dataset_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
299;detect_dataset_histograms_numeric.png;All variables should be dealt with as date.;F
300;detect_dataset_histograms_numeric.png;The variable Ic can be seen as ordinal.;F
301;detect_dataset_histograms_numeric.png;The variable Vc can be seen as ordinal without losing information.;F
302;detect_dataset_histograms_numeric.png;Variable Ia is balanced.;T
303;detect_dataset_histograms_numeric.png;It is clear that variable Va shows some outliers, but we can’t be sure of the same for variable Vc.;F
304;detect_dataset_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
305;detect_dataset_histograms_numeric.png;Variable Ia shows a high number of outlier values.;T
306;detect_dataset_histograms_numeric.png;Variable Ic doesn’t have any outliers.;F
307;detect_dataset_histograms_numeric.png;Variable Ic presents some outliers.;T
308;detect_dataset_histograms_numeric.png;At least 60% of the variables present outliers.;F
309;detect_dataset_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;T
310;detect_dataset_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
311;detect_dataset_histograms_numeric.png;Considering the common semantics for Ia and Ib variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
312;detect_dataset_histograms_numeric.png;Considering the common semantics for Vc variable, dummification would be the most adequate encoding.;F
313;detect_dataset_histograms_numeric.png;The variable Vb can be coded as ordinal without losing information.;F
314;detect_dataset_histograms_numeric.png;Feature generation based on variable Vb seems to be promising.;F
315;detect_dataset_histograms_numeric.png;Feature generation based on the use of variable Ic wouldn’t be useful, but the use of Ia seems to be promising.;F
316;detect_dataset_histograms_numeric.png;Given the usual semantics of Ib variable, dummification would have been a better codification.;F
317;detect_dataset_histograms_numeric.png;Not knowing the semantics of Ia variable, dummification could have been a more adequate codification.;F
318;diabetes_decision_tree.png;The variable BMI discriminates between the target values, as shown in the decision tree.;T
319;diabetes_decision_tree.png;Variable BMI is one of the most relevant variables.;T
320;diabetes_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;F
321;diabetes_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;F
322;diabetes_decision_tree.png;The precision for the presented tree is higher than 60%, consider 1 as the positive class and 0 as the negative class.;T
323;diabetes_decision_tree.png;The number of True Positives reported in the same tree is 30, consider 1 as the positive class and 0 as the negative class.;F
324;diabetes_decision_tree.png;The number of False Positives is lower than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
325;diabetes_decision_tree.png;The accuracy for the presented tree is higher than its specificity, consider 1 as the positive class and 0 as the negative class.;F
326;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], the Decision Tree presented classifies (not A, not B) as 1.;T
327;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], the Decision Tree presented classifies (not A, not B) as 1.;T
328;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 98.;F
329;diabetes_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
330;diabetes_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;F
331;diabetes_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;F
332;diabetes_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
333;diabetes_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
334;diabetes_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
335;diabetes_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;T
336;diabetes_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
337;diabetes_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;T
338;diabetes_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.;T
339;diabetes_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 3.;F
340;diabetes_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.;T
341;diabetes_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;T
342;diabetes_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
343;diabetes_pca.png;The first 7 principal components are enough for explaining half the data variance.;T
344;diabetes_pca.png;Using the first 2 principal components would imply an error between 10 and 20%.;F
345;diabetes_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.;F
346;diabetes_correlation_heatmap.png;One of the variables Age or Insulin can be discarded without losing information.;F
347;diabetes_correlation_heatmap.png;The variable DiabetesPedigreeFunction can be discarded without risking losing information.;F
348;diabetes_correlation_heatmap.png;Variables Age and SkinThickness are redundant, but we can’t say the same for the pair BMI and BloodPressure.;F
349;diabetes_correlation_heatmap.png;Variables DiabetesPedigreeFunction and Age are redundant.;F
350;diabetes_correlation_heatmap.png;Considering that the target variable is BMI we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
351;diabetes_correlation_heatmap.png;Considering that the target variable is BMI we can say that variable SkinThickness seems to be relevant for the majority of mining tasks.;F
352;diabetes_correlation_heatmap.png;Considering that the target variable is BMI we can say that variables Insulin and Glucose seem to be useful for classification tasks.;F
353;diabetes_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
354;diabetes_correlation_heatmap.png;Removing variable Insulin might improve the training of decision trees .;F
355;diabetes_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable DiabetesPedigreeFunction previously than variable Pregnancies.;F
356;diabetes_boxplots.png;Variable DiabetesPedigreeFunction is balanced.;F
357;diabetes_boxplots.png;Those boxplots show that the data is not normalized.;T
358;diabetes_boxplots.png;It is clear that variable Glucose shows some outliers, but we can’t be sure of the same for variable Age.;F
359;diabetes_boxplots.png;Outliers seem to be a problem in the dataset.;T
360;diabetes_boxplots.png;Variable Pregnancies shows some outlier values.;T
361;diabetes_boxplots.png;Variable Insulin doesn’t have any outliers.;F
362;diabetes_boxplots.png;Variable BMI presents some outliers.;T
363;diabetes_boxplots.png;At least 85% of the variables present outliers.;T
364;diabetes_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;T
365;diabetes_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
366;diabetes_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
367;diabetes_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
368;diabetes_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
369;diabetes_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
370;diabetes_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;T
371;diabetes_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;T
372;diabetes_histograms_numeric.png;All variables should be dealt with as numeric.;F
373;diabetes_histograms_numeric.png;The variable Age can be seen as ordinal.;F
374;diabetes_histograms_numeric.png;The variable Age can be seen as ordinal without losing information.;F
375;diabetes_histograms_numeric.png;Variable Pregnancies is balanced.;F
376;diabetes_histograms_numeric.png;It is clear that variable DiabetesPedigreeFunction shows some outliers, but we can’t be sure of the same for variable Glucose.;F
377;diabetes_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
378;diabetes_histograms_numeric.png;Variable Age shows a high number of outlier values.;F
379;diabetes_histograms_numeric.png;Variable BMI doesn’t have any outliers.;F
380;diabetes_histograms_numeric.png;Variable BloodPressure presents some outliers.;T
381;diabetes_histograms_numeric.png;At least 60% of the variables present outliers.;T
382;diabetes_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
383;diabetes_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
384;diabetes_histograms_numeric.png;Considering the common semantics for BloodPressure and Pregnancies variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
385;diabetes_histograms_numeric.png;Considering the common semantics for BMI variable, dummification would be the most adequate encoding.;F
386;diabetes_histograms_numeric.png;The variable Age can be coded as ordinal without losing information.;F
387;diabetes_histograms_numeric.png;Feature generation based on variable BMI seems to be promising.;F
388;diabetes_histograms_numeric.png;Feature generation based on the use of variable Age wouldn’t be useful, but the use of Pregnancies seems to be promising.;F
389;diabetes_histograms_numeric.png;Given the usual semantics of BMI variable, dummification would have been a better codification.;F
390;diabetes_histograms_numeric.png;Not knowing the semantics of SkinThickness variable, dummification could have been a more adequate codification.;F
391;Placement_decision_tree.png;The variable ssc_p discriminates between the target values, as shown in the decision tree.;T
392;Placement_decision_tree.png;Variable hsc_p is one of the most relevant variables.;T
393;Placement_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T
394;Placement_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider Placed as the positive class and Not Placed as the negative class.;T
395;Placement_decision_tree.png;The recall for the presented tree is higher than 90%, consider Placed as the positive class and Not Placed as the negative class.;T
396;Placement_decision_tree.png;The number of False Negatives is lower than the number of True Negatives for the presented tree, consider Placed as the positive class and Not Placed as the negative class.;T
397;Placement_decision_tree.png;The number of True Negatives is lower than the number of True Positives for the presented tree, consider Placed as the positive class and Not Placed as the negative class.;T
398;Placement_decision_tree.png;The accuracy for the presented tree is higher than 75%.;T
399;Placement_decision_tree.png;Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], it is possible to state that KNN algorithm classifies (not A, not B) as Placed for any k ≤ 68.;T
400;Placement_decision_tree.png;Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], it is possible to state that KNN algorithm classifies (not A, not B) as Placed for any k ≤ 68.;T
401;Placement_decision_tree.png;Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], the Decision Tree presented classifies (A, not B) as Placed.;F
402;Placement_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
403;Placement_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;T
404;Placement_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;T
405;Placement_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
406;Placement_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
407;Placement_overfitting_knn.png;KNN is in overfitting for k less than 13.;T
408;Placement_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;T
409;Placement_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
410;Placement_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
411;Placement_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.;T
412;Placement_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 3.;F
413;Placement_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.;T
414;Placement_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;F
415;Placement_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
416;Placement_pca.png;The first 3 principal components are enough for explaining half the data variance.;T
417;Placement_pca.png;Using the first 2 principal components would imply an error between 10 and 30%.;T
418;Placement_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.;F
419;Placement_correlation_heatmap.png;One of the variables hsc_p or mba_p can be discarded without losing information.;F
420;Placement_correlation_heatmap.png;The variable mba_p can be discarded without risking losing information.;F
421;Placement_correlation_heatmap.png;Variables hsc_p and ssc_p are redundant, but we can’t say the same for the pair degree_p and etest_p.;F
422;Placement_correlation_heatmap.png;Variables hsc_p and etest_p are redundant.;F
423;Placement_correlation_heatmap.png;Considering that the target variable is mba_p we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
424;Placement_correlation_heatmap.png;Considering that the target variable is mba_p we can say that variable ssc_p seems to be relevant for the majority of mining tasks.;F
425;Placement_correlation_heatmap.png;Considering that the target variable is mba_p we can say that variables hsc_p and degree_p seem to be useful for classification tasks.;F
426;Placement_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
427;Placement_correlation_heatmap.png;Removing variable degree_p might improve the training of decision trees .;F
428;Placement_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable hsc_p previously than variable mba_p.;F
429;Placement_boxplots.png;Variable etest_p is balanced.;T
430;Placement_boxplots.png;Those boxplots show that the data is not normalized.;T
431;Placement_boxplots.png;It is clear that variable mba_p shows some outliers, but we can’t be sure of the same for variable ssc_p.;F
432;Placement_boxplots.png;Outliers seem to be a problem in the dataset.;F
433;Placement_boxplots.png;Variable hsc_p shows some outlier values.;T
434;Placement_boxplots.png;Variable hsc_p doesn’t have any outliers.;F
435;Placement_boxplots.png;Variable hsc_p presents some outliers.;T
436;Placement_boxplots.png;At least 75% of the variables present outliers.;T
437;Placement_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
438;Placement_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
439;Placement_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
440;Placement_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
441;Placement_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
442;Placement_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
443;Placement_histograms_symbolic.png;All variables should be dealt with as numeric.;F
444;Placement_histograms_symbolic.png;The variable degree_t can be seen as ordinal.;T
445;Placement_histograms_symbolic.png;The variable specialisation can be seen as ordinal without losing information.;F
446;Placement_histograms_symbolic.png;Considering the common semantics for specialisation and hsc_s variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
447;Placement_histograms_symbolic.png;Considering the common semantics for specialisation variable, dummification would be the most adequate encoding.;F
448;Placement_histograms_symbolic.png;The variable ssc_b can be coded as ordinal without losing information.;F
449;Placement_histograms_symbolic.png;Feature generation based on variable hsc_s seems to be promising.;F
450;Placement_histograms_symbolic.png;Feature generation based on the use of variable hsc_s wouldn’t be useful, but the use of degree_t seems to be promising.;F
451;Placement_histograms_symbolic.png;Given the usual semantics of hsc_s variable, dummification would have been a better codification.;F
452;Placement_histograms_symbolic.png;Not knowing the semantics of hsc_b variable, dummification could have been a more adequate codification.;T
453;Placement_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
454;Placement_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;F
455;Placement_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
456;Placement_histograms_numeric.png;All variables should be dealt with as numeric.;T
457;Placement_histograms_numeric.png;The variable etest_p can be seen as ordinal.;F
458;Placement_histograms_numeric.png;The variable mba_p can be seen as ordinal without losing information.;F
459;Placement_histograms_numeric.png;Variable degree_p is balanced.;T
460;Placement_histograms_numeric.png;It is clear that variable mba_p shows some outliers, but we can’t be sure of the same for variable hsc_p.;F
461;Placement_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
462;Placement_histograms_numeric.png;Variable mba_p shows some outlier values.;F
463;Placement_histograms_numeric.png;Variable ssc_p doesn’t have any outliers.;T
464;Placement_histograms_numeric.png;Variable degree_p presents some outliers.;F
465;Placement_histograms_numeric.png;At least 85% of the variables present outliers.;F
466;Placement_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
467;Placement_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
468;Placement_histograms_numeric.png;Considering the common semantics for ssc_p and hsc_p variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
469;Placement_histograms_numeric.png;Considering the common semantics for ssc_p variable, dummification would be the most adequate encoding.;F
470;Placement_histograms_numeric.png;The variable degree_p can be coded as ordinal without losing information.;F
471;Placement_histograms_numeric.png;Feature generation based on variable ssc_p seems to be promising.;F
472;Placement_histograms_numeric.png;Feature generation based on the use of variable etest_p wouldn’t be useful, but the use of ssc_p seems to be promising.;F
473;Placement_histograms_numeric.png;Given the usual semantics of degree_p variable, dummification would have been a better codification.;F
474;Placement_histograms_numeric.png;Not knowing the semantics of hsc_p variable, dummification could have been a more adequate codification.;F
475;Liver_Patient_decision_tree.png;The variable Sgot discriminates between the target values, as shown in the decision tree.;T
476;Liver_Patient_decision_tree.png;Variable Alkphos is one of the most relevant variables.;T
477;Liver_Patient_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T
478;Liver_Patient_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider 2 as the positive class and 1 as the negative class.;F
479;Liver_Patient_decision_tree.png;The recall for the presented tree is higher than 90%, consider 2 as the positive class and 1 as the negative class.;F
480;Liver_Patient_decision_tree.png;The number of True Negatives is higher than the number of True Positives for the presented tree, consider 2 as the positive class and 1 as the negative class.;T
481;Liver_Patient_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree, consider 2 as the positive class and 1 as the negative class.;F
482;Liver_Patient_decision_tree.png;The precision for the presented tree is higher than its recall, consider 2 as the positive class and 1 as the negative class.;T
483;Liver_Patient_decision_tree.png;Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 1.;T
484;Liver_Patient_decision_tree.png;Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], it is possible to state that KNN algorithm classifies (not A, not B) as 2 for any k ≤ 94.;F
485;Liver_Patient_decision_tree.png;Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], the Decision Tree presented classifies (not A, B) as 1.;T
486;Liver_Patient_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
487;Liver_Patient_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;T
488;Liver_Patient_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;F
489;Liver_Patient_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
490;Liver_Patient_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
491;Liver_Patient_overfitting_knn.png;KNN is in overfitting for k less than 13.;T
492;Liver_Patient_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;T
493;Liver_Patient_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
494;Liver_Patient_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
495;Liver_Patient_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.;T
496;Liver_Patient_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 4.;T
497;Liver_Patient_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.;T
498;Liver_Patient_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;T
499;Liver_Patient_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
500;Liver_Patient_pca.png;The first 3 principal components are enough for explaining half the data variance.;T
501;Liver_Patient_pca.png;Using the first 3 principal components would imply an error between 10 and 25%.;F
502;Liver_Patient_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 8.;F
503;Liver_Patient_correlation_heatmap.png;One of the variables ALB or DB can be discarded without losing information.;F
504;Liver_Patient_correlation_heatmap.png;The variable AG_Ratio can be discarded without risking losing information.;F
505;Liver_Patient_correlation_heatmap.png;Variables AG_Ratio and DB are redundant, but we can’t say the same for the pair Sgpt and Sgot.;F
506;Liver_Patient_correlation_heatmap.png;Variables Sgpt and AG_Ratio are redundant.;F
507;Liver_Patient_correlation_heatmap.png;Considering that the target variable is DB we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
508;Liver_Patient_correlation_heatmap.png;Considering that the target variable is Sgot we can say that variable Sgpt seems to be relevant for the majority of mining tasks.;T
509;Liver_Patient_correlation_heatmap.png;Considering that the target variable is TB we can say that variables Age and DB seem to be useful for classification tasks.;F
510;Liver_Patient_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
511;Liver_Patient_correlation_heatmap.png;Removing variable DB might improve the training of decision trees .;T
512;Liver_Patient_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable DB previously than variable TB.;F
513;Liver_Patient_boxplots.png;Variable ALB is balanced.;T
514;Liver_Patient_boxplots.png;Those boxplots show that the data is not normalized.;T
515;Liver_Patient_boxplots.png;It is clear that variable Sgpt shows some outliers, but we can’t be sure of the same for variable TP.;F
516;Liver_Patient_boxplots.png;Outliers seem to be a problem in the dataset.;T
517;Liver_Patient_boxplots.png;Variable Sgot shows a high number of outlier values.;T
518;Liver_Patient_boxplots.png;Variable TP doesn’t have any outliers.;F
519;Liver_Patient_boxplots.png;Variable Age presents some outliers.;T
520;Liver_Patient_boxplots.png;At least 75% of the variables present outliers.;T
521;Liver_Patient_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
522;Liver_Patient_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
523;Liver_Patient_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
524;Liver_Patient_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
525;Liver_Patient_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
526;Liver_Patient_histograms_symbolic.png;All variables should be dealt with as numeric.;F
527;Liver_Patient_histograms_symbolic.png;The variable Gender can be seen as ordinal.;T
528;Liver_Patient_histograms_symbolic.png;The variable Gender can be seen as ordinal without losing information.;F
529;Liver_Patient_histograms_symbolic.png;Considering the common semantics for Gender and <all_variables> variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
530;Liver_Patient_histograms_symbolic.png;Considering the common semantics for Gender variable, dummification would be the most adequate encoding.;F
531;Liver_Patient_histograms_symbolic.png;The variable Gender can be coded as ordinal without losing information.;F
532;Liver_Patient_histograms_symbolic.png;Feature generation based on variable Gender seems to be promising.;F
533;Liver_Patient_histograms_symbolic.png;Given the usual semantics of Gender variable, dummification would have been a better codification.;F
534;Liver_Patient_histograms_symbolic.png;Not knowing the semantics of Gender variable, dummification could have been a more adequate codification.;T
535;Liver_Patient_mv.png;Considering that the dataset has 405 records, discarding variable AG_Ratio would be better than discarding all the records with missing values for that variable.;F
536;Liver_Patient_mv.png;Considering that the dataset has 405 records, dropping all records with missing values would be better than to drop the variables with missing values.;T
537;Liver_Patient_mv.png;Considering that the dataset has 405 records, dropping all rows with missing values can lead to a dataset with less than 30% of the original data.;F
538;Liver_Patient_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;T
539;Liver_Patient_mv.png;Considering that the dataset has 405 records, it is better to drop the variable AG_Ratio than removing all records with missing values.;F
540;Liver_Patient_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
541;Liver_Patient_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
542;Liver_Patient_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
543;Liver_Patient_histograms_numeric.png;All variables should be dealt with as binary.;F
544;Liver_Patient_histograms_numeric.png;The variable ALB can be seen as ordinal.;F
545;Liver_Patient_histograms_numeric.png;The variable AG_Ratio can be seen as ordinal without losing information.;F
546;Liver_Patient_histograms_numeric.png;Variable Age is balanced.;T
547;Liver_Patient_histograms_numeric.png;It is clear that variable Sgot shows some outliers, but we can’t be sure of the same for variable Age.;T
548;Liver_Patient_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
549;Liver_Patient_histograms_numeric.png;Variable ALB shows a high number of outlier values.;F
550;Liver_Patient_histograms_numeric.png;Variable DB doesn’t have any outliers.;F
551;Liver_Patient_histograms_numeric.png;Variable Alkphos presents some outliers.;T
552;Liver_Patient_histograms_numeric.png;At least 50% of the variables present outliers.;T
553;Liver_Patient_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
554;Liver_Patient_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
555;Liver_Patient_histograms_numeric.png;Considering the common semantics for Age and TB variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
556;Liver_Patient_histograms_numeric.png;Considering the common semantics for TB variable, dummification would be the most adequate encoding.;F
557;Liver_Patient_histograms_numeric.png;The variable AG_Ratio can be coded as ordinal without losing information.;F
558;Liver_Patient_histograms_numeric.png;Feature generation based on variable ALB seems to be promising.;F
559;Liver_Patient_histograms_numeric.png;Feature generation based on the use of variable Sgpt wouldn’t be useful, but the use of Age seems to be promising.;F
560;Liver_Patient_histograms_numeric.png;Given the usual semantics of Alkphos variable, dummification would have been a better codification.;F
561;Liver_Patient_histograms_numeric.png;Not knowing the semantics of AG_Ratio variable, dummification could have been a more adequate codification.;F
562;Hotel_Reservations_decision_tree.png;The variable lead_time discriminates between the target values, as shown in the decision tree.;T
563;Hotel_Reservations_decision_tree.png;Variable no_of_special_requests is one of the most relevant variables.;T
564;Hotel_Reservations_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T
565;Hotel_Reservations_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider Canceled as the positive class and Not_Canceled as the negative class.;F
566;Hotel_Reservations_decision_tree.png;The recall for the presented tree is lower than 75%, consider Canceled as the positive class and Not_Canceled as the negative class.;T
567;Hotel_Reservations_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree, consider Canceled as the positive class and Not_Canceled as the negative class.;F
568;Hotel_Reservations_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree, consider Canceled as the positive class and Not_Canceled as the negative class.;F
569;Hotel_Reservations_decision_tree.png;The variable lead_time discriminates between the target values, as shown in the decision tree.;T
570;Hotel_Reservations_decision_tree.png;Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A, not B) as Canceled for any k ≤ 4955.;F
571;Hotel_Reservations_decision_tree.png;Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A,B) as Not_Canceled for any k ≤ 10612.;T
572;Hotel_Reservations_decision_tree.png;Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A,B) as Canceled for any k ≤ 9756.;F
573;Hotel_Reservations_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
574;Hotel_Reservations_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;T
575;Hotel_Reservations_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;T
576;Hotel_Reservations_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
577;Hotel_Reservations_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
578;Hotel_Reservations_overfitting_knn.png;KNN is in overfitting for k less than 5.;T
579;Hotel_Reservations_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;T
580;Hotel_Reservations_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
581;Hotel_Reservations_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;T
582;Hotel_Reservations_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.;F
583;Hotel_Reservations_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 3.;F
584;Hotel_Reservations_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.;F
585;Hotel_Reservations_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;F
586;Hotel_Reservations_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
587;Hotel_Reservations_pca.png;The first 3 principal components are enough for explaining half the data variance.;T
588;Hotel_Reservations_pca.png;Using the first 5 principal components would imply an error between 10 and 20%.;F
589;Hotel_Reservations_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.;F
590;Hotel_Reservations_correlation_heatmap.png;One of the variables arrival_month or no_of_special_requests can be discarded without losing information.;F
591;Hotel_Reservations_correlation_heatmap.png;The variable no_of_adults can be discarded without risking losing information.;F
592;Hotel_Reservations_correlation_heatmap.png;Variables no_of_adults and arrival_month are redundant, but we can’t say the same for the pair no_of_week_nights and no_of_weekend_nights.;F
593;Hotel_Reservations_correlation_heatmap.png;Variables no_of_adults and no_of_week_nights are redundant.;F
594;Hotel_Reservations_correlation_heatmap.png;Considering that the target variable is arrival_month we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
595;Hotel_Reservations_correlation_heatmap.png;Considering that the target variable is avg_price_per_room we can say that variable arrival_month seems to be relevant for the majority of mining tasks.;F
596;Hotel_Reservations_correlation_heatmap.png;Considering that the target variable is arrival_date we can say that variables arrival_month and no_of_adults seem to be useful for classification tasks.;F
597;Hotel_Reservations_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
598;Hotel_Reservations_correlation_heatmap.png;Removing variable no_of_adults might improve the training of decision trees .;F
599;Hotel_Reservations_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable arrival_date previously than variable no_of_week_nights.;F
600;Hotel_Reservations_boxplots.png;Variable arrival_date is balanced.;T
601;Hotel_Reservations_boxplots.png;Those boxplots show that the data is not normalized.;T
602;Hotel_Reservations_boxplots.png;It is clear that variable no_of_weekend_nights shows some outliers, but we can’t be sure of the same for variable lead_time.;F
603;Hotel_Reservations_boxplots.png;Outliers seem to be a problem in the dataset.;F
604;Hotel_Reservations_boxplots.png;Variable no_of_week_nights shows a high number of outlier values.;F
605;Hotel_Reservations_boxplots.png;Variable no_of_week_nights doesn’t have any outliers.;T
606;Hotel_Reservations_boxplots.png;Variable avg_price_per_room presents some outliers.;T
607;Hotel_Reservations_boxplots.png;At least 85% of the variables present outliers.;F
608;Hotel_Reservations_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
609;Hotel_Reservations_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
610;Hotel_Reservations_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
611;Hotel_Reservations_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
612;Hotel_Reservations_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
613;Hotel_Reservations_histograms_symbolic.png;All variables should be dealt with as symbolic.;T
614;Hotel_Reservations_histograms_symbolic.png;The variable room_type_reserved can be seen as ordinal.;T
615;Hotel_Reservations_histograms_symbolic.png;The variable type_of_meal_plan can be seen as ordinal without losing information.;T
616;Hotel_Reservations_histograms_symbolic.png;Considering the common semantics for arrival_year and type_of_meal_plan variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
617;Hotel_Reservations_histograms_symbolic.png;Considering the common semantics for type_of_meal_plan variable, dummification would be the most adequate encoding.;F
618;Hotel_Reservations_histograms_symbolic.png;The variable type_of_meal_plan can be coded as ordinal without losing information.;T
619;Hotel_Reservations_histograms_symbolic.png;Feature generation based on variable arrival_year seems to be promising.;F
620;Hotel_Reservations_histograms_symbolic.png;Feature generation based on the use of variable required_car_parking_space wouldn’t be useful, but the use of type_of_meal_plan seems to be promising.;F
621;Hotel_Reservations_histograms_symbolic.png;Given the usual semantics of required_car_parking_space variable, dummification would have been a better codification.;F
622;Hotel_Reservations_histograms_symbolic.png;Not knowing the semantics of arrival_year variable, dummification could have been a more adequate codification.;T
623;Hotel_Reservations_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
624;Hotel_Reservations_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;F
625;Hotel_Reservations_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
626;Hotel_Reservations_histograms_numeric.png;All variables should be dealt with as date.;F
627;Hotel_Reservations_histograms_numeric.png;The variable arrival_date can be seen as ordinal.;T
628;Hotel_Reservations_histograms_numeric.png;The variable no_of_children can be seen as ordinal without losing information.;T
629;Hotel_Reservations_histograms_numeric.png;Variable no_of_children is balanced.;F
630;Hotel_Reservations_histograms_numeric.png;It is clear that variable no_of_special_requests shows some outliers, but we can’t be sure of the same for variable avg_price_per_room.;F
631;Hotel_Reservations_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
632;Hotel_Reservations_histograms_numeric.png;Variable arrival_date shows a high number of outlier values.;F
633;Hotel_Reservations_histograms_numeric.png;Variable no_of_adults doesn’t have any outliers.;T
634;Hotel_Reservations_histograms_numeric.png;Variable no_of_weekend_nights presents some outliers.;T
635;Hotel_Reservations_histograms_numeric.png;At least 75% of the variables present outliers.;F
636;Hotel_Reservations_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
637;Hotel_Reservations_histograms_numeric.png;Considering the common semantics for arrival_date and no_of_adults variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
638;Hotel_Reservations_histograms_numeric.png;Considering the common semantics for no_of_special_requests variable, dummification would be the most adequate encoding.;F
639;Hotel_Reservations_histograms_numeric.png;The variable avg_price_per_room can be coded as ordinal without losing information.;F
640;Hotel_Reservations_histograms_numeric.png;Feature generation based on variable no_of_special_requests seems to be promising.;F
641;Hotel_Reservations_histograms_numeric.png;Feature generation based on the use of variable no_of_week_nights wouldn’t be useful, but the use of no_of_adults seems to be promising.;F
642;Hotel_Reservations_histograms_numeric.png;Given the usual semantics of no_of_adults variable, dummification would have been a better codification.;F
643;Hotel_Reservations_histograms_numeric.png;Not knowing the semantics of no_of_week_nights variable, dummification could have been a more adequate codification.;F
644;StressLevelDataset_decision_tree.png;The variable bullying discriminates between the target values, as shown in the decision tree.;T
645;StressLevelDataset_decision_tree.png;Variable basic_needs is one of the most relevant variables.;T
646;StressLevelDataset_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T
647;StressLevelDataset_decision_tree.png;The variable basic_needs seems to be one of the four most relevant features.;T
648;StressLevelDataset_decision_tree.png;Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], the Decision Tree presented classifies (A, not B) as 2.;T
649;StressLevelDataset_decision_tree.png;Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], the Decision Tree presented classifies (not A, B) as 1.;F
650;StressLevelDataset_decision_tree.png;Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 271.;F
651;StressLevelDataset_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
652;StressLevelDataset_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;F
653;StressLevelDataset_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;T
654;StressLevelDataset_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
655;StressLevelDataset_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
656;StressLevelDataset_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
657;StressLevelDataset_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;F
658;StressLevelDataset_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
659;StressLevelDataset_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;F
660;StressLevelDataset_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.;T
661;StressLevelDataset_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 6.;T
662;StressLevelDataset_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.;T
663;StressLevelDataset_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;F
664;StressLevelDataset_pca.png;The first 4 principal components are enough for explaining half the data variance.;T
665;StressLevelDataset_pca.png;Using the first 2 principal components would imply an error between 5 and 25%.;T
666;StressLevelDataset_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 6.;F
667;StressLevelDataset_correlation_heatmap.png;One of the variables headache or bullying can be discarded without losing information.;F
668;StressLevelDataset_correlation_heatmap.png;The variable breathing_problem can be discarded without risking losing information.;F
669;StressLevelDataset_correlation_heatmap.png;Variables anxiety_level and bullying are redundant, but we can’t say the same for the pair study_load and living_conditions.;F
670;StressLevelDataset_correlation_heatmap.png;Variables bullying and depression are redundant.;F
671;StressLevelDataset_correlation_heatmap.png;Considering that the target variable is stress_level we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
672;StressLevelDataset_correlation_heatmap.png;Considering that the target variable is anxiety_level we can say that variable breathing_problem seems to be relevant for the majority of mining tasks.;F
673;StressLevelDataset_correlation_heatmap.png;Considering that the target variable is stress_level we can say that variables living_conditions and breathing_problem seem to be useful for classification tasks.;F
674;StressLevelDataset_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
675;StressLevelDataset_correlation_heatmap.png;Removing variable basic_needs might improve the training of decision trees .;T
676;StressLevelDataset_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable basic_needs previously than variable self_esteem.;F
677;StressLevelDataset_boxplots.png;Variable study_load is balanced.;F
678;StressLevelDataset_boxplots.png;Those boxplots show that the data is not normalized.;T
679;StressLevelDataset_boxplots.png;It is clear that variable self_esteem shows some outliers, but we can’t be sure of the same for variable anxiety_level.;F
680;StressLevelDataset_boxplots.png;Outliers seem to be a problem in the dataset.;F
681;StressLevelDataset_boxplots.png;Variable basic_needs shows some outlier values.;T
682;StressLevelDataset_boxplots.png;Variable headache doesn’t have any outliers.;T
683;StressLevelDataset_boxplots.png;Variable depression presents some outliers.;F
684;StressLevelDataset_boxplots.png;At least 60% of the variables present outliers.;F
685;StressLevelDataset_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;F
686;StressLevelDataset_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
687;StressLevelDataset_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
688;StressLevelDataset_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
689;StressLevelDataset_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
690;StressLevelDataset_histograms_symbolic.png;All variables should be dealt with as symbolic.;F
691;StressLevelDataset_histograms_symbolic.png;The variable mental_health_history can be seen as ordinal.;T
692;StressLevelDataset_histograms_symbolic.png;The variable mental_health_history can be seen as ordinal without losing information.;T
693;StressLevelDataset_histograms_symbolic.png;Considering the common semantics for mental_health_history and <all_variables> variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
694;StressLevelDataset_histograms_symbolic.png;Considering the common semantics for mental_health_history variable, dummification would be the most adequate encoding.;F
695;StressLevelDataset_histograms_symbolic.png;The variable mental_health_history can be coded as ordinal without losing information.;T
696;StressLevelDataset_histograms_symbolic.png;Feature generation based on variable mental_health_history seems to be promising.;F
697;StressLevelDataset_histograms_symbolic.png;Given the usual semantics of mental_health_history variable, dummification would have been a better codification.;F
698;StressLevelDataset_histograms_symbolic.png;Not knowing the semantics of mental_health_history variable, dummification could have been a more adequate codification.;F
699;StressLevelDataset_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
700;StressLevelDataset_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;F
701;StressLevelDataset_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
702;StressLevelDataset_histograms_numeric.png;All variables should be dealt with as symbolic.;F
703;StressLevelDataset_histograms_numeric.png;The variable living_conditions can be seen as ordinal.;T
704;StressLevelDataset_histograms_numeric.png;The variable breathing_problem can be seen as ordinal without losing information.;T
705;StressLevelDataset_histograms_numeric.png;Variable breathing_problem is balanced.;F
706;StressLevelDataset_histograms_numeric.png;It is clear that variable depression shows some outliers, but we can’t be sure of the same for variable study_load.;F
707;StressLevelDataset_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
708;StressLevelDataset_histograms_numeric.png;Variable bullying shows a high number of outlier values.;F
709;StressLevelDataset_histograms_numeric.png;Variable headache doesn’t have any outliers.;T
710;StressLevelDataset_histograms_numeric.png;Variable anxiety_level presents some outliers.;F
711;StressLevelDataset_histograms_numeric.png;At least 60% of the variables present outliers.;F
712;StressLevelDataset_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
713;StressLevelDataset_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
714;StressLevelDataset_histograms_numeric.png;Considering the common semantics for sleep_quality and anxiety_level variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
715;StressLevelDataset_histograms_numeric.png;Considering the common semantics for headache variable, dummification would be the most adequate encoding.;F
716;StressLevelDataset_histograms_numeric.png;The variable breathing_problem can be coded as ordinal without losing information.;T
717;StressLevelDataset_histograms_numeric.png;Feature generation based on variable self_esteem seems to be promising.;F
718;StressLevelDataset_histograms_numeric.png;Feature generation based on the use of variable anxiety_level wouldn’t be useful, but the use of self_esteem seems to be promising.;F
719;StressLevelDataset_histograms_numeric.png;Given the usual semantics of study_load variable, dummification would have been a better codification.;F
720;StressLevelDataset_histograms_numeric.png;Not knowing the semantics of basic_needs variable, dummification could have been a more adequate codification.;F
721;WineQT_decision_tree.png;The variable chlorides discriminates between the target values, as shown in the decision tree.;T
722;WineQT_decision_tree.png;Variable density is one of the most relevant variables.;T
723;WineQT_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
724;WineQT_decision_tree.png;Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that KNN algorithm classifies (not A, not B) as 6 for any k ≤ 447.;F
725;WineQT_decision_tree.png;Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that KNN algorithm classifies (not A, not B) as 5 for any k ≤ 172.;T
726;WineQT_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
727;WineQT_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;F
728;WineQT_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;F
729;WineQT_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
730;WineQT_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
731;WineQT_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
732;WineQT_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;T
733;WineQT_overfitting_knn.png;KNN with more than 7 neighbours is in overfitting.;F
734;WineQT_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;T
735;WineQT_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;T
736;WineQT_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 8.;T
737;WineQT_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 10.;T
738;WineQT_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;F
739;WineQT_pca.png;The first 8 principal components are enough for explaining half the data variance.;T
740;WineQT_pca.png;Using the first 6 principal components would imply an error between 15 and 25%.;F
741;WineQT_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.;F
742;WineQT_correlation_heatmap.png;One of the variables citric acid or residual sugar can be discarded without losing information.;F
743;WineQT_correlation_heatmap.png;The variable chlorides can be discarded without risking losing information.;F
744;WineQT_correlation_heatmap.png;Variables sulphates and pH are redundant, but we can’t say the same for the pair free sulfur dioxide and volatile acidity.;F
745;WineQT_correlation_heatmap.png;Variables free sulfur dioxide and total sulfur dioxide are redundant.;T
746;WineQT_correlation_heatmap.png;Considering that the target variable is density we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
747;WineQT_correlation_heatmap.png;Considering that the target variable is citric acid we can say that variable volatile acidity seems to be relevant for the majority of mining tasks.;F
748;WineQT_correlation_heatmap.png;Considering that the target variable is fixed acidity we can say that variables chlorides and citric acid seem to be useful for classification tasks.;F
749;WineQT_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
750;WineQT_correlation_heatmap.png;Removing variable fixed acidity might improve the training of decision trees .;T
751;WineQT_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable pH previously than variable chlorides.;T
752;WineQT_boxplots.png;Variable citric acid is balanced.;F
753;WineQT_boxplots.png;Those boxplots show that the data is not normalized.;T
754;WineQT_boxplots.png;It is clear that variable pH shows some outliers, but we can’t be sure of the same for variable volatile acidity.;F
755;WineQT_boxplots.png;Outliers seem to be a problem in the dataset.;T
756;WineQT_boxplots.png;Variable free sulfur dioxide shows a high number of outlier values.;T
757;WineQT_boxplots.png;Variable chlorides doesn’t have any outliers.;F
758;WineQT_boxplots.png;Variable total sulfur dioxide presents some outliers.;T
759;WineQT_boxplots.png;At least 75% of the variables present outliers.;T
760;WineQT_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
761;WineQT_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
762;WineQT_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
763;WineQT_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
764;WineQT_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
765;WineQT_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
766;WineQT_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
767;WineQT_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
768;WineQT_histograms_numeric.png;All variables should be dealt with as numeric.;T
769;WineQT_histograms_numeric.png;The variable fixed acidity can be seen as ordinal.;F
770;WineQT_histograms_numeric.png;The variable pH can be seen as ordinal without losing information.;F
771;WineQT_histograms_numeric.png;Variable free sulfur dioxide is balanced.;F
772;WineQT_histograms_numeric.png;It is clear that variable alcohol shows some outliers, but we can’t be sure of the same for variable sulphates.;F
773;WineQT_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
774;WineQT_histograms_numeric.png;Variable sulphates shows a high number of outlier values.;F
775;WineQT_histograms_numeric.png;Variable pH doesn’t have any outliers.;F
776;WineQT_histograms_numeric.png;Variable citric acid presents some outliers.;F
777;WineQT_histograms_numeric.png;At least 75% of the variables present outliers.;F
778;WineQT_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
779;WineQT_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
780;WineQT_histograms_numeric.png;Considering the common semantics for citric acid and fixed acidity variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
781;WineQT_histograms_numeric.png;Considering the common semantics for citric acid variable, dummification would be the most adequate encoding.;F
782;WineQT_histograms_numeric.png;The variable pH can be coded as ordinal without losing information.;F
783;WineQT_histograms_numeric.png;Feature generation based on variable density seems to be promising.;F
784;WineQT_histograms_numeric.png;Feature generation based on the use of variable sulphates wouldn’t be useful, but the use of fixed acidity seems to be promising.;F
785;WineQT_histograms_numeric.png;Given the usual semantics of citric acid variable, dummification would have been a better codification.;F
786;WineQT_histograms_numeric.png;Not knowing the semantics of pH variable, dummification could have been a more adequate codification.;F
787;loan_data_decision_tree.png;The variable ApplicantIncome discriminates between the target values, as shown in the decision tree.;T
788;loan_data_decision_tree.png;Variable ApplicantIncome is one of the most relevant variables.;T
789;loan_data_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T
790;loan_data_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider Y as the positive class and N as the negative class.;T
791;loan_data_decision_tree.png;The recall for the presented tree is lower than 90%, consider Y as the positive class and N as the negative class.;F
792;loan_data_decision_tree.png;The number of False Positives is higher than the number of True Negatives for the presented tree, consider Y as the positive class and N as the negative class.;T
793;loan_data_decision_tree.png;The number of False Negatives is higher than the number of True Positives for the presented tree, consider Y as the positive class and N as the negative class.;F
794;loan_data_decision_tree.png;The recall for the presented tree is higher than its accuracy, consider Y as the positive class and N as the negative class.;T
795;loan_data_decision_tree.png;Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that KNN algorithm classifies (not A, not B) as Y for any k ≤ 3.;F
796;loan_data_decision_tree.png;Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that KNN algorithm classifies (not A, B) as N for any k ≤ 204.;F
797;loan_data_decision_tree.png;Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as N.;T
798;loan_data_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
799;loan_data_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;T
800;loan_data_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;F
801;loan_data_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
802;loan_data_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
803;loan_data_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
804;loan_data_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;T
805;loan_data_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
806;loan_data_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;T
807;loan_data_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;F
808;loan_data_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 5.;F
809;loan_data_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 10.;F
810;loan_data_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;F
811;loan_data_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
812;loan_data_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
813;loan_data_pca.png;Using the first 2 principal components would imply an error between 10 and 20%.;F
814;loan_data_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
815;loan_data_correlation_heatmap.png;One of the variables CoapplicantIncome or ApplicantIncome can be discarded without losing information.;F
816;loan_data_correlation_heatmap.png;The variable CoapplicantIncome can be discarded without risking losing information.;F
817;loan_data_correlation_heatmap.png;Considering that the target variable is LoanAmount we can say that variables ApplicantIncome and LoanAmount seem to be useful for classification tasks.;F
818;loan_data_correlation_heatmap.png;Variables Loan_Amount_Term and CoapplicantIncome are redundant.;F
819;loan_data_correlation_heatmap.png;Considering that the target variable is LoanAmount we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
820;loan_data_correlation_heatmap.png;Considering that the target variable is LoanAmount we can say that variable ApplicantIncome seems to be relevant for the majority of mining tasks.;F
821;loan_data_correlation_heatmap.png;Considering that the target variable is LoanAmount we can say that variables CoapplicantIncome and ApplicantIncome seem to be useful for classification tasks.;F
822;loan_data_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
823;loan_data_correlation_heatmap.png;Removing variable LoanAmount might improve the training of decision trees .;F
824;loan_data_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable CoapplicantIncome previously than variable Loan_Amount_Term.;F
825;loan_data_boxplots.png;Variable Loan_Amount_Term is balanced.;F
826;loan_data_boxplots.png;Those boxplots show that the data is not normalized.;T
827;loan_data_boxplots.png;It is clear that variable Loan_Amount_Term shows some outliers, but we can’t be sure of the same for variable ApplicantIncome.;F
828;loan_data_boxplots.png;Outliers seem to be a problem in the dataset.;T
829;loan_data_boxplots.png;Variable ApplicantIncome shows a high number of outlier values.;T
830;loan_data_boxplots.png;Variable Loan_Amount_Term doesn’t have any outliers.;F
831;loan_data_boxplots.png;Variable ApplicantIncome presents some outliers.;T
832;loan_data_boxplots.png;At least 50% of the variables present outliers.;T
833;loan_data_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
834;loan_data_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
835;loan_data_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
836;loan_data_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
837;loan_data_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
838;loan_data_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
839;loan_data_histograms_symbolic.png;All variables should be dealt with as numeric.;F
840;loan_data_histograms_symbolic.png;The variable Credit_History can be seen as ordinal.;T
841;loan_data_histograms_symbolic.png;The variable Married can be seen as ordinal without losing information.;T
842;loan_data_histograms_symbolic.png;Considering the common semantics for Credit_History and Dependents variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
843;loan_data_histograms_symbolic.png;Considering the common semantics for Property_Area variable, dummification would be the most adequate encoding.;T
844;loan_data_histograms_symbolic.png;The variable Dependents can be coded as ordinal without losing information.;T
845;loan_data_histograms_symbolic.png;Feature generation based on variable Dependents seems to be promising.;F
846;loan_data_histograms_symbolic.png;Feature generation based on the use of variable Self_Employed wouldn’t be useful, but the use of Dependents seems to be promising.;F
847;loan_data_histograms_symbolic.png;Given the usual semantics of Education variable, dummification would have been a better codification.;F
848;loan_data_histograms_symbolic.png;Not knowing the semantics of Dependents variable, dummification could have been a more adequate codification.;F
849;loan_data_mv.png;Considering that the dataset has 500 records, discarding variable Gender would be better than discarding all the records with missing values for that variable.;F
850;loan_data_mv.png;Considering that the dataset has 1175 records, dropping all records with missing values would be better than to drop the variables with missing values.;T
851;loan_data_mv.png;Considering that the dataset has 500 records, dropping all rows with missing values can lead to a dataset with less than 25% of the original data.;F
852;loan_data_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;T
853;loan_data_mv.png;Considering that the dataset has 60 records, it is better to drop the variable Self_Employed than removing all records with missing values.;T
854;loan_data_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
855;loan_data_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
856;loan_data_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
857;loan_data_histograms_numeric.png;All variables should be dealt with as date.;F
858;loan_data_histograms_numeric.png;The variable Loan_Amount_Term can be seen as ordinal.;F
859;loan_data_histograms_numeric.png;The variable CoapplicantIncome can be seen as ordinal without losing information.;F
860;loan_data_histograms_numeric.png;Variable LoanAmount is balanced.;F
861;loan_data_histograms_numeric.png;It is clear that variable LoanAmount shows some outliers, but we can’t be sure of the same for variable Loan_Amount_Term.;F
862;loan_data_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
863;loan_data_histograms_numeric.png;Variable Loan_Amount_Term shows a high number of outlier values.;F
864;loan_data_histograms_numeric.png;Variable Loan_Amount_Term doesn’t have any outliers.;T
865;loan_data_histograms_numeric.png;Variable LoanAmount presents some outliers.;F
866;loan_data_histograms_numeric.png;At least 85% of the variables present outliers.;F
867;loan_data_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
868;loan_data_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
869;loan_data_histograms_numeric.png;Considering the common semantics for LoanAmount and ApplicantIncome variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
870;loan_data_histograms_numeric.png;Considering the common semantics for LoanAmount variable, dummification would be the most adequate encoding.;F
871;loan_data_histograms_numeric.png;The variable LoanAmount can be coded as ordinal without losing information.;F
872;loan_data_histograms_numeric.png;Feature generation based on variable LoanAmount seems to be promising.;F
873;loan_data_histograms_numeric.png;Feature generation based on the use of variable CoapplicantIncome wouldn’t be useful, but the use of ApplicantIncome seems to be promising.;F
874;loan_data_histograms_numeric.png;Given the usual semantics of ApplicantIncome variable, dummification would have been a better codification.;F
875;loan_data_histograms_numeric.png;Not knowing the semantics of Loan_Amount_Term variable, dummification could have been a more adequate codification.;F
876;Dry_Bean_Dataset_decision_tree.png;The variable Area discriminates between the target values, as shown in the decision tree.;T
877;Dry_Bean_Dataset_decision_tree.png;Variable AspectRation is one of the most relevant variables.;T
878;Dry_Bean_Dataset_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;F
879;Dry_Bean_Dataset_decision_tree.png;Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], it is possible to state that KNN algorithm classifies (not A, not B) as SEKER for any k ≤ 1284.;F
880;Dry_Bean_Dataset_decision_tree.png;Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], the Decision Tree presented classifies (not A, B) as BOMBAY.;F
881;Dry_Bean_Dataset_decision_tree.png;Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], it is possible to state that KNN algorithm classifies (A,B) as DERMASON for any k ≤ 2501.;F
882;Dry_Bean_Dataset_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
883;Dry_Bean_Dataset_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;F
884;Dry_Bean_Dataset_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;F
885;Dry_Bean_Dataset_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
886;Dry_Bean_Dataset_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
887;Dry_Bean_Dataset_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
888;Dry_Bean_Dataset_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;F
889;Dry_Bean_Dataset_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
890;Dry_Bean_Dataset_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;F
891;Dry_Bean_Dataset_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;T
892;Dry_Bean_Dataset_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 8.;T
893;Dry_Bean_Dataset_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.;T
894;Dry_Bean_Dataset_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;F
895;Dry_Bean_Dataset_pca.png;The first 6 principal components are enough for explaining half the data variance.;T
896;Dry_Bean_Dataset_pca.png;Using the first 2 principal components would imply an error between 15 and 25%.;F
897;Dry_Bean_Dataset_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 9.;F
898;Dry_Bean_Dataset_correlation_heatmap.png;One of the variables MinorAxisLength or Eccentricity can be discarded without losing information.;T
899;Dry_Bean_Dataset_correlation_heatmap.png;The variable Eccentricity can be discarded without risking losing information.;T
900;Dry_Bean_Dataset_correlation_heatmap.png;Variables MinorAxisLength and Solidity are redundant, but we can’t say the same for the pair ShapeFactor1 and Extent.;F
901;Dry_Bean_Dataset_correlation_heatmap.png;Variables roundness and ShapeFactor1 are redundant.;F
902;Dry_Bean_Dataset_correlation_heatmap.png;Considering that the target variable is Area we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
903;Dry_Bean_Dataset_correlation_heatmap.png;Considering that the target variable is MinorAxisLength we can say that variable ShapeFactor1 seems to be relevant for the majority of mining tasks.;T
904;Dry_Bean_Dataset_correlation_heatmap.png;Considering that the target variable is ShapeFactor1 we can say that variables Perimeter and Eccentricity seem to be useful for classification tasks.;F
905;Dry_Bean_Dataset_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
906;Dry_Bean_Dataset_correlation_heatmap.png;Removing variable Solidity might improve the training of decision trees .;F
907;Dry_Bean_Dataset_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Eccentricity previously than variable EquivDiameter.;F
908;Dry_Bean_Dataset_boxplots.png;Variable MinorAxisLength is balanced.;F
909;Dry_Bean_Dataset_boxplots.png;Those boxplots show that the data is not normalized.;T
910;Dry_Bean_Dataset_boxplots.png;It is clear that variable Solidity shows some outliers, but we can’t be sure of the same for variable EquivDiameter.;T
911;Dry_Bean_Dataset_boxplots.png;Outliers seem to be a problem in the dataset.;T
912;Dry_Bean_Dataset_boxplots.png;Variable Solidity shows some outlier values.;T
913;Dry_Bean_Dataset_boxplots.png;Variable roundness doesn’t have any outliers.;F
914;Dry_Bean_Dataset_boxplots.png;Variable Eccentricity presents some outliers.;T
915;Dry_Bean_Dataset_boxplots.png;At least 50% of the variables present outliers.;T
916;Dry_Bean_Dataset_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;T
917;Dry_Bean_Dataset_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
918;Dry_Bean_Dataset_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
919;Dry_Bean_Dataset_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
920;Dry_Bean_Dataset_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
921;Dry_Bean_Dataset_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
922;Dry_Bean_Dataset_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
923;Dry_Bean_Dataset_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
924;Dry_Bean_Dataset_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
925;Dry_Bean_Dataset_histograms_numeric.png;All variables should be dealt with as date.;F
926;Dry_Bean_Dataset_histograms_numeric.png;The variable Perimeter can be seen as ordinal.;F
927;Dry_Bean_Dataset_histograms_numeric.png;The variable Extent can be seen as ordinal without losing information.;F
928;Dry_Bean_Dataset_histograms_numeric.png;Variable Solidity is balanced.;F
929;Dry_Bean_Dataset_histograms_numeric.png;It is clear that variable EquivDiameter shows some outliers, but we can’t be sure of the same for variable MinorAxisLength.;F
930;Dry_Bean_Dataset_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
931;Dry_Bean_Dataset_histograms_numeric.png;Variable Area shows some outlier values.;T
932;Dry_Bean_Dataset_histograms_numeric.png;Variable roundness doesn’t have any outliers.;T
933;Dry_Bean_Dataset_histograms_numeric.png;Variable Solidity presents some outliers.;T
934;Dry_Bean_Dataset_histograms_numeric.png;At least 85% of the variables present outliers.;F
935;Dry_Bean_Dataset_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;T
936;Dry_Bean_Dataset_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
937;Dry_Bean_Dataset_histograms_numeric.png;Considering the common semantics for AspectRation and Area variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
938;Dry_Bean_Dataset_histograms_numeric.png;Considering the common semantics for EquivDiameter variable, dummification would be the most adequate encoding.;F
939;Dry_Bean_Dataset_histograms_numeric.png;The variable roundness can be coded as ordinal without losing information.;F
940;Dry_Bean_Dataset_histograms_numeric.png;Feature generation based on variable EquivDiameter seems to be promising.;F
941;Dry_Bean_Dataset_histograms_numeric.png;Feature generation based on the use of variable MinorAxisLength wouldn’t be useful, but the use of Area seems to be promising.;F
942;Dry_Bean_Dataset_histograms_numeric.png;Given the usual semantics of roundness variable, dummification would have been a better codification.;F
943;Dry_Bean_Dataset_histograms_numeric.png;Not knowing the semantics of Perimeter variable, dummification could have been a more adequate codification.;F
944;credit_customers_decision_tree.png;The variable residence_since discriminates between the target values, as shown in the decision tree.;F
945;credit_customers_decision_tree.png;Variable residence_since is one of the most relevant variables.;T
946;credit_customers_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
947;credit_customers_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider good as the positive class and bad as the negative class.;T
948;credit_customers_decision_tree.png;The accuracy for the presented tree is higher than 90%.;F
949;credit_customers_decision_tree.png;The number of False Negatives is lower than the number of True Positives for the presented tree, consider good as the positive class and bad as the negative class.;T
950;credit_customers_decision_tree.png;The number of True Positives is higher than the number of False Positives for the presented tree, consider good as the positive class and bad as the negative class.;T
951;credit_customers_decision_tree.png;The accuracy for the presented tree is higher than its recall, consider good as the positive class and bad as the negative class.;F
952;credit_customers_decision_tree.png;Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 107.;F
953;credit_customers_decision_tree.png;Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as bad.;F
954;credit_customers_decision_tree.png;Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 264.;F
955;credit_customers_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
956;credit_customers_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;F
957;credit_customers_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;T
958;credit_customers_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
959;credit_customers_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
960;credit_customers_overfitting_knn.png;KNN is in overfitting for k less than 13.;T
961;credit_customers_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;T
962;credit_customers_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
963;credit_customers_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;T
964;credit_customers_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;T
965;credit_customers_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 3.;F
966;credit_customers_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.;T
967;credit_customers_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;T
968;credit_customers_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
969;credit_customers_pca.png;The first 3 principal components are enough for explaining half the data variance.;T
970;credit_customers_pca.png;Using the first 4 principal components would imply an error between 5 and 20%.;F
971;credit_customers_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
972;credit_customers_correlation_heatmap.png;One of the variables age or credit_amount can be discarded without losing information.;F
973;credit_customers_correlation_heatmap.png;The variable existing_credits can be discarded without risking losing information.;F
974;credit_customers_correlation_heatmap.png;Variables existing_credits and credit_amount are redundant, but we can’t say the same for the pair duration and installment_commitment.;F
975;credit_customers_correlation_heatmap.png;Variables residence_since and existing_credits are redundant.;F
976;credit_customers_correlation_heatmap.png;Considering that the target variable is duration we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
977;credit_customers_correlation_heatmap.png;Considering that the target variable is credit_amount we can say that variable age seems to be relevant for the majority of mining tasks.;F
978;credit_customers_correlation_heatmap.png;Considering that the target variable is residence_since we can say that variables age and installment_commitment seem to be useful for classification tasks.;F
979;credit_customers_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
980;credit_customers_correlation_heatmap.png;Removing variable credit_amount might improve the training of decision trees .;T
981;credit_customers_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable existing_credits previously than variable credit_amount.;F
982;credit_customers_boxplots.png;Variable existing_credits is balanced.;F
983;credit_customers_boxplots.png;Those boxplots show that the data is not normalized.;T
984;credit_customers_boxplots.png;It is clear that variable age shows some outliers, but we can’t be sure of the same for variable residence_since.;T
985;credit_customers_boxplots.png;Outliers seem to be a problem in the dataset.;T
986;credit_customers_boxplots.png;Variable age shows some outlier values.;T
987;credit_customers_boxplots.png;Variable residence_since doesn’t have any outliers.;T
988;credit_customers_boxplots.png;Variable age presents some outliers.;T
989;credit_customers_boxplots.png;At least 50% of the variables present outliers.;T
990;credit_customers_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
991;credit_customers_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
992;credit_customers_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
993;credit_customers_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
994;credit_customers_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
995;credit_customers_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
996;credit_customers_histograms_symbolic.png;All variables should be dealt with as numeric.;F
997;credit_customers_histograms_symbolic.png;The variable other_parties can be seen as ordinal.;T
998;credit_customers_histograms_symbolic.png;The variable employment can be seen as ordinal without losing information.;T
999;credit_customers_histograms_symbolic.png;Considering the common semantics for checking_status and employment variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1000;credit_customers_histograms_symbolic.png;Considering the common semantics for housing variable, dummification would be the most adequate encoding.;T
1001;credit_customers_histograms_symbolic.png;The variable checking_status can be coded as ordinal without losing information.;T
1002;credit_customers_histograms_symbolic.png;Feature generation based on variable num_dependents seems to be promising.;F
1003;credit_customers_histograms_symbolic.png;Feature generation based on the use of variable employment wouldn’t be useful, but the use of checking_status seems to be promising.;F
1004;credit_customers_histograms_symbolic.png;Given the usual semantics of own_telephone variable, dummification would have been a better codification.;F
1005;credit_customers_histograms_symbolic.png;Not knowing the semantics of num_dependents variable, dummification could have been a more adequate codification.;F
1006;credit_customers_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1007;credit_customers_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;F
1008;credit_customers_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1009;credit_customers_histograms_numeric.png;All variables should be dealt with as binary.;F
1010;credit_customers_histograms_numeric.png;The variable credit_amount can be seen as ordinal.;F
1011;credit_customers_histograms_numeric.png;The variable age can be seen as ordinal without losing information.;T
1012;credit_customers_histograms_numeric.png;Variable duration is balanced.;F
1013;credit_customers_histograms_numeric.png;It is clear that variable age shows some outliers, but we can’t be sure of the same for variable credit_amount.;F
1014;credit_customers_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1015;credit_customers_histograms_numeric.png;Variable residence_since shows some outlier values.;F
1016;credit_customers_histograms_numeric.png;Variable credit_amount doesn’t have any outliers.;F
1017;credit_customers_histograms_numeric.png;Variable existing_credits presents some outliers.;F
1018;credit_customers_histograms_numeric.png;At least 60% of the variables present outliers.;F
1019;credit_customers_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
1020;credit_customers_histograms_numeric.png;Considering the common semantics for residence_since and duration variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1021;credit_customers_histograms_numeric.png;Considering the common semantics for installment_commitment variable, dummification would be the most adequate encoding.;F
1022;credit_customers_histograms_numeric.png;The variable age can be coded as ordinal without losing information.;T
1023;credit_customers_histograms_numeric.png;Feature generation based on variable residence_since seems to be promising.;F
1024;credit_customers_histograms_numeric.png;Feature generation based on the use of variable credit_amount wouldn’t be useful, but the use of duration seems to be promising.;F
1025;credit_customers_histograms_numeric.png;Given the usual semantics of age variable, dummification would have been a better codification.;F
1026;credit_customers_histograms_numeric.png;Not knowing the semantics of installment_commitment variable, dummification could have been a more adequate codification.;F
1027;weatherAUS_decision_tree.png;The variable Pressure3pm discriminates between the target values, as shown in the decision tree.;T
1028;weatherAUS_decision_tree.png;Variable Rainfall is one of the most relevant variables.;T
1029;weatherAUS_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;T
1030;weatherAUS_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider Yes as the positive class and No as the negative class.;T
1031;weatherAUS_decision_tree.png;The precision for the presented tree is lower than 60%, consider Yes as the positive class and No as the negative class.;F
1032;weatherAUS_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree, consider Yes as the positive class and No as the negative class.;F
1033;weatherAUS_decision_tree.png;The number of False Negatives is lower than the number of False Positives for the presented tree, consider Yes as the positive class and No as the negative class.;F
1034;weatherAUS_decision_tree.png;The accuracy for the presented tree is higher than 75%.;T
1035;weatherAUS_decision_tree.png;Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], the Decision Tree presented classifies (not A, not B) as No.;T
1036;weatherAUS_decision_tree.png;Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], the Decision Tree presented classifies (not A, B) as Yes.;T
1037;weatherAUS_decision_tree.png;Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as No.;T
1038;weatherAUS_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
1039;weatherAUS_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;T
1040;weatherAUS_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;F
1041;weatherAUS_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1042;weatherAUS_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
1043;weatherAUS_overfitting_knn.png;KNN is in overfitting for k less than 5.;T
1044;weatherAUS_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;T
1045;weatherAUS_overfitting_knn.png;KNN with less than 17 neighbours is in overfitting.;F
1046;weatherAUS_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;T
1047;weatherAUS_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;F
1048;weatherAUS_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 3.;F
1049;weatherAUS_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.;F
1050;weatherAUS_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;F
1051;weatherAUS_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1052;weatherAUS_pca.png;The first 3 principal components are enough for explaining half the data variance.;T
1053;weatherAUS_pca.png;Using the first 6 principal components would imply an error between 5 and 20%.;F
1054;weatherAUS_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.;F
1055;weatherAUS_correlation_heatmap.png;One of the variables Pressure9am or Pressure3pm can be discarded without losing information.;T
1056;weatherAUS_correlation_heatmap.png;The variable Pressure9am can be discarded without risking losing information.;T
1057;weatherAUS_correlation_heatmap.png;Variables Rainfall and Pressure3pm are redundant, but we can’t say the same for the pair Pressure9am and Cloud3pm.;F
1058;weatherAUS_correlation_heatmap.png;Variables Temp3pm and Rainfall are redundant.;F
1059;weatherAUS_correlation_heatmap.png;Considering that the target variable is Pressure9am we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
1060;weatherAUS_correlation_heatmap.png;Considering that the target variable is Rainfall we can say that variable Temp3pm seems to be relevant for the majority of mining tasks.;F
1061;weatherAUS_correlation_heatmap.png;Considering that the target variable is Pressure9am we can say that variables Pressure9am and Cloud3pm seem to be useful for classification tasks.;F
1062;weatherAUS_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1063;weatherAUS_correlation_heatmap.png;Removing variable Cloud9am might improve the training of decision trees .;F
1064;weatherAUS_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Cloud9am previously than variable Pressure9am.;F
1065;weatherAUS_boxplots.png;Variable Pressure9am is balanced.;T
1066;weatherAUS_boxplots.png;Those boxplots show that the data is not normalized.;T
1067;weatherAUS_boxplots.png;It is clear that variable Cloud9am shows some outliers, but we can’t be sure of the same for variable WindSpeed9am.;F
1068;weatherAUS_boxplots.png;Outliers seem to be a problem in the dataset.;F
1069;weatherAUS_boxplots.png;Variable Rainfall shows a high number of outlier values.;F
1070;weatherAUS_boxplots.png;Variable Cloud9am doesn’t have any outliers.;T
1071;weatherAUS_boxplots.png;Variable Cloud9am presents some outliers.;F
1072;weatherAUS_boxplots.png;At least 60% of the variables present outliers.;T
1073;weatherAUS_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1074;weatherAUS_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1075;weatherAUS_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
1076;weatherAUS_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1077;weatherAUS_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1078;weatherAUS_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1079;weatherAUS_histograms_symbolic.png;All variables should be dealt with as symbolic.;T
1080;weatherAUS_histograms_symbolic.png;The variable WindDir9am can be seen as ordinal.;T
1081;weatherAUS_histograms_symbolic.png;The variable WindDir3pm can be seen as ordinal without losing information.;F
1082;weatherAUS_histograms_symbolic.png;Considering the common semantics for Location and WindGustDir variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1083;weatherAUS_histograms_symbolic.png;Considering the common semantics for WindGustDir variable, dummification would be the most adequate encoding.;T
1084;weatherAUS_histograms_symbolic.png;The variable WindDir3pm can be coded as ordinal without losing information.;F
1085;weatherAUS_histograms_symbolic.png;Feature generation based on variable WindDir3pm seems to be promising.;F
1086;weatherAUS_histograms_symbolic.png;Feature generation based on the use of variable WindDir3pm wouldn’t be useful, but the use of Location seems to be promising.;F
1087;weatherAUS_histograms_symbolic.png;Given the usual semantics of RainToday variable, dummification would have been a better codification.;F
1088;weatherAUS_histograms_symbolic.png;Not knowing the semantics of WindGustDir variable, dummification could have been a more adequate codification.;T
1089;weatherAUS_mv.png;Considering that the dataset has 3000 records, discarding variable Pressure9am would be better than discarding all the records with missing values for that variable.;T
1090;weatherAUS_mv.png;Considering that the dataset has 5000 records, dropping all records with missing values would be better than to drop the variables with missing values.;F
1091;weatherAUS_mv.png;Considering that the dataset has 5000 records, dropping all rows with missing values can lead to a dataset with less than 25% of the original data.;T
1092;weatherAUS_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;T
1093;weatherAUS_mv.png;Considering that the dataset has 3000 records, it is better to drop the variable Cloud9am than removing all records with missing values.;T
1094;weatherAUS_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1095;weatherAUS_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;F
1096;weatherAUS_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1097;weatherAUS_histograms_numeric.png;All variables should be dealt with as date.;F
1098;weatherAUS_histograms_numeric.png;The variable Rainfall can be seen as ordinal.;F
1099;weatherAUS_histograms_numeric.png;The variable Pressure3pm can be seen as ordinal without losing information.;F
1100;weatherAUS_histograms_numeric.png;Variable Cloud3pm is balanced.;F
1101;weatherAUS_histograms_numeric.png;It is clear that variable Pressure9am shows some outliers, but we can’t be sure of the same for variable Rainfall.;F
1102;weatherAUS_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1103;weatherAUS_histograms_numeric.png;Variable Pressure9am shows some outlier values.;F
1104;weatherAUS_histograms_numeric.png;Variable Cloud3pm doesn’t have any outliers.;T
1105;weatherAUS_histograms_numeric.png;Variable Pressure9am presents some outliers.;F
1106;weatherAUS_histograms_numeric.png;At least 85% of the variables present outliers.;F
1107;weatherAUS_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1108;weatherAUS_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
1109;weatherAUS_histograms_numeric.png;Considering the common semantics for Pressure3pm and Rainfall variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1110;weatherAUS_histograms_numeric.png;Considering the common semantics for Rainfall variable, dummification would be the most adequate encoding.;F
1111;weatherAUS_histograms_numeric.png;The variable Cloud9am can be coded as ordinal without losing information.;T
1112;weatherAUS_histograms_numeric.png;Feature generation based on variable Temp3pm seems to be promising.;F
1113;weatherAUS_histograms_numeric.png;Feature generation based on the use of variable Cloud9am wouldn’t be useful, but the use of Rainfall seems to be promising.;F
1114;weatherAUS_histograms_numeric.png;Given the usual semantics of WindSpeed9am variable, dummification would have been a better codification.;F
1115;weatherAUS_histograms_numeric.png;Not knowing the semantics of Rainfall variable, dummification could have been a more adequate codification.;F
1116;car_insurance_decision_tree.png;The variable displacement discriminates between the target values, as shown in the decision tree.;F
1117;car_insurance_decision_tree.png;Variable displacement is one of the most relevant variables.;T
1118;car_insurance_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T
1119;car_insurance_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;T
1120;car_insurance_decision_tree.png;The specificity for the presented tree is higher than 60%, consider 1 as the positive class and 0 as the negative class.;T
1121;car_insurance_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
1122;car_insurance_decision_tree.png;The number of True Negatives is lower than the number of True Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
1123;car_insurance_decision_tree.png;The number of True Positives is lower than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
1124;car_insurance_decision_tree.png;Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 2141.;T
1125;car_insurance_decision_tree.png;Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], the Decision Tree presented classifies (not A, B) as 1.;F
1126;car_insurance_decision_tree.png;Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 686.;F
1127;car_insurance_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
1128;car_insurance_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;T
1129;car_insurance_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;F
1130;car_insurance_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1131;car_insurance_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
1132;car_insurance_overfitting_knn.png;KNN is in overfitting for k larger than 5.;F
1133;car_insurance_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;F
1134;car_insurance_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.;F
1135;car_insurance_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;T
1136;car_insurance_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;T
1137;car_insurance_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 6.;T
1138;car_insurance_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.;T
1139;car_insurance_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;F
1140;car_insurance_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1141;car_insurance_pca.png;The first 4 principal components are enough for explaining half the data variance.;T
1142;car_insurance_pca.png;Using the first 6 principal components would imply an error between 5 and 25%.;F
1143;car_insurance_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 6.;T
1144;car_insurance_correlation_heatmap.png;One of the variables age_of_car or airbags can be discarded without losing information.;F
1145;car_insurance_correlation_heatmap.png;The variable length can be discarded without risking losing information.;T
1146;car_insurance_correlation_heatmap.png;Variables age_of_car and policy_tenure are redundant, but we can’t say the same for the pair height and length.;F
1147;car_insurance_correlation_heatmap.png;Variables age_of_car and gross_weight are redundant.;F
1148;car_insurance_correlation_heatmap.png;Considering that the target variable is displacement we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
1149;car_insurance_correlation_heatmap.png;Considering that the target variable is length we can say that variable height seems to be relevant for the majority of mining tasks.;F
1150;car_insurance_correlation_heatmap.png;Considering that the target variable is length we can say that variables gross_weight and width seem to be useful for classification tasks.;T
1151;car_insurance_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1152;car_insurance_correlation_heatmap.png;Removing variable length might improve the training of decision trees .;T
1153;car_insurance_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable length previously than variable gross_weight.;T
1154;car_insurance_boxplots.png;Variable height is balanced.;F
1155;car_insurance_boxplots.png;Those boxplots show that the data is not normalized.;T
1156;car_insurance_boxplots.png;It is clear that variable displacement shows some outliers, but we can’t be sure of the same for variable policy_tenure.;F
1157;car_insurance_boxplots.png;Outliers seem to be a problem in the dataset.;F
1158;car_insurance_boxplots.png;Variable airbags shows some outlier values.;F
1159;car_insurance_boxplots.png;Variable width doesn’t have any outliers.;T
1160;car_insurance_boxplots.png;Variable length presents some outliers.;F
1161;car_insurance_boxplots.png;At least 50% of the variables present outliers.;F
1162;car_insurance_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1163;car_insurance_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1164;car_insurance_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
1165;car_insurance_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1166;car_insurance_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1167;car_insurance_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1168;car_insurance_histograms_symbolic.png;All variables should be dealt with as symbolic.;T
1169;car_insurance_histograms_symbolic.png;The variable segment can be seen as ordinal.;T
1170;car_insurance_histograms_symbolic.png;The variable is_esc can be seen as ordinal without losing information.;T
1171;car_insurance_histograms_symbolic.png;Considering the common semantics for segment and area_cluster variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1172;car_insurance_histograms_symbolic.png;Considering the common semantics for max_torque variable, dummification would be the most adequate encoding.;F
1173;car_insurance_histograms_symbolic.png;The variable max_torque can be coded as ordinal without losing information.;T
1174;car_insurance_histograms_symbolic.png;Feature generation based on variable area_cluster seems to be promising.;F
1175;car_insurance_histograms_symbolic.png;Feature generation based on the use of variable steering_type wouldn’t be useful, but the use of area_cluster seems to be promising.;F
1176;car_insurance_histograms_symbolic.png;Given the usual semantics of model variable, dummification would have been a better codification.;T
1177;car_insurance_histograms_symbolic.png;Not knowing the semantics of is_esc variable, dummification could have been a more adequate codification.;T
1178;car_insurance_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1179;car_insurance_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
1180;car_insurance_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1181;car_insurance_histograms_numeric.png;All variables should be dealt with as numeric.;F
1182;car_insurance_histograms_numeric.png;The variable age_of_car can be seen as ordinal.;F
1183;car_insurance_histograms_numeric.png;The variable height can be seen as ordinal without losing information.;F
1184;car_insurance_histograms_numeric.png;Variable displacement is balanced.;F
1185;car_insurance_histograms_numeric.png;It is clear that variable displacement shows some outliers, but we can’t be sure of the same for variable age_of_car.;F
1186;car_insurance_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1187;car_insurance_histograms_numeric.png;Variable displacement shows some outlier values.;F
1188;car_insurance_histograms_numeric.png;Variable width doesn’t have any outliers.;T
1189;car_insurance_histograms_numeric.png;Variable height presents some outliers.;T
1190;car_insurance_histograms_numeric.png;At least 60% of the variables present outliers.;F
1191;car_insurance_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1192;car_insurance_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1193;car_insurance_histograms_numeric.png;Considering the common semantics for displacement and policy_tenure variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1194;car_insurance_histograms_numeric.png;Considering the common semantics for length variable, dummification would be the most adequate encoding.;F
1195;car_insurance_histograms_numeric.png;The variable age_of_car can be coded as ordinal without losing information.;F
1196;car_insurance_histograms_numeric.png;Feature generation based on variable height seems to be promising.;F
1197;car_insurance_histograms_numeric.png;Feature generation based on the use of variable age_of_car wouldn’t be useful, but the use of policy_tenure seems to be promising.;F
1198;car_insurance_histograms_numeric.png;Given the usual semantics of age_of_policyholder variable, dummification would have been a better codification.;F
1199;car_insurance_histograms_numeric.png;Not knowing the semantics of displacement variable, dummification could have been a more adequate codification.;T
1200;heart_decision_tree.png;The variable slope discriminates between the target values, as shown in the decision tree.;T
1201;heart_decision_tree.png;Variable slope is one of the most relevant variables.;T
1202;heart_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T
1203;heart_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;T
1204;heart_decision_tree.png;The specificity for the presented tree is lower than 75%, consider 1 as the positive class and 0 as the negative class.;T
1205;heart_decision_tree.png;The number of True Negatives is higher than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
1206;heart_decision_tree.png;The number of False Positives is lower than the number of True Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
1207;heart_decision_tree.png;The precision for the presented tree is lower than its specificity, consider 1 as the positive class and 0 as the negative class.;F
1208;heart_decision_tree.png;Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], the Decision Tree presented classifies (not A, B) as 1.;T
1209;heart_decision_tree.png;Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], the Decision Tree presented classifies (not A, B) as 1.;T
1210;heart_decision_tree.png;Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as 0.;F
1211;heart_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
1212;heart_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;F
1213;heart_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;F
1214;heart_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1215;heart_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
1216;heart_overfitting_knn.png;KNN is in overfitting for k less than 17.;F
1217;heart_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;F
1218;heart_overfitting_knn.png;KNN with more than 7 neighbours is in overfitting.;F
1219;heart_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;F
1220;heart_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.;F
1221;heart_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 4.;F
1222;heart_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.;F
1223;heart_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;F
1224;heart_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1225;heart_pca.png;The first 4 principal components are enough for explaining half the data variance.;T
1226;heart_pca.png;Using the first 9 principal components would imply an error between 15 and 20%.;F
1227;heart_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
1228;heart_correlation_heatmap.png;One of the variables restecg or age can be discarded without losing information.;F
1229;heart_correlation_heatmap.png;The variable trestbps can be discarded without risking losing information.;F
1230;heart_correlation_heatmap.png;Variables cp and age are redundant, but we can’t say the same for the pair ca and trestbps.;F
1231;heart_correlation_heatmap.png;Variables restecg and oldpeak are redundant.;F
1232;heart_correlation_heatmap.png;Considering that the target variable is chol we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
1233;heart_correlation_heatmap.png;Considering that the target variable is slope we can say that variable thalach seems to be relevant for the majority of mining tasks.;F
1234;heart_correlation_heatmap.png;Considering that the target variable is restecg we can say that variables cp and chol seem to be useful for classification tasks.;F
1235;heart_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
1236;heart_correlation_heatmap.png;Removing variable age might improve the training of decision trees .;F
1237;heart_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable restecg previously than variable slope.;F
1238;heart_boxplots.png;Variable thal is balanced.;F
1239;heart_boxplots.png;Those boxplots show that the data is not normalized.;T
1240;heart_boxplots.png;It is clear that variable trestbps shows some outliers, but we can’t be sure of the same for variable restecg.;F
1241;heart_boxplots.png;Outliers seem to be a problem in the dataset.;T
1242;heart_boxplots.png;Variable chol shows some outlier values.;T
1243;heart_boxplots.png;Variable restecg doesn’t have any outliers.;F
1244;heart_boxplots.png;Variable restecg presents some outliers.;T
1245;heart_boxplots.png;At least 85% of the variables present outliers.;T
1246;heart_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1247;heart_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1248;heart_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
1249;heart_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1250;heart_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1251;heart_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1252;heart_histograms_symbolic.png;All variables should be dealt with as numeric.;F
1253;heart_histograms_symbolic.png;The variable sex can be seen as ordinal.;T
1254;heart_histograms_symbolic.png;The variable sex can be seen as ordinal without losing information.;T
1255;heart_histograms_symbolic.png;Considering the common semantics for fbs and sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1256;heart_histograms_symbolic.png;Considering the common semantics for sex variable, dummification would be the most adequate encoding.;F
1257;heart_histograms_symbolic.png;The variable sex can be coded as ordinal without losing information.;T
1258;heart_histograms_symbolic.png;Feature generation based on variable exang seems to be promising.;F
1259;heart_histograms_symbolic.png;Feature generation based on the use of variable exang wouldn’t be useful, but the use of sex seems to be promising.;F
1260;heart_histograms_symbolic.png;Given the usual semantics of sex variable, dummification would have been a better codification.;F
1261;heart_histograms_symbolic.png;Not knowing the semantics of sex variable, dummification could have been a more adequate codification.;F
1262;heart_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
1263;heart_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
1264;heart_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1265;heart_histograms_numeric.png;All variables should be dealt with as binary.;F
1266;heart_histograms_numeric.png;The variable chol can be seen as ordinal.;F
1267;heart_histograms_numeric.png;The variable age can be seen as ordinal without losing information.;T
1268;heart_histograms_numeric.png;Variable restecg is balanced.;F
1269;heart_histograms_numeric.png;It is clear that variable chol shows some outliers, but we can’t be sure of the same for variable age.;T
1270;heart_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1271;heart_histograms_numeric.png;Variable age shows some outlier values.;F
1272;heart_histograms_numeric.png;Variable chol doesn’t have any outliers.;F
1273;heart_histograms_numeric.png;Variable ca presents some outliers.;F
1274;heart_histograms_numeric.png;At least 50% of the variables present outliers.;F
1275;heart_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1276;heart_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1277;heart_histograms_numeric.png;Considering the common semantics for chol and age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1278;heart_histograms_numeric.png;Considering the common semantics for restecg variable, dummification would be the most adequate encoding.;F
1279;heart_histograms_numeric.png;The variable thal can be coded as ordinal without losing information.;T
1280;heart_histograms_numeric.png;Feature generation based on variable cp seems to be promising.;F
1281;heart_histograms_numeric.png;Feature generation based on the use of variable thalach wouldn’t be useful, but the use of age seems to be promising.;F
1282;heart_histograms_numeric.png;Given the usual semantics of restecg variable, dummification would have been a better codification.;F
1283;heart_histograms_numeric.png;Not knowing the semantics of trestbps variable, dummification could have been a more adequate codification.;F
1284;Breast_Cancer_decision_tree.png;The variable texture_worst discriminates between the target values, as shown in the decision tree.;T
1285;Breast_Cancer_decision_tree.png;Variable texture_worst is one of the most relevant variables.;T
1286;Breast_Cancer_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
1287;Breast_Cancer_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider M as the positive class and B as the negative class.;F
1288;Breast_Cancer_decision_tree.png;The recall for the presented tree is higher than 60%, consider M as the positive class and B as the negative class.;T
1289;Breast_Cancer_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree, consider M as the positive class and B as the negative class.;F
1290;Breast_Cancer_decision_tree.png;The number of False Positives is lower than the number of False Negatives for the presented tree, consider M as the positive class and B as the negative class.;T
1291;Breast_Cancer_decision_tree.png;The number of True Positives is lower than the number of False Positives for the presented tree, consider M as the positive class and B as the negative class.;F
1292;Breast_Cancer_decision_tree.png;Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that Naive Bayes algorithm classifies (A, not B), as M.;F
1293;Breast_Cancer_decision_tree.png;Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that Naive Bayes algorithm classifies (not A, B), as M.;F
1294;Breast_Cancer_decision_tree.png;Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that KNN algorithm classifies (not A, B) as M for any k ≤ 20.;F
1295;Breast_Cancer_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
1296;Breast_Cancer_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;F
1297;Breast_Cancer_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;F
1298;Breast_Cancer_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1299;Breast_Cancer_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
1300;Breast_Cancer_overfitting_knn.png;KNN is in overfitting for k larger than 5.;F
1301;Breast_Cancer_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;F
1302;Breast_Cancer_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
1303;Breast_Cancer_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
1304;Breast_Cancer_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.;F
1305;Breast_Cancer_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 6.;F
1306;Breast_Cancer_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.;F
1307;Breast_Cancer_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;F
1308;Breast_Cancer_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1309;Breast_Cancer_pca.png;The first 6 principal components are enough for explaining half the data variance.;T
1310;Breast_Cancer_pca.png;Using the first 6 principal components would imply an error between 10 and 30%.;F
1311;Breast_Cancer_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.;F
1312;Breast_Cancer_correlation_heatmap.png;One of the variables symmetry_se or area_se can be discarded without losing information.;F
1313;Breast_Cancer_correlation_heatmap.png;The variable perimeter_worst can be discarded without risking losing information.;T
1314;Breast_Cancer_correlation_heatmap.png;Variables texture_worst and radius_worst are redundant, but we can’t say the same for the pair perimeter_worst and texture_se.;T
1315;Breast_Cancer_correlation_heatmap.png;Variables texture_worst and perimeter_se are redundant.;T
1316;Breast_Cancer_correlation_heatmap.png;Considering that the target variable is symmetry_se we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
1317;Breast_Cancer_correlation_heatmap.png;Considering that the target variable is perimeter_se we can say that variable area_se seems to be relevant for the majority of mining tasks.;T
1318;Breast_Cancer_correlation_heatmap.png;Considering that the target variable is radius_worst we can say that variables symmetry_se and perimeter_mean seem to be useful for classification tasks.;F
1319;Breast_Cancer_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1320;Breast_Cancer_correlation_heatmap.png;Removing variable texture_se might improve the training of decision trees .;F
1321;Breast_Cancer_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable texture_mean previously than variable perimeter_se.;F
1322;Breast_Cancer_boxplots.png;Variable perimeter_se is balanced.;F
1323;Breast_Cancer_boxplots.png;Those boxplots show that the data is not normalized.;T
1324;Breast_Cancer_boxplots.png;It is clear that variable texture_mean shows some outliers, but we can’t be sure of the same for variable perimeter_se.;F
1325;Breast_Cancer_boxplots.png;Outliers seem to be a problem in the dataset.;T
1326;Breast_Cancer_boxplots.png;Variable texture_se shows a high number of outlier values.;F
1327;Breast_Cancer_boxplots.png;Variable texture_mean doesn’t have any outliers.;F
1328;Breast_Cancer_boxplots.png;Variable radius_worst presents some outliers.;T
1329;Breast_Cancer_boxplots.png;At least 60% of the variables present outliers.;T
1330;Breast_Cancer_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1331;Breast_Cancer_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1332;Breast_Cancer_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
1333;Breast_Cancer_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1334;Breast_Cancer_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1335;Breast_Cancer_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1336;Breast_Cancer_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1337;Breast_Cancer_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
1338;Breast_Cancer_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1339;Breast_Cancer_histograms_numeric.png;All variables should be dealt with as numeric.;T
1340;Breast_Cancer_histograms_numeric.png;The variable perimeter_mean can be seen as ordinal.;F
1341;Breast_Cancer_histograms_numeric.png;The variable radius_worst can be seen as ordinal without losing information.;F
1342;Breast_Cancer_histograms_numeric.png;Variable texture_se is balanced.;F
1343;Breast_Cancer_histograms_numeric.png;It is clear that variable radius_worst shows some outliers, but we can’t be sure of the same for variable perimeter_worst.;F
1344;Breast_Cancer_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1345;Breast_Cancer_histograms_numeric.png;Variable smoothness_se shows some outlier values.;T
1346;Breast_Cancer_histograms_numeric.png;Variable smoothness_se doesn’t have any outliers.;F
1347;Breast_Cancer_histograms_numeric.png;Variable texture_worst presents some outliers.;T
1348;Breast_Cancer_histograms_numeric.png;At least 60% of the variables present outliers.;T
1349;Breast_Cancer_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1350;Breast_Cancer_histograms_numeric.png;Considering the common semantics for perimeter_mean and texture_mean variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1351;Breast_Cancer_histograms_numeric.png;Considering the common semantics for area_se variable, dummification would be the most adequate encoding.;F
1352;Breast_Cancer_histograms_numeric.png;The variable smoothness_se can be coded as ordinal without losing information.;F
1353;Breast_Cancer_histograms_numeric.png;Feature generation based on variable perimeter_worst seems to be promising.;F
1354;Breast_Cancer_histograms_numeric.png;Feature generation based on the use of variable area_se wouldn’t be useful, but the use of texture_mean seems to be promising.;F
1355;Breast_Cancer_histograms_numeric.png;Given the usual semantics of perimeter_worst variable, dummification would have been a better codification.;F
1356;Breast_Cancer_histograms_numeric.png;Not knowing the semantics of smoothness_se variable, dummification could have been a more adequate codification.;F
1357;e-commerce_decision_tree.png;The variable Prior_purchases discriminates between the target values, as shown in the decision tree.;F
1358;e-commerce_decision_tree.png;Variable Prior_purchases is one of the most relevant variables.;T
1359;e-commerce_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;T
1360;e-commerce_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider Yes as the positive class and No as the negative class.;F
1361;e-commerce_decision_tree.png;The accuracy for the presented tree is lower than 60%.;T
1362;e-commerce_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree, consider Yes as the positive class and No as the negative class.;T
1363;e-commerce_decision_tree.png;The number of True Negatives is lower than the number of False Negatives for the presented tree, consider Yes as the positive class and No as the negative class.;F
1364;e-commerce_decision_tree.png;The accuracy for the presented tree is higher than 60%.;F
1365;e-commerce_decision_tree.png;Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (A,B) as Yes for any k ≤ 1596.;T
1366;e-commerce_decision_tree.png;Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (not A, B) as No for any k ≤ 3657.;F
1367;e-commerce_decision_tree.png;Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (not A, B) as No for any k ≤ 1596.;F
1368;e-commerce_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
1369;e-commerce_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;T
1370;e-commerce_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;F
1371;e-commerce_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
1372;e-commerce_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
1373;e-commerce_overfitting_knn.png;KNN is in overfitting for k less than 17.;T
1374;e-commerce_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;T
1375;e-commerce_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
1376;e-commerce_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;T
1377;e-commerce_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.;T
1378;e-commerce_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 8.;T
1379;e-commerce_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.;T
1380;e-commerce_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;F
1381;e-commerce_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1382;e-commerce_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
1383;e-commerce_pca.png;Using the first 4 principal components would imply an error between 10 and 25%.;F
1384;e-commerce_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.;F
1385;e-commerce_correlation_heatmap.png;One of the variables Discount_offered or Prior_purchases can be discarded without losing information.;F
1386;e-commerce_correlation_heatmap.png;The variable Customer_rating can be discarded without risking losing information.;F
1387;e-commerce_correlation_heatmap.png;Variables Customer_care_calls and Cost_of_the_Product are redundant.;F
1388;e-commerce_correlation_heatmap.png;Variables Prior_purchases and Cost_of_the_Product are redundant.;F
1389;e-commerce_correlation_heatmap.png;Considering that the target variable is Weight_in_gms we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
1390;e-commerce_correlation_heatmap.png;Considering that the target variable is Weight_in_gms we can say that variable Discount_offered seems to be relevant for the majority of mining tasks.;F
1391;e-commerce_correlation_heatmap.png;Considering that the target variable is Cost_of_the_Product we can say that variables Weight_in_gms and Prior_purchases seem to be useful for classification tasks.;F
1392;e-commerce_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
1393;e-commerce_correlation_heatmap.png;Removing variable Cost_of_the_Product might improve the training of decision trees .;F
1394;e-commerce_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Discount_offered previously than variable Cost_of_the_Product.;F
1395;e-commerce_boxplots.png;Variable Discount_offered is balanced.;F
1396;e-commerce_boxplots.png;Those boxplots show that the data is not normalized.;T
1397;e-commerce_boxplots.png;It is clear that variable Customer_rating shows some outliers, but we can’t be sure of the same for variable Prior_purchases.;F
1398;e-commerce_boxplots.png;Outliers seem to be a problem in the dataset.;F
1399;e-commerce_boxplots.png;Variable Discount_offered shows some outlier values.;F
1400;e-commerce_boxplots.png;Variable Customer_rating doesn’t have any outliers.;T
1401;e-commerce_boxplots.png;Variable Prior_purchases presents some outliers.;T
1402;e-commerce_boxplots.png;At least 60% of the variables present outliers.;F
1403;e-commerce_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1404;e-commerce_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1405;e-commerce_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
1406;e-commerce_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1407;e-commerce_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1408;e-commerce_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1409;e-commerce_histograms_symbolic.png;All variables should be dealt with as symbolic.;T
1410;e-commerce_histograms_symbolic.png;The variable Warehouse_block can be seen as ordinal.;T
1411;e-commerce_histograms_symbolic.png;The variable Product_importance can be seen as ordinal without losing information.;T
1412;e-commerce_histograms_symbolic.png;Considering the common semantics for Mode_of_Shipment and Warehouse_block variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1413;e-commerce_histograms_symbolic.png;Considering the common semantics for Mode_of_Shipment variable, dummification would be the most adequate encoding.;T
1414;e-commerce_histograms_symbolic.png;The variable Product_importance can be coded as ordinal without losing information.;T
1415;e-commerce_histograms_symbolic.png;Feature generation based on variable Gender seems to be promising.;F
1416;e-commerce_histograms_symbolic.png;Feature generation based on the use of variable Warehouse_block wouldn’t be useful, but the use of Mode_of_Shipment seems to be promising.;F
1417;e-commerce_histograms_symbolic.png;Given the usual semantics of Warehouse_block variable, dummification would have been a better codification.;T
1418;e-commerce_histograms_symbolic.png;Not knowing the semantics of Product_importance variable, dummification could have been a more adequate codification.;F
1419;e-commerce_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1420;e-commerce_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;F
1421;e-commerce_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1422;e-commerce_histograms_numeric.png;All variables should be dealt with as date.;F
1423;e-commerce_histograms_numeric.png;The variable Weight_in_gms can be seen as ordinal.;F
1424;e-commerce_histograms_numeric.png;The variable Weight_in_gms can be seen as ordinal without losing information.;F
1425;e-commerce_histograms_numeric.png;Variable Customer_care_calls is balanced.;F
1426;e-commerce_histograms_numeric.png;It is clear that variable Discount_offered shows some outliers, but we can’t be sure of the same for variable Customer_care_calls.;F
1427;e-commerce_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1428;e-commerce_histograms_numeric.png;Variable Prior_purchases shows a high number of outlier values.;F
1429;e-commerce_histograms_numeric.png;Variable Prior_purchases doesn’t have any outliers.;F
1430;e-commerce_histograms_numeric.png;Variable Discount_offered presents some outliers.;F
1431;e-commerce_histograms_numeric.png;At least 85% of the variables present outliers.;F
1432;e-commerce_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1433;e-commerce_histograms_numeric.png;Considering the common semantics for Prior_purchases and Customer_care_calls variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1434;e-commerce_histograms_numeric.png;Considering the common semantics for Customer_care_calls variable, dummification would be the most adequate encoding.;F
1435;e-commerce_histograms_numeric.png;The variable Customer_care_calls can be coded as ordinal without losing information.;T
1436;e-commerce_histograms_numeric.png;Feature generation based on variable Discount_offered seems to be promising.;F
1437;e-commerce_histograms_numeric.png;Feature generation based on the use of variable Discount_offered wouldn’t be useful, but the use of Customer_care_calls seems to be promising.;F
1438;e-commerce_histograms_numeric.png;Given the usual semantics of Discount_offered variable, dummification would have been a better codification.;F
1439;e-commerce_histograms_numeric.png;Not knowing the semantics of Cost_of_the_Product variable, dummification could have been a more adequate codification.;F
1440;maintenance_decision_tree.png;The variable Rotational speed [rpm] discriminates between the target values, as shown in the decision tree.;T
1441;maintenance_decision_tree.png;Variable Rotational speed [rpm] is one of the most relevant variables.;T
1442;maintenance_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T
1443;maintenance_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;T
1444;maintenance_decision_tree.png;The precision for the presented tree is lower than 60%, consider 1 as the positive class and 0 as the negative class.;F
1445;maintenance_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
1446;maintenance_decision_tree.png;The number of True Negatives is higher than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
1447;maintenance_decision_tree.png;The number of True Negatives reported in the same tree is 50, consider 1 as the positive class and 0 as the negative class.;F
1448;maintenance_decision_tree.png;Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 5990.;F
1449;maintenance_decision_tree.png;Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 46.;T
1450;maintenance_decision_tree.png;Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 46.;F
1451;maintenance_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
1452;maintenance_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;F
1453;maintenance_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;F
1454;maintenance_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1455;maintenance_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
1456;maintenance_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
1457;maintenance_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;F
1458;maintenance_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
1459;maintenance_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
1460;maintenance_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.;T
1461;maintenance_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 8.;F
1462;maintenance_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.;F
1463;maintenance_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;F
1464;maintenance_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1465;maintenance_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
1466;maintenance_pca.png;Using the first 2 principal components would imply an error between 10 and 25%.;F
1467;maintenance_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
1468;maintenance_correlation_heatmap.png;One of the variables Process temperature [K] or Torque [Nm] can be discarded without losing information.;T
1469;maintenance_correlation_heatmap.png;The variable Rotational speed [rpm] can be discarded without risking losing information.;T
1470;maintenance_correlation_heatmap.png;Considering that the target variable is Process temperature [K] we can say that variables Air temperature [K] and Tool wear [min] seem to be useful for classification tasks.;F
1471;maintenance_correlation_heatmap.png;Variables Rotational speed [rpm] and Process temperature [K] are redundant.;T
1472;maintenance_correlation_heatmap.png;Considering that the target variable is Process temperature [K] we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
1473;maintenance_correlation_heatmap.png;Considering that the target variable is Torque [Nm] we can say that variable Tool wear [min] seems to be relevant for the majority of mining tasks.;F
1474;maintenance_correlation_heatmap.png;Considering that the target variable is Process temperature [K] we can say that variables Torque [Nm] and Tool wear [min] seem to be useful for classification tasks.;F
1475;maintenance_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1476;maintenance_correlation_heatmap.png;Removing variable Torque [Nm] might improve the training of decision trees .;T
1477;maintenance_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Rotational speed [rpm] previously than variable Torque [Nm].;F
1478;maintenance_boxplots.png;Variable Process temperature [K] is balanced.;T
1479;maintenance_boxplots.png;Those boxplots show that the data is not normalized.;T
1480;maintenance_boxplots.png;It is clear that variable Rotational speed [rpm] shows some outliers, but we can’t be sure of the same for variable Torque [Nm].;F
1481;maintenance_boxplots.png;Outliers seem to be a problem in the dataset.;F
1482;maintenance_boxplots.png;Variable Tool wear [min] shows a high number of outlier values.;F
1483;maintenance_boxplots.png;Variable Air temperature [K] doesn’t have any outliers.;T
1484;maintenance_boxplots.png;Variable Tool wear [min] presents some outliers.;F
1485;maintenance_boxplots.png;At least 85% of the variables present outliers.;F
1486;maintenance_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1487;maintenance_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1488;maintenance_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
1489;maintenance_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1490;maintenance_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1491;maintenance_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1492;maintenance_histograms_symbolic.png;All variables should be dealt with as date.;F
1493;maintenance_histograms_symbolic.png;The variable TWF can be seen as ordinal.;T
1494;maintenance_histograms_symbolic.png;The variable HDF can be seen as ordinal without losing information.;T
1495;maintenance_histograms_symbolic.png;Considering the common semantics for PWF and Type variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1496;maintenance_histograms_symbolic.png;Considering the common semantics for Type variable, dummification would be the most adequate encoding.;T
1497;maintenance_histograms_symbolic.png;The variable Type can be coded as ordinal without losing information.;F
1498;maintenance_histograms_symbolic.png;Feature generation based on variable OSF seems to be promising.;F
1499;maintenance_histograms_symbolic.png;Feature generation based on the use of variable RNF wouldn’t be useful, but the use of Type seems to be promising.;F
1500;maintenance_histograms_symbolic.png;Given the usual semantics of OSF variable, dummification would have been a better codification.;F
1501;maintenance_histograms_symbolic.png;Not knowing the semantics of RNF variable, dummification could have been a more adequate codification.;F
1502;maintenance_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1503;maintenance_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
1504;maintenance_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1505;maintenance_histograms_numeric.png;All variables should be dealt with as numeric.;T
1506;maintenance_histograms_numeric.png;The variable Rotational speed [rpm] can be seen as ordinal.;F
1507;maintenance_histograms_numeric.png;The variable Air temperature [K] can be seen as ordinal without losing information.;F
1508;maintenance_histograms_numeric.png;Variable Rotational speed [rpm] is balanced.;F
1509;maintenance_histograms_numeric.png;It is clear that variable Air temperature [K] shows some outliers, but we can’t be sure of the same for variable Torque [Nm].;F
1510;maintenance_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1511;maintenance_histograms_numeric.png;Variable Torque [Nm] shows some outlier values.;T
1512;maintenance_histograms_numeric.png;Variable Air temperature [K] doesn’t have any outliers.;T
1513;maintenance_histograms_numeric.png;Variable Process temperature [K] presents some outliers.;T
1514;maintenance_histograms_numeric.png;At least 85% of the variables present outliers.;F
1515;maintenance_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1516;maintenance_histograms_numeric.png;Considering the common semantics for Torque [Nm] and Air temperature [K] variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1517;maintenance_histograms_numeric.png;Considering the common semantics for Torque [Nm] variable, dummification would be the most adequate encoding.;F
1518;maintenance_histograms_numeric.png;The variable Rotational speed [rpm] can be coded as ordinal without losing information.;F
1519;maintenance_histograms_numeric.png;Feature generation based on variable Rotational speed [rpm] seems to be promising.;F
1520;maintenance_histograms_numeric.png;Feature generation based on the use of variable Air temperature [K] wouldn’t be useful, but the use of Process temperature [K] seems to be promising.;F
1521;maintenance_histograms_numeric.png;Given the usual semantics of Rotational speed [rpm] variable, dummification would have been a better codification.;F
1522;maintenance_histograms_numeric.png;Not knowing the semantics of Tool wear [min] variable, dummification could have been a more adequate codification.;F
1523;Churn_Modelling_decision_tree.png;The variable NumOfProducts discriminates between the target values, as shown in the decision tree.;T
1524;Churn_Modelling_decision_tree.png;Variable Age is one of the most relevant variables.;T
1525;Churn_Modelling_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;T
1526;Churn_Modelling_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;T
1527;Churn_Modelling_decision_tree.png;The specificity for the presented tree is lower than 90%, consider 1 as the positive class and 0 as the negative class.;F
1528;Churn_Modelling_decision_tree.png;The number of True Positives reported in the same tree is 50, consider 1 as the positive class and 0 as the negative class.;F
1529;Churn_Modelling_decision_tree.png;The number of True Positives is lower than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
1530;Churn_Modelling_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
1531;Churn_Modelling_decision_tree.png;Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 124.;F
1532;Churn_Modelling_decision_tree.png;Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.;F
1533;Churn_Modelling_decision_tree.png;Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 1.;T
1534;Churn_Modelling_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
1535;Churn_Modelling_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;T
1536;Churn_Modelling_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;T
1537;Churn_Modelling_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1538;Churn_Modelling_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
1539;Churn_Modelling_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
1540;Churn_Modelling_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;T
1541;Churn_Modelling_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.;T
1542;Churn_Modelling_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;T
1543;Churn_Modelling_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;T
1544;Churn_Modelling_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 8.;T
1545;Churn_Modelling_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.;T
1546;Churn_Modelling_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;F
1547;Churn_Modelling_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1548;Churn_Modelling_pca.png;The first 5 principal components are enough for explaining half the data variance.;T
1549;Churn_Modelling_pca.png;Using the first 5 principal components would imply an error between 10 and 25%.;F
1550;Churn_Modelling_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.;F
1551;Churn_Modelling_correlation_heatmap.png;One of the variables EstimatedSalary or NumOfProducts can be discarded without losing information.;F
1552;Churn_Modelling_correlation_heatmap.png;The variable EstimatedSalary can be discarded without risking losing information.;F
1553;Churn_Modelling_correlation_heatmap.png;Variables Age and CreditScore are redundant, but we can’t say the same for the pair Tenure and NumOfProducts.;F
1554;Churn_Modelling_correlation_heatmap.png;Variables NumOfProducts and CreditScore are redundant.;F
1555;Churn_Modelling_correlation_heatmap.png;Considering that the target variable is Balance we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
1556;Churn_Modelling_correlation_heatmap.png;Considering that the target variable is Tenure we can say that variable EstimatedSalary seems to be relevant for the majority of mining tasks.;F
1557;Churn_Modelling_correlation_heatmap.png;Considering that the target variable is EstimatedSalary we can say that variables NumOfProducts and CreditScore seem to be useful for classification tasks.;F
1558;Churn_Modelling_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
1559;Churn_Modelling_correlation_heatmap.png;Removing variable Balance might improve the training of decision trees .;F
1560;Churn_Modelling_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Tenure previously than variable CreditScore.;F
1561;Churn_Modelling_boxplots.png;Variable Tenure is balanced.;T
1562;Churn_Modelling_boxplots.png;Those boxplots show that the data is not normalized.;T
1563;Churn_Modelling_boxplots.png;It is clear that variable Tenure shows some outliers, but we can’t be sure of the same for variable NumOfProducts.;F
1564;Churn_Modelling_boxplots.png;Outliers seem to be a problem in the dataset.;F
1565;Churn_Modelling_boxplots.png;Variable EstimatedSalary shows some outlier values.;F
1566;Churn_Modelling_boxplots.png;Variable EstimatedSalary doesn’t have any outliers.;T
1567;Churn_Modelling_boxplots.png;Variable Age presents some outliers.;T
1568;Churn_Modelling_boxplots.png;At least 60% of the variables present outliers.;F
1569;Churn_Modelling_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1570;Churn_Modelling_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1571;Churn_Modelling_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
1572;Churn_Modelling_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1573;Churn_Modelling_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1574;Churn_Modelling_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1575;Churn_Modelling_histograms_symbolic.png;All variables should be dealt with as binary.;F
1576;Churn_Modelling_histograms_symbolic.png;The variable IsActiveMember can be seen as ordinal.;T
1577;Churn_Modelling_histograms_symbolic.png;The variable Gender can be seen as ordinal without losing information.;F
1578;Churn_Modelling_histograms_symbolic.png;Considering the common semantics for Gender and Geography variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1579;Churn_Modelling_histograms_symbolic.png;Considering the common semantics for IsActiveMember variable, dummification would be the most adequate encoding.;F
1580;Churn_Modelling_histograms_symbolic.png;The variable IsActiveMember can be coded as ordinal without losing information.;T
1581;Churn_Modelling_histograms_symbolic.png;Feature generation based on variable IsActiveMember seems to be promising.;F
1582;Churn_Modelling_histograms_symbolic.png;Feature generation based on the use of variable Gender wouldn’t be useful, but the use of Geography seems to be promising.;F
1583;Churn_Modelling_histograms_symbolic.png;Given the usual semantics of Geography variable, dummification would have been a better codification.;T
1584;Churn_Modelling_histograms_symbolic.png;Not knowing the semantics of Gender variable, dummification could have been a more adequate codification.;F
1585;Churn_Modelling_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1586;Churn_Modelling_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
1587;Churn_Modelling_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1588;Churn_Modelling_histograms_numeric.png;All variables should be dealt with as numeric.;F
1589;Churn_Modelling_histograms_numeric.png;The variable Age can be seen as ordinal.;F
1590;Churn_Modelling_histograms_numeric.png;The variable EstimatedSalary can be seen as ordinal without losing information.;F
1591;Churn_Modelling_histograms_numeric.png;Variable Age is balanced.;F
1592;Churn_Modelling_histograms_numeric.png;It is clear that variable NumOfProducts shows some outliers, but we can’t be sure of the same for variable Tenure.;F
1593;Churn_Modelling_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
1594;Churn_Modelling_histograms_numeric.png;Variable EstimatedSalary shows some outlier values.;F
1595;Churn_Modelling_histograms_numeric.png;Variable Age doesn’t have any outliers.;T
1596;Churn_Modelling_histograms_numeric.png;Variable NumOfProducts presents some outliers.;F
1597;Churn_Modelling_histograms_numeric.png;At least 85% of the variables present outliers.;F
1598;Churn_Modelling_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1599;Churn_Modelling_histograms_numeric.png;Considering the common semantics for NumOfProducts and CreditScore variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1600;Churn_Modelling_histograms_numeric.png;Considering the common semantics for Balance variable, dummification would be the most adequate encoding.;F
1601;Churn_Modelling_histograms_numeric.png;The variable CreditScore can be coded as ordinal without losing information.;F
1602;Churn_Modelling_histograms_numeric.png;Feature generation based on variable Age seems to be promising.;F
1603;Churn_Modelling_histograms_numeric.png;Feature generation based on the use of variable EstimatedSalary wouldn’t be useful, but the use of CreditScore seems to be promising.;F
1604;Churn_Modelling_histograms_numeric.png;Given the usual semantics of CreditScore variable, dummification would have been a better codification.;F
1605;Churn_Modelling_histograms_numeric.png;Not knowing the semantics of CreditScore variable, dummification could have been a more adequate codification.;F
1606;vehicle_decision_tree.png;The variable MAJORSKEWNESS discriminates between the target values, as shown in the decision tree.;T
1607;vehicle_decision_tree.png;Variable MAJORSKEWNESS is one of the most relevant variables.;T
1608;vehicle_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;T
1609;vehicle_decision_tree.png;The accuracy for the presented tree is lower than 90%.;T
1610;vehicle_decision_tree.png;The variable MAJORSKEWNESS seems to be one of the five most relevant features.;T
1611;vehicle_decision_tree.png;Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 4.;F
1612;vehicle_decision_tree.png;Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that KNN algorithm classifies (A, not B) as 2 for any k ≤ 3.;T
1613;vehicle_decision_tree.png;Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that KNN algorithm classifies (A,B) as 4 for any k ≤ 3.;T
1614;vehicle_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
1615;vehicle_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;F
1616;vehicle_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;T
1617;vehicle_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
1618;vehicle_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
1619;vehicle_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
1620;vehicle_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;F
1621;vehicle_overfitting_knn.png;KNN with more than 7 neighbours is in overfitting.;F
1622;vehicle_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;F
1623;vehicle_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.;F
1624;vehicle_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 6.;F
1625;vehicle_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.;F
1626;vehicle_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;F
1627;vehicle_pca.png;The first 10 principal components are enough for explaining half the data variance.;T
1628;vehicle_pca.png;Using the first 8 principal components would imply an error between 5 and 20%.;F
1629;vehicle_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.;F
1630;vehicle_correlation_heatmap.png;One of the variables MAJORSKEWNESS or CIRCULARITY can be discarded without losing information.;F
1631;vehicle_correlation_heatmap.png;The variable GYRATIONRADIUS can be discarded without risking losing information.;T
1632;vehicle_correlation_heatmap.png;Variables CIRCULARITY and COMPACTNESS are redundant, but we can’t say the same for the pair MINORVARIANCE and MAJORVARIANCE.;F
1633;vehicle_correlation_heatmap.png;Variables MINORVARIANCE and MINORKURTOSIS are redundant.;F
1634;vehicle_correlation_heatmap.png;Considering that the target variable is CIRCULARITY we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
1635;vehicle_correlation_heatmap.png;Considering that the target variable is MINORVARIANCE we can say that variable MAJORVARIANCE seems to be relevant for the majority of mining tasks.;T
1636;vehicle_correlation_heatmap.png;Considering that the target variable is MAJORSKEWNESS we can say that variables MINORKURTOSIS and MINORSKEWNESS seem to be useful for classification tasks.;F
1637;vehicle_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1638;vehicle_correlation_heatmap.png;Removing variable MAJORKURTOSIS might improve the training of decision trees .;T
1639;vehicle_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable MINORKURTOSIS previously than variable MAJORSKEWNESS.;F
1640;vehicle_boxplots.png;Variable COMPACTNESS is balanced.;T
1641;vehicle_boxplots.png;Those boxplots show that the data is not normalized.;T
1642;vehicle_boxplots.png;It is clear that variable MINORSKEWNESS shows some outliers, but we can’t be sure of the same for variable MINORVARIANCE.;F
1643;vehicle_boxplots.png;Outliers seem to be a problem in the dataset.;F
1644;vehicle_boxplots.png;Variable MINORKURTOSIS shows some outlier values.;T
1645;vehicle_boxplots.png;Variable COMPACTNESS doesn’t have any outliers.;F
1646;vehicle_boxplots.png;Variable CIRCULARITY presents some outliers.;T
1647;vehicle_boxplots.png;At least 75% of the variables present outliers.;T
1648;vehicle_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1649;vehicle_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1650;vehicle_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;F
1651;vehicle_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1652;vehicle_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1653;vehicle_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1654;vehicle_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
1655;vehicle_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
1656;vehicle_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1657;vehicle_histograms_numeric.png;All variables should be dealt with as date.;F
1658;vehicle_histograms_numeric.png;The variable MINORSKEWNESS can be seen as ordinal.;F
1659;vehicle_histograms_numeric.png;The variable GYRATIONRADIUS can be seen as ordinal without losing information.;F
1660;vehicle_histograms_numeric.png;Variable COMPACTNESS is balanced.;T
1661;vehicle_histograms_numeric.png;It is clear that variable MAJORSKEWNESS shows some outliers, but we can’t be sure of the same for variable MAJORVARIANCE.;T
1662;vehicle_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1663;vehicle_histograms_numeric.png;Variable MINORKURTOSIS shows a high number of outlier values.;F
1664;vehicle_histograms_numeric.png;Variable MINORSKEWNESS doesn’t have any outliers.;T
1665;vehicle_histograms_numeric.png;Variable CIRCULARITY presents some outliers.;F
1666;vehicle_histograms_numeric.png;At least 60% of the variables present outliers.;F
1667;vehicle_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1668;vehicle_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1669;vehicle_histograms_numeric.png;Considering the common semantics for RADIUS RATIO and COMPACTNESS variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1670;vehicle_histograms_numeric.png;Considering the common semantics for MINORKURTOSIS variable, dummification would be the most adequate encoding.;F
1671;vehicle_histograms_numeric.png;The variable DISTANCE CIRCULARITY can be coded as ordinal without losing information.;F
1672;vehicle_histograms_numeric.png;Feature generation based on variable GYRATIONRADIUS seems to be promising.;F
1673;vehicle_histograms_numeric.png;Feature generation based on the use of variable MAJORSKEWNESS wouldn’t be useful, but the use of COMPACTNESS seems to be promising.;F
1674;vehicle_histograms_numeric.png;Given the usual semantics of GYRATIONRADIUS variable, dummification would have been a better codification.;F
1675;vehicle_histograms_numeric.png;Not knowing the semantics of MAJORSKEWNESS variable, dummification could have been a more adequate codification.;F
1676;adult_decision_tree.png;The variable capital-loss discriminates between the target values, as shown in the decision tree.;T
1677;adult_decision_tree.png;Variable hours-per-week is one of the most relevant variables.;T
1678;adult_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
1679;adult_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider >50K as the positive class and <=50K as the negative class.;T
1680;adult_decision_tree.png;The accuracy for the presented tree is higher than 60%.;T
1681;adult_decision_tree.png;The number of True Negatives is higher than the number of False Negatives for the presented tree, consider >50K as the positive class and <=50K as the negative class.;T
1682;adult_decision_tree.png;The number of False Negatives is lower than the number of False Positives for the presented tree, consider >50K as the positive class and <=50K as the negative class.;F
1683;adult_decision_tree.png;The number of False Negatives is lower than the number of False Positives for the presented tree, consider >50K as the positive class and <=50K as the negative class.;F
1684;adult_decision_tree.png;Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as >50K.;T
1685;adult_decision_tree.png;Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that KNN algorithm classifies (A, not B) as >50K for any k ≤ 541.;T
1686;adult_decision_tree.png;Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that KNN algorithm classifies (not A, B) as >50K for any k ≤ 21974.;F
1687;adult_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
1688;adult_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;T
1689;adult_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;F
1690;adult_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1691;adult_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
1692;adult_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
1693;adult_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;T
1694;adult_overfitting_knn.png;KNN with more than 7 neighbours is in overfitting.;F
1695;adult_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;T
1696;adult_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.;T
1697;adult_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 4.;F
1698;adult_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.;F
1699;adult_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.;F
1700;adult_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1701;adult_pca.png;The first 4 principal components are enough for explaining half the data variance.;T
1702;adult_pca.png;Using the first 5 principal components would imply an error between 15 and 30%.;F
1703;adult_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.;F
1704;adult_correlation_heatmap.png;One of the variables fnlwgt or hours-per-week can be discarded without losing information.;F
1705;adult_correlation_heatmap.png;The variable hours-per-week can be discarded without risking losing information.;F
1706;adult_correlation_heatmap.png;Variables capital-loss and age are redundant.;F
1707;adult_correlation_heatmap.png;Variables age and educational-num are redundant.;F
1708;adult_correlation_heatmap.png;Considering that the target variable is capital-gain we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
1709;adult_correlation_heatmap.png;Considering that the target variable is capital-loss we can say that variable capital-gain seems to be relevant for the majority of mining tasks.;F
1710;adult_correlation_heatmap.png;Considering that the target variable is capital-gain we can say that variables fnlwgt and age seem to be useful for classification tasks.;F
1711;adult_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1712;adult_correlation_heatmap.png;Removing variable fnlwgt might improve the training of decision trees .;F
1713;adult_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable capital-gain previously than variable fnlwgt.;F
1714;adult_boxplots.png;Variable hours-per-week is balanced.;F
1715;adult_boxplots.png;Those boxplots show that the data is not normalized.;T
1716;adult_boxplots.png;It is clear that variable educational-num shows some outliers, but we can’t be sure of the same for variable fnlwgt.;F
1717;adult_boxplots.png;Outliers seem to be a problem in the dataset.;T
1718;adult_boxplots.png;Variable capital-loss shows a high number of outlier values.;T
1719;adult_boxplots.png;Variable capital-gain doesn’t have any outliers.;F
1720;adult_boxplots.png;Variable capital-gain presents some outliers.;T
1721;adult_boxplots.png;At least 75% of the variables present outliers.;F
1722;adult_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1723;adult_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
1724;adult_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1725;adult_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1726;adult_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1727;adult_histograms_symbolic.png;All variables should be dealt with as date.;F
1728;adult_histograms_symbolic.png;The variable gender can be seen as ordinal.;T
1729;adult_histograms_symbolic.png;The variable education can be seen as ordinal without losing information.;T
1730;adult_histograms_symbolic.png;Considering the common semantics for marital-status and workclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1731;adult_histograms_symbolic.png;Considering the common semantics for marital-status variable, dummification would be the most adequate encoding.;T
1732;adult_histograms_symbolic.png;The variable education can be coded as ordinal without losing information.;T
1733;adult_histograms_symbolic.png;Feature generation based on variable marital-status seems to be promising.;F
1734;adult_histograms_symbolic.png;Feature generation based on the use of variable occupation wouldn’t be useful, but the use of workclass seems to be promising.;F
1735;adult_histograms_symbolic.png;Given the usual semantics of education variable, dummification would have been a better codification.;F
1736;adult_histograms_symbolic.png;Not knowing the semantics of occupation variable, dummification could have been a more adequate codification.;T
1737;adult_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1738;adult_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;F
1739;adult_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1740;adult_histograms_numeric.png;All variables should be dealt with as date.;F
1741;adult_histograms_numeric.png;The variable fnlwgt can be seen as ordinal.;F
1742;adult_histograms_numeric.png;The variable hours-per-week can be seen as ordinal without losing information.;T
1743;adult_histograms_numeric.png;Variable fnlwgt is balanced.;F
1744;adult_histograms_numeric.png;It is clear that variable educational-num shows some outliers, but we can’t be sure of the same for variable capital-loss.;F
1745;adult_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
1746;adult_histograms_numeric.png;Variable educational-num shows some outlier values.;F
1747;adult_histograms_numeric.png;Variable capital-loss doesn’t have any outliers.;F
1748;adult_histograms_numeric.png;Variable age presents some outliers.;T
1749;adult_histograms_numeric.png;At least 85% of the variables present outliers.;F
1750;adult_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1751;adult_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1752;adult_histograms_numeric.png;Considering the common semantics for capital-gain and age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1753;adult_histograms_numeric.png;Considering the common semantics for fnlwgt variable, dummification would be the most adequate encoding.;F
1754;adult_histograms_numeric.png;The variable educational-num can be coded as ordinal without losing information.;T
1755;adult_histograms_numeric.png;Feature generation based on variable educational-num seems to be promising.;F
1756;adult_histograms_numeric.png;Feature generation based on the use of variable capital-loss wouldn’t be useful, but the use of age seems to be promising.;F
1757;adult_histograms_numeric.png;Given the usual semantics of capital-gain variable, dummification would have been a better codification.;F
1758;adult_histograms_numeric.png;Not knowing the semantics of fnlwgt variable, dummification could have been a more adequate codification.;F
1759;Covid_Data_decision_tree.png;The variable ASHTMA discriminates between the target values, as shown in the decision tree.;F
1760;Covid_Data_decision_tree.png;Variable ASHTMA is one of the most relevant variables.;T
1761;Covid_Data_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
1762;Covid_Data_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider Yes as the positive class and No as the negative class.;F
1763;Covid_Data_decision_tree.png;The precision for the presented tree is higher than 75%, consider Yes as the positive class and No as the negative class.;F
1764;Covid_Data_decision_tree.png;The number of False Positives is higher than the number of True Negatives for the presented tree, consider Yes as the positive class and No as the negative class.;T
1765;Covid_Data_decision_tree.png;The number of True Negatives is higher than the number of True Positives for the presented tree, consider Yes as the positive class and No as the negative class.;F
1766;Covid_Data_decision_tree.png;The recall for the presented tree is lower than 90%, consider Yes as the positive class and No as the negative class.;F
1767;Covid_Data_decision_tree.png;Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (not A, B) as Yes for any k ≤ 46.;F
1768;Covid_Data_decision_tree.png;Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (A,B) as No for any k ≤ 7971.;F
1769;Covid_Data_decision_tree.png;Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (A, not B) as Yes for any k ≤ 173.;T
1770;Covid_Data_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
1771;Covid_Data_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;F
1772;Covid_Data_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;F
1773;Covid_Data_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
1774;Covid_Data_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
1775;Covid_Data_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
1776;Covid_Data_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;F
1777;Covid_Data_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.;T
1778;Covid_Data_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;T
1779;Covid_Data_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.;F
1780;Covid_Data_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 6.;F
1781;Covid_Data_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.;F
1782;Covid_Data_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;F
1783;Covid_Data_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
1784;Covid_Data_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
1785;Covid_Data_pca.png;Using the first 11 principal components would imply an error between 15 and 25%.;F
1786;Covid_Data_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.;F
1787;Covid_Data_correlation_heatmap.png;One of the variables HIPERTENSION or RENAL_CHRONIC can be discarded without losing information.;T
1788;Covid_Data_correlation_heatmap.png;The variable MEDICAL_UNIT can be discarded without risking losing information.;F
1789;Covid_Data_correlation_heatmap.png;Variables PREGNANT and TOBACCO are redundant, but we can’t say the same for the pair MEDICAL_UNIT and ASTHMA.;F
1790;Covid_Data_correlation_heatmap.png;Variables COPD and AGE are redundant.;F
1791;Covid_Data_correlation_heatmap.png;Considering that the target variable is CARDIOVASCULAR we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
1792;Covid_Data_correlation_heatmap.png;Considering that the target variable is CARDIOVASCULAR we can say that variable ICU seems to be relevant for the majority of mining tasks.;F
1793;Covid_Data_correlation_heatmap.png;Considering that the target variable is CARDIOVASCULAR we can say that variables HIPERTENSION and TOBACCO seem to be useful for classification tasks.;T
1794;Covid_Data_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1795;Covid_Data_correlation_heatmap.png;Removing variable COPD might improve the training of decision trees .;T
1796;Covid_Data_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable ICU previously than variable PREGNANT.;F
1797;Covid_Data_boxplots.png;Variable OTHER_DISEASE is balanced.;F
1798;Covid_Data_boxplots.png;Those boxplots show that the data is not normalized.;T
1799;Covid_Data_boxplots.png;It is clear that variable ASTHMA shows some outliers, but we can’t be sure of the same for variable COPD.;F
1800;Covid_Data_boxplots.png;Outliers seem to be a problem in the dataset.;T
1801;Covid_Data_boxplots.png;Variable AGE shows some outlier values.;T
1802;Covid_Data_boxplots.png;Variable ASTHMA doesn’t have any outliers.;F
1803;Covid_Data_boxplots.png;Variable OTHER_DISEASE presents some outliers.;T
1804;Covid_Data_boxplots.png;At least 75% of the variables present outliers.;T
1805;Covid_Data_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
1806;Covid_Data_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1807;Covid_Data_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
1808;Covid_Data_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1809;Covid_Data_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1810;Covid_Data_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1811;Covid_Data_histograms_symbolic.png;All variables should be dealt with as date.;F
1812;Covid_Data_histograms_symbolic.png;The variable PATIENT_TYPE can be seen as ordinal.;T
1813;Covid_Data_histograms_symbolic.png;The variable USMER can be seen as ordinal without losing information.;T
1814;Covid_Data_histograms_symbolic.png;Considering the common semantics for USMER and SEX variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1815;Covid_Data_histograms_symbolic.png;Considering the common semantics for PATIENT_TYPE variable, dummification would be the most adequate encoding.;T
1816;Covid_Data_histograms_symbolic.png;The variable PATIENT_TYPE can be coded as ordinal without losing information.;T
1817;Covid_Data_histograms_symbolic.png;Feature generation based on variable SEX seems to be promising.;F
1818;Covid_Data_histograms_symbolic.png;Feature generation based on the use of variable PATIENT_TYPE wouldn’t be useful, but the use of USMER seems to be promising.;F
1819;Covid_Data_histograms_symbolic.png;Given the usual semantics of PATIENT_TYPE variable, dummification would have been a better codification.;F
1820;Covid_Data_histograms_symbolic.png;Not knowing the semantics of SEX variable, dummification could have been a more adequate codification.;F
1821;Covid_Data_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1822;Covid_Data_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
1823;Covid_Data_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1824;Covid_Data_histograms_numeric.png;All variables should be dealt with as numeric.;F
1825;Covid_Data_histograms_numeric.png;The variable TOBACCO can be seen as ordinal.;T
1826;Covid_Data_histograms_numeric.png;The variable MEDICAL_UNIT can be seen as ordinal without losing information.;T
1827;Covid_Data_histograms_numeric.png;Variable ICU is balanced.;F
1828;Covid_Data_histograms_numeric.png;It is clear that variable RENAL_CHRONIC shows some outliers, but we can’t be sure of the same for variable ICU.;F
1829;Covid_Data_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1830;Covid_Data_histograms_numeric.png;Variable OTHER_DISEASE shows some outlier values.;F
1831;Covid_Data_histograms_numeric.png;Variable MEDICAL_UNIT doesn’t have any outliers.;T
1832;Covid_Data_histograms_numeric.png;Variable PREGNANT presents some outliers.;F
1833;Covid_Data_histograms_numeric.png;At least 85% of the variables present outliers.;F
1834;Covid_Data_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
1835;Covid_Data_histograms_numeric.png;Considering the common semantics for COPD and MEDICAL_UNIT variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
1836;Covid_Data_histograms_numeric.png;Considering the common semantics for ASTHMA variable, dummification would be the most adequate encoding.;F
1837;Covid_Data_histograms_numeric.png;The variable PREGNANT can be coded as ordinal without losing information.;T
1838;Covid_Data_histograms_numeric.png;Feature generation based on variable ICU seems to be promising.;F
1839;Covid_Data_histograms_numeric.png;Feature generation based on the use of variable PNEUMONIA wouldn’t be useful, but the use of MEDICAL_UNIT seems to be promising.;F
1840;Covid_Data_histograms_numeric.png;Given the usual semantics of HIPERTENSION variable, dummification would have been a better codification.;F
1841;Covid_Data_histograms_numeric.png;Not knowing the semantics of COPD variable, dummification could have been a more adequate codification.;F
1842;sky_survey_decision_tree.png;The variable mjd discriminates between the target values, as shown in the decision tree.;T
1843;sky_survey_decision_tree.png;Variable mjd is one of the most relevant variables.;T
1844;sky_survey_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T
1845;sky_survey_decision_tree.png;Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], the Decision Tree presented classifies (A, not B) as QSO.;F
1846;sky_survey_decision_tree.png;Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], the Decision Tree presented classifies (A, not B) as QSO.;F
1847;sky_survey_decision_tree.png;Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], it is possible to state that KNN algorithm classifies (A,B) as GALAXY for any k ≤ 945.;F
1848;sky_survey_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
1849;sky_survey_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;T
1850;sky_survey_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;F
1851;sky_survey_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1852;sky_survey_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
1853;sky_survey_overfitting_knn.png;KNN is in overfitting for k less than 5.;F
1854;sky_survey_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;F
1855;sky_survey_overfitting_knn.png;KNN with less than 15 neighbours is in overfitting.;F
1856;sky_survey_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;F
1857;sky_survey_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;F
1858;sky_survey_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 8.;T
1859;sky_survey_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 7.;F
1860;sky_survey_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;F
1861;sky_survey_pca.png;The first 7 principal components are enough for explaining half the data variance.;T
1862;sky_survey_pca.png;Using the first 4 principal components would imply an error between 15 and 30%.;F
1863;sky_survey_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.;F
1864;sky_survey_correlation_heatmap.png;One of the variables redshift or plate can be discarded without losing information.;F
1865;sky_survey_correlation_heatmap.png;The variable camcol can be discarded without risking losing information.;F
1866;sky_survey_correlation_heatmap.png;Variables run and ra are redundant, but we can’t say the same for the pair mjd and dec.;F
1867;sky_survey_correlation_heatmap.png;Variables run and redshift are redundant.;F
1868;sky_survey_correlation_heatmap.png;Considering that the target variable is mjd we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
1869;sky_survey_correlation_heatmap.png;Considering that the target variable is mjd we can say that variable dec seems to be relevant for the majority of mining tasks.;F
1870;sky_survey_correlation_heatmap.png;Considering that the target variable is plate we can say that variables camcol and mjd seem to be useful for classification tasks.;F
1871;sky_survey_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1872;sky_survey_correlation_heatmap.png;Removing variable ra might improve the training of decision trees .;F
1873;sky_survey_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable camcol previously than variable mjd.;F
1874;sky_survey_boxplots.png;Variable plate is balanced.;F
1875;sky_survey_boxplots.png;Those boxplots show that the data is not normalized.;T
1876;sky_survey_boxplots.png;It is clear that variable field shows some outliers, but we can’t be sure of the same for variable ra.;F
1877;sky_survey_boxplots.png;Outliers seem to be a problem in the dataset.;T
1878;sky_survey_boxplots.png;Variable field shows some outlier values.;F
1879;sky_survey_boxplots.png;Variable field doesn’t have any outliers.;T
1880;sky_survey_boxplots.png;Variable redshift presents some outliers.;T
1881;sky_survey_boxplots.png;At least 50% of the variables present outliers.;T
1882;sky_survey_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1883;sky_survey_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
1884;sky_survey_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1885;sky_survey_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1886;sky_survey_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1887;sky_survey_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1888;sky_survey_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
1889;sky_survey_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1890;sky_survey_histograms_numeric.png;All variables should be dealt with as date.;F
1891;sky_survey_histograms_numeric.png;The variable run can be seen as ordinal.;F
1892;sky_survey_histograms_numeric.png;The variable field can be seen as ordinal without losing information.;F
1893;sky_survey_histograms_numeric.png;Variable ra is balanced.;F
1894;sky_survey_histograms_numeric.png;It is clear that variable camcol shows some outliers, but we can’t be sure of the same for variable mjd.;F
1895;sky_survey_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
1896;sky_survey_histograms_numeric.png;Variable redshift shows a high number of outlier values.;F
1897;sky_survey_histograms_numeric.png;Variable field doesn’t have any outliers.;F
1898;sky_survey_histograms_numeric.png;Variable plate presents some outliers.;T
1899;sky_survey_histograms_numeric.png;At least 60% of the variables present outliers.;T
1900;sky_survey_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1901;sky_survey_histograms_numeric.png;Considering the common semantics for ra and dec variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1902;sky_survey_histograms_numeric.png;Considering the common semantics for field variable, dummification would be the most adequate encoding.;F
1903;sky_survey_histograms_numeric.png;The variable camcol can be coded as ordinal without losing information.;T
1904;sky_survey_histograms_numeric.png;Feature generation based on variable redshift seems to be promising.;F
1905;sky_survey_histograms_numeric.png;Feature generation based on the use of variable camcol wouldn’t be useful, but the use of ra seems to be promising.;F
1906;sky_survey_histograms_numeric.png;Given the usual semantics of ra variable, dummification would have been a better codification.;F
1907;sky_survey_histograms_numeric.png;Not knowing the semantics of plate variable, dummification could have been a more adequate codification.;F
1908;Wine_decision_tree.png;The variable Proanthocyanins discriminates between the target values, as shown in the decision tree.;T
1909;Wine_decision_tree.png;Variable Proanthocyanins is one of the most relevant variables.;T
1910;Wine_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.;T
1911;Wine_decision_tree.png;Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A,B) as 3 for any k ≤ 60.;F
1912;Wine_decision_tree.png;Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 60.;F
1913;Wine_decision_tree.png;Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A, not B) as 2 for any k ≤ 49.;F
1914;Wine_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
1915;Wine_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;F
1916;Wine_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;F
1917;Wine_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
1918;Wine_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
1919;Wine_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
1920;Wine_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;F
1921;Wine_overfitting_knn.png;KNN with less than 15 neighbours is in overfitting.;F
1922;Wine_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;F
1923;Wine_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;F
1924;Wine_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 6.;F
1925;Wine_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.;F
1926;Wine_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;F
1927;Wine_pca.png;The first 4 principal components are enough for explaining half the data variance.;T
1928;Wine_pca.png;Using the first 10 principal components would imply an error between 15 and 30%.;F
1929;Wine_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
1930;Wine_correlation_heatmap.png;One of the variables Flavanoids or Hue can be discarded without losing information.;F
1931;Wine_correlation_heatmap.png;The variable Color intensity can be discarded without risking losing information.;F
1932;Wine_correlation_heatmap.png;Variables Color intensity and Alcohol are redundant, but we can’t say the same for the pair Flavanoids and Alcalinity of ash.;F
1933;Wine_correlation_heatmap.png;Variables Flavanoids and Total phenols are redundant.;T
1934;Wine_correlation_heatmap.png;Considering that the target variable is Class we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
1935;Wine_correlation_heatmap.png;Considering that the target variable is Class we can say that variable Ash seems to be relevant for the majority of mining tasks.;F
1936;Wine_correlation_heatmap.png;Considering that the target variable is Class we can say that variables Alcalinity of ash and Malic acid seem to be useful for classification tasks.;F
1937;Wine_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
1938;Wine_correlation_heatmap.png;Removing variable Alcohol might improve the training of decision trees .;F
1939;Wine_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable OD280-OD315 of diluted wines previously than variable Total phenols.;F
1940;Wine_boxplots.png;Variable OD280-OD315 of diluted wines is balanced.;F
1941;Wine_boxplots.png;Those boxplots show that the data is not normalized.;T
1942;Wine_boxplots.png;It is clear that variable Nonflavanoid phenols shows some outliers, but we can’t be sure of the same for variable Color intensity.;F
1943;Wine_boxplots.png;Outliers seem to be a problem in the dataset.;T
1944;Wine_boxplots.png;Variable Hue shows some outlier values.;T
1945;Wine_boxplots.png;Variable Malic acid doesn’t have any outliers.;F
1946;Wine_boxplots.png;Variable OD280-OD315 of diluted wines presents some outliers.;F
1947;Wine_boxplots.png;At least 50% of the variables present outliers.;T
1948;Wine_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1949;Wine_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
1950;Wine_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
1951;Wine_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
1952;Wine_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
1953;Wine_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
1954;Wine_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;F
1955;Wine_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
1956;Wine_histograms_numeric.png;All variables should be dealt with as binary.;F
1957;Wine_histograms_numeric.png;The variable Total phenols can be seen as ordinal.;F
1958;Wine_histograms_numeric.png;The variable Alcohol can be seen as ordinal without losing information.;F
1959;Wine_histograms_numeric.png;Variable Flavanoids is balanced.;F
1960;Wine_histograms_numeric.png;It is clear that variable Color intensity shows some outliers, but we can’t be sure of the same for variable Total phenols.;F
1961;Wine_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
1962;Wine_histograms_numeric.png;Variable Alcalinity of ash shows some outlier values.;F
1963;Wine_histograms_numeric.png;Variable Alcohol doesn’t have any outliers.;T
1964;Wine_histograms_numeric.png;Variable Ash presents some outliers.;T
1965;Wine_histograms_numeric.png;At least 50% of the variables present outliers.;F
1966;Wine_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
1967;Wine_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
1968;Wine_histograms_numeric.png;Considering the common semantics for OD280-OD315 of diluted wines and Alcohol variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
1969;Wine_histograms_numeric.png;Considering the common semantics for OD280-OD315 of diluted wines variable, dummification would be the most adequate encoding.;F
1970;Wine_histograms_numeric.png;The variable Hue can be coded as ordinal without losing information.;F
1971;Wine_histograms_numeric.png;Feature generation based on variable Malic acid seems to be promising.;F
1972;Wine_histograms_numeric.png;Feature generation based on the use of variable Nonflavanoid phenols wouldn’t be useful, but the use of Alcohol seems to be promising.;F
1973;Wine_histograms_numeric.png;Given the usual semantics of Total phenols variable, dummification would have been a better codification.;F
1974;Wine_histograms_numeric.png;Not knowing the semantics of Alcalinity of ash variable, dummification could have been a more adequate codification.;F
1975;water_potability_decision_tree.png;The variable Hardness discriminates between the target values, as shown in the decision tree.;T
1976;water_potability_decision_tree.png;Variable Chloramines is one of the most relevant variables.;T
1977;water_potability_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T
1978;water_potability_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;F
1979;water_potability_decision_tree.png;The specificity for the presented tree is higher than 90%, consider 1 as the positive class and 0 as the negative class.;T
1980;water_potability_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
1981;water_potability_decision_tree.png;The number of False Negatives is higher than the number of True Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
1982;water_potability_decision_tree.png;The specificity for the presented tree is lower than 60%, consider 1 as the positive class and 0 as the negative class.;F
1983;water_potability_decision_tree.png;Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 1388.;F
1984;water_potability_decision_tree.png;Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 6.;F
1985;water_potability_decision_tree.png;Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 1.;T
1986;water_potability_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
1987;water_potability_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;T
1988;water_potability_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;T
1989;water_potability_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
1990;water_potability_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
1991;water_potability_overfitting_knn.png;KNN is in overfitting for k larger than 5.;F
1992;water_potability_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;F
1993;water_potability_overfitting_knn.png;KNN with less than 15 neighbours is in overfitting.;F
1994;water_potability_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
1995;water_potability_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.;T
1996;water_potability_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 9.;T
1997;water_potability_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.;F
1998;water_potability_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;F
1999;water_potability_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
2000;water_potability_pca.png;The first 4 principal components are enough for explaining half the data variance.;T
2001;water_potability_pca.png;Using the first 3 principal components would imply an error between 5 and 30%.;F
2002;water_potability_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
2003;water_potability_correlation_heatmap.png;One of the variables Hardness or Conductivity can be discarded without losing information.;F
2004;water_potability_correlation_heatmap.png;The variable Turbidity can be discarded without risking losing information.;F
2005;water_potability_correlation_heatmap.png;Variables Trihalomethanes and Hardness are redundant, but we can’t say the same for the pair Chloramines and Sulfate.;F
2006;water_potability_correlation_heatmap.png;Variables Hardness and ph are redundant.;F
2007;water_potability_correlation_heatmap.png;Considering that the target variable is Sulfate we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
2008;water_potability_correlation_heatmap.png;Considering that the target variable is Sulfate we can say that variable Turbidity seems to be relevant for the majority of mining tasks.;F
2009;water_potability_correlation_heatmap.png;Considering that the target variable is Sulfate we can say that variables Conductivity and Turbidity seem to be useful for classification tasks.;F
2010;water_potability_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
2011;water_potability_correlation_heatmap.png;Removing variable Hardness might improve the training of decision trees .;F
2012;water_potability_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Turbidity previously than variable Chloramines.;F
2013;water_potability_boxplots.png;Variable Trihalomethanes is balanced.;T
2014;water_potability_boxplots.png;Those boxplots show that the data is not normalized.;T
2015;water_potability_boxplots.png;It is clear that variable Turbidity shows some outliers, but we can’t be sure of the same for variable Sulfate.;F
2016;water_potability_boxplots.png;Outliers seem to be a problem in the dataset.;T
2017;water_potability_boxplots.png;Variable ph shows some outlier values.;T
2018;water_potability_boxplots.png;Variable Turbidity doesn’t have any outliers.;F
2019;water_potability_boxplots.png;Variable Trihalomethanes presents some outliers.;T
2020;water_potability_boxplots.png;At least 75% of the variables present outliers.;T
2021;water_potability_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;T
2022;water_potability_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2023;water_potability_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
2024;water_potability_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2025;water_potability_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
2026;water_potability_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
2027;water_potability_mv.png;Considering that the dataset has 3000 records, discarding variable Trihalomethanes would be better than discarding all the records with missing values for that variable.;F
2028;water_potability_mv.png;Considering that the dataset has 1000 records, dropping all records with missing values would be better than to drop the variables with missing values.;F
2029;water_potability_mv.png;Considering that the dataset has 1000 records, dropping all rows with missing values can lead to a dataset with less than 30% of the original data.;T
2030;water_potability_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;T
2031;water_potability_mv.png;Considering that the dataset has 3000 records, it is better to drop the variable Trihalomethanes than removing all records with missing values.;F
2032;water_potability_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
2033;water_potability_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;F
2034;water_potability_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2035;water_potability_histograms_numeric.png;All variables should be dealt with as date.;F
2036;water_potability_histograms_numeric.png;The variable Trihalomethanes can be seen as ordinal.;F
2037;water_potability_histograms_numeric.png;The variable Chloramines can be seen as ordinal without losing information.;F
2038;water_potability_histograms_numeric.png;Variable Turbidity is balanced.;T
2039;water_potability_histograms_numeric.png;It is clear that variable Chloramines shows some outliers, but we can’t be sure of the same for variable Trihalomethanes.;F
2040;water_potability_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
2041;water_potability_histograms_numeric.png;Variable Trihalomethanes shows a high number of outlier values.;F
2042;water_potability_histograms_numeric.png;Variable Turbidity doesn’t have any outliers.;T
2043;water_potability_histograms_numeric.png;Variable Sulfate presents some outliers.;T
2044;water_potability_histograms_numeric.png;At least 50% of the variables present outliers.;T
2045;water_potability_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
2046;water_potability_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2047;water_potability_histograms_numeric.png;Considering the common semantics for ph and Hardness variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
2048;water_potability_histograms_numeric.png;Considering the common semantics for Turbidity variable, dummification would be the most adequate encoding.;F
2049;water_potability_histograms_numeric.png;The variable Conductivity can be coded as ordinal without losing information.;F
2050;water_potability_histograms_numeric.png;Feature generation based on variable Chloramines seems to be promising.;F
2051;water_potability_histograms_numeric.png;Feature generation based on the use of variable Conductivity wouldn’t be useful, but the use of ph seems to be promising.;F
2052;water_potability_histograms_numeric.png;Given the usual semantics of Chloramines variable, dummification would have been a better codification.;F
2053;water_potability_histograms_numeric.png;Not knowing the semantics of ph variable, dummification could have been a more adequate codification.;F
2054;abalone_decision_tree.png;The variable Diameter discriminates between the target values, as shown in the decision tree.;T
2055;abalone_decision_tree.png;Variable Diameter is one of the most relevant variables.;T
2056;abalone_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
2057;abalone_decision_tree.png;Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], the Decision Tree presented classifies (not A, B) as I.;F
2058;abalone_decision_tree.png;Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (A, not B) as M for any k ≤ 117.;T
2059;abalone_decision_tree.png;Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (not A, not B) as M for any k ≤ 1191.;T
2060;abalone_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
2061;abalone_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;F
2062;abalone_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;F
2063;abalone_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
2064;abalone_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
2065;abalone_overfitting_knn.png;KNN is in overfitting for k less than 5.;T
2066;abalone_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;T
2067;abalone_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
2068;abalone_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;T
2069;abalone_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;F
2070;abalone_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 4.;F
2071;abalone_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.;T
2072;abalone_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;T
2073;abalone_pca.png;The first 6 principal components are enough for explaining half the data variance.;T
2074;abalone_pca.png;Using the first 3 principal components would imply an error between 15 and 30%.;F
2075;abalone_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 6.;F
2076;abalone_correlation_heatmap.png;One of the variables Whole weight or Length can be discarded without losing information.;T
2077;abalone_correlation_heatmap.png;The variable Whole weight can be discarded without risking losing information.;T
2078;abalone_correlation_heatmap.png;Variables Length and Height are redundant, but we can’t say the same for the pair Whole weight and Viscera weight.;F
2079;abalone_correlation_heatmap.png;Variables Diameter and Length are redundant.;T
2080;abalone_correlation_heatmap.png;Considering that the target variable is Whole weight we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
2081;abalone_correlation_heatmap.png;Considering that the target variable is Viscera weight we can say that variable Whole weight seems to be relevant for the majority of mining tasks.;T
2082;abalone_correlation_heatmap.png;Considering that the target variable is Diameter we can say that variables Whole weight and Length seem to be useful for classification tasks.;T
2083;abalone_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
2084;abalone_correlation_heatmap.png;Removing variable Rings might improve the training of decision trees .;F
2085;abalone_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Length previously than variable Height.;T
2086;abalone_boxplots.png;Variable Rings is balanced.;F
2087;abalone_boxplots.png;Those boxplots show that the data is not normalized.;T
2088;abalone_boxplots.png;It is clear that variable Shell weight shows some outliers, but we can’t be sure of the same for variable Viscera weight.;F
2089;abalone_boxplots.png;Outliers seem to be a problem in the dataset.;T
2090;abalone_boxplots.png;Variable Rings shows a high number of outlier values.;F
2091;abalone_boxplots.png;Variable Shell weight doesn’t have any outliers.;F
2092;abalone_boxplots.png;Variable Shell weight presents some outliers.;T
2093;abalone_boxplots.png;At least 50% of the variables present outliers.;T
2094;abalone_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
2095;abalone_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2096;abalone_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
2097;abalone_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2098;abalone_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
2099;abalone_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
2100;abalone_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
2101;abalone_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
2102;abalone_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2103;abalone_histograms_numeric.png;All variables should be dealt with as date.;F
2104;abalone_histograms_numeric.png;The variable Shucked weight can be seen as ordinal.;F
2105;abalone_histograms_numeric.png;The variable Shucked weight can be seen as ordinal without losing information.;F
2106;abalone_histograms_numeric.png;Variable Shell weight is balanced.;F
2107;abalone_histograms_numeric.png;It is clear that variable Rings shows some outliers, but we can’t be sure of the same for variable Whole weight.;F
2108;abalone_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
2109;abalone_histograms_numeric.png;Variable Viscera weight shows some outlier values.;F
2110;abalone_histograms_numeric.png;Variable Diameter doesn’t have any outliers.;T
2111;abalone_histograms_numeric.png;Variable Length presents some outliers.;T
2112;abalone_histograms_numeric.png;At least 75% of the variables present outliers.;F
2113;abalone_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
2114;abalone_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2115;abalone_histograms_numeric.png;Considering the common semantics for Diameter and Length variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
2116;abalone_histograms_numeric.png;Considering the common semantics for Length variable, dummification would be the most adequate encoding.;F
2117;abalone_histograms_numeric.png;The variable Diameter can be coded as ordinal without losing information.;F
2118;abalone_histograms_numeric.png;Feature generation based on variable Shucked weight seems to be promising.;F
2119;abalone_histograms_numeric.png;Feature generation based on the use of variable Diameter wouldn’t be useful, but the use of Length seems to be promising.;F
2120;abalone_histograms_numeric.png;Given the usual semantics of Viscera weight variable, dummification would have been a better codification.;F
2121;abalone_histograms_numeric.png;Not knowing the semantics of Shell weight variable, dummification could have been a more adequate codification.;F
2122;smoking_drinking_decision_tree.png;The variable SMK_stat_type_cd discriminates between the target values, as shown in the decision tree.;T
2123;smoking_drinking_decision_tree.png;Variable gamma_GTP is one of the most relevant variables.;T
2124;smoking_drinking_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T
2125;smoking_drinking_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider Y as the positive class and N as the negative class.;F
2126;smoking_drinking_decision_tree.png;The specificity for the presented tree is higher than 90%, consider Y as the positive class and N as the negative class.;F
2127;smoking_drinking_decision_tree.png;The number of False Negatives reported in the same tree is 10, consider Y as the positive class and N as the negative class.;F
2128;smoking_drinking_decision_tree.png;The number of True Positives is lower than the number of False Negatives for the presented tree, consider Y as the positive class and N as the negative class.;F
2129;smoking_drinking_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree, consider Y as the positive class and N as the negative class.;F
2130;smoking_drinking_decision_tree.png;Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that KNN algorithm classifies (A,B) as N for any k ≤ 3135.;T
2131;smoking_drinking_decision_tree.png;Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as N.;F
2132;smoking_drinking_decision_tree.png;Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], the Decision Tree presented classifies (A, not B) as Y.;F
2133;smoking_drinking_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
2134;smoking_drinking_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;T
2135;smoking_drinking_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;T
2136;smoking_drinking_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
2137;smoking_drinking_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
2138;smoking_drinking_overfitting_knn.png;KNN is in overfitting for k less than 13.;T
2139;smoking_drinking_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;T
2140;smoking_drinking_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.;T
2141;smoking_drinking_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;T
2142;smoking_drinking_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.;T
2143;smoking_drinking_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 10.;T
2144;smoking_drinking_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 3.;F
2145;smoking_drinking_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;T
2146;smoking_drinking_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
2147;smoking_drinking_pca.png;The first 6 principal components are enough for explaining half the data variance.;T
2148;smoking_drinking_pca.png;Using the first 8 principal components would imply an error between 15 and 30%.;F
2149;smoking_drinking_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 7.;F
2150;smoking_drinking_correlation_heatmap.png;One of the variables waistline or age can be discarded without losing information.;F
2151;smoking_drinking_correlation_heatmap.png;The variable hemoglobin can be discarded without risking losing information.;F
2152;smoking_drinking_correlation_heatmap.png;Variables BLDS and weight are redundant, but we can’t say the same for the pair waistline and LDL_chole.;F
2153;smoking_drinking_correlation_heatmap.png;Variables age and height are redundant.;F
2154;smoking_drinking_correlation_heatmap.png;Considering that the target variable is tot_chole we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
2155;smoking_drinking_correlation_heatmap.png;Considering that the target variable is tot_chole we can say that variable age seems to be relevant for the majority of mining tasks.;F
2156;smoking_drinking_correlation_heatmap.png;Considering that the target variable is tot_chole we can say that variables height and waistline seem to be useful for classification tasks.;F
2157;smoking_drinking_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
2158;smoking_drinking_correlation_heatmap.png;Removing variable waistline might improve the training of decision trees .;T
2159;smoking_drinking_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable LDL_chole previously than variable SMK_stat_type_cd.;T
2160;smoking_drinking_boxplots.png;Variable waistline is balanced.;F
2161;smoking_drinking_boxplots.png;Those boxplots show that the data is not normalized.;T
2162;smoking_drinking_boxplots.png;It is clear that variable tot_chole shows some outliers, but we can’t be sure of the same for variable waistline.;T
2163;smoking_drinking_boxplots.png;Outliers seem to be a problem in the dataset.;T
2164;smoking_drinking_boxplots.png;Variable tot_chole shows a high number of outlier values.;T
2165;smoking_drinking_boxplots.png;Variable SMK_stat_type_cd doesn’t have any outliers.;F
2166;smoking_drinking_boxplots.png;Variable BLDS presents some outliers.;T
2167;smoking_drinking_boxplots.png;At least 85% of the variables present outliers.;T
2168;smoking_drinking_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
2169;smoking_drinking_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2170;smoking_drinking_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
2171;smoking_drinking_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2172;smoking_drinking_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
2173;smoking_drinking_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
2174;smoking_drinking_histograms_symbolic.png;All variables should be dealt with as binary.;T
2175;smoking_drinking_histograms_symbolic.png;The variable hear_right can be seen as ordinal.;T
2176;smoking_drinking_histograms_symbolic.png;The variable hear_right can be seen as ordinal without losing information.;T
2177;smoking_drinking_histograms_symbolic.png;Considering the common semantics for hear_left and sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
2178;smoking_drinking_histograms_symbolic.png;Considering the common semantics for hear_right variable, dummification would be the most adequate encoding.;F
2179;smoking_drinking_histograms_symbolic.png;The variable sex can be coded as ordinal without losing information.;F
2180;smoking_drinking_histograms_symbolic.png;Feature generation based on variable hear_left seems to be promising.;F
2181;smoking_drinking_histograms_symbolic.png;Feature generation based on the use of variable hear_right wouldn’t be useful, but the use of sex seems to be promising.;F
2182;smoking_drinking_histograms_symbolic.png;Given the usual semantics of hear_right variable, dummification would have been a better codification.;F
2183;smoking_drinking_histograms_symbolic.png;Not knowing the semantics of hear_right variable, dummification could have been a more adequate codification.;F
2184;smoking_drinking_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
2185;smoking_drinking_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
2186;smoking_drinking_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2187;smoking_drinking_histograms_numeric.png;All variables should be dealt with as date.;F
2188;smoking_drinking_histograms_numeric.png;The variable SBP can be seen as ordinal.;F
2189;smoking_drinking_histograms_numeric.png;The variable tot_chole can be seen as ordinal without losing information.;F
2190;smoking_drinking_histograms_numeric.png;Variable weight is balanced.;F
2191;smoking_drinking_histograms_numeric.png;It is clear that variable height shows some outliers, but we can’t be sure of the same for variable age.;F
2192;smoking_drinking_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
2193;smoking_drinking_histograms_numeric.png;Variable LDL_chole shows some outlier values.;T
2194;smoking_drinking_histograms_numeric.png;Variable tot_chole doesn’t have any outliers.;F
2195;smoking_drinking_histograms_numeric.png;Variable gamma_GTP presents some outliers.;T
2196;smoking_drinking_histograms_numeric.png;At least 60% of the variables present outliers.;T
2197;smoking_drinking_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
2198;smoking_drinking_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2199;smoking_drinking_histograms_numeric.png;Considering the common semantics for age and height variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
2200;smoking_drinking_histograms_numeric.png;Considering the common semantics for weight variable, dummification would be the most adequate encoding.;F
2201;smoking_drinking_histograms_numeric.png;The variable hemoglobin can be coded as ordinal without losing information.;F
2202;smoking_drinking_histograms_numeric.png;Feature generation based on variable waistline seems to be promising.;F
2203;smoking_drinking_histograms_numeric.png;Feature generation based on the use of variable height wouldn’t be useful, but the use of age seems to be promising.;F
2204;smoking_drinking_histograms_numeric.png;Given the usual semantics of BLDS variable, dummification would have been a better codification.;F
2205;smoking_drinking_histograms_numeric.png;Not knowing the semantics of hemoglobin variable, dummification could have been a more adequate codification.;F
2206;BankNoteAuthentication_decision_tree.png;The variable curtosis discriminates between the target values, as shown in the decision tree.;T
2207;BankNoteAuthentication_decision_tree.png;Variable skewness is one of the most relevant variables.;T
2208;BankNoteAuthentication_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T
2209;BankNoteAuthentication_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;F
2210;BankNoteAuthentication_decision_tree.png;The recall for the presented tree is higher than 75%, consider 1 as the positive class and 0 as the negative class.;F
2211;BankNoteAuthentication_decision_tree.png;The number of False Positives reported in the same tree is 10, consider 1 as the positive class and 0 as the negative class.;F
2212;BankNoteAuthentication_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
2213;BankNoteAuthentication_decision_tree.png;The recall for the presented tree is lower than its accuracy, consider 1 as the positive class and 0 as the negative class.;T
2214;BankNoteAuthentication_decision_tree.png;Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 214.;T
2215;BankNoteAuthentication_decision_tree.png;Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 214.;F
2216;BankNoteAuthentication_decision_tree.png;Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 131.;F
2217;BankNoteAuthentication_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
2218;BankNoteAuthentication_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;F
2219;BankNoteAuthentication_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;F
2220;BankNoteAuthentication_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
2221;BankNoteAuthentication_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
2222;BankNoteAuthentication_overfitting_knn.png;KNN is in overfitting for k larger than 5.;F
2223;BankNoteAuthentication_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;F
2224;BankNoteAuthentication_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
2225;BankNoteAuthentication_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;F
2226;BankNoteAuthentication_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;F
2227;BankNoteAuthentication_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 10.;F
2228;BankNoteAuthentication_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 10.;F
2229;BankNoteAuthentication_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;F
2230;BankNoteAuthentication_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
2231;BankNoteAuthentication_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
2232;BankNoteAuthentication_pca.png;Using the first 2 principal components would imply an error between 15 and 30%.;F
2233;BankNoteAuthentication_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.;F
2234;BankNoteAuthentication_correlation_heatmap.png;One of the variables variance or curtosis can be discarded without losing information.;F
2235;BankNoteAuthentication_correlation_heatmap.png;The variable skewness can be discarded without risking losing information.;F
2236;BankNoteAuthentication_correlation_heatmap.png;Variables entropy and curtosis are redundant, but we can’t say the same for the pair variance and skewness.;F
2237;BankNoteAuthentication_correlation_heatmap.png;Variables curtosis and entropy are redundant.;F
2238;BankNoteAuthentication_correlation_heatmap.png;Considering that the target variable is entropy we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
2239;BankNoteAuthentication_correlation_heatmap.png;Considering that the target variable is entropy we can say that variable skewness seems to be relevant for the majority of mining tasks.;F
2240;BankNoteAuthentication_correlation_heatmap.png;Considering that the target variable is entropy we can say that variables curtosis and variance seem to be useful for classification tasks.;F
2241;BankNoteAuthentication_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
2242;BankNoteAuthentication_correlation_heatmap.png;Removing variable variance might improve the training of decision trees .;F
2243;BankNoteAuthentication_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable variance previously than variable skewness.;F
2244;BankNoteAuthentication_boxplots.png;Variable curtosis is balanced.;F
2245;BankNoteAuthentication_boxplots.png;Those boxplots show that the data is not normalized.;T
2246;BankNoteAuthentication_boxplots.png;It is clear that variable curtosis shows some outliers, but we can’t be sure of the same for variable entropy.;F
2247;BankNoteAuthentication_boxplots.png;Outliers seem to be a problem in the dataset.;T
2248;BankNoteAuthentication_boxplots.png;Variable skewness shows a high number of outlier values.;T
2249;BankNoteAuthentication_boxplots.png;Variable skewness doesn’t have any outliers.;F
2250;BankNoteAuthentication_boxplots.png;Variable variance presents some outliers.;T
2251;BankNoteAuthentication_boxplots.png;At least 75% of the variables present outliers.;T
2252;BankNoteAuthentication_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2253;BankNoteAuthentication_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
2254;BankNoteAuthentication_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2255;BankNoteAuthentication_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
2256;BankNoteAuthentication_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
2257;BankNoteAuthentication_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
2258;BankNoteAuthentication_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
2259;BankNoteAuthentication_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2260;BankNoteAuthentication_histograms_numeric.png;All variables should be dealt with as symbolic.;F
2261;BankNoteAuthentication_histograms_numeric.png;The variable skewness can be seen as ordinal.;F
2262;BankNoteAuthentication_histograms_numeric.png;The variable skewness can be seen as ordinal without losing information.;F
2263;BankNoteAuthentication_histograms_numeric.png;Variable variance is balanced.;T
2264;BankNoteAuthentication_histograms_numeric.png;It is clear that variable variance shows some outliers, but we can’t be sure of the same for variable entropy.;F
2265;BankNoteAuthentication_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
2266;BankNoteAuthentication_histograms_numeric.png;Variable variance shows a high number of outlier values.;F
2267;BankNoteAuthentication_histograms_numeric.png;Variable skewness doesn’t have any outliers.;F
2268;BankNoteAuthentication_histograms_numeric.png;Variable curtosis presents some outliers.;T
2269;BankNoteAuthentication_histograms_numeric.png;At least 60% of the variables present outliers.;T
2270;BankNoteAuthentication_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2271;BankNoteAuthentication_histograms_numeric.png;Considering the common semantics for variance and skewness variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
2272;BankNoteAuthentication_histograms_numeric.png;Considering the common semantics for curtosis variable, dummification would be the most adequate encoding.;F
2273;BankNoteAuthentication_histograms_numeric.png;The variable entropy can be coded as ordinal without losing information.;F
2274;BankNoteAuthentication_histograms_numeric.png;Feature generation based on variable skewness seems to be promising.;F
2275;BankNoteAuthentication_histograms_numeric.png;Feature generation based on the use of variable curtosis wouldn’t be useful, but the use of variance seems to be promising.;F
2276;BankNoteAuthentication_histograms_numeric.png;Given the usual semantics of curtosis variable, dummification would have been a better codification.;F
2277;BankNoteAuthentication_histograms_numeric.png;Not knowing the semantics of entropy variable, dummification could have been a more adequate codification.;F
2278;Iris_decision_tree.png;The variable PetalWidthCm discriminates between the target values, as shown in the decision tree.;T
2279;Iris_decision_tree.png;Variable PetalWidthCm is one of the most relevant variables.;T
2280;Iris_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;F
2281;Iris_decision_tree.png;Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], the Decision Tree presented classifies (not A, not B) as Iris-versicolor.;F
2282;Iris_decision_tree.png;Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], it is possible to state that KNN algorithm classifies (A,B) as Iris-virginica for any k ≤ 38.;F
2283;Iris_decision_tree.png;Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], it is possible to state that KNN algorithm classifies (A,B) as Iris-setosa for any k ≤ 32.;T
2284;Iris_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
2285;Iris_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;F
2286;Iris_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;F
2287;Iris_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
2288;Iris_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
2289;Iris_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
2290;Iris_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;F
2291;Iris_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.;F
2292;Iris_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.;F
2293;Iris_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;F
2294;Iris_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 8.;F
2295;Iris_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 10.;F
2296;Iris_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.;F
2297;Iris_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
2298;Iris_pca.png;Using the first 2 principal components would imply an error between 10 and 25%.;F
2299;Iris_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;T
2300;Iris_correlation_heatmap.png;One of the variables PetalWidthCm or SepalLengthCm can be discarded without losing information.;T
2301;Iris_correlation_heatmap.png;The variable PetalLengthCm can be discarded without risking losing information.;T
2302;Iris_correlation_heatmap.png;Variables SepalLengthCm and SepalWidthCm are redundant, but we can’t say the same for the pair PetalLengthCm and PetalWidthCm.;F
2303;Iris_correlation_heatmap.png;Variables PetalLengthCm and SepalLengthCm are redundant.;T
2304;Iris_correlation_heatmap.png;Considering that the target variable is PetalWidthCm we can say that from the correlation analysis alone, it is clear that there are relevant variables.;T
2305;Iris_correlation_heatmap.png;Considering that the target variable is PetalLengthCm we can say that variable SepalLengthCm seems to be relevant for the majority of mining tasks.;T
2306;Iris_correlation_heatmap.png;Considering that the target variable is PetalWidthCm we can say that variables PetalLengthCm and SepalLengthCm seem to be useful for classification tasks.;T
2307;Iris_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
2308;Iris_correlation_heatmap.png;Removing variable PetalWidthCm might improve the training of decision trees .;T
2309;Iris_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable SepalWidthCm.;T
2310;Iris_boxplots.png;Variable PetalWidthCm is balanced.;F
2311;Iris_boxplots.png;Those boxplots show that the data is not normalized.;T
2312;Iris_boxplots.png;It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.;T
2313;Iris_boxplots.png;Outliers seem to be a problem in the dataset.;T
2314;Iris_boxplots.png;Variable PetalLengthCm shows some outlier values.;F
2315;Iris_boxplots.png;Variable SepalLengthCm doesn’t have any outliers.;F
2316;Iris_boxplots.png;Variable PetalWidthCm presents some outliers.;F
2317;Iris_boxplots.png;At least 75% of the variables present outliers.;F
2318;Iris_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
2319;Iris_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2320;Iris_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
2321;Iris_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2322;Iris_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
2323;Iris_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
2324;Iris_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
2325;Iris_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;F
2326;Iris_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2327;Iris_histograms_numeric.png;All variables should be dealt with as date.;F
2328;Iris_histograms_numeric.png;The variable PetalWidthCm can be seen as ordinal.;F
2329;Iris_histograms_numeric.png;The variable SepalLengthCm can be seen as ordinal without losing information.;F
2330;Iris_histograms_numeric.png;Variable PetalWidthCm is balanced.;F
2331;Iris_histograms_numeric.png;It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.;T
2332;Iris_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
2333;Iris_histograms_numeric.png;Variable SepalWidthCm shows a high number of outlier values.;F
2334;Iris_histograms_numeric.png;Variable SepalWidthCm doesn’t have any outliers.;T
2335;Iris_histograms_numeric.png;Variable PetalLengthCm presents some outliers.;T
2336;Iris_histograms_numeric.png;At least 75% of the variables present outliers.;T
2337;Iris_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2338;Iris_histograms_numeric.png;Considering the common semantics for PetalWidthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
2339;Iris_histograms_numeric.png;Considering the common semantics for PetalLengthCm variable, dummification would be the most adequate encoding.;F
2340;Iris_histograms_numeric.png;The variable PetalLengthCm can be coded as ordinal without losing information.;F
2341;Iris_histograms_numeric.png;Feature generation based on variable SepalWidthCm seems to be promising.;F
2342;Iris_histograms_numeric.png;Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.;F
2343;Iris_histograms_numeric.png;Given the usual semantics of PetalLengthCm variable, dummification would have been a better codification.;F
2344;Iris_histograms_numeric.png;Not knowing the semantics of SepalWidthCm variable, dummification could have been a more adequate codification.;F
2345;phone_decision_tree.png;The variable mobile_wt discriminates between the target values, as shown in the decision tree.;T
2346;phone_decision_tree.png;Variable mobile_wt is one of the most relevant variables.;T
2347;phone_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T
2348;phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], it is possible to state that KNN algorithm classifies (A,B) as 2 for any k ≤ 636.;F
2349;phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], the Decision Tree presented classifies (not A, B) as 1.;F
2350;phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], the Decision Tree presented classifies (A, not B) as 0.;T
2351;phone_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
2352;phone_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;F
2353;phone_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.;T
2354;phone_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
2355;phone_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
2356;phone_overfitting_knn.png;KNN is in overfitting for k less than 13.;T
2357;phone_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;T
2358;phone_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;T
2359;phone_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;T
2360;phone_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.;F
2361;phone_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 5.;F
2362;phone_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.;F
2363;phone_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;F
2364;phone_pca.png;The first 8 principal components are enough for explaining half the data variance.;T
2365;phone_pca.png;Using the first 11 principal components would imply an error between 10 and 25%.;F
2366;phone_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 11.;F
2367;phone_correlation_heatmap.png;One of the variables px_height or battery_power can be discarded without losing information.;F
2368;phone_correlation_heatmap.png;The variable battery_power can be discarded without risking losing information.;F
2369;phone_correlation_heatmap.png;Variables ram and px_width are redundant, but we can’t say the same for the pair mobile_wt and sc_h.;F
2370;phone_correlation_heatmap.png;Variables px_height and sc_w are redundant.;F
2371;phone_correlation_heatmap.png;Considering that the target variable is px_width we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
2372;phone_correlation_heatmap.png;Considering that the target variable is px_width we can say that variable n_cores seems to be relevant for the majority of mining tasks.;F
2373;phone_correlation_heatmap.png;Considering that the target variable is px_width we can say that variables sc_h and fc seem to be useful for classification tasks.;F
2374;phone_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
2375;phone_correlation_heatmap.png;Removing variable sc_h might improve the training of decision trees .;F
2376;phone_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable px_height previously than variable px_width.;F
2377;phone_boxplots.png;Variable n_cores is balanced.;F
2378;phone_boxplots.png;Those boxplots show that the data is not normalized.;T
2379;phone_boxplots.png;It is clear that variable talk_time shows some outliers, but we can’t be sure of the same for variable px_width.;F
2380;phone_boxplots.png;Outliers seem to be a problem in the dataset.;F
2381;phone_boxplots.png;Variable px_height shows some outlier values.;F
2382;phone_boxplots.png;Variable sc_w doesn’t have any outliers.;F
2383;phone_boxplots.png;Variable pc presents some outliers.;F
2384;phone_boxplots.png;At least 50% of the variables present outliers.;F
2385;phone_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
2386;phone_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2387;phone_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
2388;phone_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2389;phone_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
2390;phone_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
2391;phone_histograms_symbolic.png;All variables should be dealt with as date.;F
2392;phone_histograms_symbolic.png;The variable four_g can be seen as ordinal.;T
2393;phone_histograms_symbolic.png;The variable wifi can be seen as ordinal without losing information.;T
2394;phone_histograms_symbolic.png;Considering the common semantics for touch_screen and blue variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
2395;phone_histograms_symbolic.png;Considering the common semantics for three_g variable, dummification would be the most adequate encoding.;F
2396;phone_histograms_symbolic.png;The variable three_g can be coded as ordinal without losing information.;T
2397;phone_histograms_symbolic.png;Feature generation based on variable four_g seems to be promising.;F
2398;phone_histograms_symbolic.png;Feature generation based on the use of variable three_g wouldn’t be useful, but the use of blue seems to be promising.;F
2399;phone_histograms_symbolic.png;Given the usual semantics of three_g variable, dummification would have been a better codification.;F
2400;phone_histograms_symbolic.png;Not knowing the semantics of four_g variable, dummification could have been a more adequate codification.;F
2401;phone_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
2402;phone_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;F
2403;phone_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2404;phone_histograms_numeric.png;All variables should be dealt with as binary.;F
2405;phone_histograms_numeric.png;The variable int_memory can be seen as ordinal.;F
2406;phone_histograms_numeric.png;The variable fc can be seen as ordinal without losing information.;F
2407;phone_histograms_numeric.png;Variable sc_h is balanced.;F
2408;phone_histograms_numeric.png;It is clear that variable sc_w shows some outliers, but we can’t be sure of the same for variable sc_h.;F
2409;phone_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
2410;phone_histograms_numeric.png;Variable pc shows a high number of outlier values.;F
2411;phone_histograms_numeric.png;Variable ram doesn’t have any outliers.;T
2412;phone_histograms_numeric.png;Variable fc presents some outliers.;T
2413;phone_histograms_numeric.png;At least 60% of the variables present outliers.;F
2414;phone_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
2415;phone_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2416;phone_histograms_numeric.png;Considering the common semantics for px_height and battery_power variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
2417;phone_histograms_numeric.png;Considering the common semantics for px_height variable, dummification would be the most adequate encoding.;F
2418;phone_histograms_numeric.png;The variable battery_power can be coded as ordinal without losing information.;F
2419;phone_histograms_numeric.png;Feature generation based on variable mobile_wt seems to be promising.;F
2420;phone_histograms_numeric.png;Feature generation based on the use of variable sc_h wouldn’t be useful, but the use of battery_power seems to be promising.;F
2421;phone_histograms_numeric.png;Given the usual semantics of mobile_wt variable, dummification would have been a better codification.;F
2422;phone_histograms_numeric.png;Not knowing the semantics of talk_time variable, dummification could have been a more adequate codification.;F
2423;Titanic_decision_tree.png;The variable Parch discriminates between the target values, as shown in the decision tree.;T
2424;Titanic_decision_tree.png;Variable Parch is one of the most relevant variables.;T
2425;Titanic_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T
2426;Titanic_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;F
2427;Titanic_decision_tree.png;The accuracy for the presented tree is lower than 75%.;T
2428;Titanic_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
2429;Titanic_decision_tree.png;The number of True Negatives is higher than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
2430;Titanic_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
2431;Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 181.;F
2432;Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 1.;F
2433;Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.;F
2434;Titanic_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
2435;Titanic_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;T
2436;Titanic_overfitting_rf.png;Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.;F
2437;Titanic_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
2438;Titanic_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
2439;Titanic_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
2440;Titanic_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;F
2441;Titanic_overfitting_knn.png;KNN with less than 7 neighbours is in overfitting.;F
2442;Titanic_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
2443;Titanic_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.;T
2444;Titanic_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 9.;F
2445;Titanic_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.;F
2446;Titanic_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;F
2447;Titanic_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
2448;Titanic_pca.png;The first 4 principal components are enough for explaining half the data variance.;T
2449;Titanic_pca.png;Using the first 2 principal components would imply an error between 15 and 20%.;F
2450;Titanic_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 4.;F
2451;Titanic_correlation_heatmap.png;One of the variables SibSp or Parch can be discarded without losing information.;F
2452;Titanic_correlation_heatmap.png;The variable Parch can be discarded without risking losing information.;F
2453;Titanic_correlation_heatmap.png;Variables Fare and Age seem to be useful for classification tasks.;F
2454;Titanic_correlation_heatmap.png;Variables Age and Fare are redundant.;F
2455;Titanic_correlation_heatmap.png;Considering that the target variable is Pclass we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
2456;Titanic_correlation_heatmap.png;Considering that the target variable is Parch we can say that variable Pclass seems to be relevant for the majority of mining tasks.;F
2457;Titanic_correlation_heatmap.png;Considering that the target variable is Pclass we can say that variables Parch and SibSp seem to be useful for classification tasks.;F
2458;Titanic_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
2459;Titanic_correlation_heatmap.png;Removing variable SibSp might improve the training of decision trees .;F
2460;Titanic_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Fare previously than variable Age.;F
2461;Titanic_boxplots.png;Variable Age is balanced.;F
2462;Titanic_boxplots.png;Those boxplots show that the data is not normalized.;T
2463;Titanic_boxplots.png;It is clear that variable Pclass shows some outliers, but we can’t be sure of the same for variable Fare.;F
2464;Titanic_boxplots.png;Outliers seem to be a problem in the dataset.;T
2465;Titanic_boxplots.png;Variable Fare shows a high number of outlier values.;F
2466;Titanic_boxplots.png;Variable Fare doesn’t have any outliers.;F
2467;Titanic_boxplots.png;Variable Parch presents some outliers.;F
2468;Titanic_boxplots.png;At least 50% of the variables present outliers.;F
2469;Titanic_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;T
2470;Titanic_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2471;Titanic_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
2472;Titanic_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2473;Titanic_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
2474;Titanic_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
2475;Titanic_histograms_symbolic.png;All variables should be dealt with as date.;F
2476;Titanic_histograms_symbolic.png;The variable Sex can be seen as ordinal.;T
2477;Titanic_histograms_symbolic.png;The variable Sex can be seen as ordinal without losing information.;F
2478;Titanic_histograms_symbolic.png;Considering the common semantics for Sex and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
2479;Titanic_histograms_symbolic.png;Considering the common semantics for Embarked variable, dummification would be the most adequate encoding.;T
2480;Titanic_histograms_symbolic.png;The variable Embarked can be coded as ordinal without losing information.;F
2481;Titanic_histograms_symbolic.png;Feature generation based on variable Sex seems to be promising.;F
2482;Titanic_histograms_symbolic.png;Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Embarked seems to be promising.;F
2483;Titanic_histograms_symbolic.png;Given the usual semantics of Sex variable, dummification would have been a better codification.;F
2484;Titanic_histograms_symbolic.png;Not knowing the semantics of Embarked variable, dummification could have been a more adequate codification.;T
2485;Titanic_mv.png;Considering that the dataset has 200 records, discarding variable Embarked would be better than discarding all the records with missing values for that variable.;F
2486;Titanic_mv.png;Considering that the dataset has 200 records, dropping all records with missing values would be better than to drop the variables with missing values.;F
2487;Titanic_mv.png;Considering that the dataset has 200 records, dropping all rows with missing values can lead to a dataset with less than 25% of the original data.;T
2488;Titanic_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;T
2489;Titanic_mv.png;Feature generation based on variable Embarked seems to be promising.;F
2490;Titanic_mv.png;Considering that the dataset has 200 records, it is better to drop the variable Age than removing all records with missing values.;T
2491;Titanic_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
2492;Titanic_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
2493;Titanic_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2494;Titanic_histograms_numeric.png;All variables should be dealt with as symbolic.;F
2495;Titanic_histograms_numeric.png;The variable Parch can be seen as ordinal.;T
2496;Titanic_histograms_numeric.png;The variable Fare can be seen as ordinal without losing information.;F
2497;Titanic_histograms_numeric.png;Variable Pclass is balanced.;F
2498;Titanic_histograms_numeric.png;It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable SibSp.;F
2499;Titanic_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
2500;Titanic_histograms_numeric.png;Variable Age shows a high number of outlier values.;T
2501;Titanic_histograms_numeric.png;Variable Fare doesn’t have any outliers.;F
2502;Titanic_histograms_numeric.png;Variable Parch presents some outliers.;T
2503;Titanic_histograms_numeric.png;At least 60% of the variables present outliers.;T
2504;Titanic_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2505;Titanic_histograms_numeric.png;Considering the common semantics for Age and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
2506;Titanic_histograms_numeric.png;Considering the common semantics for SibSp variable, dummification would be the most adequate encoding.;F
2507;Titanic_histograms_numeric.png;The variable Pclass can be coded as ordinal without losing information.;T
2508;Titanic_histograms_numeric.png;Feature generation based on variable Parch seems to be promising.;F
2509;Titanic_histograms_numeric.png;Feature generation based on the use of variable Age wouldn’t be useful, but the use of Pclass seems to be promising.;F
2510;Titanic_histograms_numeric.png;Given the usual semantics of Age variable, dummification would have been a better codification.;F
2511;Titanic_histograms_numeric.png;Not knowing the semantics of SibSp variable, dummification could have been a more adequate codification.;F
2512;apple_quality_decision_tree.png;The variable Crunchiness discriminates between the target values, as shown in the decision tree.;T
2513;apple_quality_decision_tree.png;Variable Juiciness is one of the most relevant variables.;T
2514;apple_quality_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
2515;apple_quality_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider good as the positive class and bad as the negative class.;F
2516;apple_quality_decision_tree.png;The recall for the presented tree is higher than 75%, consider good as the positive class and bad as the negative class.;T
2517;apple_quality_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree, consider good as the positive class and bad as the negative class.;T
2518;apple_quality_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree, consider good as the positive class and bad as the negative class.;F
2519;apple_quality_decision_tree.png;The specificity for the presented tree is higher than 90%, consider good as the positive class and bad as the negative class.;F
2520;apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], the Decision Tree presented classifies (not A, not B) as bad.;F
2521;apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 1625.;F
2522;apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as good.;T
2523;apple_quality_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
2524;apple_quality_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;F
2525;apple_quality_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;F
2526;apple_quality_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
2527;apple_quality_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
2528;apple_quality_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
2529;apple_quality_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;T
2530;apple_quality_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
2531;apple_quality_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
2532;apple_quality_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;F
2533;apple_quality_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 5.;F
2534;apple_quality_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 6.;F
2535;apple_quality_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;F
2536;apple_quality_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
2537;apple_quality_pca.png;The first 5 principal components are enough for explaining half the data variance.;T
2538;apple_quality_pca.png;Using the first 2 principal components would imply an error between 15 and 20%.;F
2539;apple_quality_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
2540;apple_quality_correlation_heatmap.png;One of the variables Crunchiness or Acidity can be discarded without losing information.;F
2541;apple_quality_correlation_heatmap.png;The variable Ripeness can be discarded without risking losing information.;F
2542;apple_quality_correlation_heatmap.png;Variables Juiciness and Crunchiness are redundant, but we can’t say the same for the pair Sweetness and Ripeness.;F
2543;apple_quality_correlation_heatmap.png;Variables Juiciness and Crunchiness are redundant.;F
2544;apple_quality_correlation_heatmap.png;Considering that the target variable is Ripeness we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
2545;apple_quality_correlation_heatmap.png;Considering that the target variable is Ripeness we can say that variable Juiciness seems to be relevant for the majority of mining tasks.;F
2546;apple_quality_correlation_heatmap.png;Considering that the target variable is Ripeness we can say that variables Crunchiness and Weight seem to be useful for classification tasks.;F
2547;apple_quality_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
2548;apple_quality_correlation_heatmap.png;Removing variable Juiciness might improve the training of decision trees .;F
2549;apple_quality_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Juiciness previously than variable Ripeness.;F
2550;apple_quality_boxplots.png;Variable Weight is balanced.;T
2551;apple_quality_boxplots.png;Those boxplots show that the data is not normalized.;F
2552;apple_quality_boxplots.png;It is clear that variable Sweetness shows some outliers, but we can’t be sure of the same for variable Crunchiness.;F
2553;apple_quality_boxplots.png;Outliers seem to be a problem in the dataset.;T
2554;apple_quality_boxplots.png;Variable Ripeness shows a high number of outlier values.;F
2555;apple_quality_boxplots.png;Variable Acidity doesn’t have any outliers.;F
2556;apple_quality_boxplots.png;Variable Juiciness presents some outliers.;T
2557;apple_quality_boxplots.png;At least 75% of the variables present outliers.;T
2558;apple_quality_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2559;apple_quality_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
2560;apple_quality_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2561;apple_quality_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;T
2562;apple_quality_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;F
2563;apple_quality_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
2564;apple_quality_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;F
2565;apple_quality_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2566;apple_quality_histograms_numeric.png;All variables should be dealt with as numeric.;T
2567;apple_quality_histograms_numeric.png;The variable Acidity can be seen as ordinal.;F
2568;apple_quality_histograms_numeric.png;The variable Size can be seen as ordinal without losing information.;F
2569;apple_quality_histograms_numeric.png;Variable Juiciness is balanced.;T
2570;apple_quality_histograms_numeric.png;It is clear that variable Weight shows some outliers, but we can’t be sure of the same for variable Sweetness.;F
2571;apple_quality_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
2572;apple_quality_histograms_numeric.png;Variable Juiciness shows a high number of outlier values.;F
2573;apple_quality_histograms_numeric.png;Variable Size doesn’t have any outliers.;F
2574;apple_quality_histograms_numeric.png;Variable Weight presents some outliers.;T
2575;apple_quality_histograms_numeric.png;At least 50% of the variables present outliers.;T
2576;apple_quality_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2577;apple_quality_histograms_numeric.png;Considering the common semantics for Crunchiness and Size variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
2578;apple_quality_histograms_numeric.png;Considering the common semantics for Sweetness variable, dummification would be the most adequate encoding.;F
2579;apple_quality_histograms_numeric.png;The variable Juiciness can be coded as ordinal without losing information.;F
2580;apple_quality_histograms_numeric.png;Feature generation based on variable Acidity seems to be promising.;F
2581;apple_quality_histograms_numeric.png;Feature generation based on the use of variable Acidity wouldn’t be useful, but the use of Size seems to be promising.;F
2582;apple_quality_histograms_numeric.png;Given the usual semantics of Acidity variable, dummification would have been a better codification.;F
2583;apple_quality_histograms_numeric.png;Not knowing the semantics of Acidity variable, dummification could have been a more adequate codification.;F
2584;Employee_decision_tree.png;The variable JoiningYear discriminates between the target values, as shown in the decision tree.;T
2585;Employee_decision_tree.png;Variable ExperienceInCurrentDomain is one of the most relevant variables.;T
2586;Employee_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
2587;Employee_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;T
2588;Employee_decision_tree.png;The recall for the presented tree is lower than 60%, consider 1 as the positive class and 0 as the negative class.;F
2589;Employee_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
2590;Employee_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F
2591;Employee_decision_tree.png;The number of True Negatives is higher than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T
2592;Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 44.;T
2593;Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], the Decision Tree presented classifies (not A, B) as 1.;T
2594;Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], the Decision Tree presented classifies (A,B) as 0.;T
2595;Employee_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.;F
2596;Employee_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;F
2597;Employee_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.;T
2598;Employee_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
2599;Employee_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
2600;Employee_overfitting_knn.png;KNN is in overfitting for k less than 13.;F
2601;Employee_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;F
2602;Employee_overfitting_knn.png;KNN with more than 15 neighbours is in overfitting.;F
2603;Employee_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;T
2604;Employee_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.;T
2605;Employee_overfitting_decision_tree.png;The chart reporting the accuracy for different trees shows that the model enters in overfitting for models with depth higher than 9.;T
2606;Employee_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 4.;F
2607;Employee_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.;F
2608;Employee_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
2609;Employee_pca.png;The first 3 principal components are enough for explaining half the data variance.;T
2610;Employee_pca.png;Using the first 3 principal components would imply an error between 15 and 25%.;F
2611;Employee_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
2612;Employee_correlation_heatmap.png;One of the variables PaymentTier or JoiningYear can be discarded without losing information.;F
2613;Employee_correlation_heatmap.png;The variable JoiningYear can be discarded without risking losing information.;F
2614;Employee_correlation_heatmap.png;Variables Age and PaymentTier are redundant, but we can’t say the same for the pair ExperienceInCurrentDomain and JoiningYear.;F
2615;Employee_correlation_heatmap.png;Variables PaymentTier and JoiningYear are redundant.;F
2616;Employee_correlation_heatmap.png;Considering that the target variable is PaymentTier we can say that from the correlation analysis alone, it is clear that there are relevant variables.;F
2617;Employee_correlation_heatmap.png;Considering that the target variable is PaymentTier we can say that variable JoiningYear seems to be relevant for the majority of mining tasks.;F
2618;Employee_correlation_heatmap.png;Considering that the target variable is PaymentTier we can say that variables Age and ExperienceInCurrentDomain seem to be useful for classification tasks.;F
2619;Employee_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;F
2620;Employee_correlation_heatmap.png;Removing variable PaymentTier might improve the training of decision trees .;F
2621;Employee_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable ExperienceInCurrentDomain previously than variable PaymentTier.;F
2622;Employee_boxplots.png;Variable ExperienceInCurrentDomain is balanced.;F
2623;Employee_boxplots.png;Those boxplots show that the data is not normalized.;T
2624;Employee_boxplots.png;It is clear that variable PaymentTier shows some outliers, but we can’t be sure of the same for variable Age.;F
2625;Employee_boxplots.png;Outliers seem to be a problem in the dataset.;T
2626;Employee_boxplots.png;Variable JoiningYear shows a high number of outlier values.;F
2627;Employee_boxplots.png;Variable JoiningYear doesn’t have any outliers.;T
2628;Employee_boxplots.png;Variable PaymentTier presents some outliers.;T
2629;Employee_boxplots.png;At least 60% of the variables present outliers.;T
2630;Employee_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
2631;Employee_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
2632;Employee_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
2633;Employee_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;F
2634;Employee_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
2635;Employee_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
2636;Employee_histograms_symbolic.png;All variables should be dealt with as date.;F
2637;Employee_histograms_symbolic.png;The variable Gender can be seen as ordinal.;T
2638;Employee_histograms_symbolic.png;The variable EverBenched can be seen as ordinal without losing information.;T
2639;Employee_histograms_symbolic.png;Considering the common semantics for Education and City variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
2640;Employee_histograms_symbolic.png;Considering the common semantics for City variable, dummification would be the most adequate encoding.;T
2641;Employee_histograms_symbolic.png;The variable City can be coded as ordinal without losing information.;F
2642;Employee_histograms_symbolic.png;Feature generation based on variable City seems to be promising.;F
2643;Employee_histograms_symbolic.png;Feature generation based on the use of variable EverBenched wouldn’t be useful, but the use of Education seems to be promising.;F
2644;Employee_histograms_symbolic.png;Given the usual semantics of Gender variable, dummification would have been a better codification.;F
2645;Employee_histograms_symbolic.png;Not knowing the semantics of Education variable, dummification could have been a more adequate codification.;T
2646;Employee_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
2647;Employee_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
2648;Employee_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
2649;Employee_histograms_numeric.png;All variables should be dealt with as date.;F
2650;Employee_histograms_numeric.png;The variable PaymentTier can be seen as ordinal.;T
2651;Employee_histograms_numeric.png;The variable Age can be seen as ordinal without losing information.;F
2652;Employee_histograms_numeric.png;Variable Age is balanced.;F
2653;Employee_histograms_numeric.png;It is clear that variable PaymentTier shows some outliers, but we can’t be sure of the same for variable Age.;F
2654;Employee_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
2655;Employee_histograms_numeric.png;Variable JoiningYear shows some outlier values.;F
2656;Employee_histograms_numeric.png;Variable ExperienceInCurrentDomain doesn’t have any outliers.;T
2657;Employee_histograms_numeric.png;Variable PaymentTier presents some outliers.;F
2658;Employee_histograms_numeric.png;At least 50% of the variables present outliers.;F
2659;Employee_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
2660;Employee_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
2661;Employee_histograms_numeric.png;Considering the common semantics for JoiningYear and PaymentTier variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
2662;Employee_histograms_numeric.png;Considering the common semantics for PaymentTier variable, dummification would be the most adequate encoding.;F
2663;Employee_histograms_numeric.png;The variable PaymentTier can be coded as ordinal without losing information.;T
2664;Employee_histograms_numeric.png;Feature generation based on variable PaymentTier seems to be promising.;F
2665;Employee_histograms_numeric.png;Feature generation based on the use of variable ExperienceInCurrentDomain wouldn’t be useful, but the use of JoiningYear seems to be promising.;F
2666;Employee_histograms_numeric.png;Given the usual semantics of PaymentTier variable, dummification would have been a better codification.;T
2667;Employee_histograms_numeric.png;Not knowing the semantics of Age variable, dummification could have been a more adequate codification.;F
