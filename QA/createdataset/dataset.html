 
<html> 
<head></head> 
<body> 
    <img src="../images/smoking_drinking_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>0: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="../images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="../images/smoking_drinking_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="../images/smoking_drinking_pca.png" width="auto" height = "600"/> 
    <p>3: The first 10 principal components are enough to explain half the data variance.</p>
    <img src="../images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>4: Removing variable hemoglobin would not improve the training of the decision tree algorithm .</p>
    <img src="../images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>5: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="../images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>6: Considering the common semantics for sex variable, dummification would be the most adequate encoding.</p>
    <img src="../images/smoking_drinking_class_histogram.png" width="auto" height = "600"/> 
    <p>7: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="../images/smoking_drinking_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>8: As we are facing the curse of dimensionality, dummification could be a better option than using binary variables.</p>
    <img src="../images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>9: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="../images/BankNoteAuthentication_pca.png" width="auto" height = "600"/> 
    <p>10: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="../images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>11: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="../images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>12: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="../images/BankNoteAuthentication_class_histogram.png" width="auto" height = "600"/> 
    <p>13: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="../images/BankNoteAuthentication_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>14: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="../images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>15: Normalization of this dataset could not help a KNN algorithm to outperform a Naive Bayes.</p>
    <img src="../images/Iris_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>16: As reported in the chart, the MLP enters into overfitting after 500 iterations.</p>
    <img src="../images/Iris_overfitting_rf.png" width="auto" height = "600"/> 
    <p>17: The random forests results shown can be explained by the fact that the models become more complex with the number of estimators.</p>
    <img src="../images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>18: KNN is in overfitting for k less than 5.</p>
    <img src="../images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>19: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="../images/Iris_pca.png" width="auto" height = "600"/> 
    <p>20: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="../images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>21: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="../images/Iris_class_histogram.png" width="auto" height = "600"/> 
    <p>22: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="../images/Iris_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>23: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="../images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>24: It is clear that variable SepalLengthCm shows some outliers, but we canâ€™t be sure of the same for variable PetalWidthCm.</p>
    <img src="../images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>25: Pruning can only improve the decision tree presented if it is based on post-pruning.</p>
    <img src="../images/phone_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>26: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="../images/phone_overfitting_gb.png" width="auto" height = "600"/> 
    <p>27: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="../images/phone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>28: Results for Random Forests identified as 20, may be explained by its estimators being in overfitting.</p>
    <img src="../images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>29: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="../images/phone_pca.png" width="auto" height = "600"/> 
    <p>30: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="../images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>31: The intrinsic dimensionality of this dataset is 11.</p>
    <img src="../images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>32: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="../images/phone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>33: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="../images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>34: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="../images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>35: Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="../images/Titanic_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>36: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="../images/Titanic_overfitting_gb.png" width="auto" height = "600"/> 
    <p>37: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="../images/Titanic_overfitting_rf.png" width="auto" height = "600"/> 
    <p>38: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="../images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>39: KNN with 7 neighbour is in overfitting.</p>
    <img src="../images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>40: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="../images/Titanic_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>41: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="../images/Titanic_pca.png" width="auto" height = "600"/> 
    <p>42: Using the first 4 principal components would imply an error between 5 and 20%.</p>
    <img src="../images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>43: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="../images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>44: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="../images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>45: All variables, but the class, should be dealt with as numeric.</p>
    <img src="../images/Titanic_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>46: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="../images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>47: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="../images/apple_quality_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>48: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="../images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>49: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="../images/apple_quality_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>50: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="../images/apple_quality_pca.png" width="auto" height = "600"/> 
    <p>51: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="../images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>52: The variable Juiciness can be discarded without risking losing information.</p>
    <img src="../images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>53: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="../images/apple_quality_class_histogram.png" width="auto" height = "600"/> 
    <p>54: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="../images/apple_quality_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>55: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="../images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>56: The histograms presented show a large number of outliers for most of the numeric variables.</p>
</body> 
</html> 
