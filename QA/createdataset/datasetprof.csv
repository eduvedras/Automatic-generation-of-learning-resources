Id;Chart;Question;Answer
0;ObesityDataSet_decision_tree.png;The variable FAF discriminates between the target values, as shown in the decision tree.;T
1;ObesityDataSet_decision_tree.png;Variable Height is one of the most relevant variables.;T
2;ObesityDataSet_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
7;ObesityDataSet_decision_tree.png;The variable FAF seems to be one of the two most relevant features.;T
8;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that Naive Bayes algorithm classifies (not A, B), as Overweight_Level_I.;F
9;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], the Decision Tree presented classifies (A, not B) as Obesity_Type_III.;F
10;ObesityDataSet_decision_tree.png;Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that KNN algorithm classifies (A, not B) as Insufficient_Weight for any k ≤ 160.;F
11;ObesityDataSet_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
12;ObesityDataSet_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.;F
13;ObesityDataSet_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.;T
14;ObesityDataSet_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
15;ObesityDataSet_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
16;ObesityDataSet_overfitting_knn.png;KNN is in overfitting for k larger than 17.;F
17;ObesityDataSet_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;F
18;ObesityDataSet_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
19;ObesityDataSet_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.;F
20;ObesityDataSet_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.;F
22;ObesityDataSet_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.;F
23;ObesityDataSet_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.;F
24;ObesityDataSet_pca.png;The first 7 principal components are enough for explaining half the data variance.;T
25;ObesityDataSet_pca.png;Using the first 7 principal components would imply an error between 15 and 20%.;F
26;ObesityDataSet_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 3.;F
27;ObesityDataSet_correlation_heatmap.png;One of the variables Age or Height can be discarded without losing information.;F
28;ObesityDataSet_correlation_heatmap.png;The variable Weight can be discarded without risking losing information.;F
29;ObesityDataSet_correlation_heatmap.png;Variables NCP and TUE are redundant, but we can’t say the same for the pair Weight and Height.;F
30;ObesityDataSet_correlation_heatmap.png;Variables FAF and TUE are redundant.;F
31;ObesityDataSet_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.;F
32;ObesityDataSet_correlation_heatmap.png;Variable Height seems to be relevant for the majority of mining tasks.;F
33;ObesityDataSet_correlation_heatmap.png;Variables FAF and Height seem to be useful for classification tasks.;F
34;ObesityDataSet_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
35;ObesityDataSet_correlation_heatmap.png;Removing variable CH2O might improve the training of decision trees .;F
36;ObesityDataSet_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable Height.;F
37;ObesityDataSet_boxplots.png;Variable CH2O is balanced.;T
38;ObesityDataSet_boxplots.png;Those boxplots show that the data is not normalized.;T
39;ObesityDataSet_boxplots.png;It is clear that variable FCVC shows some outliers, but we can’t be sure of the same for variable TUE.;F
40;ObesityDataSet_boxplots.png;Outliers seem to be a problem in the dataset.;F
41;ObesityDataSet_boxplots.png;Variable FAF shows some outlier values.;F
42;ObesityDataSet_boxplots.png;Variable NCP doesn’t have any outliers.;T
43;ObesityDataSet_boxplots.png;Variable Height presents some outliers.;T
44;ObesityDataSet_boxplots.png;At least 75% of the variables present outliers.;F
45;ObesityDataSet_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
46;ObesityDataSet_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;F
47;ObesityDataSet_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
48;ObesityDataSet_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;T
49;ObesityDataSet_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;T
50;ObesityDataSet_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
51;ObesityDataSet_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.;F
52;ObesityDataSet_histograms_symbolic.png;The variable SMOKE can be seen as ordinal.;T
53;ObesityDataSet_histograms_symbolic.png;The variable FAVC can be seen as ordinal without losing information.;T
54;ObesityDataSet_histograms_symbolic.png;Considering the common semantics for FAVC and CAEC variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
55;ObesityDataSet_histograms_symbolic.png;Considering the common semantics for family_history_with_overweight variable, dummification would be the most adequate encoding.;F
56;ObesityDataSet_histograms_symbolic.png;The variable MTRANS can be coded as ordinal without losing information.;F
57;ObesityDataSet_histograms_symbolic.png;Feature generation based on variable family_history_with_overweight seems to be promising.;F
58;ObesityDataSet_histograms_symbolic.png;Feature generation based on the use of variable SCC wouldn’t be useful, but the use of CAEC seems to be promising.;F
59;ObesityDataSet_histograms_symbolic.png;Given the usual semantics of family_history_with_overweight variable, dummification would have been a better codification.;F
61;ObesityDataSet_histograms_symbolic.png;Not knowing the semantics of family_history_with_overweight variable, dummification could have been a more adequate codification.;F
62;ObesityDataSet_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
63;ObesityDataSet_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
64;ObesityDataSet_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
66;ObesityDataSet_histograms_numeric.png;All variables, but the class, should be dealt with as symbolic.;F
67;ObesityDataSet_histograms_numeric.png;The variable Height can be seen as ordinal.;F
68;ObesityDataSet_histograms_numeric.png;The variable NCP can be seen as ordinal without losing information.;F
69;ObesityDataSet_histograms_numeric.png;Variable FAF is balanced.;F
70;ObesityDataSet_histograms_numeric.png;It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable CH2O.;F
71;ObesityDataSet_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
72;ObesityDataSet_histograms_numeric.png;Variable Height shows a high number of outlier values.;F
73;ObesityDataSet_histograms_numeric.png;Variable TUE doesn’t have any outliers.;T
74;ObesityDataSet_histograms_numeric.png;Variable FCVC presents some outliers.;F
75;ObesityDataSet_histograms_numeric.png;At least 60% of the variables present outliers.;F
76;ObesityDataSet_histograms_numeric.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
77;ObesityDataSet_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
78;ObesityDataSet_histograms_numeric.png;Considering the common semantics for Weight and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
79;ObesityDataSet_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.;F
80;ObesityDataSet_histograms_numeric.png;The variable Weight can be coded as ordinal without losing information.;F
81;ObesityDataSet_histograms_numeric.png;Feature generation based on variable TUE seems to be promising.;F
82;ObesityDataSet_histograms_numeric.png;Feature generation based on the use of variable Weight wouldn’t be useful, but the use of Age seems to be promising.;F
85;ObesityDataSet_histograms_numeric.png;Not knowing the semantics of CH2O variable, dummification could have been a more adequate codification.;F
86;customer_segmentation_decision_tree.png;The variable Family_Size discriminates between the target values, as shown in the decision tree.;F
87;customer_segmentation_decision_tree.png;Variable Work_Experience is one of the most relevant variables.;T
88;customer_segmentation_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;F
94;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A,B) as B for any k ≤ 11.;F
95;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A, not B) as C for any k ≤ 723.;F
96;customer_segmentation_decision_tree.png;Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (not A, B) as B for any k ≤ 524.;F
97;customer_segmentation_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.;F
98;customer_segmentation_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.;T
99;customer_segmentation_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;F
100;customer_segmentation_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;T
101;customer_segmentation_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.;F
102;customer_segmentation_overfitting_knn.png;KNN is in overfitting for k larger than 13.;F
103;customer_segmentation_overfitting_knn.png;KNN with 11 neighbour is in overfitting.;T
104;customer_segmentation_overfitting_knn.png;KNN with more than 17 neighbours is in overfitting.;F
105;customer_segmentation_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.;T
106;customer_segmentation_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;T
108;customer_segmentation_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 5.;T
109;customer_segmentation_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;T
110;customer_segmentation_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
111;customer_segmentation_pca.png;Using the first 2 principal components would imply an error between 10 and 20%.;F
112;customer_segmentation_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
113;customer_segmentation_correlation_heatmap.png;One of the variables Age or Family_Size can be discarded without losing information.;F
114;customer_segmentation_correlation_heatmap.png;The variable Age can be discarded without risking losing information.;F
115;customer_segmentation_correlation_heatmap.png;Variables Age and Work_Experience seem to be useful for classification tasks.;F
116;customer_segmentation_correlation_heatmap.png;Variables Age and Work_Experience are redundant.;F
117;customer_segmentation_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.;F
118;customer_segmentation_correlation_heatmap.png;Variable Family_Size seems to be relevant for the majority of mining tasks.;F
119;customer_segmentation_correlation_heatmap.png;Variables Family_Size and Work_Experience seem to be useful for classification tasks.;F
120;customer_segmentation_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
121;customer_segmentation_correlation_heatmap.png;Removing variable Work_Experience might improve the training of decision trees .;F
122;customer_segmentation_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable Family_Size.;F
123;customer_segmentation_boxplots.png;Variable Family_Size is balanced.;F
124;customer_segmentation_boxplots.png;Those boxplots show that the data is not normalized.;T
125;customer_segmentation_boxplots.png;It is clear that variable Work_Experience shows some outliers, but we can’t be sure of the same for variable Family_Size.;F
126;customer_segmentation_boxplots.png;Outliers seem to be a problem in the dataset.;F
127;customer_segmentation_boxplots.png;Variable Work_Experience shows a high number of outlier values.;F
128;customer_segmentation_boxplots.png;Variable Work_Experience doesn’t have any outliers.;T
129;customer_segmentation_boxplots.png;Variable Age presents some outliers.;F
130;customer_segmentation_boxplots.png;At least 50% of the variables present outliers.;F
131;customer_segmentation_boxplots.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
132;customer_segmentation_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;F
133;customer_segmentation_boxplots.png;A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.;T
134;customer_segmentation_boxplots.png;Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;T
135;customer_segmentation_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;T
136;customer_segmentation_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
137;customer_segmentation_histograms_symbolic.png;All variables, but the class, should be dealt with as numeric.;F
138;customer_segmentation_histograms_symbolic.png;The variable Gender can be seen as ordinal.;T
139;customer_segmentation_histograms_symbolic.png;The variable Ever_Married can be seen as ordinal without losing information.;T
140;customer_segmentation_histograms_symbolic.png;Considering the common semantics for Var_1 and Profession variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
141;customer_segmentation_histograms_symbolic.png;Considering the common semantics for Profession variable, dummification would be the most adequate encoding.;F
142;customer_segmentation_histograms_symbolic.png;The variable Graduated can be coded as ordinal without losing information.;T
143;customer_segmentation_histograms_symbolic.png;Feature generation based on variable Gender seems to be promising.;F
145;customer_segmentation_histograms_symbolic.png;Given the usual semantics of Profession variable, dummification would have been a better codification.;F
147;customer_segmentation_histograms_symbolic.png;Not knowing the semantics of Spending_Score variable, dummification could have been a more adequate codification.;T
149;customer_segmentation_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.;F
151;customer_segmentation_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;T
152;customer_segmentation_mv.png;Feature generation based on variable Var_1 seems to be promising.;F
153;customer_segmentation_mv.png;It is better to drop the variable Family_Size than removing all records with missing values.;F
154;customer_segmentation_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
155;customer_segmentation_nr_records_nr_variables.png;Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.;F
156;customer_segmentation_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
158;customer_segmentation_histograms_numeric.png;All variables, but the class, should be dealt with as symbolic.;F
159;customer_segmentation_histograms_numeric.png;The variable Family_Size can be seen as ordinal.;T
160;customer_segmentation_histograms_numeric.png;The variable Age can be seen as ordinal without losing information.;F
161;customer_segmentation_histograms_numeric.png;Variable Family_Size is balanced.;F
162;customer_segmentation_histograms_numeric.png;It is clear that variable Work_Experience shows some outliers, but we can’t be sure of the same for variable Age.;F
163;customer_segmentation_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
164;customer_segmentation_histograms_numeric.png;Variable Age shows some outlier values.;F
165;customer_segmentation_histograms_numeric.png;Variable Family_Size doesn’t have any outliers.;T
166;customer_segmentation_histograms_numeric.png;Variable Work_Experience presents some outliers.;F
167;customer_segmentation_histograms_numeric.png;At least 75% of the variables present outliers.;F
168;customer_segmentation_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;F
169;customer_segmentation_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
170;customer_segmentation_histograms_numeric.png;Considering the common semantics for Family_Size and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
171;customer_segmentation_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.;F
172;customer_segmentation_histograms_numeric.png;The variable Age can be coded as ordinal without losing information.;F
173;customer_segmentation_histograms_numeric.png;Feature generation based on variable Work_Experience seems to be promising.;T
174;customer_segmentation_histograms_numeric.png;Feature generation based on the use of variable Work_Experience wouldn’t be useful, but the use of Age seems to be promising.;F
175;customer_segmentation_histograms_numeric.png;Given the usual semantics of Age variable, dummification would have been a better codification.;F
177;customer_segmentation_histograms_numeric.png;Not knowing the semantics of Family_Size variable, dummification could have been a more adequate codification.;F
178;urinalysis_tests_decision_tree.png;The variable Age discriminates between the target values, as shown in the decision tree.;F
179;urinalysis_tests_decision_tree.png;Variable Age is one of the most relevant variables.;T
180;urinalysis_tests_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T
181;urinalysis_tests_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.;F
182;urinalysis_tests_decision_tree.png;The specificity for the presented tree is higher than 60%.;T
183;urinalysis_tests_decision_tree.png;The number of True Positives reported in the same tree is 10.;F
184;urinalysis_tests_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree.;F
185;urinalysis_tests_decision_tree.png;The recall for the presented tree is lower than its specificity.;T
186;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as NEGATIVE.;T
187;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as POSITIVE.;F
188;urinalysis_tests_decision_tree.png;Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], it is possible to state that KNN algorithm classifies (not A, B) as NEGATIVE for any k ≤ 763.;F
189;urinalysis_tests_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
190;urinalysis_tests_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;T
191;urinalysis_tests_overfitting_rf.png;Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.;F
192;urinalysis_tests_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
193;urinalysis_tests_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.;F
194;urinalysis_tests_overfitting_knn.png;KNN is in overfitting for k larger than 5.;F
195;urinalysis_tests_overfitting_knn.png;KNN with 7 neighbour is in overfitting.;F
196;urinalysis_tests_overfitting_knn.png;KNN with less than 17 neighbours is in overfitting.;F
197;urinalysis_tests_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.;T
198;urinalysis_tests_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.;T
200;urinalysis_tests_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 8.;T
201;urinalysis_tests_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;T
202;urinalysis_tests_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
203;urinalysis_tests_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
204;urinalysis_tests_pca.png;Using the first 2 principal components would imply an error between 5 and 20%.;F
205;urinalysis_tests_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 2.;F
206;urinalysis_tests_correlation_heatmap.png;One of the variables pH or Age can be discarded without losing information.;F
207;urinalysis_tests_correlation_heatmap.png;The variable Age can be discarded without risking losing information.;F
208;urinalysis_tests_correlation_heatmap.png;Variables Specific Gravity and Age seem to be useful for classification tasks.;F
209;urinalysis_tests_correlation_heatmap.png;Variables Age and pH are redundant.;F
210;urinalysis_tests_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.;F
211;urinalysis_tests_correlation_heatmap.png;Variable Specific Gravity seems to be relevant for the majority of mining tasks.;F
212;urinalysis_tests_correlation_heatmap.png;Variables Specific Gravity and pH seem to be useful for classification tasks.;F
213;urinalysis_tests_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
214;urinalysis_tests_correlation_heatmap.png;Removing variable pH might improve the training of decision trees .;F
215;urinalysis_tests_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Age previously than variable pH.;F
216;urinalysis_tests_boxplots.png;Variable pH is balanced.;F
217;urinalysis_tests_boxplots.png;Those boxplots show that the data is not normalized.;T
218;urinalysis_tests_boxplots.png;It is clear that variable Specific Gravity shows some outliers, but we can’t be sure of the same for variable Age.;F
219;urinalysis_tests_boxplots.png;Outliers seem to be a problem in the dataset.;T
220;urinalysis_tests_boxplots.png;Variable Specific Gravity shows a high number of outlier values.;F
221;urinalysis_tests_boxplots.png;Variable Age doesn’t have any outliers.;F
222;urinalysis_tests_boxplots.png;Variable Age presents some outliers.;T
223;urinalysis_tests_boxplots.png;At least 60 of the variables present outliers.;T
224;urinalysis_tests_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
225;urinalysis_tests_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;T
226;urinalysis_tests_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
228;urinalysis_tests_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
229;urinalysis_tests_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
230;urinalysis_tests_histograms_symbolic.png;All variables, but the class, should be dealt with as symbolic.;T
231;urinalysis_tests_histograms_symbolic.png;The variable Gender can be seen as ordinal.;T
232;urinalysis_tests_histograms_symbolic.png;The variable Mucous Threads can be seen as ordinal without losing information.;T
233;urinalysis_tests_histograms_symbolic.png;Considering the common semantics for Epithelial Cells and Color variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
234;urinalysis_tests_histograms_symbolic.png;Considering the common semantics for Amorphous Urates variable, dummification would be the most adequate encoding.;F
235;urinalysis_tests_histograms_symbolic.png;The variable Color can be coded as ordinal without losing information.;T
236;urinalysis_tests_histograms_symbolic.png;Feature generation based on variable Amorphous Urates seems to be promising.;F
237;urinalysis_tests_histograms_symbolic.png;Feature generation based on the use of variable Protein wouldn’t be useful, but the use of Color seems to be promising.;F
238;urinalysis_tests_histograms_symbolic.png;Given the usual semantics of Bacteria variable, dummification would have been a better codification.;F
240;urinalysis_tests_histograms_symbolic.png;Not knowing the semantics of Epithelial Cells variable, dummification could have been a more adequate codification.;F
241;urinalysis_tests_mv.png;Discarding variable Color would be better than discarding all the records with missing values for that variable.;F
242;urinalysis_tests_mv.png;Dropping all records with missing values would be better than to drop the variables with missing values.;T
244;urinalysis_tests_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;F
245;urinalysis_tests_mv.png;Feature generation based on variable Color seems to be promising.;F
246;urinalysis_tests_mv.png;It is better to drop the variable Color than removing all records with missing values.;F
247;urinalysis_tests_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T
248;urinalysis_tests_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F
249;urinalysis_tests_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
251;urinalysis_tests_histograms_numeric.png;All variables, but the class, should be dealt with as binary.;F
252;urinalysis_tests_histograms_numeric.png;The variable Specific Gravity can be seen as ordinal.;T
253;urinalysis_tests_histograms_numeric.png;The variable Specific Gravity can be seen as ordinal without losing information.;T
254;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity is balanced.;F
255;urinalysis_tests_histograms_numeric.png;It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable pH.;F
256;urinalysis_tests_histograms_numeric.png;Outliers seem to be a problem in the dataset.;F
257;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity shows a high number of outlier values.;F
258;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity doesn’t have any outliers.;F
259;urinalysis_tests_histograms_numeric.png;Variable Specific Gravity presents some outliers.;F
260;urinalysis_tests_histograms_numeric.png;At least 50% of the variables present outliers.;F
262;urinalysis_tests_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;F
263;urinalysis_tests_histograms_numeric.png;Considering the common semantics for Age and pH variables, dummification if applied would increase the risk of facing the curse of dimensionality.;T
264;urinalysis_tests_histograms_numeric.png;Considering the common semantics for Age variable, dummification would be the most adequate encoding.;F
265;urinalysis_tests_histograms_numeric.png;The variable pH can be coded as ordinal without losing information.;T
266;urinalysis_tests_histograms_numeric.png;Feature generation based on variable Age seems to be promising.;T
267;urinalysis_tests_histograms_numeric.png;Feature generation based on the use of variable Age wouldn’t be useful, but the use of pH seems to be promising.;F
268;urinalysis_tests_histograms_numeric.png;Given the usual semantics of Specific Gravity variable, dummification would have been a better codification.;F
270;urinalysis_tests_histograms_numeric.png;Not knowing the semantics of Age variable, dummification could have been a more adequate codification.;F
271;detect_dataset_decision_tree.png;The variable Ic discriminates between the target values, as shown in the decision tree.;T
272;detect_dataset_decision_tree.png;Variable Vb is one of the most relevant variables.;T
273;detect_dataset_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T
274;detect_dataset_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives.;T
275;detect_dataset_decision_tree.png;The precision for the presented tree is higher than 75%.;T
276;detect_dataset_decision_tree.png;The number of False Negatives is lower than the number of True Negatives for the presented tree.;T
277;detect_dataset_decision_tree.png;The number of True Positives is lower than the number of False Positives for the presented tree.;F
278;detect_dataset_decision_tree.png;The number of False Negatives reported in the same tree is 50.;F
279;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 3.;F
280;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], the Decision Tree presented classifies (A, not B) as 0.;T
281;detect_dataset_decision_tree.png;Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], the Decision Tree presented classifies (A,B) as 0.;F
282;detect_dataset_overfitting_mlp.png;We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.;F
283;detect_dataset_overfitting_gb.png;We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.;F
284;detect_dataset_overfitting_rf.png;Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.;F
285;detect_dataset_overfitting_rf.png;The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;F
286;detect_dataset_overfitting_rf.png;We are able to identify the existence of overfitting for random forest models with more than 502 estimators.;F
287;detect_dataset_overfitting_knn.png;KNN is in overfitting for k less than 17.;F
288;detect_dataset_overfitting_knn.png;KNN with 5 neighbour is in overfitting.;F
289;detect_dataset_overfitting_knn.png;KNN with less than 17 neighbours is in overfitting.;F
290;detect_dataset_overfitting_knn.png;We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.;F
291;detect_dataset_overfitting_decision_tree.png;According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.;F
293;detect_dataset_overfitting_decision_tree.png;The decision tree is in overfitting for depths above 9.;F
294;detect_dataset_overfitting_decision_tree.png;We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.;F
295;detect_dataset_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F
296;detect_dataset_pca.png;The first 2 principal components are enough for explaining half the data variance.;T
297;detect_dataset_pca.png;Using the first 3 principal components would imply an error between 10 and 20%.;F
298;detect_dataset_correlation_heatmap.png;The intrinsic dimensionality of this dataset is 5.;F
299;detect_dataset_correlation_heatmap.png;One of the variables Vc or Va can be discarded without losing information.;F
300;detect_dataset_correlation_heatmap.png;The variable Ic can be discarded without risking losing information.;F
301;detect_dataset_correlation_heatmap.png;Variables Ia and Ic are redundant, but we can’t say the same for the pair Vc and Vb.;F
302;detect_dataset_correlation_heatmap.png;Variables Ib and Vc are redundant.;F
303;detect_dataset_correlation_heatmap.png;From the correlation analysis alone, it is clear that there are relevant variables.;F
304;detect_dataset_correlation_heatmap.png;Variable Vb seems to be relevant for the majority of mining tasks.;F
305;detect_dataset_correlation_heatmap.png;Variables Ib and Ic seem to be useful for classification tasks.;F
306;detect_dataset_correlation_heatmap.png;Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;T
307;detect_dataset_correlation_heatmap.png;Removing variable Ic might improve the training of decision trees .;F
308;detect_dataset_correlation_heatmap.png;There is evidence in favour for sequential backward selection to select variable Ic previously than variable Va.;F
309;detect_dataset_boxplots.png;Variable Vb is balanced.;T
310;detect_dataset_boxplots.png;Those boxplots show that the data is not normalized.;T
311;detect_dataset_boxplots.png;It is clear that variable Vb shows some outliers, but we can’t be sure of the same for variable Va.;F
312;detect_dataset_boxplots.png;Outliers seem to be a problem in the dataset.;F
313;detect_dataset_boxplots.png;Variable Vb shows some outlier values.;F
314;detect_dataset_boxplots.png;Variable Vb doesn’t have any outliers.;T
315;detect_dataset_boxplots.png;Variable Ia presents some outliers.;F
316;detect_dataset_boxplots.png;At least 75 of the variables present outliers.;F
317;detect_dataset_boxplots.png;The boxplots presented show a large number of outliers for most of the numeric variables.;F
318;detect_dataset_boxplots.png;The existence of outliers is one of the problems to tackle in this dataset.;F
319;detect_dataset_boxplots.png;A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.;F
321;detect_dataset_boxplots.png;Normalization of this dataset could not have impact on a KNN classifier.;F
322;detect_dataset_boxplots.png;Scaling this dataset would be mandatory to improve the results with distance-based methods.;T
323;detect_dataset_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F
324;detect_dataset_nr_records_nr_variables.png;Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.;F
325;detect_dataset_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F
327;detect_dataset_histograms_numeric.png;All variables, but the class, should be dealt with as date.;F
328;detect_dataset_histograms_numeric.png;The variable Ic can be seen as ordinal.;F
329;detect_dataset_histograms_numeric.png;The variable Vc can be seen as ordinal without losing information.;F
330;detect_dataset_histograms_numeric.png;Variable Ia is balanced.;T
331;detect_dataset_histograms_numeric.png;It is clear that variable Va shows some outliers, but we can’t be sure of the same for variable Vc.;F
332;detect_dataset_histograms_numeric.png;Outliers seem to be a problem in the dataset.;T
333;detect_dataset_histograms_numeric.png;Variable Ia shows a high number of outlier values.;T
334;detect_dataset_histograms_numeric.png;Variable Ic doesn’t have any outliers.;F
335;detect_dataset_histograms_numeric.png;Variable Ic presents some outliers.;T
336;detect_dataset_histograms_numeric.png;At least 60% of the variables present outliers.;F
337;detect_dataset_histograms_numeric.png;The histograms presented show a large number of outliers for most of the numeric variables.;T
338;detect_dataset_histograms_numeric.png;The existence of outliers is one of the problems to tackle in this dataset.;T
339;detect_dataset_histograms_numeric.png;Considering the common semantics for Ia and Ib variables, dummification if applied would increase the risk of facing the curse of dimensionality.;F
340;detect_dataset_histograms_numeric.png;Considering the common semantics for Vc variable, dummification would be the most adequate encoding.;F
341;detect_dataset_histograms_numeric.png;The variable Vb can be coded as ordinal without losing information.;F
342;detect_dataset_histograms_numeric.png;Feature generation based on variable Vb seems to be promising.;F
343;detect_dataset_histograms_numeric.png;Feature generation based on the use of variable Ic wouldn’t be useful, but the use of Ia seems to be promising.;F
344;detect_dataset_histograms_numeric.png;Given the usual semantics of Ib variable, dummification would have been a better codification.;F
346;detect_dataset_histograms_numeric.png;Not knowing the semantics of Ia variable, dummification could have been a more adequate codification.;F
347;diabetes_decision_tree.png;The variable BMI discriminates between the target values, as shown in the decision tree.;T
348;diabetes_decision_tree.png;Variable BMI is one of the most relevant variables.;T
349;diabetes_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;F
350;diabetes_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives.;F
351;diabetes_decision_tree.png;The precision for the presented tree is higher than 60%.;T
352;diabetes_decision_tree.png;The number of True Positives reported in the same tree is 30.;F
353;diabetes_decision_tree.png;The number of False Positives is lower than the number of False Negatives for the presented tree.;T
354;diabetes_decision_tree.png;The accuracy for the presented tree is higher than its specificity.;F
355;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], the Decision Tree presented classifies (not A, not B) as 1.;T
356;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], the Decision Tree presented classifies (not A, not B) as 1.;T
357;diabetes_decision_tree.png;Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 98.;F
