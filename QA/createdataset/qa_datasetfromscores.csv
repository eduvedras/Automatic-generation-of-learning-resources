Id,Chart,Question,Answer
1,smoking_drinking_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.,F
4,smoking_drinking_overfitting_knn.png,KNN with less than 7 neighbours is in overfitting.,T
6,smoking_drinking_overfitting_dt_acc_rec.png,The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.,F
7,smoking_drinking_pca.png,The first 10 principal components are enough to explain half the data variance.,T
8,smoking_drinking_correlation_heatmap.png,Removing variable hemoglobin would not improve the training of the decision tree algorithm .,T
9,smoking_drinking_boxplots.png,"A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.",F
10,smoking_drinking_histograms_symbolic.png,"Considering the common semantics for sex variable, dummification would be the most adequate encoding.",F
11,smoking_drinking_class_histogram.png,Balancing this dataset would be mandatory to improve the results.,F
12,smoking_drinking_nr_records_nr_variables.png,"As we are facing the curse of dimensionality, dummification could be a better option than using binary variables.",F
13,smoking_drinking_histograms_numeric.png,The existence of outliers is one of the problems to tackle in this dataset.,F
21,BankNoteAuthentication_pca.png,The first 2 principal components are enough for explaining half the data variance.,T
22,BankNoteAuthentication_correlation_heatmap.png,"Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.",T
23,BankNoteAuthentication_boxplots.png,"A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.",F
24,BankNoteAuthentication_class_histogram.png,Balancing this dataset would be mandatory to improve the results.,F
25,BankNoteAuthentication_nr_records_nr_variables.png,We face the curse of dimensionality when training a classifier with this dataset.,F
26,BankNoteAuthentication_histograms_numeric.png,Normalization of this dataset could not help a KNN algorithm to outperform a Naive Bayes.,T
28,Iris_overfitting_mlp.png,"As reported in the chart, the MLP enters into overfitting after 500 iterations.",F
30,Iris_overfitting_rf.png,The random forests results shown can be explained by the fact that the models become more complex with the number of estimators.,F
31,Iris_overfitting_knn.png,KNN is in overfitting for k less than 5.,F
32,Iris_overfitting_decision_tree.png,"According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.",F
33,Iris_pca.png,The first 2 principal components are enough for explaining half the data variance.,T
34,Iris_correlation_heatmap.png,The intrinsic dimensionality of this dataset is 3.,T
36,Iris_class_histogram.png,Balancing this dataset would be mandatory to improve the results.,F
37,Iris_nr_records_nr_variables.png,We face the curse of dimensionality when training a classifier with this dataset.,T
38,Iris_histograms_numeric.png,"It is clear that variable SepalLengthCm shows some outliers, but we can’t be sure of the same for variable PetalWidthCm.",F
39,phone_decision_tree.png,Pruning can only improve the decision tree presented if it is based on post-pruning.,F
40,phone_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.,F
41,phone_overfitting_gb.png,We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.,F
42,phone_overfitting_rf.png,"Results for Random Forests identified as 20, may be explained by its estimators being in overfitting.",F
43,phone_overfitting_knn.png,We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.,T
45,phone_pca.png,The first 3 principal components are enough for explaining half the data variance.,T
46,phone_correlation_heatmap.png,The intrinsic dimensionality of this dataset is 11.,F
47,phone_boxplots.png,The existence of outliers is one of the problems to tackle in this dataset.,F
50,phone_nr_records_nr_variables.png,We face the curse of dimensionality when training a classifier with this dataset.,F
51,phone_histograms_numeric.png,The histograms presented show a large number of outliers for most of the numeric variables.,F
52,Titanic_decision_tree.png,"Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], the Decision Tree presented classifies (not A, B) as 1.",F
53,Titanic_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.,F
54,Titanic_overfitting_gb.png,We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.,T
55,Titanic_overfitting_rf.png,We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.,T
56,Titanic_overfitting_knn.png,KNN with 7 neighbour is in overfitting.,T
57,Titanic_overfitting_decision_tree.png,We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.,F
58,Titanic_overfitting_dt_acc_rec.png,The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.,F
59,Titanic_pca.png,Using the first 4 principal components would imply an error between 5 and 20%.,F
60,Titanic_correlation_heatmap.png,The intrinsic dimensionality of this dataset is 2.,F
61,Titanic_boxplots.png,"Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.",F
62,Titanic_histograms_symbolic.png,"All variables, but the class, should be dealt with as numeric.",F
65,Titanic_nr_records_nr_variables.png,We face the curse of dimensionality when training a classifier with this dataset.,F
67,apple_quality_decision_tree.png,The number of True Positives is lower than the number of False Negatives for the presented tree.,T
68,apple_quality_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.,F
71,apple_quality_overfitting_knn.png,We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.,T
73,apple_quality_overfitting_dt_acc_rec.png,The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.,F
74,apple_quality_pca.png,The first 3 principal components are enough for explaining half the data variance.,T
75,apple_quality_correlation_heatmap.png,The variable Juiciness can be discarded without risking losing information.,T
76,apple_quality_boxplots.png,Scaling this dataset would be mandatory to improve the results with distance-based methods.,F
77,apple_quality_class_histogram.png,Balancing this dataset would be mandatory to improve the results.,F
78,apple_quality_nr_records_nr_variables.png,We face the curse of dimensionality when training a classifier with this dataset.,F
79,apple_quality_histograms_numeric.png,The histograms presented show a large number of outliers for most of the numeric variables.,F
80,Employee_decision_tree.png,"Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 471.",T
84,Employee_overfitting_knn.png,KNN with more than 7 neighbours is in overfitting.,F
86,Employee_overfitting_dt_acc_rec.png,We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.,F
87,Employee_pca.png,The first 2 principal components are enough for explaining half the data variance.,T
88,Employee_correlation_heatmap.png,Variable JoiningYear seems to be important for the majority of mining tasks.,F
89,Employee_boxplots.png,The boxplots presented show a large number of outliers for most of the analysed variables.,F
90,Employee_histograms_symbolic.png,"Given the usual semantics of Gender variable, dummification would have been a better codification.",F
91,Employee_class_histogram.png,Balancing this dataset would be mandatory to improve the results.,T
93,Employee_histograms_numeric.png,"A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.",T
95,smoking_drinking_overfitting_mlp.png,"The data chart suggests that MLP models begin to overfit after a certain number of iterations, which is not explicitly indicated in the chart but can be inferred.",F
96,smoking_drinking_overfitting_gb.png,The gradient boosting model's accuracy reaches a plateau and then decreases beyond approximately 1000 estimatorsdue to overfitting.,F
98,smoking_drinking_overfitting_knn.png,"The accuracy of KNN increases with the number of neighbors up to a certain point, beyond which it starts to overfit the data. Specifically, overfitting begins around 13 or more neighbors.",F
99,smoking_drinking_overfitting_decision_tree.png,"According to the decision tree overfitting chart, a tree with a max depth greater than the point where accuracy on the training data starts decreasing is in overfitting.",F
100,smoking_drinking_overfitting_dt_acc_rec.png," The performance of both accuracy and recall increases with the max depth up to a point, but beyond that, the model begins to overfit, resulting in a decrease in accuracy and an unstable recall.",F
101,smoking_drinking_pca.png, The bar chart reveals that the first 12 principal components together explain 100.0% of the data variance.,T
112,BankNoteAuthentication_overfitting_knn.png," KNN exhibits overfitting when the number of neighbors is greater than 5, as indicated by the decreasing accuracy on the chart.",F
113,BankNoteAuthentication_overfitting_decision_tree.png,"According to the multi-line chart, the decision tree exhibits increasing overfitting as the max depth increases, reaching high accuracy levels but eventually leading to overfitting beyond a max depth of approximately 15.",F
114,BankNoteAuthentication_overfitting_dt_acc_rec.png,"The performance of both accuracy and recall increase as the max depth of the decision tree increases, but the extent of the increase in accuracy contrasts the decrease in recall, indicating overfitting.",F
115,BankNoteAuthentication_pca.png,The four principal components collectively explain a significant proportion of the variance in the data.,T
122,Iris_overfitting_mlp.png,The MLP model shows signs of overfitting after approximately 800 iterations.,F
124,Iris_overfitting_rf.png,"The random forest model's accuracy increases as the number of estimators grows, but it eventually reaches a plateau and starts to decrease due to overfitting.",F
125,Iris_overfitting_knn.png, KNN shows overfitting when the number of neighbors is greater than 17.,F
126,Iris_overfitting_decision_tree.png,"According to the given chart, the decision tree exhibits increasing accuracy with max depth up to a certain point, after which accuracy starts decreasing, indicating overfitting.",F
127,Iris_pca.png, The bar chart illustrates the percentage of total data variance explained by each of the four principal components.,T
134,phone_overfitting_mlp.png,The MLP model shows increasing accuracy but also signs of overfitting past 800 iterations.,F
135,phone_overfitting_gb.png,The gradient boosting model displays increasing accuracy but also higher overfitting as the number of estimators exceeds 500.,F
136,phone_overfitting_rf.png,"The accuracy of Random Forests continuously increases as the number of estimators goes up, but eventually it plateaus and then starts decreasing due to overfitting.",F
137,phone_overfitting_knn.png,KNN is not overfitting until the number of neighbors is greater than 15.,F
138,phone_overfitting_decision_tree.png,"According to the chart, the decision tree exhibits increasing accuracy with max depth up to a certain point, beyond which the accuracy starts to decrease, indicating overfitting.",F
139,phone_pca.png,The first 12 principal components collectively explain 100% of the data variance.,T
148,Titanic_overfitting_gb.png,Gradient boosting models display clear signs of overfitting with more than 1000 estimators.,T
149,Titanic_overfitting_rf.png,"The accuracy of Random Forests increases as the number of estimators grows, but at a certain point, the model begins to overfit, resulting in a decrease in accuracy.",F
150,Titanic_overfitting_knn.png,"KNN is overfitting for values of k greater than the optimal number, which is not explicitly stated in the chart but can be identified as the point where the accuracy starts to plateau or decreases slightly.",F
151,Titanic_overfitting_decision_tree.png,"According to the chart, the decision tree shows increasing accuracy with max depth up to a certain point, after which accuracy begins to decrease, indicating overfitting.",T
152,Titanic_overfitting_dt_acc_rec.png,"The performance of both accuracy and recall increases as the max depth of the decision tree increases, but the trend of each metric may differ and the gap between them may narrow down at larger depths due to overfitting.",T
153,Titanic_pca.png,"Based on the given data chart, the six principal components together explain 100.0% of the data variance.",T
162,apple_quality_overfitting_mlp.png,"We can observe that the accuracy of the MLP model continues to increase beyond 500 iterations, indicating overfitting.",F
164,apple_quality_overfitting_rf.png,"The accuracy of Random Forests continues to increase as the number of estimators goes up, but at a decreasing rate, eventually reaching a plateau, indicating the model is Overfitting.",F
165,apple_quality_overfitting_knn.png, KNN shows overfitting for values of neighbors greater than 17.,F
167,apple_quality_overfitting_dt_acc_rec.png," The performance of both accuracy and recall increases with max depth up to a certain point, after which overfitting causes a significant decrease in both metrics.",F
168,apple_quality_pca.png,The seven principal components together explain a significant proportion of the variance in the data.,F
178,Employee_overfitting_knn.png," The accuracy of K-Nearest Neighbors (KNN) increases as the number of neighbors grows, up until a point where it begins to overfit the data, around 13 or more neighbors.",F
179,Employee_overfitting_decision_tree.png," The decision tree chart indicates an increasing accuracy as the max depth increases up to a point, after which accuracy starts to decrease, suggesting overfitting.",T
180,Employee_overfitting_dt_acc_rec.png," The performance of both accuracy and recall shows increasing variability with an increasing max depth, indicating the decision tree is overfitting the data.",F
188,smoking_drinking_decision_tree.png,The number of False Positives is higher than the number of False Negatives for the presented tree.,T
192,smoking_drinking_overfitting_knn.png,KNN with more than 13 neighbors is in overfitting.,F
193,smoking_drinking_overfitting_decision_tree.png,The overfitting in the decision tree chart increases with depth.,T
194,smoking_drinking_overfitting_dt_acc_rec.png,The difference between accuracy and recall increases as the max depth of the decision tree increases due to overfitting.,F
195,smoking_drinking_pca.png,The first two principal components explain more variance than the last two principal components.,T
196,smoking_drinking_correlation_heatmap.png,The variable weight and BLDS are redundant in this dataset.,F
198,smoking_drinking_histograms_symbolic.png,The variable 'sex' can be seen as binary for this data chart description.,T
201,smoking_drinking_histograms_numeric.png,'The variable LDL_chole shows some outlier values.',F
207,BankNoteAuthentication_overfitting_decision_tree.png,The decision tree starts overfitting for depths above 20.,F
210,BankNoteAuthentication_correlation_heatmap.png,"'Variables entropy and curtosis are redundant, but we can’t say the same for the pair variance and skewness.'",F
211,BankNoteAuthentication_boxplots.png,The boxplots suggest that variable curtosis has outliers.,T
214,BankNoteAuthentication_histograms_numeric.png,The variable skewness doesn't have any outliers.,F
216,Iris_overfitting_mlp.png,Overfitting in the MLP model worsens as the number of iterations increases past a certain point.,F
217,Iris_overfitting_gb.png,"The accuracy decreases as the number of estimators in the gradient boosting model increases, indicating overfitting.",F
218,Iris_overfitting_rf.png,"The chart shows that as the number of estimators increases, the accuracy also increases, indicating a typical case of overfitting.",F
219,Iris_overfitting_knn.png,KNN with more than 23 neighbors is in overfitting.,F
220,Iris_overfitting_decision_tree.png,"According to the chart, as the decision tree depth increases, the accuracy also increases.",F
221,Iris_pca.png,The sum of the explained variance ratios of the 4 principal components should be equal to 1.,F
222,Iris_correlation_heatmap.png,'The variable PetalWidthCm can be discarded without risking losing information.',T
223,Iris_boxplots.png,'Those boxplots show that the data is not normalized.',T
226,Iris_histograms_numeric.png,The variable PetalWidthCm can be seen as ordinal.,F
228,phone_overfitting_mlp.png,It is possible to observe overfitting in MLP models after a large number of iterations.,F
229,phone_overfitting_gb.png,"As the number of estimators increases, the accuracy of the model decreases due to overfitting.",F
230,phone_overfitting_rf.png,"The accuracy of the random forest model decreases as the number of estimators increases, indicating potential overfitting.",F
231,phone_overfitting_knn.png,KNN is in overfitting for k less than 13.,T
232,phone_overfitting_decision_tree.png,The chart illustrates that the decision tree begins to overfit at a max depth of 10.,F
233,phone_pca.png,The second principal component explains more data variance than the fourth principal component.,T
235,phone_boxplots.png,Variable px_height shows some outlier values.,F
239,phone_histograms_numeric.png,The histograms presented show a large number of outliers for most of the numeric variables.,F
240,Titanic_decision_tree.png,"The variable Parch discriminates between the target values, as shown in the decision tree.",T
241,Titanic_overfitting_mlp.png,Overfitting occurs in the MLP model as the number of iterations increases beyond a certain point.,F
242,Titanic_overfitting_gb.png,The accuracy of the gradient boosting model decreases as the number of estimators increases from 2 to 2002.,F
244,Titanic_overfitting_knn.png,KNN is in overfitting for k larger than 13.,F
245,Titanic_overfitting_decision_tree.png,"""The chart shows that the decision tree accuracy continues to increase as the max depth increases.""",F
250,Titanic_histograms_symbolic.png,"The variable 'Embarked' is categorical, so it should be encoded using one-hot encoding.",F
251,Titanic_mv.png,Removing all records with missing values for 'Embarked' variable would result in losing more information than removing the variable itself.,F
254,Titanic_histograms_numeric.png,The variable Fare can be seen as ordinal without losing information.,F
256,apple_quality_overfitting_mlp.png,The overfitting on the MLP model is visible on the chart.,F
258,apple_quality_overfitting_rf.png,The overfitting of random forest models increases as the number of estimators increases from 2 to 2002.,F
259,apple_quality_overfitting_knn.png,"In the chart, KNN is in overfitting for k larger than 17.",F
260,apple_quality_overfitting_decision_tree.png,"In the multi-line chart showing the overfitting of a decision tree, the accuracy increases continuously as the max depth of the tree increases.",F
262,apple_quality_pca.png,The total explained variance ratio of the 7 principal components is equal to 1.,F
263,apple_quality_correlation_heatmap.png,"'Variables Juiciness and Crunchiness are redundant, but we can’t say the same for the pair Sweetness and Ripeness.'",F
267,apple_quality_histograms_numeric.png,The variable Size can be seen as ordinal without losing information.,F
268,Employee_decision_tree.png,The number of False Positives is higher than the number of False Negatives for the presented tree.,F
269,Employee_overfitting_mlp.png,Overfitting is observed in the multi-line chart for MLP models as the number of iterations increases.,F
270,Employee_overfitting_gb.png,The accuracy decreases as the number of estimators increases in the overfitting of gradient boosting model.,F
272,Employee_overfitting_knn.png,KNN with 1 neighbor is in overfitting.,F
273,Employee_overfitting_decision_tree.png,The chart illustrates that the decision tree starts overfitting after a depth of 10.,F
274,Employee_overfitting_dt_acc_rec.png,The accuracy increases with the depth while the recall decreases due to overfitting.,F
275,Employee_pca.png,The sum of the explained variance ratios of the 4 principal components is equal to 1.,T
277,Employee_boxplots.png,"Variable Age shows a high number of outlier values, based on the boxplots.",F
281,Employee_histograms_numeric.png,The variable PaymentTier can be seen as ordinal.,T
283,smoking_drinking_overfitting_mlp.png, The model begins to underfit after 800 iterations.,F
284,smoking_drinking_overfitting_gb.png, Gradient boosting with 1003 estimators is not overfitting the data.,T
285,smoking_drinking_overfitting_rf.png, Random forest is not affected by overfitting when using 2 estimators.,T
287,smoking_drinking_overfitting_decision_tree.png, The decision tree performs better when max depth is greater than 15.,F
288,smoking_drinking_overfitting_dt_acc_rec.png, The decision tree with a max depth of 15 shows better recall than the one with a max depth of 13.,F
289,smoking_drinking_pca.png, A component with an explained variance ratio of 0.5 has a greater explanatory power than the first principal component.,F
295,smoking_drinking_histograms_numeric.png, Height and weight have similar distributions.,F
299,BankNoteAuthentication_overfitting_rf.png, Random forest starts overfitting at approximately 250 estimators.,F
300,BankNoteAuthentication_overfitting_knn.png, The chart indicates that overfitting occurs with k-nearest neighbors starting from 10 neighbors.,F
301,BankNoteAuthentication_overfitting_decision_tree.png, The decision tree model reaches its optimal accuracy for max depth equal to 11.,F
302,BankNoteAuthentication_overfitting_dt_acc_rec.png,Max depth of 10 is best for both accuracy and recall.,F
303,BankNoteAuthentication_pca.png, The second principal component explains less variance than the third principal component.,F
310,Iris_overfitting_mlp.png,The MLP model is not overfitting when 300 iterations are reached.,T
312,Iris_overfitting_rf.png, The random forest model with 201 estimators underfits the data.,T
313,Iris_overfitting_knn.png, The accuracy starts to decrease when the number of neighbors is greater than 12.,F
314,Iris_overfitting_decision_tree.png, The decision tree starts to overfit from a depth of 15.,F
315,Iris_pca.png, The first principal component explains more than 50% of the variance.,T
322,phone_overfitting_mlp.png, The model starts to underfit at around 750 iterations.,F
323,phone_overfitting_gb.png, The model is underfitting when 50 estimators are used.,F
326,phone_overfitting_decision_tree.png,The decision tree with max depth 12 shows the best performance.,F
327,phone_pca.png, The explained variance ratios of the first five principal components are higher than those of the last five principal components.,T
341,Titanic_pca.png, The fourth principal component explains 15% of the variance.,F
354,apple_quality_overfitting_decision_tree.png, The decision tree with a max depth of 10 is not overfitting the data.,T
356,apple_quality_pca.png, The first principal component explains more than 80% of the variance.,F
358,apple_quality_boxplots.png,Size has more outliers than other variables.,F
363,Employee_overfitting_mlp.png, The overfitting of the MLP starts after 600 iterations.,F
364,Employee_overfitting_gb.png, The model becomes underfitting after 850 estimators.,F
366,Employee_overfitting_knn.png, The accuracy is higher when k = 17 than when k = 13.,F
367,Employee_overfitting_decision_tree.png, Overfitting starts between a max depth of 15 to 20.,F
368,Employee_overfitting_dt_acc_rec.png, The decision tree model starts overfitting at a max depth of 7.,T
371,Employee_boxplots.png, There's no outlier on 'Age' variable.,T
377,smoking_drinking_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.,F
378,smoking_drinking_overfitting_gb.png,We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.,F
379,smoking_drinking_overfitting_rf.png,The random forests results shown can be explained by the fact that they are in overfitting.,F
383,smoking_drinking_pca.png,The first 4 principal components are enough for explaining half the data variance.,T
385,smoking_drinking_boxplots.png,Scaling this dataset would be mandatory to improve the results with distance-based methods.,T
389,smoking_drinking_histograms_numeric.png,At least 50 of the variables present outliers.,F
390,BankNoteAuthentication_decision_tree.png,"Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to verify that KNN algorithm classifies (not A, B) as 0 for any k ≤ 182.",T
393,BankNoteAuthentication_overfitting_rf.png,"Results for Random Forests identified as 3, can be explained by its estimators being in underfitting.",F
397,BankNoteAuthentication_pca.png,The first 2 principal components are enough for explaining half the data variance.,T
402,BankNoteAuthentication_histograms_numeric.png,The variable skewness doesn’t have any outliers.,F
404,Iris_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.,F
406,Iris_overfitting_rf.png,The random forests results shown can be explained by the absence of balance in the training set.,F
407,Iris_overfitting_knn.png,KNN with more than 17 neighbours is in overfitting.,F
408,Iris_overfitting_decision_tree.png,We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.,F
409,Iris_pca.png,The first 2 principal components are enough for explaining half the data variance.,T
410,Iris_correlation_heatmap.png,The intrinsic dimensionality of this dataset is 2.,F
411,Iris_boxplots.png,The existence of outliers is one of the problems to tackle in this dataset.,T
413,Iris_nr_records_nr_variables.png,"Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.",F
414,Iris_histograms_numeric.png,At least 85 of the variables present outliers.,F
416,phone_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.,F
417,phone_overfitting_gb.png,We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.,F
418,phone_overfitting_rf.png,The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.,F
419,phone_overfitting_knn.png,KNN with more than 15 neighbours is in overfitting.,F
420,phone_overfitting_decision_tree.png,The decision tree is in overfitting for depths above 6.,F
421,phone_pca.png,The first 8 principal components are enough for explaining half the data variance.,T
422,phone_correlation_heatmap.png,The intrinsic dimensionality of this dataset is 4.,F
423,phone_boxplots.png,Scaling this dataset would be mandatory to improve the results with distance-based methods.,T
427,phone_histograms_numeric.png,"It is clear that variable px_width shows some outliers, but we can’t be sure of the same for variable sc_w.",F
428,Titanic_decision_tree.png,"As reported in the tree, the number of False Positive is smaller than the number of False Negatives.",T
429,Titanic_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.,F
430,Titanic_overfitting_gb.png,We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.,T
431,Titanic_overfitting_rf.png,We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.,F
432,Titanic_overfitting_knn.png,KNN is in overfitting for k larger than 17.,F
433,Titanic_overfitting_decision_tree.png,We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.,F
434,Titanic_overfitting_dt_acc_rec.png,We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.,T
435,Titanic_pca.png,The first 2 principal components are enough for explaining half the data variance.,T
436,Titanic_correlation_heatmap.png,Removing variable SibSp might improve the training of decision trees .,F
437,Titanic_boxplots.png,The variable Age doesn’t have any outliers.,F
440,Titanic_class_histogram.png,Balancing this dataset would be mandatory to improve the results.,F
441,Titanic_nr_records_nr_variables.png,We face the curse of dimensionality when training a classifier with this dataset.,F
442,Titanic_histograms_numeric.png,Outliers seem to be a problem in the dataset.,T
444,apple_quality_overfitting_mlp.png,We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.,F
445,apple_quality_overfitting_gb.png,Results for Gradient Boosting up to 1502 estimators can be explained by overfitting.,F
446,apple_quality_overfitting_rf.png,"Results for Random Forest identified as 3, may be explained by its estimators being in underfitting.",T
447,apple_quality_overfitting_knn.png,KNN with 5 neighbour is in overfitting.,T
448,apple_quality_overfitting_decision_tree.png,The decision tree is in overfitting for depths above 8.,T
449,apple_quality_overfitting_dt_acc_rec.png,The difference between recall and accuracy grows with the depth due to the overfitting phenomenon.,F
450,apple_quality_pca.png,Using the first 2 principal components would imply an error between 5 and 20%.,F
451,apple_quality_correlation_heatmap.png,The variable Ripeness can be discarded without risking losing information.,F
452,apple_quality_boxplots.png,The histograms presented show a large number of outliers for most of the numeric variables.,F
454,apple_quality_nr_records_nr_variables.png,"Given the number of records and that some variables are date, we might be facing the curse of dimensionality.",F
455,apple_quality_histograms_numeric.png,"Given the usual semantics of Ripeness variable, dummification would have been a better codification.",F
456,Employee_decision_tree.png,Variable JoiningYear is one of the most relevant variables.,T
458,Employee_overfitting_gb.png,We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.,F
460,Employee_overfitting_knn.png,KNN with less than 9 neighbours is in overfitting.,T
462,Employee_overfitting_dt_acc_rec.png,We are able to identify the existence of overfitting for decision tree models with more than 8 nodes of depth.,T
463,Employee_pca.png,The first 3 principal components are enough for explaining half the data variance.,T
464,Employee_correlation_heatmap.png,It is clear that variable Age and variable JoiningYear are redundant.,F
465,Employee_boxplots.png,The existence of outliers is one of the problems to tackle in this dataset.,F
467,Employee_class_histogram.png,Balancing this dataset would be mandatory to improve the results.,T
468,Employee_nr_records_nr_variables.png,We face the curse of dimensionality when training a classifier with this dataset.,F
469,Employee_histograms_numeric.png,"It is clear that variable ExperienceInCurrentDomain shows some outliers, but we can’t be sure of the same for variable Age.",F
