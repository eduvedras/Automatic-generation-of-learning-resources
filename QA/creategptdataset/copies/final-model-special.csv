Image;Question;Prediction;Reference
phone_decision_tree.png;The variable mobile_wt discriminates between the target values, as shown in the decision tree.;T;T
phone_decision_tree.png;Variable mobile_wt is one of the most relevant variables.;T;T
phone_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.;T;T
phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], it is possible to state that KNN algorithm classifies (A,B) as 2 for any k ≤ 636.;F;F
phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], the Decision Tree presented classifies (not A, B) as 1.;F;F
phone_decision_tree.png;Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], the Decision Tree presented classifies (A, not B) as 0.;T;T
phone_pca.png;The first 8 principal components are enough for explaining half the data variance.;T;T
phone_pca.png;Using the first 11 principal components would imply an error between 10 and 25%.;F;F
phone_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;F;F
phone_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;F;F
phone_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F;F
Titanic_decision_tree.png;The variable Parch discriminates between the target values, as shown in the decision tree.;T;T
Titanic_decision_tree.png;Variable Parch is one of the most relevant variables.;T;T
Titanic_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.;T;T
Titanic_decision_tree.png;As reported in the tree, the number of False Positive is bigger than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;F;F
Titanic_decision_tree.png;The accuracy for the presented tree is lower than 75%.;T;T
Titanic_decision_tree.png;The number of True Negatives is lower than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F;F
Titanic_decision_tree.png;The number of True Negatives is higher than the number of False Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T;T
Titanic_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T;F
Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 181.;F;F
Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 1.;F;F
Titanic_decision_tree.png;Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.;T;F
Titanic_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F;F
Titanic_pca.png;The first 4 principal components are enough for explaining half the data variance.;T;T
Titanic_pca.png;Using the first 2 principal components would imply an error between 15 and 20%.;F;F
Titanic_mv.png;Considering that the dataset has 200 records, discarding variable Embarked would be better than discarding all the records with missing values for that variable.;F;F
Titanic_mv.png;Considering that the dataset has 200 records, dropping all records with missing values would be better than to drop the variables with missing values.;T;F
Titanic_mv.png;Considering that the dataset has 200 records, dropping all rows with missing values can lead to a dataset with less than 25% of the original data.;T;T
Titanic_mv.png;There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;T;T
Titanic_mv.png;Feature generation based on variable Embarked seems to be promising.;F;F
Titanic_mv.png;Considering that the dataset has 200 records, it is better to drop the variable Age than removing all records with missing values.;T;T
Titanic_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T;T
Titanic_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F;F
Titanic_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F;F
apple_quality_decision_tree.png;The variable Crunchiness discriminates between the target values, as shown in the decision tree.;T;T
apple_quality_decision_tree.png;Variable Juiciness is one of the most relevant variables.;T;T
apple_quality_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T;T
apple_quality_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider good as the positive class and bad as the negative class.;T;F
apple_quality_decision_tree.png;The recall for the presented tree is higher than 75%, consider good as the positive class and bad as the negative class.;T;T
apple_quality_decision_tree.png;The number of True Positives is higher than the number of False Negatives for the presented tree, consider good as the positive class and bad as the negative class.;T;T
apple_quality_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree, consider good as the positive class and bad as the negative class.;F;F
apple_quality_decision_tree.png;The specificity for the presented tree is higher than 90%, consider good as the positive class and bad as the negative class.;T;F
apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], the Decision Tree presented classifies (not A, not B) as bad.;F;F
apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 1625.;F;F
apple_quality_decision_tree.png;Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as good.;T;T
apple_quality_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F;F
apple_quality_pca.png;The first 5 principal components are enough for explaining half the data variance.;T;T
apple_quality_pca.png;Using the first 2 principal components would imply an error between 15 and 20%.;F;F
apple_quality_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T;F
apple_quality_nr_records_nr_variables.png;Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.;F;F
apple_quality_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F;F
Employee_decision_tree.png;The variable JoiningYear discriminates between the target values, as shown in the decision tree.;T;T
Employee_decision_tree.png;Variable ExperienceInCurrentDomain is one of the most relevant variables.;T;T
Employee_decision_tree.png;A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.;T;T
Employee_decision_tree.png;As reported in the tree, the number of False Positive is smaller than the number of False Negatives, consider 1 as the positive class and 0 as the negative class.;T;T
Employee_decision_tree.png;The recall for the presented tree is lower than 60%, consider 1 as the positive class and 0 as the negative class.;F;F
Employee_decision_tree.png;The number of False Positives is higher than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F;F
Employee_decision_tree.png;The number of False Positives is higher than the number of True Positives for the presented tree, consider 1 as the positive class and 0 as the negative class.;F;F
Employee_decision_tree.png;The number of True Negatives is higher than the number of False Negatives for the presented tree, consider 1 as the positive class and 0 as the negative class.;T;T
Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 44.;F;T
Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], the Decision Tree presented classifies (not A, B) as 1.;F;T
Employee_decision_tree.png;Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], the Decision Tree presented classifies (A,B) as 0.;F;T
Employee_overfitting_dt_acc_rec.png;The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;F;F
Employee_pca.png;The first 3 principal components are enough for explaining half the data variance.;T;T
Employee_pca.png;Using the first 3 principal components would imply an error between 15 and 25%.;F;F
Employee_class_histogram.png;Balancing this dataset would be mandatory to improve the results.;T;T
Employee_nr_records_nr_variables.png;Given the number of records and that some variables are date, we might be facing the curse of dimensionality.;F;F
Employee_nr_records_nr_variables.png;We face the curse of dimensionality when training a classifier with this dataset.;F;F
