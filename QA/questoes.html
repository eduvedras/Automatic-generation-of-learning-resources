 
<html> 
<head></head> 
<body> 
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>0: The variable FAF discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>1: Variable Height is one of the most relevant variables.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>2: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>3: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>4: The specificity for the presented tree is higher than 75%.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>5: The number of True Negatives reported in the same tree is 50.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>6: The number of False Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>7: The variable FAF seems to be one of the two most relevant features.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>8: Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that Naive Bayes algorithm classifies (not A, B), as Overweight_Level_I.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>9: Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], the Decision Tree presented classifies (A, not B) as Obesity_Type_III.</p>
    <img src="images/ObesityDataSet_decision_tree.png" width="auto" height = "600"/> 
    <p>10: Considering that A=True<=>[FAF <= 2.0] and B=True<=>[Height <= 1.72], it is possible to state that KNN algorithm classifies (A, not B) as Insufficient_Weight for any k ≤ 160.</p>
    <img src="images/ObesityDataSet_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>11: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/ObesityDataSet_overfitting_gb.png" width="auto" height = "600"/> 
    <p>12: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/ObesityDataSet_overfitting_rf.png" width="auto" height = "600"/> 
    <p>13: Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.</p>
    <img src="images/ObesityDataSet_overfitting_rf.png" width="auto" height = "600"/> 
    <p>14: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/ObesityDataSet_overfitting_rf.png" width="auto" height = "600"/> 
    <p>15: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/ObesityDataSet_overfitting_knn.png" width="auto" height = "600"/> 
    <p>16: KNN is in overfitting for k larger than 17.</p>
    <img src="images/ObesityDataSet_overfitting_knn.png" width="auto" height = "600"/> 
    <p>17: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/ObesityDataSet_overfitting_knn.png" width="auto" height = "600"/> 
    <p>18: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/ObesityDataSet_overfitting_knn.png" width="auto" height = "600"/> 
    <p>19: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/ObesityDataSet_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>20: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/ObesityDataSet_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>21: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/ObesityDataSet_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>22: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/ObesityDataSet_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>23: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/ObesityDataSet_pca.png" width="auto" height = "600"/> 
    <p>24: The first 7 principal components are enough for explaining half the data variance.</p>
    <img src="images/ObesityDataSet_pca.png" width="auto" height = "600"/> 
    <p>25: Using the first 7 principal components would imply an error between 15 and 20%.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>26: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>27: One of the variables Age or Height can be discarded without losing information.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>28: The variable Weight can be discarded without risking losing information.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>29: Variables NCP and TUE are redundant, but we can’t say the same for the pair Weight and Height.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>30: Variables FAF and TUE are redundant.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>31: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>32: Variable Height seems to be relevant for the majority of mining tasks.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>33: Variables FAF and Height seem to be useful for classification tasks.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>34: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>35: Removing variable CH2O might improve the training of decision trees .</p>
    <img src="images/ObesityDataSet_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>36: There is evidence in favour for sequential backward selection to select variable Age previously than variable Height.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>37: Variable CH2O is balanced.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>38: Those boxplots show that the data is not normalized.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>39: It is clear that variable FCVC shows some outliers, but we can’t be sure of the same for variable TUE.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>40: Outliers seem to be a problem in the dataset.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>41: Variable FAF shows some outlier values.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>42: Variable NCP doesn’t have any outliers.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>43: Variable Height presents some outliers.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>44: At least 75 of the variables present outliers.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>45: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>46: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>47: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>48: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>49: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/ObesityDataSet_boxplots.png" width="auto" height = "600"/> 
    <p>50: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>51: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>52: The variable SMOKE can be seen as ordinal.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>53: The variable FAVC can be seen as ordinal without losing information.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>54: Considering the common semantics for FAVC and CAEC variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>55: Considering the common semantics for family_history_with_overweight variable, dummification would be the most adequate encoding.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>56: The variable MTRANS can be coded as ordinal without losing information.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>57: Feature generation based on variable family_history_with_overweight seems to be promising.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>58: Feature generation based on the use of variable SCC wouldn’t be useful, but the use of CAEC seems to be promising.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>59: Given the usual semantics of family_history_with_overweight variable, dummification would have been a better codification.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>60: It is better to drop the variable CALC than removing all records with missing values.</p>
    <img src="images/ObesityDataSet_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>61: Not knowing the semantics of family_history_with_overweight variable, dummification could have been a more adequate codification.</p>
    <img src="images/ObesityDataSet_class_histogram.png" width="auto" height = "600"/> 
    <p>62: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/ObesityDataSet_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>63: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/ObesityDataSet_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>64: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/ObesityDataSet_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>65: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>66: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>67: The variable Height can be seen as ordinal.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>68: The variable NCP can be seen as ordinal without losing information.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>69: Variable FAF is balanced.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>70: It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable CH2O.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>71: Outliers seem to be a problem in the dataset.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>72: Variable Height shows a high number of outlier values.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>73: Variable TUE doesn’t have any outliers.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>74: Variable FCVC presents some outliers.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>75: At least 60 of the variables present outliers.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>76: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>77: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>78: Considering the common semantics for Weight and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>79: Considering the common semantics for Age variable, dummification would be the most adequate encoding.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>80: The variable Weight can be coded as ordinal without losing information.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>81: Feature generation based on variable TUE seems to be promising.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>82: Feature generation based on the use of variable Weight wouldn’t be useful, but the use of Age seems to be promising.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>83: Given the usual semantics of FAF variable, dummification would have been a better codification.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>84: It is better to drop the variable Age than removing all records with missing values.</p>
    <img src="images/ObesityDataSet_histograms_numeric.png" width="auto" height = "600"/> 
    <p>85: Not knowing the semantics of CH2O variable, dummification could have been a more adequate codification.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>86: The variable Family_Size discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>87: Variable Work_Experience is one of the most relevant variables.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>88: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>89: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>90: The precision for the presented tree is lower than 60%.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>91: The number of True Negatives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>92: The number of True Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>93: The recall for the presented tree is higher than its accuracy.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>94: Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A,B) as B for any k ≤ 11.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>95: Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (A, not B) as C for any k ≤ 723.</p>
    <img src="images/customer_segmentation_decision_tree.png" width="auto" height = "600"/> 
    <p>96: Considering that A=True<=>[Family_Size <= 2.5] and B=True<=>[Work_Experience <= 9.5], it is possible to state that KNN algorithm classifies (not A, B) as B for any k ≤ 524.</p>
    <img src="images/customer_segmentation_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>97: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/customer_segmentation_overfitting_gb.png" width="auto" height = "600"/> 
    <p>98: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/customer_segmentation_overfitting_rf.png" width="auto" height = "600"/> 
    <p>99: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/customer_segmentation_overfitting_rf.png" width="auto" height = "600"/> 
    <p>100: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/customer_segmentation_overfitting_rf.png" width="auto" height = "600"/> 
    <p>101: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/customer_segmentation_overfitting_knn.png" width="auto" height = "600"/> 
    <p>102: KNN is in overfitting for k larger than 13.</p>
    <img src="images/customer_segmentation_overfitting_knn.png" width="auto" height = "600"/> 
    <p>103: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/customer_segmentation_overfitting_knn.png" width="auto" height = "600"/> 
    <p>104: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/customer_segmentation_overfitting_knn.png" width="auto" height = "600"/> 
    <p>105: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/customer_segmentation_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>106: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/customer_segmentation_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>107: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 7.</p>
    <img src="images/customer_segmentation_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>108: The decision tree is in overfitting for depths above 5.</p>
    <img src="images/customer_segmentation_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>109: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/customer_segmentation_pca.png" width="auto" height = "600"/> 
    <p>110: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/customer_segmentation_pca.png" width="auto" height = "600"/> 
    <p>111: Using the first 2 principal components would imply an error between 10 and 20%.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>112: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>113: One of the variables Age or Family_Size can be discarded without losing information.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>114: The variable Age can be discarded without risking losing information.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>115: Variables Age and Work_Experience seem to be useful for classification tasks.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>116: Variables Age and Work_Experience are redundant.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>117: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>118: Variable Family_Size seems to be relevant for the majority of mining tasks.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>119: Variables Family_Size and Work_Experience seem to be useful for classification tasks.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>120: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>121: Removing variable Work_Experience might improve the training of decision trees .</p>
    <img src="images/customer_segmentation_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>122: There is evidence in favour for sequential backward selection to select variable Age previously than variable Family_Size.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>123: Variable Family_Size is balanced.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>124: Those boxplots show that the data is not normalized.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>125: It is clear that variable Work_Experience shows some outliers, but we can’t be sure of the same for variable Family_Size.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>126: Outliers seem to be a problem in the dataset.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>127: Variable Work_Experience shows a high number of outlier values.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>128: Variable Work_Experience doesn’t have any outliers.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>129: Variable Age presents some outliers.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>130: At least 50 of the variables present outliers.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>131: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>132: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>133: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>134: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>135: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/customer_segmentation_boxplots.png" width="auto" height = "600"/> 
    <p>136: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>137: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>138: The variable Gender can be seen as ordinal.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>139: The variable Ever_Married can be seen as ordinal without losing information.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>140: Considering the common semantics for Var_1 and Profession variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>141: Considering the common semantics for Profession variable, dummification would be the most adequate encoding.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>142: The variable Graduated can be coded as ordinal without losing information.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>143: Feature generation based on variable Gender seems to be promising.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>144: Feature generation based on the use of variable Graduated wouldn’t be useful, but the use of Profession seems to be promising.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>145: Given the usual semantics of Profession variable, dummification would have been a better codification.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>146: It is better to drop the variable Graduated than removing all records with missing values.</p>
    <img src="images/customer_segmentation_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>147: Not knowing the semantics of Spending_Score variable, dummification could have been a more adequate codification.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>148: Discarding variable Var_1 would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>149: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>150: Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>151: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>152: Feature generation based on variable Var_1 seems to be promising.</p>
    <img src="images/customer_segmentation_mv.png" width="auto" height = "600"/> 
    <p>153: It is better to drop the variable Family_Size than removing all records with missing values.</p>
    <img src="images/customer_segmentation_class_histogram.png" width="auto" height = "600"/> 
    <p>154: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/customer_segmentation_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>155: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/customer_segmentation_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>156: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/customer_segmentation_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>157: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>158: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>159: The variable Family_Size can be seen as ordinal.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>160: The variable Age can be seen as ordinal without losing information.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>161: Variable Family_Size is balanced.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>162: It is clear that variable Work_Experience shows some outliers, but we can’t be sure of the same for variable Age.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>163: Outliers seem to be a problem in the dataset.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>164: Variable Age shows some outlier values.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>165: Variable Family_Size doesn’t have any outliers.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>166: Variable Work_Experience presents some outliers.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>167: At least 75 of the variables present outliers.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>168: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>169: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>170: Considering the common semantics for Family_Size and Age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>171: Considering the common semantics for Age variable, dummification would be the most adequate encoding.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>172: The variable Age can be coded as ordinal without losing information.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>173: Feature generation based on variable Work_Experience seems to be promising.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>174: Feature generation based on the use of variable Work_Experience wouldn’t be useful, but the use of Age seems to be promising.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>175: Given the usual semantics of Age variable, dummification would have been a better codification.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>176: It is better to drop the variable Age than removing all records with missing values.</p>
    <img src="images/customer_segmentation_histograms_numeric.png" width="auto" height = "600"/> 
    <p>177: Not knowing the semantics of Family_Size variable, dummification could have been a more adequate codification.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>178: The variable Age discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>179: Variable Age is one of the most relevant variables.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>180: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>181: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>182: The specificity for the presented tree is higher than 60%.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>183: The number of True Positives reported in the same tree is 10.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>184: The number of False Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>185: The recall for the presented tree is lower than its specificity.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>186: Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as NEGATIVE.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>187: Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], the Decision Tree presented classifies (not A, B) as POSITIVE.</p>
    <img src="images/urinalysis_tests_decision_tree.png" width="auto" height = "600"/> 
    <p>188: Considering that A=True<=>[Age <= 0.1] and B=True<=>[pH <= 5.5], it is possible to state that KNN algorithm classifies (not A, B) as NEGATIVE for any k ≤ 763.</p>
    <img src="images/urinalysis_tests_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>189: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/urinalysis_tests_overfitting_gb.png" width="auto" height = "600"/> 
    <p>190: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/urinalysis_tests_overfitting_rf.png" width="auto" height = "600"/> 
    <p>191: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/urinalysis_tests_overfitting_rf.png" width="auto" height = "600"/> 
    <p>192: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/urinalysis_tests_overfitting_rf.png" width="auto" height = "600"/> 
    <p>193: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/urinalysis_tests_overfitting_knn.png" width="auto" height = "600"/> 
    <p>194: KNN is in overfitting for k larger than 5.</p>
    <img src="images/urinalysis_tests_overfitting_knn.png" width="auto" height = "600"/> 
    <p>195: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/urinalysis_tests_overfitting_knn.png" width="auto" height = "600"/> 
    <p>196: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/urinalysis_tests_overfitting_knn.png" width="auto" height = "600"/> 
    <p>197: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/urinalysis_tests_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>198: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/urinalysis_tests_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>199: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/urinalysis_tests_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>200: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/urinalysis_tests_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>201: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/urinalysis_tests_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>202: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/urinalysis_tests_pca.png" width="auto" height = "600"/> 
    <p>203: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/urinalysis_tests_pca.png" width="auto" height = "600"/> 
    <p>204: Using the first 2 principal components would imply an error between 5 and 20%.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>205: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>206: One of the variables pH or Age can be discarded without losing information.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>207: The variable Age can be discarded without risking losing information.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>208: Variables Specific Gravity and Age seem to be useful for classification tasks.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>209: Variables Age and pH are redundant.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>210: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>211: Variable Specific Gravity seems to be relevant for the majority of mining tasks.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>212: Variables Specific Gravity and pH seem to be useful for classification tasks.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>213: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>214: Removing variable pH might improve the training of decision trees .</p>
    <img src="images/urinalysis_tests_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>215: There is evidence in favour for sequential backward selection to select variable Age previously than variable pH.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>216: Variable pH is balanced.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>217: Those boxplots show that the data is not normalized.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>218: It is clear that variable Specific Gravity shows some outliers, but we can’t be sure of the same for variable Age.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>219: Outliers seem to be a problem in the dataset.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>220: Variable Specific Gravity shows a high number of outlier values.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>221: Variable Age doesn’t have any outliers.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>222: Variable Age presents some outliers.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>223: At least 60 of the variables present outliers.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>224: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>225: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>226: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>227: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>228: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/urinalysis_tests_boxplots.png" width="auto" height = "600"/> 
    <p>229: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>230: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>231: The variable Gender can be seen as ordinal.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>232: The variable Mucous Threads can be seen as ordinal without losing information.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>233: Considering the common semantics for Epithelial Cells and Color variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>234: Considering the common semantics for Amorphous Urates variable, dummification would be the most adequate encoding.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>235: The variable Color can be coded as ordinal without losing information.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>236: Feature generation based on variable Amorphous Urates seems to be promising.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>237: Feature generation based on the use of variable Protein wouldn’t be useful, but the use of Color seems to be promising.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>238: Given the usual semantics of Bacteria variable, dummification would have been a better codification.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>239: It is better to drop the variable Bacteria than removing all records with missing values.</p>
    <img src="images/urinalysis_tests_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>240: Not knowing the semantics of Epithelial Cells variable, dummification could have been a more adequate codification.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>241: Discarding variable Color would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>242: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>243: Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>244: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>245: Feature generation based on variable Color seems to be promising.</p>
    <img src="images/urinalysis_tests_mv.png" width="auto" height = "600"/> 
    <p>246: It is better to drop the variable Color than removing all records with missing values.</p>
    <img src="images/urinalysis_tests_class_histogram.png" width="auto" height = "600"/> 
    <p>247: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/urinalysis_tests_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>248: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/urinalysis_tests_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>249: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/urinalysis_tests_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>250: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>251: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>252: The variable Specific Gravity can be seen as ordinal.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>253: The variable Specific Gravity can be seen as ordinal without losing information.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>254: Variable Specific Gravity is balanced.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>255: It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable pH.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>256: Outliers seem to be a problem in the dataset.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>257: Variable Specific Gravity shows a high number of outlier values.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>258: Variable Specific Gravity doesn’t have any outliers.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>259: Variable Specific Gravity presents some outliers.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>260: At least 50 of the variables present outliers.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>261: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>262: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>263: Considering the common semantics for Age and pH variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>264: Considering the common semantics for Age variable, dummification would be the most adequate encoding.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>265: The variable pH can be coded as ordinal without losing information.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>266: Feature generation based on variable Age seems to be promising.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>267: Feature generation based on the use of variable Age wouldn’t be useful, but the use of pH seems to be promising.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>268: Given the usual semantics of Specific Gravity variable, dummification would have been a better codification.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>269: It is better to drop the variable pH than removing all records with missing values.</p>
    <img src="images/urinalysis_tests_histograms_numeric.png" width="auto" height = "600"/> 
    <p>270: Not knowing the semantics of Age variable, dummification could have been a more adequate codification.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>271: The variable Ic discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>272: Variable Vb is one of the most relevant variables.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>273: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>274: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>275: The precision for the presented tree is higher than 75%.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>276: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>277: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>278: The number of False Negatives reported in the same tree is 50.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>279: Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 3.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>280: Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], the Decision Tree presented classifies (A, not B) as 0.</p>
    <img src="images/detect_dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>281: Considering that A=True<=>[Ic <= 71.01] and B=True<=>[Vb <= -0.37], the Decision Tree presented classifies (A,B) as 0.</p>
    <img src="images/detect_dataset_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>282: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/detect_dataset_overfitting_gb.png" width="auto" height = "600"/> 
    <p>283: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/detect_dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>284: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/detect_dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>285: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/detect_dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>286: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/detect_dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>287: KNN is in overfitting for k less than 17.</p>
    <img src="images/detect_dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>288: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/detect_dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>289: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/detect_dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>290: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/detect_dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>291: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/detect_dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>292: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.</p>
    <img src="images/detect_dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>293: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/detect_dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>294: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/detect_dataset_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>295: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/detect_dataset_pca.png" width="auto" height = "600"/> 
    <p>296: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/detect_dataset_pca.png" width="auto" height = "600"/> 
    <p>297: Using the first 3 principal components would imply an error between 10 and 20%.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>298: The intrinsic dimensionality of this dataset is 5.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>299: One of the variables Vc or Va can be discarded without losing information.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>300: The variable Ic can be discarded without risking losing information.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>301: Variables Ia and Ic are redundant, but we can’t say the same for the pair Vc and Vb.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>302: Variables Ib and Vc are redundant.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>303: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>304: Variable Vb seems to be relevant for the majority of mining tasks.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>305: Variables Ib and Ic seem to be useful for classification tasks.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>306: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>307: Removing variable Ic might improve the training of decision trees .</p>
    <img src="images/detect_dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>308: There is evidence in favour for sequential backward selection to select variable Ic previously than variable Va.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>309: Variable Vb is balanced.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>310: Those boxplots show that the data is not normalized.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>311: It is clear that variable Vb shows some outliers, but we can’t be sure of the same for variable Va.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>312: Outliers seem to be a problem in the dataset.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>313: Variable Vb shows some outlier values.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>314: Variable Vb doesn’t have any outliers.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>315: Variable Ia presents some outliers.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>316: At least 75 of the variables present outliers.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>317: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>318: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>319: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>320: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>321: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/detect_dataset_boxplots.png" width="auto" height = "600"/> 
    <p>322: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/detect_dataset_class_histogram.png" width="auto" height = "600"/> 
    <p>323: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/detect_dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>324: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/detect_dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>325: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/detect_dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>326: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>327: All variables, but the class, should be dealt with as date.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>328: The variable Ic can be seen as ordinal.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>329: The variable Vc can be seen as ordinal without losing information.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>330: Variable Ia is balanced.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>331: It is clear that variable Va shows some outliers, but we can’t be sure of the same for variable Vc.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>332: Outliers seem to be a problem in the dataset.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>333: Variable Ia shows a high number of outlier values.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>334: Variable Ic doesn’t have any outliers.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>335: Variable Ic presents some outliers.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>336: At least 60 of the variables present outliers.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>337: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>338: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>339: Considering the common semantics for Ia and Ib variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>340: Considering the common semantics for Vc variable, dummification would be the most adequate encoding.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>341: The variable Vb can be coded as ordinal without losing information.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>342: Feature generation based on variable Vb seems to be promising.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>343: Feature generation based on the use of variable Ic wouldn’t be useful, but the use of Ia seems to be promising.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>344: Given the usual semantics of Ib variable, dummification would have been a better codification.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>345: It is better to drop the variable Ia than removing all records with missing values.</p>
    <img src="images/detect_dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>346: Not knowing the semantics of Ia variable, dummification could have been a more adequate codification.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>347: The variable BMI discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>348: Variable BMI is one of the most relevant variables.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>349: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>350: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>351: The precision for the presented tree is higher than 60%.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>352: The number of True Positives reported in the same tree is 30.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>353: The number of False Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>354: The accuracy for the presented tree is higher than its specificity.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>355: Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], the Decision Tree presented classifies (not A, not B) as 1.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>356: Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], the Decision Tree presented classifies (not A, not B) as 1.</p>
    <img src="images/diabetes_decision_tree.png" width="auto" height = "600"/> 
    <p>357: Considering that A=True<=>[BMI <= 29.85] and B=True<=>[Age <= 27.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 98.</p>
    <img src="images/diabetes_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>358: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/diabetes_overfitting_gb.png" width="auto" height = "600"/> 
    <p>359: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/diabetes_overfitting_rf.png" width="auto" height = "600"/> 
    <p>360: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/diabetes_overfitting_rf.png" width="auto" height = "600"/> 
    <p>361: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/diabetes_overfitting_rf.png" width="auto" height = "600"/> 
    <p>362: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/diabetes_overfitting_knn.png" width="auto" height = "600"/> 
    <p>363: KNN is in overfitting for k larger than 13.</p>
    <img src="images/diabetes_overfitting_knn.png" width="auto" height = "600"/> 
    <p>364: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/diabetes_overfitting_knn.png" width="auto" height = "600"/> 
    <p>365: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/diabetes_overfitting_knn.png" width="auto" height = "600"/> 
    <p>366: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/diabetes_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>367: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/diabetes_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>368: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.</p>
    <img src="images/diabetes_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>369: The decision tree is in overfitting for depths above 4.</p>
    <img src="images/diabetes_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>370: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/diabetes_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>371: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/diabetes_pca.png" width="auto" height = "600"/> 
    <p>372: The first 7 principal components are enough for explaining half the data variance.</p>
    <img src="images/diabetes_pca.png" width="auto" height = "600"/> 
    <p>373: Using the first 2 principal components would imply an error between 10 and 20%.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>374: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>375: One of the variables Age or Insulin can be discarded without losing information.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>376: The variable DiabetesPedigreeFunction can be discarded without risking losing information.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>377: Variables Age and SkinThickness are redundant, but we can’t say the same for the pair BMI and BloodPressure.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>378: Variables DiabetesPedigreeFunction and Age are redundant.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>379: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>380: Variable SkinThickness seems to be relevant for the majority of mining tasks.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>381: Variables Insulin and Glucose seem to be useful for classification tasks.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>382: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>383: Removing variable Insulin might improve the training of decision trees .</p>
    <img src="images/diabetes_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>384: There is evidence in favour for sequential backward selection to select variable DiabetesPedigreeFunction previously than variable Pregnancies.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>385: Variable DiabetesPedigreeFunction is balanced.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>386: Those boxplots show that the data is not normalized.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>387: It is clear that variable Glucose shows some outliers, but we can’t be sure of the same for variable Age.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>388: Outliers seem to be a problem in the dataset.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>389: Variable Pregnancies shows some outlier values.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>390: Variable Insulin doesn’t have any outliers.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>391: Variable BMI presents some outliers.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>392: At least 85 of the variables present outliers.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>393: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>394: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>395: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>396: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>397: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/diabetes_boxplots.png" width="auto" height = "600"/> 
    <p>398: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/diabetes_class_histogram.png" width="auto" height = "600"/> 
    <p>399: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/diabetes_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>400: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/diabetes_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>401: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/diabetes_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>402: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>403: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>404: The variable Age can be seen as ordinal.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>405: The variable Age can be seen as ordinal without losing information.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>406: Variable Pregnancies is balanced.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>407: It is clear that variable DiabetesPedigreeFunction shows some outliers, but we can’t be sure of the same for variable Glucose.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>408: Outliers seem to be a problem in the dataset.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>409: Variable Age shows a high number of outlier values.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>410: Variable BMI doesn’t have any outliers.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>411: Variable BloodPressure presents some outliers.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>412: At least 60 of the variables present outliers.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>413: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>414: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>415: Considering the common semantics for BloodPressure and Pregnancies variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>416: Considering the common semantics for BMI variable, dummification would be the most adequate encoding.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>417: The variable Age can be coded as ordinal without losing information.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>418: Feature generation based on variable BMI seems to be promising.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>419: Feature generation based on the use of variable Age wouldn’t be useful, but the use of Pregnancies seems to be promising.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>420: Given the usual semantics of BMI variable, dummification would have been a better codification.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>421: It is better to drop the variable BMI than removing all records with missing values.</p>
    <img src="images/diabetes_histograms_numeric.png" width="auto" height = "600"/> 
    <p>422: Not knowing the semantics of SkinThickness variable, dummification could have been a more adequate codification.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>423: The variable ssc_p discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>424: Variable hsc_p is one of the most relevant variables.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>425: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>426: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>427: The recall for the presented tree is higher than 90%.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>428: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>429: The number of True Negatives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>430: The accuracy for the presented tree is higher than 75%.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>431: Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], it is possible to state that KNN algorithm classifies (not A, not B) as Placed for any k ≤ 68.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>432: Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], it is possible to state that KNN algorithm classifies (not A, not B) as Placed for any k ≤ 68.</p>
    <img src="images/Placement_decision_tree.png" width="auto" height = "600"/> 
    <p>433: Considering that A=True<=>[ssc_p <= 60.09] and B=True<=>[hsc_p <= 70.24], the Decision Tree presented classifies (A, not B) as Placed.</p>
    <img src="images/Placement_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>434: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Placement_overfitting_gb.png" width="auto" height = "600"/> 
    <p>435: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Placement_overfitting_rf.png" width="auto" height = "600"/> 
    <p>436: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/Placement_overfitting_rf.png" width="auto" height = "600"/> 
    <p>437: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Placement_overfitting_rf.png" width="auto" height = "600"/> 
    <p>438: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Placement_overfitting_knn.png" width="auto" height = "600"/> 
    <p>439: KNN is in overfitting for k less than 13.</p>
    <img src="images/Placement_overfitting_knn.png" width="auto" height = "600"/> 
    <p>440: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/Placement_overfitting_knn.png" width="auto" height = "600"/> 
    <p>441: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/Placement_overfitting_knn.png" width="auto" height = "600"/> 
    <p>442: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/Placement_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>443: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/Placement_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>444: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.</p>
    <img src="images/Placement_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>445: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/Placement_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>446: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Placement_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>447: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Placement_pca.png" width="auto" height = "600"/> 
    <p>448: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/Placement_pca.png" width="auto" height = "600"/> 
    <p>449: Using the first 2 principal components would imply an error between 10 and 30%.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>450: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>451: One of the variables hsc_p or mba_p can be discarded without losing information.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>452: The variable mba_p can be discarded without risking losing information.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>453: Variables hsc_p and ssc_p are redundant, but we can’t say the same for the pair degree_p and etest_p.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>454: Variables hsc_p and etest_p are redundant.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>455: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>456: Variable ssc_p seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>457: Variables hsc_p and degree_p seem to be useful for classification tasks.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>458: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>459: Removing variable degree_p might improve the training of decision trees .</p>
    <img src="images/Placement_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>460: There is evidence in favour for sequential backward selection to select variable hsc_p previously than variable mba_p.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>461: Variable etest_p is balanced.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>462: Those boxplots show that the data is not normalized.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>463: It is clear that variable mba_p shows some outliers, but we can’t be sure of the same for variable ssc_p.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>464: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>465: Variable hsc_p shows some outlier values.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>466: Variable hsc_p doesn’t have any outliers.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>467: Variable hsc_p presents some outliers.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>468: At least 75 of the variables present outliers.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>469: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>470: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>471: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>472: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>473: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Placement_boxplots.png" width="auto" height = "600"/> 
    <p>474: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>475: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>476: The variable degree_t can be seen as ordinal.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>477: The variable specialisation can be seen as ordinal without losing information.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>478: Considering the common semantics for specialisation and hsc_s variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>479: Considering the common semantics for specialisation variable, dummification would be the most adequate encoding.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>480: The variable ssc_b can be coded as ordinal without losing information.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>481: Feature generation based on variable hsc_s seems to be promising.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>482: Feature generation based on the use of variable hsc_s wouldn’t be useful, but the use of degree_t seems to be promising.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>483: Given the usual semantics of hsc_s variable, dummification would have been a better codification.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>484: It is better to drop the variable ssc_b than removing all records with missing values.</p>
    <img src="images/Placement_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>485: Not knowing the semantics of hsc_b variable, dummification could have been a more adequate codification.</p>
    <img src="images/Placement_class_histogram.png" width="auto" height = "600"/> 
    <p>486: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Placement_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>487: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/Placement_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>488: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Placement_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>489: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>490: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>491: The variable etest_p can be seen as ordinal.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>492: The variable mba_p can be seen as ordinal without losing information.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>493: Variable degree_p is balanced.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>494: It is clear that variable mba_p shows some outliers, but we can’t be sure of the same for variable hsc_p.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>495: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>496: Variable mba_p shows some outlier values.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>497: Variable ssc_p doesn’t have any outliers.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>498: Variable degree_p presents some outliers.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>499: At least 85 of the variables present outliers.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>500: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>501: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>502: Considering the common semantics for ssc_p and hsc_p variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>503: Considering the common semantics for ssc_p variable, dummification would be the most adequate encoding.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>504: The variable degree_p can be coded as ordinal without losing information.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>505: Feature generation based on variable ssc_p seems to be promising.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>506: Feature generation based on the use of variable etest_p wouldn’t be useful, but the use of ssc_p seems to be promising.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>507: Given the usual semantics of degree_p variable, dummification would have been a better codification.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>508: It is better to drop the variable etest_p than removing all records with missing values.</p>
    <img src="images/Placement_histograms_numeric.png" width="auto" height = "600"/> 
    <p>509: Not knowing the semantics of hsc_p variable, dummification could have been a more adequate codification.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>510: The variable Sgot discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>511: Variable Alkphos is one of the most relevant variables.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>512: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>513: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>514: The recall for the presented tree is higher than 90%.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>515: The number of True Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>516: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>517: The precision for the presented tree is higher than its recall.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>518: Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 1.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>519: Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], it is possible to state that KNN algorithm classifies (not A, not B) as 2 for any k ≤ 94.</p>
    <img src="images/Liver_Patient_decision_tree.png" width="auto" height = "600"/> 
    <p>520: Considering that A=True<=>[Alkphos <= 211.5] and B=True<=>[Sgot <= 26.5], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="images/Liver_Patient_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>521: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Liver_Patient_overfitting_gb.png" width="auto" height = "600"/> 
    <p>522: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/Liver_Patient_overfitting_rf.png" width="auto" height = "600"/> 
    <p>523: Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.</p>
    <img src="images/Liver_Patient_overfitting_rf.png" width="auto" height = "600"/> 
    <p>524: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Liver_Patient_overfitting_rf.png" width="auto" height = "600"/> 
    <p>525: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/Liver_Patient_overfitting_knn.png" width="auto" height = "600"/> 
    <p>526: KNN is in overfitting for k less than 13.</p>
    <img src="images/Liver_Patient_overfitting_knn.png" width="auto" height = "600"/> 
    <p>527: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/Liver_Patient_overfitting_knn.png" width="auto" height = "600"/> 
    <p>528: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/Liver_Patient_overfitting_knn.png" width="auto" height = "600"/> 
    <p>529: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/Liver_Patient_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>530: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/Liver_Patient_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>531: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/Liver_Patient_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>532: The decision tree is in overfitting for depths above 4.</p>
    <img src="images/Liver_Patient_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>533: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/Liver_Patient_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>534: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Liver_Patient_pca.png" width="auto" height = "600"/> 
    <p>535: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/Liver_Patient_pca.png" width="auto" height = "600"/> 
    <p>536: Using the first 3 principal components would imply an error between 10 and 25%.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>537: The intrinsic dimensionality of this dataset is 8.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>538: One of the variables ALB or DB can be discarded without losing information.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>539: The variable AG_Ratio can be discarded without risking losing information.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>540: Variables AG_Ratio and DB are redundant, but we can’t say the same for the pair Sgpt and Sgot.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>541: Variables Sgpt and AG_Ratio are redundant.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>542: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>543: Variable Sgpt seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>544: Variables Age and DB seem to be useful for classification tasks.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>545: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>546: Removing variable DB might improve the training of decision trees .</p>
    <img src="images/Liver_Patient_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>547: There is evidence in favour for sequential backward selection to select variable DB previously than variable TB.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>548: Variable ALB is balanced.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>549: Those boxplots show that the data is not normalized.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>550: It is clear that variable Sgpt shows some outliers, but we can’t be sure of the same for variable TP.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>551: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>552: Variable Sgot shows a high number of outlier values.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>553: Variable TP doesn’t have any outliers.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>554: Variable Age presents some outliers.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>555: At least 75 of the variables present outliers.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>556: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>557: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>558: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>559: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>560: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Liver_Patient_boxplots.png" width="auto" height = "600"/> 
    <p>561: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>562: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>563: The variable Gender can be seen as ordinal.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>564: The variable Gender can be seen as ordinal without losing information.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>565: Considering the common semantics for Gender and <all_variables> variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>566: Considering the common semantics for Gender variable, dummification would be the most adequate encoding.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>567: The variable Gender can be coded as ordinal without losing information.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>568: Feature generation based on variable Gender seems to be promising.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>569: Feature generation based on the use of variable Gender wouldn’t be useful, but the use of <all_variables> seems to be promising.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>570: Given the usual semantics of Gender variable, dummification would have been a better codification.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>571: It is better to drop the variable Gender than removing all records with missing values.</p>
    <img src="images/Liver_Patient_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>572: Not knowing the semantics of Gender variable, dummification could have been a more adequate codification.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>573: Discarding variable AG_Ratio would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>574: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>575: Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>576: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>577: Feature generation based on variable AG_Ratio seems to be promising.</p>
    <img src="images/Liver_Patient_mv.png" width="auto" height = "600"/> 
    <p>578: It is better to drop the variable AG_Ratio than removing all records with missing values.</p>
    <img src="images/Liver_Patient_class_histogram.png" width="auto" height = "600"/> 
    <p>579: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Liver_Patient_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>580: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Liver_Patient_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>581: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Liver_Patient_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>582: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>583: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>584: The variable ALB can be seen as ordinal.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>585: The variable AG_Ratio can be seen as ordinal without losing information.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>586: Variable Age is balanced.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>587: It is clear that variable Sgot shows some outliers, but we can’t be sure of the same for variable Age.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>588: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>589: Variable ALB shows a high number of outlier values.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>590: Variable DB doesn’t have any outliers.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>591: Variable Alkphos presents some outliers.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>592: At least 50 of the variables present outliers.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>593: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>594: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>595: Considering the common semantics for Age and TB variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>596: Considering the common semantics for TB variable, dummification would be the most adequate encoding.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>597: The variable AG_Ratio can be coded as ordinal without losing information.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>598: Feature generation based on variable ALB seems to be promising.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>599: Feature generation based on the use of variable Sgpt wouldn’t be useful, but the use of Age seems to be promising.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>600: Given the usual semantics of Alkphos variable, dummification would have been a better codification.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>601: It is better to drop the variable AG_Ratio than removing all records with missing values.</p>
    <img src="images/Liver_Patient_histograms_numeric.png" width="auto" height = "600"/> 
    <p>602: Not knowing the semantics of AG_Ratio variable, dummification could have been a more adequate codification.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>603: The variable lead_time discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>604: Variable no_of_special_requests is one of the most relevant variables.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>605: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>606: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>607: The recall for the presented tree is lower than 75%.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>608: The number of True Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>609: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>610: The variable lead_time discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>611: Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A, not B) as Canceled for any k ≤ 4955.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>612: Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A,B) as Not_Canceled for any k ≤ 10612.</p>
    <img src="images/Hotel_Reservations_decision_tree.png" width="auto" height = "600"/> 
    <p>613: Considering that A=True<=>[lead_time <= 151.5] and B=True<=>[no_of_special_requests <= 2.5], it is possible to state that KNN algorithm classifies (A,B) as Canceled for any k ≤ 9756.</p>
    <img src="images/Hotel_Reservations_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>614: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Hotel_Reservations_overfitting_gb.png" width="auto" height = "600"/> 
    <p>615: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Hotel_Reservations_overfitting_rf.png" width="auto" height = "600"/> 
    <p>616: Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.</p>
    <img src="images/Hotel_Reservations_overfitting_rf.png" width="auto" height = "600"/> 
    <p>617: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Hotel_Reservations_overfitting_rf.png" width="auto" height = "600"/> 
    <p>618: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/Hotel_Reservations_overfitting_knn.png" width="auto" height = "600"/> 
    <p>619: KNN is in overfitting for k less than 5.</p>
    <img src="images/Hotel_Reservations_overfitting_knn.png" width="auto" height = "600"/> 
    <p>620: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/Hotel_Reservations_overfitting_knn.png" width="auto" height = "600"/> 
    <p>621: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/Hotel_Reservations_overfitting_knn.png" width="auto" height = "600"/> 
    <p>622: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/Hotel_Reservations_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>623: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/Hotel_Reservations_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>624: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.</p>
    <img src="images/Hotel_Reservations_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>625: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/Hotel_Reservations_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>626: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/Hotel_Reservations_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>627: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Hotel_Reservations_pca.png" width="auto" height = "600"/> 
    <p>628: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/Hotel_Reservations_pca.png" width="auto" height = "600"/> 
    <p>629: Using the first 5 principal components would imply an error between 10 and 20%.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>630: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>631: One of the variables arrival_month or no_of_special_requests can be discarded without losing information.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>632: The variable no_of_adults can be discarded without risking losing information.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>633: Variables no_of_adults and arrival_month are redundant, but we can’t say the same for the pair no_of_week_nights and no_of_weekend_nights.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>634: Variables no_of_adults and no_of_week_nights are redundant.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>635: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>636: Variable arrival_month seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>637: Variables arrival_month and no_of_adults seem to be useful for classification tasks.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>638: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>639: Removing variable no_of_adults might improve the training of decision trees .</p>
    <img src="images/Hotel_Reservations_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>640: There is evidence in favour for sequential backward selection to select variable arrival_date previously than variable no_of_week_nights.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>641: Variable arrival_date is balanced.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>642: Those boxplots show that the data is not normalized.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>643: It is clear that variable no_of_weekend_nights shows some outliers, but we can’t be sure of the same for variable lead_time.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>644: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>645: Variable no_of_week_nights shows a high number of outlier values.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>646: Variable no_of_week_nights doesn’t have any outliers.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>647: Variable avg_price_per_room presents some outliers.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>648: At least 85 of the variables present outliers.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>649: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>650: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>651: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>652: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>653: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Hotel_Reservations_boxplots.png" width="auto" height = "600"/> 
    <p>654: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>655: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>656: The variable room_type_reserved can be seen as ordinal.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>657: The variable type_of_meal_plan can be seen as ordinal without losing information.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>658: Considering the common semantics for arrival_year and type_of_meal_plan variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>659: Considering the common semantics for type_of_meal_plan variable, dummification would be the most adequate encoding.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>660: The variable type_of_meal_plan can be coded as ordinal without losing information.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>661: Feature generation based on variable arrival_year seems to be promising.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>662: Feature generation based on the use of variable required_car_parking_space wouldn’t be useful, but the use of type_of_meal_plan seems to be promising.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>663: Given the usual semantics of required_car_parking_space variable, dummification would have been a better codification.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>664: It is better to drop the variable required_car_parking_space than removing all records with missing values.</p>
    <img src="images/Hotel_Reservations_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>665: Not knowing the semantics of arrival_year variable, dummification could have been a more adequate codification.</p>
    <img src="images/Hotel_Reservations_class_histogram.png" width="auto" height = "600"/> 
    <p>666: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Hotel_Reservations_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>667: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/Hotel_Reservations_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>668: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Hotel_Reservations_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>669: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>670: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>671: The variable arrival_date can be seen as ordinal.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>672: The variable no_of_children can be seen as ordinal without losing information.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>673: Variable no_of_children is balanced.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>674: It is clear that variable no_of_special_requests shows some outliers, but we can’t be sure of the same for variable avg_price_per_room.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>675: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>676: Variable arrival_date shows a high number of outlier values.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>677: Variable no_of_adults doesn’t have any outliers.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>678: Variable no_of_weekend_nights presents some outliers.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>679: At least 75 of the variables present outliers.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>680: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>681: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>682: Considering the common semantics for arrival_date and no_of_adults variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>683: Considering the common semantics for no_of_special_requests variable, dummification would be the most adequate encoding.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>684: The variable avg_price_per_room can be coded as ordinal without losing information.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>685: Feature generation based on variable no_of_special_requests seems to be promising.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>686: Feature generation based on the use of variable no_of_week_nights wouldn’t be useful, but the use of no_of_adults seems to be promising.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>687: Given the usual semantics of no_of_adults variable, dummification would have been a better codification.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>688: It is better to drop the variable arrival_date than removing all records with missing values.</p>
    <img src="images/Hotel_Reservations_histograms_numeric.png" width="auto" height = "600"/> 
    <p>689: Not knowing the semantics of no_of_week_nights variable, dummification could have been a more adequate codification.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>690: The variable bullying discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>691: Variable basic_needs is one of the most relevant variables.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>692: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>693: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>694: The precision for the presented tree is higher than 60%.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>695: The number of False Positives reported in the same tree is 30.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>696: The number of True Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>697: The variable basic_needs seems to be one of the four most relevant features.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>698: Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], the Decision Tree presented classifies (A, not B) as 2.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>699: Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="images/StressLevelDataset_decision_tree.png" width="auto" height = "600"/> 
    <p>700: Considering that A=True<=>[basic_needs <= 3.5] and B=True<=>[bullying <= 1.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 271.</p>
    <img src="images/StressLevelDataset_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>701: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/StressLevelDataset_overfitting_gb.png" width="auto" height = "600"/> 
    <p>702: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/StressLevelDataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>703: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/StressLevelDataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>704: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/StressLevelDataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>705: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/StressLevelDataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>706: KNN is in overfitting for k larger than 17.</p>
    <img src="images/StressLevelDataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>707: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/StressLevelDataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>708: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/StressLevelDataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>709: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/StressLevelDataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>710: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/StressLevelDataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>711: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/StressLevelDataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>712: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/StressLevelDataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>713: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/StressLevelDataset_pca.png" width="auto" height = "600"/> 
    <p>714: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/StressLevelDataset_pca.png" width="auto" height = "600"/> 
    <p>715: Using the first 2 principal components would imply an error between 5 and 25%.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>716: The intrinsic dimensionality of this dataset is 6.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>717: One of the variables headache or bullying can be discarded without losing information.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>718: The variable breathing_problem can be discarded without risking losing information.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>719: Variables anxiety_level and bullying are redundant, but we can’t say the same for the pair study_load and living_conditions.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>720: Variables bullying and depression are redundant.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>721: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>722: Variable breathing_problem seems to be relevant for the majority of mining tasks.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>723: Variables living_conditions and breathing_problem seem to be useful for classification tasks.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>724: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>725: Removing variable basic_needs might improve the training of decision trees .</p>
    <img src="images/StressLevelDataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>726: There is evidence in favour for sequential backward selection to select variable basic_needs previously than variable self_esteem.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>727: Variable study_load is balanced.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>728: Those boxplots show that the data is not normalized.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>729: It is clear that variable self_esteem shows some outliers, but we can’t be sure of the same for variable anxiety_level.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>730: Outliers seem to be a problem in the dataset.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>731: Variable basic_needs shows some outlier values.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>732: Variable headache doesn’t have any outliers.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>733: Variable depression presents some outliers.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>734: At least 60 of the variables present outliers.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>735: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>736: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>737: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>738: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>739: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/StressLevelDataset_boxplots.png" width="auto" height = "600"/> 
    <p>740: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>741: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>742: The variable mental_health_history can be seen as ordinal.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>743: The variable mental_health_history can be seen as ordinal without losing information.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>744: Considering the common semantics for mental_health_history and <all_variables> variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>745: Considering the common semantics for mental_health_history variable, dummification would be the most adequate encoding.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>746: The variable mental_health_history can be coded as ordinal without losing information.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>747: Feature generation based on variable mental_health_history seems to be promising.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>748: Feature generation based on the use of variable mental_health_history wouldn’t be useful, but the use of <all_variables> seems to be promising.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>749: Given the usual semantics of mental_health_history variable, dummification would have been a better codification.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>750: It is better to drop the variable mental_health_history than removing all records with missing values.</p>
    <img src="images/StressLevelDataset_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>751: Not knowing the semantics of mental_health_history variable, dummification could have been a more adequate codification.</p>
    <img src="images/StressLevelDataset_class_histogram.png" width="auto" height = "600"/> 
    <p>752: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/StressLevelDataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>753: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/StressLevelDataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>754: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/StressLevelDataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>755: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>756: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>757: The variable living_conditions can be seen as ordinal.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>758: The variable breathing_problem can be seen as ordinal without losing information.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>759: Variable breathing_problem is balanced.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>760: It is clear that variable depression shows some outliers, but we can’t be sure of the same for variable study_load.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>761: Outliers seem to be a problem in the dataset.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>762: Variable bullying shows a high number of outlier values.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>763: Variable headache doesn’t have any outliers.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>764: Variable anxiety_level presents some outliers.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>765: At least 60 of the variables present outliers.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>766: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>767: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>768: Considering the common semantics for sleep_quality and anxiety_level variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>769: Considering the common semantics for headache variable, dummification would be the most adequate encoding.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>770: The variable breathing_problem can be coded as ordinal without losing information.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>771: Feature generation based on variable self_esteem seems to be promising.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>772: Feature generation based on the use of variable anxiety_level wouldn’t be useful, but the use of self_esteem seems to be promising.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>773: Given the usual semantics of study_load variable, dummification would have been a better codification.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>774: It is better to drop the variable depression than removing all records with missing values.</p>
    <img src="images/StressLevelDataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>775: Not knowing the semantics of basic_needs variable, dummification could have been a more adequate codification.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>776: The variable chlorides discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>777: Variable density is one of the most relevant variables.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>778: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>779: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>780: The precision for the presented tree is higher than 75%.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>781: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>782: The number of False Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>783: The number of False Positives reported in the same tree is 10.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>784: Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that KNN algorithm classifies (not A, not B) as 6 for any k ≤ 447.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>785: Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 3.</p>
    <img src="images/WineQT_decision_tree.png" width="auto" height = "600"/> 
    <p>786: Considering that A=True<=>[density <= 1.0] and B=True<=>[chlorides <= 0.08], it is possible to state that KNN algorithm classifies (not A, not B) as 5 for any k ≤ 172.</p>
    <img src="images/WineQT_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>787: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/WineQT_overfitting_gb.png" width="auto" height = "600"/> 
    <p>788: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/WineQT_overfitting_rf.png" width="auto" height = "600"/> 
    <p>789: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/WineQT_overfitting_rf.png" width="auto" height = "600"/> 
    <p>790: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/WineQT_overfitting_rf.png" width="auto" height = "600"/> 
    <p>791: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/WineQT_overfitting_knn.png" width="auto" height = "600"/> 
    <p>792: KNN is in overfitting for k larger than 13.</p>
    <img src="images/WineQT_overfitting_knn.png" width="auto" height = "600"/> 
    <p>793: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/WineQT_overfitting_knn.png" width="auto" height = "600"/> 
    <p>794: KNN with more than 7 neighbours is in overfitting.</p>
    <img src="images/WineQT_overfitting_knn.png" width="auto" height = "600"/> 
    <p>795: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/WineQT_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>796: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/WineQT_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>797: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/WineQT_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>798: The decision tree is in overfitting for depths above 10.</p>
    <img src="images/WineQT_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>799: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/WineQT_pca.png" width="auto" height = "600"/> 
    <p>800: The first 8 principal components are enough for explaining half the data variance.</p>
    <img src="images/WineQT_pca.png" width="auto" height = "600"/> 
    <p>801: Using the first 6 principal components would imply an error between 15 and 25%.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>802: The intrinsic dimensionality of this dataset is 5.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>803: One of the variables citric acid or residual sugar can be discarded without losing information.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>804: The variable chlorides can be discarded without risking losing information.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>805: Variables sulphates and pH are redundant, but we can’t say the same for the pair free sulfur dioxide and volatile acidity.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>806: Variables free sulfur dioxide and total sulfur dioxide are redundant.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>807: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>808: Variable volatile acidity seems to be relevant for the majority of mining tasks.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>809: Variables chlorides and citric acid seem to be useful for classification tasks.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>810: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>811: Removing variable fixed acidity might improve the training of decision trees .</p>
    <img src="images/WineQT_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>812: There is evidence in favour for sequential backward selection to select variable pH previously than variable chlorides.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>813: Variable citric acid is balanced.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>814: Those boxplots show that the data is not normalized.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>815: It is clear that variable pH shows some outliers, but we can’t be sure of the same for variable volatile acidity.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>816: Outliers seem to be a problem in the dataset.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>817: Variable free sulfur dioxide shows a high number of outlier values.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>818: Variable chlorides doesn’t have any outliers.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>819: Variable total sulfur dioxide presents some outliers.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>820: At least 75 of the variables present outliers.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>821: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>822: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>823: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>824: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>825: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/WineQT_boxplots.png" width="auto" height = "600"/> 
    <p>826: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/WineQT_class_histogram.png" width="auto" height = "600"/> 
    <p>827: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/WineQT_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>828: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/WineQT_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>829: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/WineQT_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>830: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>831: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>832: The variable fixed acidity can be seen as ordinal.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>833: The variable pH can be seen as ordinal without losing information.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>834: Variable free sulfur dioxide is balanced.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>835: It is clear that variable alcohol shows some outliers, but we can’t be sure of the same for variable sulphates.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>836: Outliers seem to be a problem in the dataset.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>837: Variable sulphates shows a high number of outlier values.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>838: Variable pH doesn’t have any outliers.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>839: Variable citric acid presents some outliers.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>840: At least 75 of the variables present outliers.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>841: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>842: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>843: Considering the common semantics for citric acid and fixed acidity variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>844: Considering the common semantics for citric acid variable, dummification would be the most adequate encoding.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>845: The variable pH can be coded as ordinal without losing information.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>846: Feature generation based on variable density seems to be promising.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>847: Feature generation based on the use of variable sulphates wouldn’t be useful, but the use of fixed acidity seems to be promising.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>848: Given the usual semantics of citric acid variable, dummification would have been a better codification.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>849: It is better to drop the variable free sulfur dioxide than removing all records with missing values.</p>
    <img src="images/WineQT_histograms_numeric.png" width="auto" height = "600"/> 
    <p>850: Not knowing the semantics of pH variable, dummification could have been a more adequate codification.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>851: The variable ApplicantIncome discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>852: Variable ApplicantIncome is one of the most relevant variables.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>853: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>854: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>855: The recall for the presented tree is lower than 90%.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>856: The number of False Positives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>857: The number of False Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>858: The recall for the presented tree is higher than its accuracy.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>859: Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that KNN algorithm classifies (not A, not B) as Y for any k ≤ 3.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>860: Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that KNN algorithm classifies (not A, B) as N for any k ≤ 204.</p>
    <img src="images/loan_data_decision_tree.png" width="auto" height = "600"/> 
    <p>861: Considering that A=True<=>[Loan_Amount_Term <= 420.0] and B=True<=>[ApplicantIncome <= 1519.0], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as N.</p>
    <img src="images/loan_data_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>862: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/loan_data_overfitting_gb.png" width="auto" height = "600"/> 
    <p>863: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/loan_data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>864: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/loan_data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>865: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/loan_data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>866: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/loan_data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>867: KNN is in overfitting for k larger than 13.</p>
    <img src="images/loan_data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>868: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/loan_data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>869: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/loan_data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>870: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/loan_data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>871: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/loan_data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>872: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.</p>
    <img src="images/loan_data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>873: The decision tree is in overfitting for depths above 10.</p>
    <img src="images/loan_data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>874: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/loan_data_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>875: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/loan_data_pca.png" width="auto" height = "600"/> 
    <p>876: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/loan_data_pca.png" width="auto" height = "600"/> 
    <p>877: Using the first 2 principal components would imply an error between 10 and 20%.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>878: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>879: One of the variables CoapplicantIncome or ApplicantIncome can be discarded without losing information.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>880: The variable CoapplicantIncome can be discarded without risking losing information.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>881: Variables ApplicantIncome and LoanAmount seem to be useful for classification tasks.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>882: Variables Loan_Amount_Term and CoapplicantIncome are redundant.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>883: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>884: Variable ApplicantIncome seems to be relevant for the majority of mining tasks.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>885: Variables CoapplicantIncome and ApplicantIncome seem to be useful for classification tasks.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>886: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>887: Removing variable LoanAmount might improve the training of decision trees .</p>
    <img src="images/loan_data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>888: There is evidence in favour for sequential backward selection to select variable CoapplicantIncome previously than variable Loan_Amount_Term.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>889: Variable Loan_Amount_Term is balanced.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>890: Those boxplots show that the data is not normalized.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>891: It is clear that variable Loan_Amount_Term shows some outliers, but we can’t be sure of the same for variable ApplicantIncome.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>892: Outliers seem to be a problem in the dataset.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>893: Variable ApplicantIncome shows a high number of outlier values.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>894: Variable Loan_Amount_Term doesn’t have any outliers.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>895: Variable ApplicantIncome presents some outliers.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>896: At least 50 of the variables present outliers.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>897: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>898: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>899: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>900: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>901: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/loan_data_boxplots.png" width="auto" height = "600"/> 
    <p>902: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>903: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>904: The variable Credit_History can be seen as ordinal.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>905: The variable Married can be seen as ordinal without losing information.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>906: Considering the common semantics for Credit_History and Dependents variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>907: Considering the common semantics for Property_Area variable, dummification would be the most adequate encoding.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>908: The variable Dependents can be coded as ordinal without losing information.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>909: Feature generation based on variable Dependents seems to be promising.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>910: Feature generation based on the use of variable Self_Employed wouldn’t be useful, but the use of Dependents seems to be promising.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>911: Given the usual semantics of Education variable, dummification would have been a better codification.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>912: It is better to drop the variable Property_Area than removing all records with missing values.</p>
    <img src="images/loan_data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>913: Not knowing the semantics of Dependents variable, dummification could have been a more adequate codification.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>914: Discarding variable Gender would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>915: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>916: Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>917: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>918: Feature generation based on variable Dependents seems to be promising.</p>
    <img src="images/loan_data_mv.png" width="auto" height = "600"/> 
    <p>919: It is better to drop the variable Self_Employed than removing all records with missing values.</p>
    <img src="images/loan_data_class_histogram.png" width="auto" height = "600"/> 
    <p>920: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/loan_data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>921: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/loan_data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>922: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/loan_data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>923: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>924: All variables, but the class, should be dealt with as date.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>925: The variable Loan_Amount_Term can be seen as ordinal.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>926: The variable CoapplicantIncome can be seen as ordinal without losing information.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>927: Variable LoanAmount is balanced.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>928: It is clear that variable LoanAmount shows some outliers, but we can’t be sure of the same for variable Loan_Amount_Term.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>929: Outliers seem to be a problem in the dataset.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>930: Variable Loan_Amount_Term shows a high number of outlier values.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>931: Variable Loan_Amount_Term doesn’t have any outliers.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>932: Variable LoanAmount presents some outliers.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>933: At least 85 of the variables present outliers.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>934: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>935: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>936: Considering the common semantics for LoanAmount and ApplicantIncome variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>937: Considering the common semantics for LoanAmount variable, dummification would be the most adequate encoding.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>938: The variable LoanAmount can be coded as ordinal without losing information.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>939: Feature generation based on variable LoanAmount seems to be promising.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>940: Feature generation based on the use of variable CoapplicantIncome wouldn’t be useful, but the use of ApplicantIncome seems to be promising.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>941: Given the usual semantics of ApplicantIncome variable, dummification would have been a better codification.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>942: It is better to drop the variable ApplicantIncome than removing all records with missing values.</p>
    <img src="images/loan_data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>943: Not knowing the semantics of Loan_Amount_Term variable, dummification could have been a more adequate codification.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>944: The variable Area discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>945: Variable AspectRation is one of the most relevant variables.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>946: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>947: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>948: The recall for the presented tree is higher than 60%.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>949: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>950: The number of True Positives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>951: The precision for the presented tree is higher than its specificity.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>952: Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], it is possible to state that KNN algorithm classifies (not A, not B) as SEKER for any k ≤ 1284.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>953: Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], the Decision Tree presented classifies (not A, B) as BOMBAY.</p>
    <img src="images/Dry_Bean_Dataset_decision_tree.png" width="auto" height = "600"/> 
    <p>954: Considering that A=True<=>[Area <= 39172.5] and B=True<=>[AspectRation <= 1.86], it is possible to state that KNN algorithm classifies (A,B) as DERMASON for any k ≤ 2501.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>955: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_gb.png" width="auto" height = "600"/> 
    <p>956: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>957: Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>958: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_rf.png" width="auto" height = "600"/> 
    <p>959: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>960: KNN is in overfitting for k larger than 17.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>961: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>962: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_knn.png" width="auto" height = "600"/> 
    <p>963: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>964: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>965: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>966: The decision tree is in overfitting for depths above 8.</p>
    <img src="images/Dry_Bean_Dataset_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>967: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/Dry_Bean_Dataset_pca.png" width="auto" height = "600"/> 
    <p>968: The first 6 principal components are enough for explaining half the data variance.</p>
    <img src="images/Dry_Bean_Dataset_pca.png" width="auto" height = "600"/> 
    <p>969: Using the first 2 principal components would imply an error between 15 and 25%.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>970: The intrinsic dimensionality of this dataset is 9.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>971: One of the variables MinorAxisLength or Eccentricity can be discarded without losing information.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>972: The variable Eccentricity can be discarded without risking losing information.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>973: Variables MinorAxisLength and Solidity are redundant, but we can’t say the same for the pair ShapeFactor1 and Extent.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>974: Variables roundness and ShapeFactor1 are redundant.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>975: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>976: Variable ShapeFactor1 seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>977: Variables Perimeter and Eccentricity seem to be useful for classification tasks.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>978: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>979: Removing variable Solidity might improve the training of decision trees .</p>
    <img src="images/Dry_Bean_Dataset_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>980: There is evidence in favour for sequential backward selection to select variable Eccentricity previously than variable EquivDiameter.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>981: Variable MinorAxisLength is balanced.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>982: Those boxplots show that the data is not normalized.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>983: It is clear that variable Solidity shows some outliers, but we can’t be sure of the same for variable EquivDiameter.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>984: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>985: Variable Solidity shows some outlier values.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>986: Variable roundness doesn’t have any outliers.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>987: Variable Eccentricity presents some outliers.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>988: At least 50 of the variables present outliers.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>989: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>990: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>991: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>992: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>993: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Dry_Bean_Dataset_boxplots.png" width="auto" height = "600"/> 
    <p>994: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Dry_Bean_Dataset_class_histogram.png" width="auto" height = "600"/> 
    <p>995: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Dry_Bean_Dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>996: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Dry_Bean_Dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>997: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Dry_Bean_Dataset_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>998: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>999: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1000: The variable Perimeter can be seen as ordinal.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1001: The variable Extent can be seen as ordinal without losing information.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1002: Variable Solidity is balanced.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1003: It is clear that variable EquivDiameter shows some outliers, but we can’t be sure of the same for variable MinorAxisLength.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1004: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1005: Variable Area shows some outlier values.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1006: Variable roundness doesn’t have any outliers.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1007: Variable Solidity presents some outliers.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1008: At least 85 of the variables present outliers.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1009: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1010: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1011: Considering the common semantics for AspectRation and Area variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1012: Considering the common semantics for EquivDiameter variable, dummification would be the most adequate encoding.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1013: The variable roundness can be coded as ordinal without losing information.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1014: Feature generation based on variable EquivDiameter seems to be promising.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1015: Feature generation based on the use of variable MinorAxisLength wouldn’t be useful, but the use of Area seems to be promising.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1016: Given the usual semantics of roundness variable, dummification would have been a better codification.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1017: It is better to drop the variable Solidity than removing all records with missing values.</p>
    <img src="images/Dry_Bean_Dataset_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1018: Not knowing the semantics of Perimeter variable, dummification could have been a more adequate codification.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1019: The variable residence_since discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1020: Variable residence_since is one of the most relevant variables.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1021: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1022: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1023: The accuracy for the presented tree is higher than 90%.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1024: The number of False Negatives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1025: The number of True Positives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1026: The accuracy for the presented tree is higher than its recall.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1027: Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 107.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1028: Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as bad.</p>
    <img src="images/credit_customers_decision_tree.png" width="auto" height = "600"/> 
    <p>1029: Considering that A=True<=>[existing_credits <= 1.5] and B=True<=>[residence_since <= 3.5], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 264.</p>
    <img src="images/credit_customers_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1030: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/credit_customers_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1031: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/credit_customers_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1032: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/credit_customers_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1033: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/credit_customers_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1034: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/credit_customers_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1035: KNN is in overfitting for k less than 13.</p>
    <img src="images/credit_customers_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1036: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/credit_customers_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1037: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/credit_customers_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1038: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/credit_customers_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1039: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/credit_customers_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1040: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.</p>
    <img src="images/credit_customers_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1041: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/credit_customers_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1042: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/credit_customers_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1043: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/credit_customers_pca.png" width="auto" height = "600"/> 
    <p>1044: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/credit_customers_pca.png" width="auto" height = "600"/> 
    <p>1045: Using the first 4 principal components would imply an error between 5 and 20%.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1046: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1047: One of the variables age or credit_amount can be discarded without losing information.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1048: The variable existing_credits can be discarded without risking losing information.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1049: Variables existing_credits and credit_amount are redundant, but we can’t say the same for the pair duration and installment_commitment.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1050: Variables residence_since and existing_credits are redundant.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1051: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1052: Variable age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1053: Variables age and installment_commitment seem to be useful for classification tasks.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1054: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1055: Removing variable credit_amount might improve the training of decision trees .</p>
    <img src="images/credit_customers_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1056: There is evidence in favour for sequential backward selection to select variable existing_credits previously than variable credit_amount.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1057: Variable existing_credits is balanced.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1058: Those boxplots show that the data is not normalized.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1059: It is clear that variable age shows some outliers, but we can’t be sure of the same for variable residence_since.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1060: Outliers seem to be a problem in the dataset.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1061: Variable age shows some outlier values.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1062: Variable residence_since doesn’t have any outliers.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1063: Variable age presents some outliers.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1064: At least 50 of the variables present outliers.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1065: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1066: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1067: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1068: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1069: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/credit_customers_boxplots.png" width="auto" height = "600"/> 
    <p>1070: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1071: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1072: The variable other_parties can be seen as ordinal.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1073: The variable employment can be seen as ordinal without losing information.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1074: Considering the common semantics for checking_status and employment variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1075: Considering the common semantics for housing variable, dummification would be the most adequate encoding.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1076: The variable checking_status can be coded as ordinal without losing information.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1077: Feature generation based on variable num_dependents seems to be promising.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1078: Feature generation based on the use of variable employment wouldn’t be useful, but the use of checking_status seems to be promising.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1079: Given the usual semantics of own_telephone variable, dummification would have been a better codification.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1080: It is better to drop the variable num_dependents than removing all records with missing values.</p>
    <img src="images/credit_customers_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1081: Not knowing the semantics of num_dependents variable, dummification could have been a more adequate codification.</p>
    <img src="images/credit_customers_class_histogram.png" width="auto" height = "600"/> 
    <p>1082: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/credit_customers_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1083: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/credit_customers_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1084: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/credit_customers_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1085: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1086: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1087: The variable credit_amount can be seen as ordinal.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1088: The variable age can be seen as ordinal without losing information.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1089: Variable duration is balanced.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1090: It is clear that variable age shows some outliers, but we can’t be sure of the same for variable credit_amount.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1091: Outliers seem to be a problem in the dataset.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1092: Variable residence_since shows some outlier values.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1093: Variable credit_amount doesn’t have any outliers.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1094: Variable existing_credits presents some outliers.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1095: At least 60 of the variables present outliers.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1096: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1097: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1098: Considering the common semantics for residence_since and duration variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1099: Considering the common semantics for installment_commitment variable, dummification would be the most adequate encoding.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1100: The variable age can be coded as ordinal without losing information.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1101: Feature generation based on variable residence_since seems to be promising.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1102: Feature generation based on the use of variable credit_amount wouldn’t be useful, but the use of duration seems to be promising.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1103: Given the usual semantics of age variable, dummification would have been a better codification.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1104: It is better to drop the variable residence_since than removing all records with missing values.</p>
    <img src="images/credit_customers_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1105: Not knowing the semantics of installment_commitment variable, dummification could have been a more adequate codification.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1106: The variable Pressure3pm discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1107: Variable Rainfall is one of the most relevant variables.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1108: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1109: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1110: The precision for the presented tree is lower than 60%.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1111: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1112: The number of False Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1113: The accuracy for the presented tree is higher than 75%.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1114: Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], the Decision Tree presented classifies (not A, not B) as No.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1115: Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], the Decision Tree presented classifies (not A, B) as Yes.</p>
    <img src="images/weatherAUS_decision_tree.png" width="auto" height = "600"/> 
    <p>1116: Considering that A=True<=>[Rainfall <= 0.1] and B=True<=>[Pressure3pm <= 1009.65], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as No.</p>
    <img src="images/weatherAUS_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1117: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/weatherAUS_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1118: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/weatherAUS_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1119: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/weatherAUS_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1120: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/weatherAUS_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1121: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/weatherAUS_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1122: KNN is in overfitting for k less than 5.</p>
    <img src="images/weatherAUS_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1123: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/weatherAUS_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1124: KNN with less than 17 neighbours is in overfitting.</p>
    <img src="images/weatherAUS_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1125: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/weatherAUS_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1126: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/weatherAUS_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1127: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 3.</p>
    <img src="images/weatherAUS_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1128: The decision tree is in overfitting for depths above 4.</p>
    <img src="images/weatherAUS_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1129: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/weatherAUS_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1130: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/weatherAUS_pca.png" width="auto" height = "600"/> 
    <p>1131: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/weatherAUS_pca.png" width="auto" height = "600"/> 
    <p>1132: Using the first 6 principal components would imply an error between 5 and 20%.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1133: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1134: One of the variables Pressure9am or Pressure3pm can be discarded without losing information.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1135: The variable Pressure9am can be discarded without risking losing information.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1136: Variables Rainfall and Pressure3pm are redundant, but we can’t say the same for the pair Pressure9am and Cloud3pm.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1137: Variables Temp3pm and Rainfall are redundant.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1138: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1139: Variable Temp3pm seems to be relevant for the majority of mining tasks.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1140: Variables Pressure9am and Cloud3pm seem to be useful for classification tasks.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1141: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1142: Removing variable Cloud9am might improve the training of decision trees .</p>
    <img src="images/weatherAUS_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1143: There is evidence in favour for sequential backward selection to select variable Cloud9am previously than variable Pressure9am.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1144: Variable Pressure9am is balanced.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1145: Those boxplots show that the data is not normalized.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1146: It is clear that variable Cloud9am shows some outliers, but we can’t be sure of the same for variable WindSpeed9am.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1147: Outliers seem to be a problem in the dataset.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1148: Variable Rainfall shows a high number of outlier values.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1149: Variable Cloud9am doesn’t have any outliers.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1150: Variable Cloud9am presents some outliers.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1151: At least 60 of the variables present outliers.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1152: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1153: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1154: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1155: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1156: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/weatherAUS_boxplots.png" width="auto" height = "600"/> 
    <p>1157: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1158: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1159: The variable WindDir9am can be seen as ordinal.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1160: The variable WindDir3pm can be seen as ordinal without losing information.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1161: Considering the common semantics for Location and WindGustDir variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1162: Considering the common semantics for WindGustDir variable, dummification would be the most adequate encoding.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1163: The variable WindDir3pm can be coded as ordinal without losing information.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1164: Feature generation based on variable WindDir3pm seems to be promising.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1165: Feature generation based on the use of variable WindDir3pm wouldn’t be useful, but the use of Location seems to be promising.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1166: Given the usual semantics of RainToday variable, dummification would have been a better codification.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1167: It is better to drop the variable Location than removing all records with missing values.</p>
    <img src="images/weatherAUS_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1168: Not knowing the semantics of WindGustDir variable, dummification could have been a more adequate codification.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1169: Discarding variable Pressure9am would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1170: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1171: Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1172: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1173: Feature generation based on variable RainToday seems to be promising.</p>
    <img src="images/weatherAUS_mv.png" width="auto" height = "600"/> 
    <p>1174: It is better to drop the variable Cloud9am than removing all records with missing values.</p>
    <img src="images/weatherAUS_class_histogram.png" width="auto" height = "600"/> 
    <p>1175: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/weatherAUS_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1176: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/weatherAUS_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1177: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/weatherAUS_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1178: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1179: All variables, but the class, should be dealt with as date.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1180: The variable Rainfall can be seen as ordinal.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1181: The variable Pressure3pm can be seen as ordinal without losing information.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1182: Variable Cloud3pm is balanced.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1183: It is clear that variable Pressure9am shows some outliers, but we can’t be sure of the same for variable Rainfall.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1184: Outliers seem to be a problem in the dataset.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1185: Variable Pressure9am shows some outlier values.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1186: Variable Cloud3pm doesn’t have any outliers.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1187: Variable Pressure9am presents some outliers.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1188: At least 85 of the variables present outliers.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1189: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1190: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1191: Considering the common semantics for Pressure3pm and Rainfall variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1192: Considering the common semantics for Rainfall variable, dummification would be the most adequate encoding.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1193: The variable Cloud9am can be coded as ordinal without losing information.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1194: Feature generation based on variable Temp3pm seems to be promising.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1195: Feature generation based on the use of variable Cloud9am wouldn’t be useful, but the use of Rainfall seems to be promising.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1196: Given the usual semantics of WindSpeed9am variable, dummification would have been a better codification.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1197: It is better to drop the variable Rainfall than removing all records with missing values.</p>
    <img src="images/weatherAUS_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1198: Not knowing the semantics of Rainfall variable, dummification could have been a more adequate codification.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1199: The variable displacement discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1200: Variable displacement is one of the most relevant variables.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1201: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1202: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1203: The specificity for the presented tree is higher than 60%.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1204: The number of True Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1205: The number of True Negatives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1206: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1207: Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 2141.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1208: Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="images/car_insurance_decision_tree.png" width="auto" height = "600"/> 
    <p>1209: Considering that A=True<=>[displacement <= 1196.5] and B=True<=>[height <= 1519.0], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 686.</p>
    <img src="images/car_insurance_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1210: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/car_insurance_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1211: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/car_insurance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1212: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/car_insurance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1213: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/car_insurance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1214: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/car_insurance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1215: KNN is in overfitting for k larger than 5.</p>
    <img src="images/car_insurance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1216: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/car_insurance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1217: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/car_insurance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1218: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/car_insurance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1219: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/car_insurance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1220: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/car_insurance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1221: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/car_insurance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1222: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/car_insurance_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1223: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/car_insurance_pca.png" width="auto" height = "600"/> 
    <p>1224: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/car_insurance_pca.png" width="auto" height = "600"/> 
    <p>1225: Using the first 6 principal components would imply an error between 5 and 25%.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1226: The intrinsic dimensionality of this dataset is 6.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1227: One of the variables age_of_car or airbags can be discarded without losing information.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1228: The variable length can be discarded without risking losing information.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1229: Variables age_of_car and policy_tenure are redundant, but we can’t say the same for the pair height and length.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1230: Variables age_of_car and gross_weight are redundant.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1231: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1232: Variable height seems to be relevant for the majority of mining tasks.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1233: Variables gross_weight and width seem to be useful for classification tasks.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1234: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1235: Removing variable length might improve the training of decision trees .</p>
    <img src="images/car_insurance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1236: There is evidence in favour for sequential backward selection to select variable length previously than variable gross_weight.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1237: Variable height is balanced.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1238: Those boxplots show that the data is not normalized.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1239: It is clear that variable displacement shows some outliers, but we can’t be sure of the same for variable policy_tenure.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1240: Outliers seem to be a problem in the dataset.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1241: Variable airbags shows some outlier values.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1242: Variable width doesn’t have any outliers.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1243: Variable length presents some outliers.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1244: At least 50 of the variables present outliers.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1245: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1246: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1247: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1248: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1249: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/car_insurance_boxplots.png" width="auto" height = "600"/> 
    <p>1250: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1251: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1252: The variable segment can be seen as ordinal.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1253: The variable is_esc can be seen as ordinal without losing information.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1254: Considering the common semantics for segment and area_cluster variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1255: Considering the common semantics for max_torque variable, dummification would be the most adequate encoding.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1256: The variable max_torque can be coded as ordinal without losing information.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1257: Feature generation based on variable area_cluster seems to be promising.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1258: Feature generation based on the use of variable steering_type wouldn’t be useful, but the use of area_cluster seems to be promising.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1259: Given the usual semantics of model variable, dummification would have been a better codification.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1260: It is better to drop the variable steering_type than removing all records with missing values.</p>
    <img src="images/car_insurance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1261: Not knowing the semantics of is_esc variable, dummification could have been a more adequate codification.</p>
    <img src="images/car_insurance_class_histogram.png" width="auto" height = "600"/> 
    <p>1262: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/car_insurance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1263: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/car_insurance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1264: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/car_insurance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1265: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1266: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1267: The variable age_of_car can be seen as ordinal.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1268: The variable height can be seen as ordinal without losing information.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1269: Variable displacement is balanced.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1270: It is clear that variable displacement shows some outliers, but we can’t be sure of the same for variable age_of_car.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1271: Outliers seem to be a problem in the dataset.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1272: Variable displacement shows some outlier values.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1273: Variable width doesn’t have any outliers.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1274: Variable height presents some outliers.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1275: At least 60 of the variables present outliers.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1276: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1277: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1278: Considering the common semantics for displacement and policy_tenure variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1279: Considering the common semantics for length variable, dummification would be the most adequate encoding.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1280: The variable age_of_car can be coded as ordinal without losing information.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1281: Feature generation based on variable height seems to be promising.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1282: Feature generation based on the use of variable age_of_car wouldn’t be useful, but the use of policy_tenure seems to be promising.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1283: Given the usual semantics of age_of_policyholder variable, dummification would have been a better codification.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1284: It is better to drop the variable gross_weight than removing all records with missing values.</p>
    <img src="images/car_insurance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1285: Not knowing the semantics of displacement variable, dummification could have been a more adequate codification.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1286: The variable slope discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1287: Variable slope is one of the most relevant variables.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1288: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1289: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1290: The specificity for the presented tree is lower than 75%.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1291: The number of True Negatives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1292: The number of False Positives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1293: The precision for the presented tree is lower than its specificity.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1294: Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1295: Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="images/heart_decision_tree.png" width="auto" height = "600"/> 
    <p>1296: Considering that A=True<=>[slope <= 1.5] and B=True<=>[restecg <= 0.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as 0.</p>
    <img src="images/heart_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1297: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/heart_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1298: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/heart_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1299: Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.</p>
    <img src="images/heart_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1300: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/heart_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1301: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/heart_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1302: KNN is in overfitting for k less than 17.</p>
    <img src="images/heart_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1303: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/heart_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1304: KNN with more than 7 neighbours is in overfitting.</p>
    <img src="images/heart_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1305: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/heart_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1306: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/heart_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1307: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/heart_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1308: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/heart_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1309: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/heart_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1310: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/heart_pca.png" width="auto" height = "600"/> 
    <p>1311: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/heart_pca.png" width="auto" height = "600"/> 
    <p>1312: Using the first 9 principal components would imply an error between 15 and 20%.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1313: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1314: One of the variables restecg or age can be discarded without losing information.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1315: The variable trestbps can be discarded without risking losing information.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1316: Variables cp and age are redundant, but we can’t say the same for the pair ca and trestbps.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1317: Variables restecg and oldpeak are redundant.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1318: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1319: Variable thalach seems to be relevant for the majority of mining tasks.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1320: Variables cp and chol seem to be useful for classification tasks.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1321: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1322: Removing variable age might improve the training of decision trees .</p>
    <img src="images/heart_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1323: There is evidence in favour for sequential backward selection to select variable restecg previously than variable slope.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1324: Variable thal is balanced.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1325: Those boxplots show that the data is not normalized.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1326: It is clear that variable trestbps shows some outliers, but we can’t be sure of the same for variable restecg.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1327: Outliers seem to be a problem in the dataset.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1328: Variable chol shows some outlier values.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1329: Variable restecg doesn’t have any outliers.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1330: Variable restecg presents some outliers.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1331: At least 85 of the variables present outliers.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1332: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1333: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1334: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1335: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1336: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/heart_boxplots.png" width="auto" height = "600"/> 
    <p>1337: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1338: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1339: The variable sex can be seen as ordinal.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1340: The variable sex can be seen as ordinal without losing information.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1341: Considering the common semantics for fbs and sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1342: Considering the common semantics for sex variable, dummification would be the most adequate encoding.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1343: The variable sex can be coded as ordinal without losing information.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1344: Feature generation based on variable exang seems to be promising.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1345: Feature generation based on the use of variable exang wouldn’t be useful, but the use of sex seems to be promising.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1346: Given the usual semantics of sex variable, dummification would have been a better codification.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1347: It is better to drop the variable exang than removing all records with missing values.</p>
    <img src="images/heart_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1348: Not knowing the semantics of sex variable, dummification could have been a more adequate codification.</p>
    <img src="images/heart_class_histogram.png" width="auto" height = "600"/> 
    <p>1349: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/heart_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1350: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/heart_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1351: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/heart_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1352: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1353: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1354: The variable chol can be seen as ordinal.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1355: The variable age can be seen as ordinal without losing information.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1356: Variable restecg is balanced.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1357: It is clear that variable chol shows some outliers, but we can’t be sure of the same for variable age.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1358: Outliers seem to be a problem in the dataset.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1359: Variable age shows some outlier values.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1360: Variable chol doesn’t have any outliers.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1361: Variable ca presents some outliers.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1362: At least 50 of the variables present outliers.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1363: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1364: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1365: Considering the common semantics for chol and age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1366: Considering the common semantics for restecg variable, dummification would be the most adequate encoding.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1367: The variable thal can be coded as ordinal without losing information.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1368: Feature generation based on variable cp seems to be promising.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1369: Feature generation based on the use of variable thalach wouldn’t be useful, but the use of age seems to be promising.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1370: Given the usual semantics of restecg variable, dummification would have been a better codification.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1371: It is better to drop the variable trestbps than removing all records with missing values.</p>
    <img src="images/heart_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1372: Not knowing the semantics of trestbps variable, dummification could have been a more adequate codification.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1373: The variable texture_worst discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1374: Variable texture_worst is one of the most relevant variables.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1375: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1376: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1377: The recall for the presented tree is higher than 60%.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1378: The number of False Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1379: The number of False Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1380: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1381: Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that Naive Bayes algorithm classifies (A, not B), as M.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1382: Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that Naive Bayes algorithm classifies (not A, B), as M.</p>
    <img src="images/Breast_Cancer_decision_tree.png" width="auto" height = "600"/> 
    <p>1383: Considering that A=True<=>[perimeter_mean <= 90.47] and B=True<=>[texture_worst <= 27.89], it is possible to state that KNN algorithm classifies (not A, B) as M for any k ≤ 20.</p>
    <img src="images/Breast_Cancer_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1384: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Breast_Cancer_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1385: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/Breast_Cancer_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1386: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/Breast_Cancer_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1387: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Breast_Cancer_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1388: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Breast_Cancer_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1389: KNN is in overfitting for k larger than 5.</p>
    <img src="images/Breast_Cancer_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1390: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/Breast_Cancer_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1391: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/Breast_Cancer_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1392: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/Breast_Cancer_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1393: According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.</p>
    <img src="images/Breast_Cancer_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1394: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/Breast_Cancer_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1395: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/Breast_Cancer_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1396: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/Breast_Cancer_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1397: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Breast_Cancer_pca.png" width="auto" height = "600"/> 
    <p>1398: The first 6 principal components are enough for explaining half the data variance.</p>
    <img src="images/Breast_Cancer_pca.png" width="auto" height = "600"/> 
    <p>1399: Using the first 6 principal components would imply an error between 10 and 30%.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1400: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1401: One of the variables symmetry_se or area_se can be discarded without losing information.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1402: The variable perimeter_worst can be discarded without risking losing information.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1403: Variables texture_worst and radius_worst are redundant, but we can’t say the same for the pair perimeter_worst and texture_se.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1404: Variables texture_worst and perimeter_se are redundant.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1405: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1406: Variable area_se seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1407: Variables symmetry_se and perimeter_mean seem to be useful for classification tasks.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1408: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1409: Removing variable texture_se might improve the training of decision trees .</p>
    <img src="images/Breast_Cancer_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1410: There is evidence in favour for sequential backward selection to select variable texture_mean previously than variable perimeter_se.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1411: Variable perimeter_se is balanced.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1412: Those boxplots show that the data is not normalized.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1413: It is clear that variable texture_mean shows some outliers, but we can’t be sure of the same for variable perimeter_se.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1414: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1415: Variable texture_se shows a high number of outlier values.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1416: Variable texture_mean doesn’t have any outliers.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1417: Variable radius_worst presents some outliers.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1418: At least 60 of the variables present outliers.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1419: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1420: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1421: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1422: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1423: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Breast_Cancer_boxplots.png" width="auto" height = "600"/> 
    <p>1424: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Breast_Cancer_class_histogram.png" width="auto" height = "600"/> 
    <p>1425: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Breast_Cancer_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1426: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/Breast_Cancer_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1427: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Breast_Cancer_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1428: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1429: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1430: The variable perimeter_mean can be seen as ordinal.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1431: The variable radius_worst can be seen as ordinal without losing information.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1432: Variable texture_se is balanced.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1433: It is clear that variable radius_worst shows some outliers, but we can’t be sure of the same for variable perimeter_worst.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1434: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1435: Variable smoothness_se shows some outlier values.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1436: Variable smoothness_se doesn’t have any outliers.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1437: Variable texture_worst presents some outliers.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1438: At least 60 of the variables present outliers.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1439: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1440: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1441: Considering the common semantics for perimeter_mean and texture_mean variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1442: Considering the common semantics for area_se variable, dummification would be the most adequate encoding.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1443: The variable smoothness_se can be coded as ordinal without losing information.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1444: Feature generation based on variable perimeter_worst seems to be promising.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1445: Feature generation based on the use of variable area_se wouldn’t be useful, but the use of texture_mean seems to be promising.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1446: Given the usual semantics of perimeter_worst variable, dummification would have been a better codification.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1447: It is better to drop the variable texture_mean than removing all records with missing values.</p>
    <img src="images/Breast_Cancer_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1448: Not knowing the semantics of smoothness_se variable, dummification could have been a more adequate codification.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1449: The variable Prior_purchases discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1450: Variable Prior_purchases is one of the most relevant variables.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1451: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1452: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1453: The accuracy for the presented tree is lower than 60%.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1454: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1455: The number of True Negatives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1456: The accuracy for the presented tree is higher than 60%.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1457: Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (A,B) as Yes for any k ≤ 1596.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1458: Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (not A, B) as No for any k ≤ 3657.</p>
    <img src="images/e-commerce_decision_tree.png" width="auto" height = "600"/> 
    <p>1459: Considering that A=True<=>[Prior_purchases <= 3.5] and B=True<=>[Customer_care_calls <= 4.5], it is possible to state that KNN algorithm classifies (not A, B) as No for any k ≤ 1596.</p>
    <img src="images/e-commerce_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1460: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/e-commerce_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1461: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/e-commerce_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1462: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/e-commerce_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1463: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/e-commerce_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1464: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/e-commerce_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1465: KNN is in overfitting for k less than 17.</p>
    <img src="images/e-commerce_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1466: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/e-commerce_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1467: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/e-commerce_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1468: We are able to identify the existence of overfitting for KNN models with less than 6 neighbors.</p>
    <img src="images/e-commerce_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1469: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/e-commerce_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1470: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/e-commerce_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1471: The decision tree is in overfitting for depths above 5.</p>
    <img src="images/e-commerce_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1472: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/e-commerce_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1473: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/e-commerce_pca.png" width="auto" height = "600"/> 
    <p>1474: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/e-commerce_pca.png" width="auto" height = "600"/> 
    <p>1475: Using the first 4 principal components would imply an error between 10 and 25%.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1476: The intrinsic dimensionality of this dataset is 5.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1477: One of the variables Discount_offered or Prior_purchases can be discarded without losing information.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1478: The variable Customer_rating can be discarded without risking losing information.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1479: Variables Customer_care_calls and Cost_of_the_Product are redundant.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1480: Variables Prior_purchases and Cost_of_the_Product are redundant.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1481: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1482: Variable Discount_offered seems to be relevant for the majority of mining tasks.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1483: Variables Weight_in_gms and Prior_purchases seem to be useful for classification tasks.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1484: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1485: Removing variable Cost_of_the_Product might improve the training of decision trees .</p>
    <img src="images/e-commerce_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1486: There is evidence in favour for sequential backward selection to select variable Discount_offered previously than variable Cost_of_the_Product.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1487: Variable Discount_offered is balanced.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1488: Those boxplots show that the data is not normalized.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1489: It is clear that variable Customer_rating shows some outliers, but we can’t be sure of the same for variable Prior_purchases.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1490: Outliers seem to be a problem in the dataset.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1491: Variable Discount_offered shows some outlier values.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1492: Variable Customer_rating doesn’t have any outliers.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1493: Variable Prior_purchases presents some outliers.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1494: At least 60 of the variables present outliers.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1495: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1496: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1497: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1498: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1499: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/e-commerce_boxplots.png" width="auto" height = "600"/> 
    <p>1500: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1501: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1502: The variable Warehouse_block can be seen as ordinal.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1503: The variable Product_importance can be seen as ordinal without losing information.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1504: Considering the common semantics for Mode_of_Shipment and Warehouse_block variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1505: Considering the common semantics for Mode_of_Shipment variable, dummification would be the most adequate encoding.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1506: The variable Product_importance can be coded as ordinal without losing information.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1507: Feature generation based on variable Gender seems to be promising.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1508: Feature generation based on the use of variable Warehouse_block wouldn’t be useful, but the use of Mode_of_Shipment seems to be promising.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1509: Given the usual semantics of Warehouse_block variable, dummification would have been a better codification.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1510: It is better to drop the variable Product_importance than removing all records with missing values.</p>
    <img src="images/e-commerce_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1511: Not knowing the semantics of Product_importance variable, dummification could have been a more adequate codification.</p>
    <img src="images/e-commerce_class_histogram.png" width="auto" height = "600"/> 
    <p>1512: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/e-commerce_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1513: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/e-commerce_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1514: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/e-commerce_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1515: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1516: All variables, but the class, should be dealt with as date.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1517: The variable Weight_in_gms can be seen as ordinal.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1518: The variable Weight_in_gms can be seen as ordinal without losing information.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1519: Variable Customer_care_calls is balanced.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1520: It is clear that variable Discount_offered shows some outliers, but we can’t be sure of the same for variable Customer_care_calls.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1521: Outliers seem to be a problem in the dataset.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1522: Variable Prior_purchases shows a high number of outlier values.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1523: Variable Prior_purchases doesn’t have any outliers.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1524: Variable Discount_offered presents some outliers.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1525: At least 85 of the variables present outliers.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1526: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1527: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1528: Considering the common semantics for Prior_purchases and Customer_care_calls variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1529: Considering the common semantics for Customer_care_calls variable, dummification would be the most adequate encoding.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1530: The variable Customer_care_calls can be coded as ordinal without losing information.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1531: Feature generation based on variable Discount_offered seems to be promising.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1532: Feature generation based on the use of variable Discount_offered wouldn’t be useful, but the use of Customer_care_calls seems to be promising.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1533: Given the usual semantics of Discount_offered variable, dummification would have been a better codification.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1534: It is better to drop the variable Discount_offered than removing all records with missing values.</p>
    <img src="images/e-commerce_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1535: Not knowing the semantics of Cost_of_the_Product variable, dummification could have been a more adequate codification.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1536: The variable Rotational speed [rpm] discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1537: Variable Rotational speed [rpm] is one of the most relevant variables.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1538: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1539: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1540: The precision for the presented tree is lower than 60%.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1541: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1542: The number of True Negatives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1543: The number of True Negatives reported in the same tree is 50.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1544: Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 5990.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1545: Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 46.</p>
    <img src="images/maintenance_decision_tree.png" width="auto" height = "600"/> 
    <p>1546: Considering that A=True<=>[Rotational speed [rpm] <= 1381.5] and B=True<=>[Torque [Nm] <= 65.05], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 46.</p>
    <img src="images/maintenance_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1547: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/maintenance_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1548: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/maintenance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1549: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/maintenance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1550: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/maintenance_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1551: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/maintenance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1552: KNN is in overfitting for k larger than 13.</p>
    <img src="images/maintenance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1553: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/maintenance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1554: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/maintenance_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1555: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/maintenance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1556: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/maintenance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1557: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/maintenance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1558: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/maintenance_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1559: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/maintenance_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1560: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/maintenance_pca.png" width="auto" height = "600"/> 
    <p>1561: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/maintenance_pca.png" width="auto" height = "600"/> 
    <p>1562: Using the first 2 principal components would imply an error between 10 and 25%.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1563: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1564: One of the variables Process temperature [K] or Torque [Nm] can be discarded without losing information.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1565: The variable Rotational speed [rpm] can be discarded without risking losing information.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1566: Variables Air temperature [K] and Tool wear [min] seem to be useful for classification tasks.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1567: Variables Rotational speed [rpm] and Process temperature [K] are redundant.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1568: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1569: Variable Tool wear [min] seems to be relevant for the majority of mining tasks.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1570: Variables Torque [Nm] and Tool wear [min] seem to be useful for classification tasks.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1571: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1572: Removing variable Torque [Nm] might improve the training of decision trees .</p>
    <img src="images/maintenance_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1573: There is evidence in favour for sequential backward selection to select variable Rotational speed [rpm] previously than variable Torque [Nm].</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1574: Variable Process temperature [K] is balanced.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1575: Those boxplots show that the data is not normalized.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1576: It is clear that variable Rotational speed [rpm] shows some outliers, but we can’t be sure of the same for variable Torque [Nm].</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1577: Outliers seem to be a problem in the dataset.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1578: Variable Tool wear [min] shows a high number of outlier values.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1579: Variable Air temperature [K] doesn’t have any outliers.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1580: Variable Tool wear [min] presents some outliers.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1581: At least 85 of the variables present outliers.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1582: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1583: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1584: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1585: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1586: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/maintenance_boxplots.png" width="auto" height = "600"/> 
    <p>1587: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1588: All variables, but the class, should be dealt with as date.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1589: The variable TWF can be seen as ordinal.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1590: The variable HDF can be seen as ordinal without losing information.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1591: Considering the common semantics for PWF and Type variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1592: Considering the common semantics for Type variable, dummification would be the most adequate encoding.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1593: The variable Type can be coded as ordinal without losing information.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1594: Feature generation based on variable OSF seems to be promising.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1595: Feature generation based on the use of variable RNF wouldn’t be useful, but the use of Type seems to be promising.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1596: Given the usual semantics of OSF variable, dummification would have been a better codification.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1597: It is better to drop the variable PWF than removing all records with missing values.</p>
    <img src="images/maintenance_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1598: Not knowing the semantics of RNF variable, dummification could have been a more adequate codification.</p>
    <img src="images/maintenance_class_histogram.png" width="auto" height = "600"/> 
    <p>1599: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/maintenance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1600: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/maintenance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1601: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/maintenance_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1602: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1603: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1604: The variable Rotational speed [rpm] can be seen as ordinal.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1605: The variable Air temperature [K] can be seen as ordinal without losing information.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1606: Variable Rotational speed [rpm] is balanced.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1607: It is clear that variable Air temperature [K] shows some outliers, but we can’t be sure of the same for variable Torque [Nm].</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1608: Outliers seem to be a problem in the dataset.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1609: Variable Torque [Nm] shows some outlier values.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1610: Variable Air temperature [K] doesn’t have any outliers.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1611: Variable Process temperature [K] presents some outliers.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1612: At least 85 of the variables present outliers.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1613: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1614: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1615: Considering the common semantics for Torque [Nm] and Air temperature [K] variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1616: Considering the common semantics for Torque [Nm] variable, dummification would be the most adequate encoding.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1617: The variable Rotational speed [rpm] can be coded as ordinal without losing information.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1618: Feature generation based on variable Rotational speed [rpm] seems to be promising.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1619: Feature generation based on the use of variable Air temperature [K] wouldn’t be useful, but the use of Process temperature [K] seems to be promising.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1620: Given the usual semantics of Rotational speed [rpm] variable, dummification would have been a better codification.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1621: It is better to drop the variable Process temperature [K] than removing all records with missing values.</p>
    <img src="images/maintenance_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1622: Not knowing the semantics of Tool wear [min] variable, dummification could have been a more adequate codification.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1623: The variable NumOfProducts discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1624: Variable Age is one of the most relevant variables.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1625: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1626: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1627: The specificity for the presented tree is lower than 90%.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1628: The number of True Positives reported in the same tree is 50.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1629: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1630: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1631: Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 124.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1632: Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.</p>
    <img src="images/Churn_Modelling_decision_tree.png" width="auto" height = "600"/> 
    <p>1633: Considering that A=True<=>[Age <= 42.5] and B=True<=>[NumOfProducts <= 2.5], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 1.</p>
    <img src="images/Churn_Modelling_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1634: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Churn_Modelling_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1635: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/Churn_Modelling_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1636: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/Churn_Modelling_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1637: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Churn_Modelling_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1638: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Churn_Modelling_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1639: KNN is in overfitting for k larger than 13.</p>
    <img src="images/Churn_Modelling_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1640: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/Churn_Modelling_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1641: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/Churn_Modelling_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1642: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/Churn_Modelling_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1643: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Churn_Modelling_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1644: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/Churn_Modelling_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1645: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/Churn_Modelling_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1646: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/Churn_Modelling_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1647: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Churn_Modelling_pca.png" width="auto" height = "600"/> 
    <p>1648: The first 5 principal components are enough for explaining half the data variance.</p>
    <img src="images/Churn_Modelling_pca.png" width="auto" height = "600"/> 
    <p>1649: Using the first 5 principal components would imply an error between 10 and 25%.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1650: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1651: One of the variables EstimatedSalary or NumOfProducts can be discarded without losing information.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1652: The variable EstimatedSalary can be discarded without risking losing information.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1653: Variables Age and CreditScore are redundant, but we can’t say the same for the pair Tenure and NumOfProducts.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1654: Variables NumOfProducts and CreditScore are redundant.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1655: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1656: Variable EstimatedSalary seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1657: Variables NumOfProducts and CreditScore seem to be useful for classification tasks.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1658: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1659: Removing variable Balance might improve the training of decision trees .</p>
    <img src="images/Churn_Modelling_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1660: There is evidence in favour for sequential backward selection to select variable Tenure previously than variable CreditScore.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1661: Variable Tenure is balanced.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1662: Those boxplots show that the data is not normalized.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1663: It is clear that variable Tenure shows some outliers, but we can’t be sure of the same for variable NumOfProducts.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1664: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1665: Variable EstimatedSalary shows some outlier values.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1666: Variable EstimatedSalary doesn’t have any outliers.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1667: Variable Age presents some outliers.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1668: At least 60 of the variables present outliers.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1669: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1670: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1671: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1672: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1673: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Churn_Modelling_boxplots.png" width="auto" height = "600"/> 
    <p>1674: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1675: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1676: The variable IsActiveMember can be seen as ordinal.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1677: The variable Gender can be seen as ordinal without losing information.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1678: Considering the common semantics for Gender and Geography variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1679: Considering the common semantics for IsActiveMember variable, dummification would be the most adequate encoding.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1680: The variable IsActiveMember can be coded as ordinal without losing information.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1681: Feature generation based on variable IsActiveMember seems to be promising.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1682: Feature generation based on the use of variable Gender wouldn’t be useful, but the use of Geography seems to be promising.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1683: Given the usual semantics of Geography variable, dummification would have been a better codification.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1684: It is better to drop the variable Gender than removing all records with missing values.</p>
    <img src="images/Churn_Modelling_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1685: Not knowing the semantics of Gender variable, dummification could have been a more adequate codification.</p>
    <img src="images/Churn_Modelling_class_histogram.png" width="auto" height = "600"/> 
    <p>1686: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Churn_Modelling_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1687: Given the number of records and that some variables are symbolic, we might be facing the curse of dimensionality.</p>
    <img src="images/Churn_Modelling_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1688: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Churn_Modelling_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1689: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1690: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1691: The variable Age can be seen as ordinal.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1692: The variable EstimatedSalary can be seen as ordinal without losing information.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1693: Variable Age is balanced.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1694: It is clear that variable NumOfProducts shows some outliers, but we can’t be sure of the same for variable Tenure.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1695: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1696: Variable EstimatedSalary shows some outlier values.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1697: Variable Age doesn’t have any outliers.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1698: Variable NumOfProducts presents some outliers.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1699: At least 85 of the variables present outliers.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1700: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1701: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1702: Considering the common semantics for NumOfProducts and CreditScore variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1703: Considering the common semantics for Balance variable, dummification would be the most adequate encoding.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1704: The variable CreditScore can be coded as ordinal without losing information.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1705: Feature generation based on variable Age seems to be promising.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1706: Feature generation based on the use of variable EstimatedSalary wouldn’t be useful, but the use of CreditScore seems to be promising.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1707: Given the usual semantics of CreditScore variable, dummification would have been a better codification.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1708: It is better to drop the variable EstimatedSalary than removing all records with missing values.</p>
    <img src="images/Churn_Modelling_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1709: Not knowing the semantics of CreditScore variable, dummification could have been a more adequate codification.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1710: The variable MAJORSKEWNESS discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1711: Variable MAJORSKEWNESS is one of the most relevant variables.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1712: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1713: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1714: The accuracy for the presented tree is lower than 90%.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1715: The number of False Negatives is lower than the number of True Positives for the presented tree.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1716: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1717: The variable MAJORSKEWNESS seems to be one of the five most relevant features.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1718: Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 4.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1719: Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that KNN algorithm classifies (A, not B) as 2 for any k ≤ 3.</p>
    <img src="images/vehicle_decision_tree.png" width="auto" height = "600"/> 
    <p>1720: Considering that A=True<=>[MAJORSKEWNESS <= 74.5] and B=True<=>[CIRCULARITY <= 49.5], it is possible to state that KNN algorithm classifies (A,B) as 4 for any k ≤ 3.</p>
    <img src="images/vehicle_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1721: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/vehicle_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1722: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/vehicle_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1723: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/vehicle_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1724: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/vehicle_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1725: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/vehicle_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1726: KNN is in overfitting for k larger than 17.</p>
    <img src="images/vehicle_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1727: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/vehicle_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1728: KNN with more than 7 neighbours is in overfitting.</p>
    <img src="images/vehicle_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1729: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/vehicle_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1730: According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.</p>
    <img src="images/vehicle_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1731: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/vehicle_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1732: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/vehicle_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1733: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/vehicle_pca.png" width="auto" height = "600"/> 
    <p>1734: The first 10 principal components are enough for explaining half the data variance.</p>
    <img src="images/vehicle_pca.png" width="auto" height = "600"/> 
    <p>1735: Using the first 8 principal components would imply an error between 5 and 20%.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1736: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1737: One of the variables MAJORSKEWNESS or CIRCULARITY can be discarded without losing information.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1738: The variable GYRATIONRADIUS can be discarded without risking losing information.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1739: Variables CIRCULARITY and COMPACTNESS are redundant, but we can’t say the same for the pair MINORVARIANCE and MAJORVARIANCE.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1740: Variables MINORVARIANCE and MINORKURTOSIS are redundant.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1741: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1742: Variable MAJORVARIANCE seems to be relevant for the majority of mining tasks.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1743: Variables MINORKURTOSIS and MINORSKEWNESS seem to be useful for classification tasks.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1744: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1745: Removing variable MAJORKURTOSIS might improve the training of decision trees .</p>
    <img src="images/vehicle_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1746: There is evidence in favour for sequential backward selection to select variable MINORKURTOSIS previously than variable MAJORSKEWNESS.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1747: Variable COMPACTNESS is balanced.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1748: Those boxplots show that the data is not normalized.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1749: It is clear that variable MINORSKEWNESS shows some outliers, but we can’t be sure of the same for variable MINORVARIANCE.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1750: Outliers seem to be a problem in the dataset.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1751: Variable MINORKURTOSIS shows some outlier values.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1752: Variable COMPACTNESS doesn’t have any outliers.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1753: Variable CIRCULARITY presents some outliers.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1754: At least 75 of the variables present outliers.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1755: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1756: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1757: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1758: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1759: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/vehicle_boxplots.png" width="auto" height = "600"/> 
    <p>1760: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/vehicle_class_histogram.png" width="auto" height = "600"/> 
    <p>1761: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/vehicle_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1762: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/vehicle_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1763: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/vehicle_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1764: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1765: All variables, but the class, should be dealt with as date.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1766: The variable MINORSKEWNESS can be seen as ordinal.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1767: The variable GYRATIONRADIUS can be seen as ordinal without losing information.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1768: Variable COMPACTNESS is balanced.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1769: It is clear that variable MAJORSKEWNESS shows some outliers, but we can’t be sure of the same for variable MAJORVARIANCE.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1770: Outliers seem to be a problem in the dataset.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1771: Variable MINORKURTOSIS shows a high number of outlier values.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1772: Variable MINORSKEWNESS doesn’t have any outliers.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1773: Variable CIRCULARITY presents some outliers.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1774: At least 60 of the variables present outliers.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1775: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1776: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1777: Considering the common semantics for RADIUS RATIO and COMPACTNESS variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1778: Considering the common semantics for MINORKURTOSIS variable, dummification would be the most adequate encoding.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1779: The variable DISTANCE CIRCULARITY can be coded as ordinal without losing information.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1780: Feature generation based on variable GYRATIONRADIUS seems to be promising.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1781: Feature generation based on the use of variable MAJORSKEWNESS wouldn’t be useful, but the use of COMPACTNESS seems to be promising.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1782: Given the usual semantics of GYRATIONRADIUS variable, dummification would have been a better codification.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1783: It is better to drop the variable COMPACTNESS than removing all records with missing values.</p>
    <img src="images/vehicle_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1784: Not knowing the semantics of MAJORSKEWNESS variable, dummification could have been a more adequate codification.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1785: The variable capital-loss discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1786: Variable hours-per-week is one of the most relevant variables.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1787: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1788: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1789: The accuracy for the presented tree is higher than 60%.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1790: The number of True Negatives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1791: The number of False Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1792: The number of False Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1793: Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as >50K.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1794: Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that KNN algorithm classifies (A, not B) as >50K for any k ≤ 541.</p>
    <img src="images/adult_decision_tree.png" width="auto" height = "600"/> 
    <p>1795: Considering that A=True<=>[hours-per-week <= 41.5] and B=True<=>[capital-loss <= 1820.5], it is possible to state that KNN algorithm classifies (not A, B) as >50K for any k ≤ 21974.</p>
    <img src="images/adult_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1796: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/adult_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1797: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/adult_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1798: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/adult_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1799: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/adult_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1800: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/adult_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1801: KNN is in overfitting for k larger than 17.</p>
    <img src="images/adult_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1802: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/adult_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1803: KNN with more than 7 neighbours is in overfitting.</p>
    <img src="images/adult_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1804: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/adult_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1805: According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.</p>
    <img src="images/adult_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1806: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/adult_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1807: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/adult_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1808: We are able to identify the existence of overfitting for decision tree models with more than 4 nodes of depth.</p>
    <img src="images/adult_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1809: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/adult_pca.png" width="auto" height = "600"/> 
    <p>1810: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/adult_pca.png" width="auto" height = "600"/> 
    <p>1811: Using the first 5 principal components would imply an error between 15 and 30%.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1812: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1813: One of the variables fnlwgt or hours-per-week can be discarded without losing information.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1814: The variable hours-per-week can be discarded without risking losing information.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1815: Variables capital-loss and age are redundant.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1816: Variables age and educational-num are redundant.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1817: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1818: Variable capital-gain seems to be relevant for the majority of mining tasks.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1819: Variables fnlwgt and age seem to be useful for classification tasks.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1820: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1821: Removing variable fnlwgt might improve the training of decision trees .</p>
    <img src="images/adult_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1822: There is evidence in favour for sequential backward selection to select variable capital-gain previously than variable fnlwgt.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1823: Variable hours-per-week is balanced.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1824: Those boxplots show that the data is not normalized.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1825: It is clear that variable educational-num shows some outliers, but we can’t be sure of the same for variable fnlwgt.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1826: Outliers seem to be a problem in the dataset.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1827: Variable capital-loss shows a high number of outlier values.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1828: Variable capital-gain doesn’t have any outliers.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1829: Variable capital-gain presents some outliers.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1830: At least 75 of the variables present outliers.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1831: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1832: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1833: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1834: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1835: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/adult_boxplots.png" width="auto" height = "600"/> 
    <p>1836: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1837: All variables, but the class, should be dealt with as date.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1838: The variable gender can be seen as ordinal.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1839: The variable education can be seen as ordinal without losing information.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1840: Considering the common semantics for marital-status and workclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1841: Considering the common semantics for marital-status variable, dummification would be the most adequate encoding.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1842: The variable education can be coded as ordinal without losing information.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1843: Feature generation based on variable marital-status seems to be promising.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1844: Feature generation based on the use of variable occupation wouldn’t be useful, but the use of workclass seems to be promising.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1845: Given the usual semantics of education variable, dummification would have been a better codification.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1846: It is better to drop the variable relationship than removing all records with missing values.</p>
    <img src="images/adult_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1847: Not knowing the semantics of occupation variable, dummification could have been a more adequate codification.</p>
    <img src="images/adult_class_histogram.png" width="auto" height = "600"/> 
    <p>1848: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/adult_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1849: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/adult_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1850: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/adult_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1851: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1852: All variables, but the class, should be dealt with as date.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1853: The variable fnlwgt can be seen as ordinal.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1854: The variable hours-per-week can be seen as ordinal without losing information.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1855: Variable fnlwgt is balanced.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1856: It is clear that variable educational-num shows some outliers, but we can’t be sure of the same for variable capital-loss.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1857: Outliers seem to be a problem in the dataset.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1858: Variable educational-num shows some outlier values.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1859: Variable capital-loss doesn’t have any outliers.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1860: Variable age presents some outliers.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1861: At least 85 of the variables present outliers.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1862: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1863: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1864: Considering the common semantics for capital-gain and age variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1865: Considering the common semantics for fnlwgt variable, dummification would be the most adequate encoding.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1866: The variable educational-num can be coded as ordinal without losing information.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1867: Feature generation based on variable educational-num seems to be promising.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1868: Feature generation based on the use of variable capital-loss wouldn’t be useful, but the use of age seems to be promising.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1869: Given the usual semantics of capital-gain variable, dummification would have been a better codification.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1870: It is better to drop the variable hours-per-week than removing all records with missing values.</p>
    <img src="images/adult_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1871: Not knowing the semantics of fnlwgt variable, dummification could have been a more adequate codification.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1872: The variable ASHTMA discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1873: Variable ASHTMA is one of the most relevant variables.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1874: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1875: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1876: The precision for the presented tree is higher than 75%.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1877: The number of False Positives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1878: The number of True Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1879: The recall for the presented tree is lower than 90%.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1880: Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (not A, B) as Yes for any k ≤ 46.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1881: Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (A,B) as No for any k ≤ 7971.</p>
    <img src="images/Covid_Data_decision_tree.png" width="auto" height = "600"/> 
    <p>1882: Considering that A=True<=>[CARDIOVASCULAR <= 50.0] and B=True<=>[ASHTMA <= 1.5], it is possible to state that KNN algorithm classifies (A, not B) as Yes for any k ≤ 173.</p>
    <img src="images/Covid_Data_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1883: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/Covid_Data_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1884: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/Covid_Data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1885: Results for Random Forests identified as 3, may be explained by its estimators being in underfitting.</p>
    <img src="images/Covid_Data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1886: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Covid_Data_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1887: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/Covid_Data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1888: KNN is in overfitting for k larger than 13.</p>
    <img src="images/Covid_Data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1889: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/Covid_Data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1890: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/Covid_Data_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1891: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/Covid_Data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1892: According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.</p>
    <img src="images/Covid_Data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1893: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/Covid_Data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1894: The decision tree is in overfitting for depths above 4.</p>
    <img src="images/Covid_Data_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1895: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Covid_Data_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>1896: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Covid_Data_pca.png" width="auto" height = "600"/> 
    <p>1897: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/Covid_Data_pca.png" width="auto" height = "600"/> 
    <p>1898: Using the first 11 principal components would imply an error between 15 and 25%.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1899: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1900: One of the variables HIPERTENSION or RENAL_CHRONIC can be discarded without losing information.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1901: The variable MEDICAL_UNIT can be discarded without risking losing information.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1902: Variables PREGNANT and TOBACCO are redundant, but we can’t say the same for the pair MEDICAL_UNIT and ASTHMA.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1903: Variables COPD and AGE are redundant.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1904: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1905: Variable ICU seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1906: Variables HIPERTENSION and TOBACCO seem to be useful for classification tasks.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1907: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1908: Removing variable COPD might improve the training of decision trees .</p>
    <img src="images/Covid_Data_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1909: There is evidence in favour for sequential backward selection to select variable ICU previously than variable PREGNANT.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1910: Variable OTHER_DISEASE is balanced.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1911: Those boxplots show that the data is not normalized.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1912: It is clear that variable ASTHMA shows some outliers, but we can’t be sure of the same for variable COPD.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1913: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1914: Variable AGE shows some outlier values.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1915: Variable ASTHMA doesn’t have any outliers.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1916: Variable OTHER_DISEASE presents some outliers.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1917: At least 75 of the variables present outliers.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1918: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1919: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1920: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1921: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1922: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Covid_Data_boxplots.png" width="auto" height = "600"/> 
    <p>1923: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1924: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1925: The variable PATIENT_TYPE can be seen as ordinal.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1926: The variable USMER can be seen as ordinal without losing information.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1927: Considering the common semantics for USMER and SEX variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1928: Considering the common semantics for PATIENT_TYPE variable, dummification would be the most adequate encoding.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1929: The variable PATIENT_TYPE can be coded as ordinal without losing information.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1930: Feature generation based on variable SEX seems to be promising.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1931: Feature generation based on the use of variable PATIENT_TYPE wouldn’t be useful, but the use of USMER seems to be promising.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1932: Given the usual semantics of PATIENT_TYPE variable, dummification would have been a better codification.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1933: It is better to drop the variable PATIENT_TYPE than removing all records with missing values.</p>
    <img src="images/Covid_Data_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>1934: Not knowing the semantics of SEX variable, dummification could have been a more adequate codification.</p>
    <img src="images/Covid_Data_class_histogram.png" width="auto" height = "600"/> 
    <p>1935: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Covid_Data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1936: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Covid_Data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1937: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Covid_Data_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>1938: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1939: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1940: The variable TOBACCO can be seen as ordinal.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1941: The variable MEDICAL_UNIT can be seen as ordinal without losing information.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1942: Variable ICU is balanced.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1943: It is clear that variable RENAL_CHRONIC shows some outliers, but we can’t be sure of the same for variable ICU.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1944: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1945: Variable OTHER_DISEASE shows some outlier values.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1946: Variable MEDICAL_UNIT doesn’t have any outliers.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1947: Variable PREGNANT presents some outliers.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1948: At least 85 of the variables present outliers.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1949: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1950: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1951: Considering the common semantics for COPD and MEDICAL_UNIT variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1952: Considering the common semantics for ASTHMA variable, dummification would be the most adequate encoding.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1953: The variable PREGNANT can be coded as ordinal without losing information.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1954: Feature generation based on variable ICU seems to be promising.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1955: Feature generation based on the use of variable PNEUMONIA wouldn’t be useful, but the use of MEDICAL_UNIT seems to be promising.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1956: Given the usual semantics of HIPERTENSION variable, dummification would have been a better codification.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1957: It is better to drop the variable PREGNANT than removing all records with missing values.</p>
    <img src="images/Covid_Data_histograms_numeric.png" width="auto" height = "600"/> 
    <p>1958: Not knowing the semantics of COPD variable, dummification could have been a more adequate codification.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1959: The variable mjd discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1960: Variable mjd is one of the most relevant variables.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1961: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1962: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1963: The specificity for the presented tree is lower than 75%.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1964: The number of False Negatives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1965: The number of False Positives is higher than the number of True Negatives for the presented tree.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1966: The number of False Negatives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1967: Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], the Decision Tree presented classifies (A, not B) as QSO.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1968: Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], the Decision Tree presented classifies (A, not B) as QSO.</p>
    <img src="images/sky_survey_decision_tree.png" width="auto" height = "600"/> 
    <p>1969: Considering that A=True<=>[dec <= 22.21] and B=True<=>[mjd <= 55090.5], it is possible to state that KNN algorithm classifies (A,B) as GALAXY for any k ≤ 945.</p>
    <img src="images/sky_survey_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>1970: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/sky_survey_overfitting_gb.png" width="auto" height = "600"/> 
    <p>1971: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/sky_survey_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1972: Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.</p>
    <img src="images/sky_survey_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1973: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/sky_survey_overfitting_rf.png" width="auto" height = "600"/> 
    <p>1974: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/sky_survey_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1975: KNN is in overfitting for k less than 5.</p>
    <img src="images/sky_survey_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1976: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/sky_survey_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1977: KNN with less than 15 neighbours is in overfitting.</p>
    <img src="images/sky_survey_overfitting_knn.png" width="auto" height = "600"/> 
    <p>1978: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/sky_survey_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1979: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/sky_survey_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1980: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/sky_survey_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1981: The decision tree is in overfitting for depths above 7.</p>
    <img src="images/sky_survey_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>1982: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/sky_survey_pca.png" width="auto" height = "600"/> 
    <p>1983: The first 7 principal components are enough for explaining half the data variance.</p>
    <img src="images/sky_survey_pca.png" width="auto" height = "600"/> 
    <p>1984: Using the first 4 principal components would imply an error between 15 and 30%.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1985: The intrinsic dimensionality of this dataset is 5.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1986: One of the variables redshift or plate can be discarded without losing information.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1987: The variable camcol can be discarded without risking losing information.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1988: Variables run and ra are redundant, but we can’t say the same for the pair mjd and dec.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1989: Variables run and redshift are redundant.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1990: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1991: Variable dec seems to be relevant for the majority of mining tasks.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1992: Variables camcol and mjd seem to be useful for classification tasks.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1993: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1994: Removing variable ra might improve the training of decision trees .</p>
    <img src="images/sky_survey_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>1995: There is evidence in favour for sequential backward selection to select variable camcol previously than variable mjd.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>1996: Variable plate is balanced.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>1997: Those boxplots show that the data is not normalized.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>1998: It is clear that variable field shows some outliers, but we can’t be sure of the same for variable ra.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>1999: Outliers seem to be a problem in the dataset.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2000: Variable field shows some outlier values.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2001: Variable field doesn’t have any outliers.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2002: Variable redshift presents some outliers.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2003: At least 50 of the variables present outliers.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2004: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2005: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2006: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2007: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2008: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/sky_survey_boxplots.png" width="auto" height = "600"/> 
    <p>2009: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/sky_survey_class_histogram.png" width="auto" height = "600"/> 
    <p>2010: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/sky_survey_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2011: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/sky_survey_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2012: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/sky_survey_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2013: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2014: All variables, but the class, should be dealt with as date.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2015: The variable run can be seen as ordinal.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2016: The variable field can be seen as ordinal without losing information.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2017: Variable ra is balanced.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2018: It is clear that variable camcol shows some outliers, but we can’t be sure of the same for variable mjd.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2019: Outliers seem to be a problem in the dataset.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2020: Variable redshift shows a high number of outlier values.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2021: Variable field doesn’t have any outliers.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2022: Variable plate presents some outliers.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2023: At least 60 of the variables present outliers.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2024: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2025: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2026: Considering the common semantics for ra and dec variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2027: Considering the common semantics for field variable, dummification would be the most adequate encoding.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2028: The variable camcol can be coded as ordinal without losing information.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2029: Feature generation based on variable redshift seems to be promising.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2030: Feature generation based on the use of variable camcol wouldn’t be useful, but the use of ra seems to be promising.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2031: Given the usual semantics of ra variable, dummification would have been a better codification.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2032: It is better to drop the variable redshift than removing all records with missing values.</p>
    <img src="images/sky_survey_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2033: Not knowing the semantics of plate variable, dummification could have been a more adequate codification.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2034: The variable Proanthocyanins discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2035: Variable Proanthocyanins is one of the most relevant variables.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2036: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2037: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2038: The specificity for the presented tree is higher than 75%.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2039: The number of True Positives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2040: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2041: The number of False Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2042: Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A,B) as 3 for any k ≤ 60.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2043: Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A,B) as 1 for any k ≤ 60.</p>
    <img src="images/Wine_decision_tree.png" width="auto" height = "600"/> 
    <p>2044: Considering that A=True<=>[Total phenols <= 2.36] and B=True<=>[Proanthocyanins <= 1.58], it is possible to state that KNN algorithm classifies (A, not B) as 2 for any k ≤ 49.</p>
    <img src="images/Wine_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2045: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Wine_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2046: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Wine_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2047: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/Wine_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2048: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Wine_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2049: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/Wine_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2050: KNN is in overfitting for k larger than 17.</p>
    <img src="images/Wine_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2051: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/Wine_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2052: KNN with less than 15 neighbours is in overfitting.</p>
    <img src="images/Wine_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2053: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/Wine_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2054: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Wine_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2055: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 6.</p>
    <img src="images/Wine_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2056: The decision tree is in overfitting for depths above 9.</p>
    <img src="images/Wine_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2057: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Wine_pca.png" width="auto" height = "600"/> 
    <p>2058: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/Wine_pca.png" width="auto" height = "600"/> 
    <p>2059: Using the first 10 principal components would imply an error between 15 and 30%.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2060: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2061: One of the variables Flavanoids or Hue can be discarded without losing information.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2062: The variable Color intensity can be discarded without risking losing information.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2063: Variables Color intensity and Alcohol are redundant, but we can’t say the same for the pair Flavanoids and Alcalinity of ash.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2064: Variables Flavanoids and Total phenols are redundant.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2065: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2066: Variable Ash seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2067: Variables Alcalinity of ash and Malic acid seem to be useful for classification tasks.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2068: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2069: Removing variable Alcohol might improve the training of decision trees .</p>
    <img src="images/Wine_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2070: There is evidence in favour for sequential backward selection to select variable OD280-OD315 of diluted wines previously than variable Total phenols.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2071: Variable OD280-OD315 of diluted wines is balanced.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2072: Those boxplots show that the data is not normalized.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2073: It is clear that variable Nonflavanoid phenols shows some outliers, but we can’t be sure of the same for variable Color intensity.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2074: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2075: Variable Hue shows some outlier values.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2076: Variable Malic acid doesn’t have any outliers.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2077: Variable OD280-OD315 of diluted wines presents some outliers.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2078: At least 50 of the variables present outliers.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2079: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2080: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2081: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2082: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2083: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Wine_boxplots.png" width="auto" height = "600"/> 
    <p>2084: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Wine_class_histogram.png" width="auto" height = "600"/> 
    <p>2085: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Wine_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2086: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/Wine_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2087: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Wine_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2088: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2089: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2090: The variable Total phenols can be seen as ordinal.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2091: The variable Alcohol can be seen as ordinal without losing information.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2092: Variable Flavanoids is balanced.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2093: It is clear that variable Color intensity shows some outliers, but we can’t be sure of the same for variable Total phenols.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2094: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2095: Variable Alcalinity of ash shows some outlier values.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2096: Variable Alcohol doesn’t have any outliers.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2097: Variable Ash presents some outliers.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2098: At least 50 of the variables present outliers.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2099: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2100: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2101: Considering the common semantics for OD280-OD315 of diluted wines and Alcohol variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2102: Considering the common semantics for OD280-OD315 of diluted wines variable, dummification would be the most adequate encoding.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2103: The variable Hue can be coded as ordinal without losing information.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2104: Feature generation based on variable Malic acid seems to be promising.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2105: Feature generation based on the use of variable Nonflavanoid phenols wouldn’t be useful, but the use of Alcohol seems to be promising.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2106: Given the usual semantics of Total phenols variable, dummification would have been a better codification.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2107: It is better to drop the variable Alcalinity of ash than removing all records with missing values.</p>
    <img src="images/Wine_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2108: Not knowing the semantics of Alcalinity of ash variable, dummification could have been a more adequate codification.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2109: The variable Hardness discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2110: Variable Chloramines is one of the most relevant variables.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2111: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2112: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2113: The specificity for the presented tree is higher than 90%.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2114: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2115: The number of False Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2116: The specificity for the presented tree is lower than 60%.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2117: Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 1388.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2118: Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that KNN algorithm classifies (A, not B) as 1 for any k ≤ 6.</p>
    <img src="images/water_potability_decision_tree.png" width="auto" height = "600"/> 
    <p>2119: Considering that A=True<=>[Hardness <= 278.29] and B=True<=>[Chloramines <= 6.7], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as 1.</p>
    <img src="images/water_potability_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2120: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/water_potability_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2121: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/water_potability_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2122: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/water_potability_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2123: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/water_potability_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2124: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/water_potability_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2125: KNN is in overfitting for k larger than 5.</p>
    <img src="images/water_potability_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2126: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/water_potability_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2127: KNN with less than 15 neighbours is in overfitting.</p>
    <img src="images/water_potability_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2128: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/water_potability_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2129: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/water_potability_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2130: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/water_potability_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2131: The decision tree is in overfitting for depths above 5.</p>
    <img src="images/water_potability_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2132: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/water_potability_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2133: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/water_potability_pca.png" width="auto" height = "600"/> 
    <p>2134: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/water_potability_pca.png" width="auto" height = "600"/> 
    <p>2135: Using the first 3 principal components would imply an error between 5 and 30%.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2136: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2137: One of the variables Hardness or Conductivity can be discarded without losing information.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2138: The variable Turbidity can be discarded without risking losing information.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2139: Variables Trihalomethanes and Hardness are redundant, but we can’t say the same for the pair Chloramines and Sulfate.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2140: Variables Hardness and ph are redundant.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2141: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2142: Variable Turbidity seems to be relevant for the majority of mining tasks.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2143: Variables Conductivity and Turbidity seem to be useful for classification tasks.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2144: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2145: Removing variable Hardness might improve the training of decision trees .</p>
    <img src="images/water_potability_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2146: There is evidence in favour for sequential backward selection to select variable Turbidity previously than variable Chloramines.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2147: Variable Trihalomethanes is balanced.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2148: Those boxplots show that the data is not normalized.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2149: It is clear that variable Turbidity shows some outliers, but we can’t be sure of the same for variable Sulfate.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2150: Outliers seem to be a problem in the dataset.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2151: Variable ph shows some outlier values.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2152: Variable Turbidity doesn’t have any outliers.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2153: Variable Trihalomethanes presents some outliers.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2154: At least 75 of the variables present outliers.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2155: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2156: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2157: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2158: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2159: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/water_potability_boxplots.png" width="auto" height = "600"/> 
    <p>2160: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2161: Discarding variable Trihalomethanes would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2162: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2163: Dropping all rows with missing values can lead to a dataset with less than 30% of the original data.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2164: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2165: Feature generation based on variable Sulfate seems to be promising.</p>
    <img src="images/water_potability_mv.png" width="auto" height = "600"/> 
    <p>2166: It is better to drop the variable Trihalomethanes than removing all records with missing values.</p>
    <img src="images/water_potability_class_histogram.png" width="auto" height = "600"/> 
    <p>2167: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/water_potability_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2168: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/water_potability_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2169: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/water_potability_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2170: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2171: All variables, but the class, should be dealt with as date.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2172: The variable Trihalomethanes can be seen as ordinal.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2173: The variable Chloramines can be seen as ordinal without losing information.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2174: Variable Turbidity is balanced.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2175: It is clear that variable Chloramines shows some outliers, but we can’t be sure of the same for variable Trihalomethanes.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2176: Outliers seem to be a problem in the dataset.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2177: Variable Trihalomethanes shows a high number of outlier values.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2178: Variable Turbidity doesn’t have any outliers.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2179: Variable Sulfate presents some outliers.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2180: At least 50 of the variables present outliers.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2181: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2182: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2183: Considering the common semantics for ph and Hardness variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2184: Considering the common semantics for Turbidity variable, dummification would be the most adequate encoding.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2185: The variable Conductivity can be coded as ordinal without losing information.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2186: Feature generation based on variable Chloramines seems to be promising.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2187: Feature generation based on the use of variable Conductivity wouldn’t be useful, but the use of ph seems to be promising.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2188: Given the usual semantics of Chloramines variable, dummification would have been a better codification.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2189: It is better to drop the variable Hardness than removing all records with missing values.</p>
    <img src="images/water_potability_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2190: Not knowing the semantics of ph variable, dummification could have been a more adequate codification.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2191: The variable Diameter discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2192: Variable Diameter is one of the most relevant variables.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2193: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2194: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2195: The recall for the presented tree is higher than 60%.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2196: The number of False Negatives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2197: The number of True Positives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2198: The number of False Negatives is lower than the number of True Negatives for the presented tree.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2199: Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], the Decision Tree presented classifies (not A, B) as I.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2200: Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (A, not B) as M for any k ≤ 117.</p>
    <img src="images/abalone_decision_tree.png" width="auto" height = "600"/> 
    <p>2201: Considering that A=True<=>[Height <= 0.13] and B=True<=>[Diameter <= 0.45], it is possible to state that KNN algorithm classifies (not A, not B) as M for any k ≤ 1191.</p>
    <img src="images/abalone_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2202: We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.</p>
    <img src="images/abalone_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2203: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/abalone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2204: Results for Random Forests identified as 2, may be explained by its estimators being in overfitting.</p>
    <img src="images/abalone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2205: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/abalone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2206: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/abalone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2207: KNN is in overfitting for k less than 5.</p>
    <img src="images/abalone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2208: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/abalone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2209: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/abalone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2210: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/abalone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2211: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/abalone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2212: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 4.</p>
    <img src="images/abalone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2213: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/abalone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2214: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/abalone_pca.png" width="auto" height = "600"/> 
    <p>2215: The first 6 principal components are enough for explaining half the data variance.</p>
    <img src="images/abalone_pca.png" width="auto" height = "600"/> 
    <p>2216: Using the first 3 principal components would imply an error between 15 and 30%.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2217: The intrinsic dimensionality of this dataset is 6.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2218: One of the variables Whole weight or Length can be discarded without losing information.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2219: The variable Whole weight can be discarded without risking losing information.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2220: Variables Length and Height are redundant, but we can’t say the same for the pair Whole weight and Viscera weight.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2221: Variables Diameter and Length are redundant.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2222: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2223: Variable Whole weight seems to be relevant for the majority of mining tasks.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2224: Variables Whole weight and Length seem to be useful for classification tasks.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2225: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2226: Removing variable Rings might improve the training of decision trees .</p>
    <img src="images/abalone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2227: There is evidence in favour for sequential backward selection to select variable Length previously than variable Height.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2228: Variable Rings is balanced.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2229: Those boxplots show that the data is not normalized.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2230: It is clear that variable Shell weight shows some outliers, but we can’t be sure of the same for variable Viscera weight.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2231: Outliers seem to be a problem in the dataset.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2232: Variable Rings shows a high number of outlier values.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2233: Variable Shell weight doesn’t have any outliers.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2234: Variable Shell weight presents some outliers.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2235: At least 50 of the variables present outliers.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2236: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2237: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2238: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2239: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2240: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/abalone_boxplots.png" width="auto" height = "600"/> 
    <p>2241: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/abalone_class_histogram.png" width="auto" height = "600"/> 
    <p>2242: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/abalone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2243: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/abalone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2244: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/abalone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2245: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2246: All variables, but the class, should be dealt with as date.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2247: The variable Shucked weight can be seen as ordinal.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2248: The variable Shucked weight can be seen as ordinal without losing information.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2249: Variable Shell weight is balanced.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2250: It is clear that variable Rings shows some outliers, but we can’t be sure of the same for variable Whole weight.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2251: Outliers seem to be a problem in the dataset.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2252: Variable Viscera weight shows some outlier values.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2253: Variable Diameter doesn’t have any outliers.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2254: Variable Length presents some outliers.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2255: At least 75 of the variables present outliers.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2256: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2257: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2258: Considering the common semantics for Diameter and Length variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2259: Considering the common semantics for Length variable, dummification would be the most adequate encoding.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2260: The variable Diameter can be coded as ordinal without losing information.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2261: Feature generation based on variable Shucked weight seems to be promising.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2262: Feature generation based on the use of variable Diameter wouldn’t be useful, but the use of Length seems to be promising.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2263: Given the usual semantics of Viscera weight variable, dummification would have been a better codification.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2264: It is better to drop the variable Shucked weight than removing all records with missing values.</p>
    <img src="images/abalone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2265: Not knowing the semantics of Shell weight variable, dummification could have been a more adequate codification.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2266: The variable SMK_stat_type_cd discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2267: Variable gamma_GTP is one of the most relevant variables.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2268: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2269: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2270: The specificity for the presented tree is higher than 90%.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2271: The number of False Negatives reported in the same tree is 10.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2272: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2273: The number of False Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2274: Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that KNN algorithm classifies (A,B) as N for any k ≤ 3135.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2275: Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that Naive Bayes algorithm classifies (not A, B), as N.</p>
    <img src="images/smoking_drinking_decision_tree.png" width="auto" height = "600"/> 
    <p>2276: Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], the Decision Tree presented classifies (A, not B) as Y.</p>
    <img src="images/smoking_drinking_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2277: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/smoking_drinking_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2278: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/smoking_drinking_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2279: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/smoking_drinking_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2280: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/smoking_drinking_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2281: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2282: KNN is in overfitting for k less than 13.</p>
    <img src="images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2283: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2284: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/smoking_drinking_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2285: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/smoking_drinking_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2286: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/smoking_drinking_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2287: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.</p>
    <img src="images/smoking_drinking_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2288: The decision tree is in overfitting for depths above 3.</p>
    <img src="images/smoking_drinking_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2289: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/smoking_drinking_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2290: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/smoking_drinking_pca.png" width="auto" height = "600"/> 
    <p>2291: The first 6 principal components are enough for explaining half the data variance.</p>
    <img src="images/smoking_drinking_pca.png" width="auto" height = "600"/> 
    <p>2292: Using the first 8 principal components would imply an error between 15 and 30%.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2293: The intrinsic dimensionality of this dataset is 7.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2294: One of the variables waistline or age can be discarded without losing information.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2295: The variable hemoglobin can be discarded without risking losing information.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2296: Variables BLDS and weight are redundant, but we can’t say the same for the pair waistline and LDL_chole.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2297: Variables age and height are redundant.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2298: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2299: Variable age seems to be relevant for the majority of mining tasks.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2300: Variables height and waistline seem to be useful for classification tasks.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2301: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2302: Removing variable waistline might improve the training of decision trees .</p>
    <img src="images/smoking_drinking_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2303: There is evidence in favour for sequential backward selection to select variable LDL_chole previously than variable SMK_stat_type_cd.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2304: Variable waistline is balanced.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2305: Those boxplots show that the data is not normalized.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2306: It is clear that variable tot_chole shows some outliers, but we can’t be sure of the same for variable waistline.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2307: Outliers seem to be a problem in the dataset.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2308: Variable tot_chole shows a high number of outlier values.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2309: Variable SMK_stat_type_cd doesn’t have any outliers.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2310: Variable BLDS presents some outliers.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2311: At least 85 of the variables present outliers.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2312: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2313: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2314: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2315: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2316: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/smoking_drinking_boxplots.png" width="auto" height = "600"/> 
    <p>2317: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2318: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2319: The variable hear_right can be seen as ordinal.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2320: The variable hear_right can be seen as ordinal without losing information.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2321: Considering the common semantics for hear_left and sex variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2322: Considering the common semantics for hear_right variable, dummification would be the most adequate encoding.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2323: The variable sex can be coded as ordinal without losing information.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2324: Feature generation based on variable hear_left seems to be promising.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2325: Feature generation based on the use of variable hear_right wouldn’t be useful, but the use of sex seems to be promising.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2326: Given the usual semantics of hear_right variable, dummification would have been a better codification.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2327: It is better to drop the variable hear_left than removing all records with missing values.</p>
    <img src="images/smoking_drinking_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2328: Not knowing the semantics of hear_right variable, dummification could have been a more adequate codification.</p>
    <img src="images/smoking_drinking_class_histogram.png" width="auto" height = "600"/> 
    <p>2329: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/smoking_drinking_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2330: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/smoking_drinking_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2331: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/smoking_drinking_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2332: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2333: All variables, but the class, should be dealt with as date.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2334: The variable SBP can be seen as ordinal.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2335: The variable tot_chole can be seen as ordinal without losing information.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2336: Variable weight is balanced.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2337: It is clear that variable height shows some outliers, but we can’t be sure of the same for variable age.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2338: Outliers seem to be a problem in the dataset.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2339: Variable LDL_chole shows some outlier values.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2340: Variable tot_chole doesn’t have any outliers.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2341: Variable gamma_GTP presents some outliers.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2342: At least 60 of the variables present outliers.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2343: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2344: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2345: Considering the common semantics for age and height variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2346: Considering the common semantics for weight variable, dummification would be the most adequate encoding.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2347: The variable hemoglobin can be coded as ordinal without losing information.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2348: Feature generation based on variable waistline seems to be promising.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2349: Feature generation based on the use of variable height wouldn’t be useful, but the use of age seems to be promising.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2350: Given the usual semantics of BLDS variable, dummification would have been a better codification.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2351: It is better to drop the variable SMK_stat_type_cd than removing all records with missing values.</p>
    <img src="images/smoking_drinking_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2352: Not knowing the semantics of hemoglobin variable, dummification could have been a more adequate codification.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2353: The variable curtosis discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2354: Variable skewness is one of the most relevant variables.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2355: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2356: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2357: The recall for the presented tree is higher than 75%.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2358: The number of False Positives reported in the same tree is 10.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2359: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2360: The recall for the presented tree is lower than its accuracy.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2361: Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (A, not B) as 0 for any k ≤ 214.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2362: Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 214.</p>
    <img src="images/BankNoteAuthentication_decision_tree.png" width="auto" height = "600"/> 
    <p>2363: Considering that A=True<=>[skewness <= 5.16] and B=True<=>[curtosis <= 0.19], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 131.</p>
    <img src="images/BankNoteAuthentication_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2364: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/BankNoteAuthentication_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2365: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/BankNoteAuthentication_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2366: Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.</p>
    <img src="images/BankNoteAuthentication_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2367: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/BankNoteAuthentication_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2368: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/BankNoteAuthentication_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2369: KNN is in overfitting for k larger than 5.</p>
    <img src="images/BankNoteAuthentication_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2370: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/BankNoteAuthentication_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2371: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/BankNoteAuthentication_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2372: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/BankNoteAuthentication_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2373: According to the decision tree overfitting chart, the tree with 5 nodes of depth is in overfitting.</p>
    <img src="images/BankNoteAuthentication_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2374: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 10.</p>
    <img src="images/BankNoteAuthentication_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2375: The decision tree is in overfitting for depths above 10.</p>
    <img src="images/BankNoteAuthentication_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2376: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/BankNoteAuthentication_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2377: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/BankNoteAuthentication_pca.png" width="auto" height = "600"/> 
    <p>2378: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/BankNoteAuthentication_pca.png" width="auto" height = "600"/> 
    <p>2379: Using the first 2 principal components would imply an error between 15 and 30%.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2380: The intrinsic dimensionality of this dataset is 3.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2381: One of the variables variance or curtosis can be discarded without losing information.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2382: The variable skewness can be discarded without risking losing information.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2383: Variables entropy and curtosis are redundant, but we can’t say the same for the pair variance and skewness.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2384: Variables curtosis and entropy are redundant.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2385: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2386: Variable skewness seems to be relevant for the majority of mining tasks.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2387: Variables curtosis and variance seem to be useful for classification tasks.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2388: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2389: Removing variable variance might improve the training of decision trees .</p>
    <img src="images/BankNoteAuthentication_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2390: There is evidence in favour for sequential backward selection to select variable variance previously than variable skewness.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2391: Variable curtosis is balanced.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2392: Those boxplots show that the data is not normalized.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2393: It is clear that variable curtosis shows some outliers, but we can’t be sure of the same for variable entropy.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2394: Outliers seem to be a problem in the dataset.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2395: Variable skewness shows a high number of outlier values.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2396: Variable skewness doesn’t have any outliers.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2397: Variable variance presents some outliers.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2398: At least 75 of the variables present outliers.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2399: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2400: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2401: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2402: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2403: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/BankNoteAuthentication_boxplots.png" width="auto" height = "600"/> 
    <p>2404: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/BankNoteAuthentication_class_histogram.png" width="auto" height = "600"/> 
    <p>2405: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/BankNoteAuthentication_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2406: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/BankNoteAuthentication_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2407: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/BankNoteAuthentication_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2408: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2409: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2410: The variable skewness can be seen as ordinal.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2411: The variable skewness can be seen as ordinal without losing information.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2412: Variable variance is balanced.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2413: It is clear that variable variance shows some outliers, but we can’t be sure of the same for variable entropy.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2414: Outliers seem to be a problem in the dataset.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2415: Variable variance shows a high number of outlier values.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2416: Variable skewness doesn’t have any outliers.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2417: Variable curtosis presents some outliers.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2418: At least 60 of the variables present outliers.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2419: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2420: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2421: Considering the common semantics for variance and skewness variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2422: Considering the common semantics for curtosis variable, dummification would be the most adequate encoding.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2423: The variable entropy can be coded as ordinal without losing information.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2424: Feature generation based on variable skewness seems to be promising.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2425: Feature generation based on the use of variable curtosis wouldn’t be useful, but the use of variance seems to be promising.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2426: Given the usual semantics of curtosis variable, dummification would have been a better codification.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2427: It is better to drop the variable skewness than removing all records with missing values.</p>
    <img src="images/BankNoteAuthentication_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2428: Not knowing the semantics of entropy variable, dummification could have been a more adequate codification.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2429: The variable PetalWidthCm discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2430: Variable PetalWidthCm is one of the most relevant variables.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2431: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2432: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2433: The recall for the presented tree is lower than 75%.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2434: The number of True Negatives reported in the same tree is 30.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2435: The number of True Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2436: The precision for the presented tree is lower than 90%.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2437: Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], the Decision Tree presented classifies (not A, not B) as Iris-versicolor.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2438: Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], it is possible to state that KNN algorithm classifies (A,B) as Iris-virginica for any k ≤ 38.</p>
    <img src="images/Iris_decision_tree.png" width="auto" height = "600"/> 
    <p>2439: Considering that A=True<=>[PetalWidthCm <= 0.7] and B=True<=>[PetalWidthCm <= 1.75], it is possible to state that KNN algorithm classifies (A,B) as Iris-setosa for any k ≤ 32.</p>
    <img src="images/Iris_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2440: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/Iris_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2441: We are able to identify the existence of overfitting for gradient boosting models with more than 1002 estimators.</p>
    <img src="images/Iris_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2442: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/Iris_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2443: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Iris_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2444: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2445: KNN is in overfitting for k larger than 17.</p>
    <img src="images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2446: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2447: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/Iris_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2448: We are able to identify the existence of overfitting for KNN models with less than 4 neighbors.</p>
    <img src="images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2449: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2450: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 8.</p>
    <img src="images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2451: The decision tree is in overfitting for depths above 10.</p>
    <img src="images/Iris_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2452: We are able to identify the existence of overfitting for decision tree models with more than 3 nodes of depth.</p>
    <img src="images/Iris_pca.png" width="auto" height = "600"/> 
    <p>2453: The first 2 principal components are enough for explaining half the data variance.</p>
    <img src="images/Iris_pca.png" width="auto" height = "600"/> 
    <p>2454: Using the first 2 principal components would imply an error between 10 and 25%.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2455: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2456: One of the variables PetalWidthCm or SepalLengthCm can be discarded without losing information.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2457: The variable PetalLengthCm can be discarded without risking losing information.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2458: Variables SepalLengthCm and SepalWidthCm are redundant, but we can’t say the same for the pair PetalLengthCm and PetalWidthCm.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2459: Variables PetalLengthCm and SepalLengthCm are redundant.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2460: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2461: Variable SepalLengthCm seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2462: Variables PetalLengthCm and SepalLengthCm seem to be useful for classification tasks.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2463: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2464: Removing variable PetalWidthCm might improve the training of decision trees .</p>
    <img src="images/Iris_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2465: There is evidence in favour for sequential backward selection to select variable PetalLengthCm previously than variable SepalWidthCm.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2466: Variable PetalWidthCm is balanced.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2467: Those boxplots show that the data is not normalized.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2468: It is clear that variable SepalWidthCm shows some outliers, but we can’t be sure of the same for variable PetalLengthCm.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2469: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2470: Variable PetalLengthCm shows some outlier values.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2471: Variable SepalLengthCm doesn’t have any outliers.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2472: Variable PetalWidthCm presents some outliers.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2473: At least 75 of the variables present outliers.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2474: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2475: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2476: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2477: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2478: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Iris_boxplots.png" width="auto" height = "600"/> 
    <p>2479: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Iris_class_histogram.png" width="auto" height = "600"/> 
    <p>2480: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Iris_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2481: Given the number of records and that some variables are numeric, we might be facing the curse of dimensionality.</p>
    <img src="images/Iris_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2482: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Iris_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2483: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2484: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2485: The variable PetalWidthCm can be seen as ordinal.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2486: The variable SepalLengthCm can be seen as ordinal without losing information.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2487: Variable PetalWidthCm is balanced.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2488: It is clear that variable PetalWidthCm shows some outliers, but we can’t be sure of the same for variable SepalLengthCm.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2489: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2490: Variable SepalWidthCm shows a high number of outlier values.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2491: Variable SepalWidthCm doesn’t have any outliers.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2492: Variable PetalLengthCm presents some outliers.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2493: At least 75 of the variables present outliers.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2494: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2495: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2496: Considering the common semantics for PetalWidthCm and SepalLengthCm variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2497: Considering the common semantics for PetalLengthCm variable, dummification would be the most adequate encoding.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2498: The variable PetalLengthCm can be coded as ordinal without losing information.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2499: Feature generation based on variable SepalWidthCm seems to be promising.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2500: Feature generation based on the use of variable PetalLengthCm wouldn’t be useful, but the use of SepalLengthCm seems to be promising.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2501: Given the usual semantics of PetalLengthCm variable, dummification would have been a better codification.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2502: It is better to drop the variable SepalWidthCm than removing all records with missing values.</p>
    <img src="images/Iris_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2503: Not knowing the semantics of SepalWidthCm variable, dummification could have been a more adequate codification.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2504: The variable mobile_wt discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2505: Variable mobile_wt is one of the most relevant variables.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2506: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 6%.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2507: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2508: The specificity for the presented tree is lower than 60%.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2509: The number of False Positives is lower than the number of False Negatives for the presented tree.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2510: The number of True Negatives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2511: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2512: Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], it is possible to state that KNN algorithm classifies (A,B) as 2 for any k ≤ 636.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2513: Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="images/phone_decision_tree.png" width="auto" height = "600"/> 
    <p>2514: Considering that A=True<=>[int_memory <= 30.5] and B=True<=>[mobile_wt <= 91.5], the Decision Tree presented classifies (A, not B) as 0.</p>
    <img src="images/phone_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2515: We are able to identify the existence of overfitting for MLP models trained longer than 300 episodes.</p>
    <img src="images/phone_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2516: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/phone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2517: Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.</p>
    <img src="images/phone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2518: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/phone_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2519: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2520: KNN is in overfitting for k less than 13.</p>
    <img src="images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2521: KNN with 5 neighbour is in overfitting.</p>
    <img src="images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2522: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/phone_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2523: We are able to identify the existence of overfitting for KNN models with less than 5 neighbors.</p>
    <img src="images/phone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2524: According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.</p>
    <img src="images/phone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2525: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.</p>
    <img src="images/phone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2526: The decision tree is in overfitting for depths above 4.</p>
    <img src="images/phone_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2527: We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.</p>
    <img src="images/phone_pca.png" width="auto" height = "600"/> 
    <p>2528: The first 8 principal components are enough for explaining half the data variance.</p>
    <img src="images/phone_pca.png" width="auto" height = "600"/> 
    <p>2529: Using the first 11 principal components would imply an error between 10 and 25%.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2530: The intrinsic dimensionality of this dataset is 11.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2531: One of the variables px_height or battery_power can be discarded without losing information.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2532: The variable battery_power can be discarded without risking losing information.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2533: Variables ram and px_width are redundant, but we can’t say the same for the pair mobile_wt and sc_h.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2534: Variables px_height and sc_w are redundant.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2535: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2536: Variable n_cores seems to be relevant for the majority of mining tasks.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2537: Variables sc_h and fc seem to be useful for classification tasks.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2538: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2539: Removing variable sc_h might improve the training of decision trees .</p>
    <img src="images/phone_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2540: There is evidence in favour for sequential backward selection to select variable px_height previously than variable px_width.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2541: Variable n_cores is balanced.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2542: Those boxplots show that the data is not normalized.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2543: It is clear that variable talk_time shows some outliers, but we can’t be sure of the same for variable px_width.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2544: Outliers seem to be a problem in the dataset.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2545: Variable px_height shows some outlier values.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2546: Variable sc_w doesn’t have any outliers.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2547: Variable pc presents some outliers.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2548: At least 50 of the variables present outliers.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2549: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2550: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2551: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2552: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2553: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/phone_boxplots.png" width="auto" height = "600"/> 
    <p>2554: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2555: All variables, but the class, should be dealt with as date.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2556: The variable four_g can be seen as ordinal.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2557: The variable wifi can be seen as ordinal without losing information.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2558: Considering the common semantics for touch_screen and blue variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2559: Considering the common semantics for three_g variable, dummification would be the most adequate encoding.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2560: The variable three_g can be coded as ordinal without losing information.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2561: Feature generation based on variable four_g seems to be promising.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2562: Feature generation based on the use of variable three_g wouldn’t be useful, but the use of blue seems to be promising.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2563: Given the usual semantics of three_g variable, dummification would have been a better codification.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2564: It is better to drop the variable three_g than removing all records with missing values.</p>
    <img src="images/phone_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2565: Not knowing the semantics of four_g variable, dummification could have been a more adequate codification.</p>
    <img src="images/phone_class_histogram.png" width="auto" height = "600"/> 
    <p>2566: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/phone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2567: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/phone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2568: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/phone_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2569: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2570: All variables, but the class, should be dealt with as binary.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2571: The variable int_memory can be seen as ordinal.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2572: The variable fc can be seen as ordinal without losing information.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2573: Variable sc_h is balanced.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2574: It is clear that variable sc_w shows some outliers, but we can’t be sure of the same for variable sc_h.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2575: Outliers seem to be a problem in the dataset.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2576: Variable pc shows a high number of outlier values.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2577: Variable ram doesn’t have any outliers.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2578: Variable fc presents some outliers.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2579: At least 60 of the variables present outliers.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2580: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2581: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2582: Considering the common semantics for px_height and battery_power variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2583: Considering the common semantics for px_height variable, dummification would be the most adequate encoding.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2584: The variable battery_power can be coded as ordinal without losing information.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2585: Feature generation based on variable mobile_wt seems to be promising.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2586: Feature generation based on the use of variable sc_h wouldn’t be useful, but the use of battery_power seems to be promising.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2587: Given the usual semantics of mobile_wt variable, dummification would have been a better codification.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2588: It is better to drop the variable mobile_wt than removing all records with missing values.</p>
    <img src="images/phone_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2589: Not knowing the semantics of talk_time variable, dummification could have been a more adequate codification.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2590: The variable Parch discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2591: Variable Parch is one of the most relevant variables.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2592: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 5%.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2593: As reported in the tree, the number of False Positive is bigger than the number of False Negatives.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2594: The accuracy for the presented tree is lower than 75%.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2595: The number of True Negatives is lower than the number of False Positives for the presented tree.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2596: The number of True Negatives is higher than the number of False Positives for the presented tree.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2597: The number of True Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2598: Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, B) as 1 for any k ≤ 181.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2599: Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 1.</p>
    <img src="images/Titanic_decision_tree.png" width="auto" height = "600"/> 
    <p>2600: Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as 0.</p>
    <img src="images/Titanic_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2601: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Titanic_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2602: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/Titanic_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2603: Results for Random Forests identified as 10, may be explained by its estimators being in overfitting.</p>
    <img src="images/Titanic_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2604: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Titanic_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2605: We are able to identify the existence of overfitting for random forest models with more than 502 estimators.</p>
    <img src="images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2606: KNN is in overfitting for k larger than 13.</p>
    <img src="images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2607: KNN with 11 neighbour is in overfitting.</p>
    <img src="images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2608: KNN with less than 7 neighbours is in overfitting.</p>
    <img src="images/Titanic_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2609: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2610: According to the decision tree overfitting chart, the tree with 20 nodes of depth is in overfitting.</p>
    <img src="images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2611: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2612: The decision tree is in overfitting for depths above 5.</p>
    <img src="images/Titanic_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2613: We are able to identify the existence of overfitting for decision tree models with more than 6 nodes of depth.</p>
    <img src="images/Titanic_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2614: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Titanic_pca.png" width="auto" height = "600"/> 
    <p>2615: The first 4 principal components are enough for explaining half the data variance.</p>
    <img src="images/Titanic_pca.png" width="auto" height = "600"/> 
    <p>2616: Using the first 2 principal components would imply an error between 15 and 20%.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2617: The intrinsic dimensionality of this dataset is 4.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2618: One of the variables SibSp or Parch can be discarded without losing information.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2619: The variable Parch can be discarded without risking losing information.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2620: Variables Fare and Age seem to be useful for classification tasks.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2621: Variables Age and Fare are redundant.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2622: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2623: Variable Pclass seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2624: Variables Parch and SibSp seem to be useful for classification tasks.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2625: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2626: Removing variable SibSp might improve the training of decision trees .</p>
    <img src="images/Titanic_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2627: There is evidence in favour for sequential backward selection to select variable Fare previously than variable Age.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2628: Variable Age is balanced.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2629: Those boxplots show that the data is not normalized.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2630: It is clear that variable Pclass shows some outliers, but we can’t be sure of the same for variable Fare.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2631: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2632: Variable Fare shows a high number of outlier values.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2633: Variable Fare doesn’t have any outliers.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2634: Variable Parch presents some outliers.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2635: At least 50 of the variables present outliers.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2636: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2637: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2638: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2639: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2640: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Titanic_boxplots.png" width="auto" height = "600"/> 
    <p>2641: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2642: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2643: The variable Sex can be seen as ordinal.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2644: The variable Sex can be seen as ordinal without losing information.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2645: Considering the common semantics for Sex and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2646: Considering the common semantics for Embarked variable, dummification would be the most adequate encoding.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2647: The variable Embarked can be coded as ordinal without losing information.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2648: Feature generation based on variable Sex seems to be promising.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2649: Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Embarked seems to be promising.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2650: Given the usual semantics of Sex variable, dummification would have been a better codification.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2651: It is better to drop the variable Embarked than removing all records with missing values.</p>
    <img src="images/Titanic_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2652: Not knowing the semantics of Embarked variable, dummification could have been a more adequate codification.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2653: Discarding variable Embarked would be better than discarding all the records with missing values for that variable.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2654: Dropping all records with missing values would be better than to drop the variables with missing values.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2655: Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2656: There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2657: Feature generation based on variable Embarked seems to be promising.</p>
    <img src="images/Titanic_mv.png" width="auto" height = "600"/> 
    <p>2658: It is better to drop the variable Age than removing all records with missing values.</p>
    <img src="images/Titanic_class_histogram.png" width="auto" height = "600"/> 
    <p>2659: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Titanic_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2660: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Titanic_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2661: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Titanic_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2662: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2663: All variables, but the class, should be dealt with as symbolic.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2664: The variable Parch can be seen as ordinal.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2665: The variable Fare can be seen as ordinal without losing information.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2666: Variable Pclass is balanced.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2667: It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable SibSp.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2668: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2669: Variable Age shows a high number of outlier values.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2670: Variable Fare doesn’t have any outliers.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2671: Variable Parch presents some outliers.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2672: At least 60 of the variables present outliers.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2673: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2674: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2675: Considering the common semantics for Age and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2676: Considering the common semantics for SibSp variable, dummification would be the most adequate encoding.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2677: The variable Pclass can be coded as ordinal without losing information.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2678: Feature generation based on variable Parch seems to be promising.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2679: Feature generation based on the use of variable Age wouldn’t be useful, but the use of Pclass seems to be promising.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2680: Given the usual semantics of Age variable, dummification would have been a better codification.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2681: It is better to drop the variable Pclass than removing all records with missing values.</p>
    <img src="images/Titanic_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2682: Not knowing the semantics of SibSp variable, dummification could have been a more adequate codification.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2683: The variable Crunchiness discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2684: Variable Juiciness is one of the most relevant variables.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2685: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2686: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2687: The recall for the presented tree is higher than 75%.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2688: The number of True Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2689: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2690: The specificity for the presented tree is higher than 90%.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2691: Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], the Decision Tree presented classifies (not A, not B) as bad.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2692: Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that KNN algorithm classifies (not A, not B) as bad for any k ≤ 1625.</p>
    <img src="images/apple_quality_decision_tree.png" width="auto" height = "600"/> 
    <p>2693: Considering that A=True<=>[Juiciness <= -0.3] and B=True<=>[Crunchiness <= 2.25], it is possible to state that Naive Bayes algorithm classifies (not A, not B), as good.</p>
    <img src="images/apple_quality_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2694: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/apple_quality_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2695: We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.</p>
    <img src="images/apple_quality_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2696: Results for Random Forests identified as 3, may be explained by its estimators being in overfitting.</p>
    <img src="images/apple_quality_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2697: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/apple_quality_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2698: We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.</p>
    <img src="images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2699: KNN is in overfitting for k larger than 17.</p>
    <img src="images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2700: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2701: KNN with more than 17 neighbours is in overfitting.</p>
    <img src="images/apple_quality_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2702: We are able to identify the existence of overfitting for KNN models with less than 3 neighbors.</p>
    <img src="images/apple_quality_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2703: According to the decision tree overfitting chart, the tree with 16 nodes of depth is in overfitting.</p>
    <img src="images/apple_quality_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2704: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.</p>
    <img src="images/apple_quality_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2705: The decision tree is in overfitting for depths above 6.</p>
    <img src="images/apple_quality_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2706: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/apple_quality_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2707: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/apple_quality_pca.png" width="auto" height = "600"/> 
    <p>2708: The first 5 principal components are enough for explaining half the data variance.</p>
    <img src="images/apple_quality_pca.png" width="auto" height = "600"/> 
    <p>2709: Using the first 2 principal components would imply an error between 15 and 20%.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2710: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2711: One of the variables Crunchiness or Acidity can be discarded without losing information.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2712: The variable Ripeness can be discarded without risking losing information.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2713: Variables Juiciness and Crunchiness are redundant, but we can’t say the same for the pair Sweetness and Ripeness.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2714: Variables Juiciness and Crunchiness are redundant.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2715: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2716: Variable Juiciness seems to be relevant for the majority of mining tasks.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2717: Variables Crunchiness and Weight seem to be useful for classification tasks.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2718: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2719: Removing variable Juiciness might improve the training of decision trees .</p>
    <img src="images/apple_quality_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2720: There is evidence in favour for sequential backward selection to select variable Juiciness previously than variable Ripeness.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2721: Variable Weight is balanced.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2722: Those boxplots show that the data is not normalized.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2723: It is clear that variable Sweetness shows some outliers, but we can’t be sure of the same for variable Crunchiness.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2724: Outliers seem to be a problem in the dataset.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2725: Variable Ripeness shows a high number of outlier values.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2726: Variable Acidity doesn’t have any outliers.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2727: Variable Juiciness presents some outliers.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2728: At least 75 of the variables present outliers.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2729: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2730: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2731: A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2732: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2733: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/apple_quality_boxplots.png" width="auto" height = "600"/> 
    <p>2734: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/apple_quality_class_histogram.png" width="auto" height = "600"/> 
    <p>2735: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/apple_quality_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2736: Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.</p>
    <img src="images/apple_quality_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2737: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/apple_quality_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2738: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2739: All variables, but the class, should be dealt with as numeric.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2740: The variable Acidity can be seen as ordinal.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2741: The variable Size can be seen as ordinal without losing information.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2742: Variable Juiciness is balanced.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2743: It is clear that variable Weight shows some outliers, but we can’t be sure of the same for variable Sweetness.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2744: Outliers seem to be a problem in the dataset.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2745: Variable Juiciness shows a high number of outlier values.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2746: Variable Size doesn’t have any outliers.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2747: Variable Weight presents some outliers.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2748: At least 50 of the variables present outliers.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2749: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2750: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2751: Considering the common semantics for Crunchiness and Size variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2752: Considering the common semantics for Sweetness variable, dummification would be the most adequate encoding.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2753: The variable Juiciness can be coded as ordinal without losing information.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2754: Feature generation based on variable Acidity seems to be promising.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2755: Feature generation based on the use of variable Acidity wouldn’t be useful, but the use of Size seems to be promising.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2756: Given the usual semantics of Acidity variable, dummification would have been a better codification.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2757: It is better to drop the variable Ripeness than removing all records with missing values.</p>
    <img src="images/apple_quality_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2758: Not knowing the semantics of Acidity variable, dummification could have been a more adequate codification.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2759: The variable JoiningYear discriminates between the target values, as shown in the decision tree.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2760: Variable ExperienceInCurrentDomain is one of the most relevant variables.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2761: A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 8%.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2762: As reported in the tree, the number of False Positive is smaller than the number of False Negatives.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2763: The recall for the presented tree is lower than 60%.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2764: The number of False Positives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2765: The number of False Positives is higher than the number of True Positives for the presented tree.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2766: The number of True Negatives is higher than the number of False Negatives for the presented tree.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2767: Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], it is possible to state that KNN algorithm classifies (A,B) as 0 for any k ≤ 44.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2768: Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], the Decision Tree presented classifies (not A, B) as 1.</p>
    <img src="images/Employee_decision_tree.png" width="auto" height = "600"/> 
    <p>2769: Considering that A=True<=>[JoiningYear <= 2017.5] and B=True<=>[ExperienceInCurrentDomain <= 3.5], the Decision Tree presented classifies (A,B) as 0.</p>
    <img src="images/Employee_overfitting_mlp.png" width="auto" height = "600"/> 
    <p>2770: We are able to identify the existence of overfitting for MLP models trained longer than 500 episodes.</p>
    <img src="images/Employee_overfitting_gb.png" width="auto" height = "600"/> 
    <p>2771: We are able to identify the existence of overfitting for gradient boosting models with more than 502 estimators.</p>
    <img src="images/Employee_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2772: Results for Random Forests identified as 2, may be explained by its estimators being in underfitting.</p>
    <img src="images/Employee_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2773: The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.</p>
    <img src="images/Employee_overfitting_rf.png" width="auto" height = "600"/> 
    <p>2774: We are able to identify the existence of overfitting for random forest models with more than 1002 estimators.</p>
    <img src="images/Employee_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2775: KNN is in overfitting for k less than 13.</p>
    <img src="images/Employee_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2776: KNN with 7 neighbour is in overfitting.</p>
    <img src="images/Employee_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2777: KNN with more than 15 neighbours is in overfitting.</p>
    <img src="images/Employee_overfitting_knn.png" width="auto" height = "600"/> 
    <p>2778: We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.</p>
    <img src="images/Employee_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2779: According to the decision tree overfitting chart, the tree with 9 nodes of depth is in overfitting.</p>
    <img src="images/Employee_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2780: The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 9.</p>
    <img src="images/Employee_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2781: The decision tree is in overfitting for depths above 4.</p>
    <img src="images/Employee_overfitting_decision_tree.png" width="auto" height = "600"/> 
    <p>2782: We are able to identify the existence of overfitting for decision tree models with more than 2 nodes of depth.</p>
    <img src="images/Employee_overfitting_dt_acc_rec.png" width="auto" height = "600"/> 
    <p>2783: The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.</p>
    <img src="images/Employee_pca.png" width="auto" height = "600"/> 
    <p>2784: The first 3 principal components are enough for explaining half the data variance.</p>
    <img src="images/Employee_pca.png" width="auto" height = "600"/> 
    <p>2785: Using the first 3 principal components would imply an error between 15 and 25%.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2786: The intrinsic dimensionality of this dataset is 2.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2787: One of the variables PaymentTier or JoiningYear can be discarded without losing information.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2788: The variable JoiningYear can be discarded without risking losing information.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2789: Variables Age and PaymentTier are redundant, but we can’t say the same for the pair ExperienceInCurrentDomain and JoiningYear.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2790: Variables PaymentTier and JoiningYear are redundant.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2791: From the correlation analysis alone, it is clear that there are relevant variables.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2792: Variable JoiningYear seems to be relevant for the majority of mining tasks.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2793: Variables Age and ExperienceInCurrentDomain seem to be useful for classification tasks.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2794: Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2795: Removing variable PaymentTier might improve the training of decision trees .</p>
    <img src="images/Employee_correlation_heatmap.png" width="auto" height = "600"/> 
    <p>2796: There is evidence in favour for sequential backward selection to select variable ExperienceInCurrentDomain previously than variable PaymentTier.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2797: Variable ExperienceInCurrentDomain is balanced.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2798: Those boxplots show that the data is not normalized.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2799: It is clear that variable PaymentTier shows some outliers, but we can’t be sure of the same for variable Age.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2800: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2801: Variable JoiningYear shows a high number of outlier values.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2802: Variable JoiningYear doesn’t have any outliers.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2803: Variable PaymentTier presents some outliers.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2804: At least 60 of the variables present outliers.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2805: The boxplots presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2806: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2807: A scaling transformation is mandatory, in order to improve the  KNN performance in this dataset.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2808: Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2809: Normalization of this dataset could not have impact on a KNN classifier.</p>
    <img src="images/Employee_boxplots.png" width="auto" height = "600"/> 
    <p>2810: Scaling this dataset would be mandatory to improve the results with distance-based methods.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2811: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2812: The variable Gender can be seen as ordinal.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2813: The variable EverBenched can be seen as ordinal without losing information.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2814: Considering the common semantics for Education and City variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2815: Considering the common semantics for City variable, dummification would be the most adequate encoding.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2816: The variable City can be coded as ordinal without losing information.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2817: Feature generation based on variable City seems to be promising.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2818: Feature generation based on the use of variable EverBenched wouldn’t be useful, but the use of Education seems to be promising.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2819: Given the usual semantics of Gender variable, dummification would have been a better codification.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2820: It is better to drop the variable EverBenched than removing all records with missing values.</p>
    <img src="images/Employee_histograms_symbolic.png" width="auto" height = "600"/> 
    <p>2821: Not knowing the semantics of Education variable, dummification could have been a more adequate codification.</p>
    <img src="images/Employee_class_histogram.png" width="auto" height = "600"/> 
    <p>2822: Balancing this dataset would be mandatory to improve the results.</p>
    <img src="images/Employee_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2823: Given the number of records and that some variables are date, we might be facing the curse of dimensionality.</p>
    <img src="images/Employee_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2824: We face the curse of dimensionality when training a classifier with this dataset.</p>
    <img src="images/Employee_nr_records_nr_variables.png" width="auto" height = "600"/> 
    <p>2825: Balancing this dataset by SMOTE would most probably be preferable over undersampling.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2826: All variables, but the class, should be dealt with as date.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2827: The variable PaymentTier can be seen as ordinal.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2828: The variable Age can be seen as ordinal without losing information.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2829: Variable Age is balanced.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2830: It is clear that variable PaymentTier shows some outliers, but we can’t be sure of the same for variable Age.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2831: Outliers seem to be a problem in the dataset.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2832: Variable JoiningYear shows some outlier values.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2833: Variable ExperienceInCurrentDomain doesn’t have any outliers.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2834: Variable PaymentTier presents some outliers.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2835: At least 50 of the variables present outliers.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2836: The histograms presented show a large number of outliers for most of the numeric variables.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2837: The existence of outliers is one of the problems to tackle in this dataset.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2838: Considering the common semantics for JoiningYear and PaymentTier variables, dummification if applied would increase the risk of facing the curse of dimensionality.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2839: Considering the common semantics for PaymentTier variable, dummification would be the most adequate encoding.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2840: The variable PaymentTier can be coded as ordinal without losing information.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2841: Feature generation based on variable PaymentTier seems to be promising.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2842: Feature generation based on the use of variable ExperienceInCurrentDomain wouldn’t be useful, but the use of JoiningYear seems to be promising.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2843: Given the usual semantics of PaymentTier variable, dummification would have been a better codification.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2844: It is better to drop the variable JoiningYear than removing all records with missing values.</p>
    <img src="images/Employee_histograms_numeric.png" width="auto" height = "600"/> 
    <p>2845: Not knowing the semantics of Age variable, dummification could have been a more adequate codification.</p>
</body> 
</html> 
