{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduvedras/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"['It is clear that variable weight is one of the four most relevant features.', 'The variable triglyceride seems to be one of the five most relevant features.', 'The variable age discriminates between the target values, as shown in the decision tree.', 'It is possible to state that gamma_GTP is the first most discriminative variable regarding the class.', 'Variable height is one of the most relevant variables.', 'Variable SMK_stat_type_cd seems to be relevant for the majority of mining tasks.', 'Variables LDL_chole and hemoglobin seem to be useful for classification tasks.', 'A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.', 'As reported in the tree, the number of False Positive is bigger than the number of False Negatives.', 'The precision for the presented tree is lower than 75%.', 'The number of True Positives is higher than the number of True Negatives for the presented tree.', 'The number of True Negatives is higher than the number of True Positives for the presented tree.', 'The variable SBP seems to be one of the five most relevant features.', 'Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that KNN algorithm classifies (not A, B) as N for any k ≤ 3135.', 'Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that Naive Bayes algorithm classifies (A, not B), as Y.', 'Considering that A=True<=>[SMK_stat_type_cd <= 1.5] and B=True<=>[gamma_GTP <= 35.5], it is possible to state that KNN algorithm classifies (not A, B) as Y for any k ≤ 2793.']\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "filetag = 'gpt4-zeroshot'\n",
    "\n",
    "dataset = load_dataset('eduvedras/QA',split='test',trust_remote_code=True)\n",
    "dataset[0]['Questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The variable SMK_stat_type_cd is used as a splitting feature in the decision tree.'\n",
      "Overfitting occurs in a multi-layer perceptron (MLP) model when the training accuracy continues to increase while the validation accuracy starts to decrease.\n",
      "Overfitting in gradient boosting models typically occurs when the training accuracy continues to increase while the validation accuracy starts to decrease.\n",
      "Overfitting occurs in the chart when the number of estimators increases beyond a certain point.\n",
      "KNN is in overfitting for k less than 17.\n",
      "The overfitting of the decision tree is observed for higher depths in the chart.\n",
      "The accuracy continues to increase as the max depth of the decision tree increases.\n",
      "The sum of the explained variance ratios of the 12 principal components equals 1.\n",
      "'The variable SBP can be discarded without risking losing information.'\n",
      "'Variable waistline does not show any outliers in the boxplots.'\n",
      "The variable 'sex' can be seen as ordinal without losing information.\n",
      "A bar chart highlights the categorical values of the target variable DRK_YN.\n",
      "The number of records in the dataset is equal to the number of variables.\n",
      "The variable SMK_stat_type_cd can be seen as ordinal without losing information.\n",
      "\"The number of False Positives is higher than the number of True Negatives for the presented tree.\"\n",
      "Overfitting occurs when the training accuracy keeps increasing while the validation accuracy starts decreasing in the chart described.\n",
      "The overfitting of gradient boosting occurs as the number of estimators increases beyond a certain point.\n",
      "The overfitting of random forest models always increases as the number of estimators increases.\n",
      "Overfitting occurs in the chart for k=1.\n",
      "The decision tree model starts overfitting when the max depth is above 9.\n",
      "The performance of accuracy decreases while the performance of recall increases as the max depth of the decision tree increases.\n",
      "The sum of the explained variance ratios of the 4 principal components is equal to 100%.\n",
      "'Variables entropy and skewness seem to be useful for classification tasks.'\n",
      "The boxplots show that the variable 'entropy' has a high number of outlier values.\n",
      "A bar chart is used to show the distribution of categorical data.\n",
      "The number of records is typically represented on the x-axis, and the number of variables is represented on the y-axis in a bar chart.\n",
      "Variable skewness presents some outliers.\n",
      "'The variable Nan contains missing values that need to be addressed before analyzing the data.'\n",
      "A multi-line chart can help visualize the overfitting of a MLP model based on accuracy across different numbers of iterations.\n",
      "The model's accuracy generally increases as the number of estimators in gradient boosting increases, showing potential overfitting.\n",
      "The accuracy of the random forest model decreases as the number of estimators increases beyond a certain point.\n",
      "The statement \"The accuracy consistently decreases as the number of neighbors increases in the KNN model\" is true based on the description of the chart.\n",
      "The chart shows that overfitting starts to occur after a maximum depth of 10.\n",
      "The sum of the explained variance ratios of the 4 principal components is equal to 100%.\n",
      "There is high correlation between SepalLengthCm and PetalWidthCm in the dataset.\n",
      "'Variable SepalWidthCm doesn’t have any outliers.'\n",
      "The bar chart is a suitable visualization for displaying categorical data like species distribution.\n",
      "The number of records is typically shown on the x-axis of a bar chart.\n",
      "All variables, but the class, should be dealt with as numeric.\n",
      "The variable int_memory discriminates between the target values, as shown in the decision tree.\n",
      "Overfitting occurs in the mlp model as the number of iterations increases beyond a certain point.\n",
      "The accuracy generally increases with the number of estimators in a gradient boosting model but reaches a point where it starts to decline due to overfitting.\n",
      "The overfitting of random forest models increases as the number of estimators goes beyond a certain threshold.\n",
      "KNN with 1 neighbor is in overfitting.\n",
      "According to the multi-line chart, the accuracy decreases as the max depth of the decision tree increases beyond a certain point.\n",
      "The first 6 principal components explain more variance than the next 6 principal components.\n",
      "The variable 'ram' has a strong positive correlation with 'px_width'.\n",
      "['Variable battery_power shows outliers, but we can’t be sure of the same for variable talk_time.']\n",
      "The variable 'blue' can be coded as ordinal without losing information.\n",
      "The bar chart is used to visualize the distribution of the target variable price_range.\n",
      "The bar chart displays the count of records and variables in the dataset.\n",
      "'Variable ram presents some outliers.'\n",
      "'Variable Pclass is one of the most relevant variables.'\n",
      "True: The chart demonstrates that as the number of iterations increases, the accuracy on the training set continues to improve, while the accuracy on the test set starts to decrease.\n",
      "The accuracy tends to increase as the number of estimators in gradient boosting models increases from 2 to 2002.\n",
      "The overfitting of the random forest model increases as the number of estimators increase.\n",
      "KNN with more than 7 neighbours is in overfitting.\n",
      "According to the multi-line chart representing the overfitting of a decision tree, higher max depths are associated with decreasing accuracy.\n",
      "The accuracy tends to increase with the depth in the decision tree model.\n",
      "The sum of the explained variance ratios of the 5 principal components is equal to 1.\n",
      "The variable Pclass can be discarded without risking losing information.\n",
      "'Variable Age shows outliers' is an ambiguous statement based on the description provided.\n",
      "The variable 'Embarked' can be seen as ordinal without losing information.\n",
      "In this dataset, the variable \"Embarked\" has more missing values than the variable \"Age\".\n",
      "The bar chart displays the distribution of the target variable Survived.\n",
      "The data chart displays the count of records and variables in the dataset.\n",
      "'At least 60 of the variables present outliers.'\n",
      "'It is possible to state that Crunchiness is the second most discriminative variable regarding the class.'\n",
      "A true statement is: \"Overfitting is observable as the number of iterations increases beyond a certain point for MLP models.\"\n",
      "The accuracy of the model decreases as the number of estimators increases for gradient boosting.\n",
      "A random forest model with a higher number of estimators is more likely to suffer from overfitting.\n",
      "KNN with 1 neighbor is in overfitting.\n",
      "The decision tree overfits for depths above 20 in the chart.\n",
      "Increasing the maximum depth of the decision tree leads to an increase in both accuracy and recall in the chart.\n",
      "The sum of the explained variance ratios of the 7 principal components is equal to 1.\n",
      "Variable Sweetness and Juiciness seem to be useful for classification tasks.\n",
      "The boxplots show that variable Acidity doesn’t have any outliers.\n",
      "The bar chart visualizes the distribution of the target variable Quality.\n",
      "The number of records can be visualized using a bar chart.\n",
      "The variable Ripeness can be seen as ordinal.\n",
      "'The variable JoiningYear seems to be one of the four most relevant features.'\n",
      "Overfitting decreases as the number of iterations increases in the MLP model.\n",
      "Overfitting occurs when the accuracy on the training data is much higher than the accuracy on the testing data for gradient boosting with a large number of estimators.\n",
      "In the chart, the accuracy decreases as the number of estimators in the random forest increases.\n",
      "KNN with more than 17 neighbours is in overfitting.\n",
      "The decision tree is in overfitting for depths above 15.\n",
      "As the depth of the decision tree increases, the accuracy may continue to improve while the recall tends to decrease due to overfitting.\n",
      "The bar chart will show the distribution of explained variance across the 4 principal components.\n",
      "The variables 'JoiningYear' and 'Age' show a strong negative correlation in the heatmap.\n",
      "Variable Age presents some outliers.\n",
      "The variable EverBenched can be seen as ordinal without losing information.\n",
      "A bar chart is a useful visualization technique to display categorical data distributions.\n",
      "The number of records is typically represented on the x-axis of the bar chart.\n",
      "The variable Age can be seen as ordinal without losing information.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "for i in range(len(dataset)):\n",
    "    inputs = []\n",
    "    inputs.append({\"role\": \"system\", \"content\": \"You are a data science teacher creating exam questions.\"})\n",
    "    inputs.append({\"role\": \"user\", \"content\": \"Consider the following description of a data chart \\\"\" + dataset[i][\"Description\"] + \"\\\".\"})\n",
    "    inputs.append({\"role\": \"assistant\", \"content\": \"I understand, the data chart is \\\"\" + dataset[i][\"Description\"] + \"\\\".\"})\n",
    "    inputs.append({\"role\": \"user\", \"content\": \"Generate a true or false sentence based on this description, in your answer generate only the sentence. As an example consider the following sentences: \" + dataset[i][\"Questions\"] + \".\"})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=inputs\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    predictions.append(response.choices[0].message.content)\n",
    "    references.append(dataset[i][\"Questions\"][2:-2].split('\", \"'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['It is clear that variable Pclass is one of the five most relevant features.', 'The variable Pclass seems to be one of the four most relevant features.', 'The variable Age discriminates between the target values, as shown in the decision tree.', 'It is possible to state that Age is the first most discriminative variable regarding the class.', 'Variable Pclass is one of the most relevant variables.', 'Variable Age seems to be relevant for the majority of mining tasks.', 'Variables Parch and SibSp seem to be useful for classification tasks.', 'A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.', 'As reported in the tree, the number of False Positive is bigger than the number of False Negatives.', 'The recall for the presented tree is higher than 60%.', 'The number of False Negatives is lower than the number of True Negatives for the presented tree.', 'The number of True Negatives is higher than the number of False Positives for the presented tree.', 'The number of False Negatives is lower than the number of True Negatives for the presented tree.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 0.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 72.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 181.']\", \"['We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.']\", \"['We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.']\", \"['Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.', 'The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.', 'We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.']\", \"['KNN is in overfitting for k less than 17.', 'KNN with 11 neighbour is in overfitting.', 'KNN with more than 7 neighbours is in overfitting.', 'We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.']\", \"['According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.', 'The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.', 'The decision tree is in overfitting for depths above 3.', 'We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.']\", \"['The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.']\", \"['The first 4 principal components are enough for explaining half the data variance.', 'Using the first 4 principal components would imply an error between 10 and 20%.']\", \"['The intrinsic dimensionality of this dataset is 2.', 'One of the variables Fare or Pclass can be discarded without losing information.', 'The variable Pclass can be discarded without risking losing information.', 'Variables Age and Parch are redundant, but we can’t say the same for the pair Fare and Pclass.', 'Variables SibSp and Fare are redundant.', 'From the correlation analysis alone, it is clear that there are relevant variables.', 'Variable Age seems to be relevant for the majority of mining tasks.', 'Variables Parch and Fare seem to be useful for classification tasks.', 'Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.', 'Removing variable Parch might improve the training of decision trees .', 'There is evidence in favour for sequential backward selection to select variable Parch previously than variable Age.']\", \"['Variable Fare is balanced.', 'Those boxplots show that the data is not normalized.', 'It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Pclass.', 'Outliers seem to be a problem in the dataset.', 'Variable Parch shows some outlier values.', 'Variable Parch doesn’t have any outliers.', 'Variable Parch presents some outliers.', 'At least 60 of the variables present outliers.', 'The boxplots presented show a large number of outliers for most of the numeric variables.', 'The existence of outliers is one of the problems to tackle in this dataset.', 'A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.', 'Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.', 'Normalization of this dataset could not have impact on a KNN classifier.', 'Scaling this dataset would be mandatory to improve the results with distance-based methods.']\", \"['All variables, but the class, should be dealt with as date.', 'The variable Embarked can be seen as ordinal.', 'The variable Embarked can be seen as ordinal without losing information.', 'Considering the common semantics for Sex and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.', 'Considering the common semantics for Embarked variable, dummification would be the most adequate encoding.', 'The variable Embarked can be coded as ordinal without losing information.', 'Feature generation based on variable Sex seems to be promising.', 'Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Embarked seems to be promising.', 'Given the usual semantics of Embarked variable, dummification would have been a better codification.', 'It is better to drop the variable Embarked than removing all records with missing values.', 'Not knowing the semantics of Sex variable, dummification could have been a more adequate codification.']\", \"['Discarding variable Age would be better than discarding all the records with missing values for that variable.', 'Dropping all records with missing values would be better than to drop the variables with missing values.', 'Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.', 'There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.', 'Feature generation based on variable Embarked seems to be promising.', 'It is better to drop the variable Embarked than removing all records with missing values.']\", \"['Balancing this dataset would be mandatory to improve the results.']\", \"['Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.', 'We face the curse of dimensionality when training a classifier with this dataset.', 'Balancing this dataset by SMOTE would most probably be preferable over undersampling.']\", \"['All variables, but the class, should be dealt with as date.', 'The variable Age can be seen as ordinal.', 'The variable Fare can be seen as ordinal without losing information.', 'Variable Age is balanced.', 'It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable Age.', 'Outliers seem to be a problem in the dataset.', 'Variable Parch shows some outlier values.', 'Variable Fare doesn’t have any outliers.', 'Variable Age presents some outliers.', 'At least 60 of the variables present outliers.', 'The boxplots presented show a large number of outliers for most of the numeric variables.', 'The existence of outliers is one of the problems to tackle in this dataset.', 'Considering the common semantics for Fare and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.', 'Considering the common semantics for Fare variable, dummification would be the most adequate encoding.', 'The variable Age can be coded as ordinal without losing information.', 'Feature generation based on variable SibSp seems to be promising.', 'Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Pclass seems to be promising.', 'Given the usual semantics of Age variable, dummification would have been a better codification.', 'It is better to drop the variable SibSp than removing all records with missing values.', 'Not knowing the semantics of Parch variable, dummification could have been a more adequate codification.']\"]\n",
      "[['It is clear that variable Pclass is one of the five most relevant features.', 'The variable Pclass seems to be one of the four most relevant features.', 'The variable Age discriminates between the target values, as shown in the decision tree.', 'It is possible to state that Age is the first most discriminative variable regarding the class.', 'Variable Pclass is one of the most relevant variables.', 'Variable Age seems to be relevant for the majority of mining tasks.', 'Variables Parch and SibSp seem to be useful for classification tasks.', 'A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of 10%.', 'As reported in the tree, the number of False Positive is bigger than the number of False Negatives.', 'The recall for the presented tree is higher than 60%.', 'The number of False Negatives is lower than the number of True Negatives for the presented tree.', 'The number of True Negatives is higher than the number of False Positives for the presented tree.', 'The number of False Negatives is lower than the number of True Negatives for the presented tree.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that Naive Bayes algorithm classifies (A,B), as 0.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, B) as 0 for any k ≤ 72.', 'Considering that A=True<=>[Pclass <= 2.5] and B=True<=>[Parch <= 0.5], it is possible to state that KNN algorithm classifies (not A, not B) as 0 for any k ≤ 181.'], ['We are able to identify the existence of overfitting for MLP models trained longer than 700 episodes.'], ['We are able to identify the existence of overfitting for gradient boosting models with more than 1502 estimators.'], ['Results for Random Forests identified as 10, may be explained by its estimators being in underfitting.', 'The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.', 'We are able to identify the existence of overfitting for random forest models with more than 1502 estimators.'], ['KNN is in overfitting for k less than 17.', 'KNN with 11 neighbour is in overfitting.', 'KNN with more than 7 neighbours is in overfitting.', 'We are able to identify the existence of overfitting for KNN models with less than 2 neighbors.'], ['According to the decision tree overfitting chart, the tree with 12 nodes of depth is in overfitting.', 'The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than 5.', 'The decision tree is in overfitting for depths above 3.', 'We are able to identify the existence of overfitting for decision tree models with more than 5 nodes of depth.'], ['The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.'], ['The first 4 principal components are enough for explaining half the data variance.', 'Using the first 4 principal components would imply an error between 10 and 20%.'], ['The intrinsic dimensionality of this dataset is 2.', 'One of the variables Fare or Pclass can be discarded without losing information.', 'The variable Pclass can be discarded without risking losing information.', 'Variables Age and Parch are redundant, but we can’t say the same for the pair Fare and Pclass.', 'Variables SibSp and Fare are redundant.', 'From the correlation analysis alone, it is clear that there are relevant variables.', 'Variable Age seems to be relevant for the majority of mining tasks.', 'Variables Parch and Fare seem to be useful for classification tasks.', 'Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.', 'Removing variable Parch might improve the training of decision trees .', 'There is evidence in favour for sequential backward selection to select variable Parch previously than variable Age.'], ['Variable Fare is balanced.', 'Those boxplots show that the data is not normalized.', 'It is clear that variable Age shows some outliers, but we can’t be sure of the same for variable Pclass.', 'Outliers seem to be a problem in the dataset.', 'Variable Parch shows some outlier values.', 'Variable Parch doesn’t have any outliers.', 'Variable Parch presents some outliers.', 'At least 60 of the variables present outliers.', 'The boxplots presented show a large number of outliers for most of the numeric variables.', 'The existence of outliers is one of the problems to tackle in this dataset.', 'A scaling transformation is mandatory, in order to improve the Naive Bayes performance in this dataset.', 'Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.', 'Normalization of this dataset could not have impact on a KNN classifier.', 'Scaling this dataset would be mandatory to improve the results with distance-based methods.'], ['All variables, but the class, should be dealt with as date.', 'The variable Embarked can be seen as ordinal.', 'The variable Embarked can be seen as ordinal without losing information.', 'Considering the common semantics for Sex and Embarked variables, dummification if applied would increase the risk of facing the curse of dimensionality.', 'Considering the common semantics for Embarked variable, dummification would be the most adequate encoding.', 'The variable Embarked can be coded as ordinal without losing information.', 'Feature generation based on variable Sex seems to be promising.', 'Feature generation based on the use of variable Sex wouldn’t be useful, but the use of Embarked seems to be promising.', 'Given the usual semantics of Embarked variable, dummification would have been a better codification.', 'It is better to drop the variable Embarked than removing all records with missing values.', 'Not knowing the semantics of Sex variable, dummification could have been a more adequate codification.'], ['Discarding variable Age would be better than discarding all the records with missing values for that variable.', 'Dropping all records with missing values would be better than to drop the variables with missing values.', 'Dropping all rows with missing values can lead to a dataset with less than 25% of the original data.', 'There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.', 'Feature generation based on variable Embarked seems to be promising.', 'It is better to drop the variable Embarked than removing all records with missing values.'], ['Balancing this dataset would be mandatory to improve the results.'], ['Given the number of records and that some variables are binary, we might be facing the curse of dimensionality.', 'We face the curse of dimensionality when training a classifier with this dataset.', 'Balancing this dataset by SMOTE would most probably be preferable over undersampling.'], ['All variables, but the class, should be dealt with as date.', 'The variable Age can be seen as ordinal.', 'The variable Fare can be seen as ordinal without losing information.', 'Variable Age is balanced.', 'It is clear that variable Parch shows some outliers, but we can’t be sure of the same for variable Age.', 'Outliers seem to be a problem in the dataset.', 'Variable Parch shows some outlier values.', 'Variable Fare doesn’t have any outliers.', 'Variable Age presents some outliers.', 'At least 60 of the variables present outliers.', 'The boxplots presented show a large number of outliers for most of the numeric variables.', 'The existence of outliers is one of the problems to tackle in this dataset.', 'Considering the common semantics for Fare and Pclass variables, dummification if applied would increase the risk of facing the curse of dimensionality.', 'Considering the common semantics for Fare variable, dummification would be the most adequate encoding.', 'The variable Age can be coded as ordinal without losing information.', 'Feature generation based on variable SibSp seems to be promising.', 'Feature generation based on the use of variable Fare wouldn’t be useful, but the use of Pclass seems to be promising.', 'Given the usual semantics of Age variable, dummification would have been a better codification.', 'It is better to drop the variable SibSp than removing all records with missing values.', 'Not knowing the semantics of Parch variable, dummification could have been a more adequate codification.']]\n"
     ]
    }
   ],
   "source": [
    "print(references)\n",
    "refs = []\n",
    "for ref in references:\n",
    "    refs.append(ref[2:-2].split('\\', \\''))\n",
    "references = refs\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.3227829465045162, 'precisions': [0.6244343891402715, 0.36893203883495146, 0.2513089005235602, 0.1875], 'brevity_penalty': 1.0, 'length_ratio': 1.2556818181818181, 'translation_length': 221, 'reference_length': 176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/eduvedras/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/eduvedras/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/eduvedras/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': 0.5712696485998412}\n",
      "{'rouge1': 0.6075141190227397, 'rouge2': 0.41401813990340575, 'rougeL': 0.5503415103415104, 'rougeLsum': 0.5591957340233202}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "file = open(f\"./resultados-ger-perguntas/{filetag}.txt\", \"a\")\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "file.write(f\"BLEU: {results}\\n\")\n",
    "\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "results = meteor.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "file.write(f\"METEOR: {results}\\n\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "file.write(f\"ROUGE: {results}\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_df = pd.DataFrame(columns=['Image','Prediction','References'])\n",
    "\n",
    "i=0\n",
    "while i < len(dataset):\n",
    "    if ((new_df['Image'] == dataset[i]['Chart_name']) & (new_df['Prediction'] == predictions[i])).any():\n",
    "        i += 1\n",
    "        continue\n",
    "    else:\n",
    "        new_df.loc[len(new_df)] = {'Image': dataset[i]['Chart_name'], 'Prediction': predictions[i], 'References': references[i]}\n",
    "        i += 1\n",
    "    \n",
    "new_df.to_csv(f'./results-final/{filetag}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
