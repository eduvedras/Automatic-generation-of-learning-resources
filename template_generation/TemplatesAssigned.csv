Template;Space1;Space2;Space3;Space4;Charts
Given the number of records and that some variables are [1], we might be facing the curse of dimensionality.;[numeric,binary,date,symbolic];;;;['nr_records_nr_variables']
We face the curse of dimensionality when training a classifier with this dataset.;;;;;['nr_records_nr_variables']
The intrinsic dimensionality of this dataset is [1].;[<pca>];;;;['correlation_heatmap']
All variables, but the class, should be dealt with as [1].;[numeric,binary,date,symbolic];;;;['histograms_numeric','histograms_symbolic']
The variable [1] can be seen as ordinal.;[<all_variables>];;;;['histograms_numeric','histograms_symbolic']
The variable [1] can be seen as ordinal without losing information.;[<all_variables>];;;;['histograms_numeric','histograms_symbolic']
Variable [1] is balanced.;[<variables>];;;;['histograms_numeric','boxplots']
Those boxplots show that the data is not normalized.;;;;;['boxplots']
It is clear that variable [1] shows some outliers, but we can’t be sure of the same for variable [2].;[<variables>];[<variables>];;;['boxplots','histograms_numeric']
Outliers seem to be a problem in the dataset.;;;;;['boxplots','histograms_numeric']
Variable [1] shows [2] outlier values.;[<variables>];[some,a high number of];;;['boxplots','histograms_numeric']
Variable [1] doesn’t have any outliers.;[<variables>];;;;['boxplots','histograms_numeric']
Variable [1] presents some outliers.;[<variables>];;;;['boxplots','histograms_numeric']
At least [1] of the variables present outliers.;[50,60,75,85];;;;['boxplots','histograms_numeric']
The [1] presented show a large number of outliers for most of the numeric variables.;[boxplots,histograms];;;;['boxplots','histograms_numeric']
The existence of outliers is one of the problems to tackle in this dataset.;;;;;['boxplots','histograms_numeric']
One of the variables [1] or [2] can be discarded without losing information.;[<variables>];[<variables>];;;['correlation_heatmap']
The variable [1] can be discarded without risking losing information.;[<variables>];;;;['correlation_heatmap']
Variables [1] and [2] are redundant, but we can’t say the same for the pair [3] and [4].;[<variables>];[<variables>];[<variables>];[<variables>];['correlation_heatmap']
Variables [1] and [2] are redundant.;[<variables>];[<variables>];;;['correlation_heatmap']
From the correlation analysis alone, it is clear that there are relevant variables.;;;;;['correlation_heatmap']
It is clear that variable [1] is one of the [2] most relevant features.;[<variables>];[two,three,four,five];;;['decision_tree']
The variable [1] seems to be one of the [2] most relevant features.;[<variables>];[two,three,four,five];;;['decision_tree']
The variable [1] discriminates between the target values, as shown in the decision tree.;[<variables>];;;;['decision_tree']
It is possible to state that [1] is the [2] most discriminative variable regarding the class.;[<variables>];[first,second];;;['decision_tree']
Variable [1] is one of the most relevant variables.;[<variables>];;;;['decision_tree']
Variable [1] seems to be relevant for the majority of mining tasks.;[<variables>];;;;['decision_tree','correlation_heatmap']
Variables [1] and [2] seem to be useful for classification tasks.;[<variables>];[<variables>];;;['decision_tree','correlation_heatmap']
A scaling transformation is mandatory, in order to improve the [1] performance in this dataset.;[Naive Bayes, KNN];;;;['boxplots']
Applying a non-supervised feature selection based on the redundancy, would not increase the performance of the generality of the training algorithms in this dataset.;;;;;['correlation_heatmap']
Balancing this dataset by SMOTE would most probably be preferable over undersampling.;;;;;['nr_records_nr_variables']
Balancing this dataset would be mandatory to improve the results.;;;;;['class_histogram']
Considering the common semantics for [1] and [2] variables, dummification if applied would increase the risk of facing the curse of dimensionality.;[<all_variables>];[<all_variables>];;;['histograms_numeric','histograms_symbolic']
Considering the common semantics for [1] variable, dummification would be the most adequate encoding.;[<all_variables>];;;;['histograms_numeric','histograms_symbolic']
The variable [1] can be coded as ordinal without losing information.;[<all_variables>];;;;['histograms_numeric','histograms_symbolic']
Discarding variable [1] would be better than discarding all the records with missing values for that variable.;[<all_variables>];;;;['mv']
Dropping all records with missing values would be better than to drop the variables with missing values.;;;;;['mv']
Dropping all rows with missing values can lead to a dataset with less than [1]% of the original data.;[25,30,40];;;;['mv']
There is no reason to believe that discarding records showing missing values is safer than discarding the corresponding variables in this case.;;;;;['mv']
Feature generation based on variable [1] seems to be promising.;[<all_variables>];;;;['mv','histograms_numeric','histograms_symbolic']
Feature generation based on the use of variable [1] wouldn’t be useful, but the use of [2] seems to be promising.;[<all_variables>];[<all_variables>];;;['histograms_numeric','histograms_symbolic']
Given the usual semantics of [1] variable, dummification would have been a better codification.;[<all_variables>];;;;['histograms_numeric','histograms_symbolic']
It is better to drop the variable [1] than removing all records with missing values.;[<all_variables>];;;;['mv','histograms_numeric','histograms_symbolic']
Multiplying ratio and Boolean variables by 100, and variables with a range between 0 and 10 by 10, would have an impact similar to other scaling transformations.;;;;;['boxplots']
Normalization of this dataset could not have impact on a KNN classifier.;;;;;['boxplots']
Not knowing the semantics of [1] variable, dummification could have been a more adequate codification.;[<all_variables>];;;;['histograms_numeric','histograms_symbolic']
Removing variable [1] might improve the training of decision trees .;[<variables>];;;;['correlation_heatmap']
Scaling this dataset would be mandatory to improve the results with distance-based methods.;;;;;['boxplots']
The first [1] principal components are enough for explaining half the data variance.;[<pca>];;;;['pca']
There is evidence in favour for sequential backward selection to select variable [1] previously than variable [2].;[<variables>];[<variables>];;;['correlation_heatmap']
Using the first [1] principal components would imply an error between [2] and [3]%.;[<pca>];[5,10,15];[20,25,30];;['pca']
A smaller tree would be delivered if we would apply post-pruning, accepting an accuracy reduction of [1]%.;[5,6,8,10];;;;['decision_tree']
According to the decision tree overfitting chart, the tree with [1] nodes of depth is in overfitting.;[5,9,16,12,20];;;;['overfitting_decision_tree']
As reported in the tree, the number of False Positive is [1] than the number of False Negatives.;[smaller,bigger];;;;['decision_tree']
KNN is in overfitting for k [1] than [2].;[less,larger];[5,13,17];;;['overfitting_knn']
KNN with [1] neighbour is in overfitting.;[5,7,11];;;;['overfitting_knn']
KNN with [1] than [2] neighbours is in overfitting.;[more,less];[7,15,17];;;['overfitting_knn']
Results for Random Forests identified as [1], may be explained by its estimators being in [2].;[2,3,10];[underfitting,overfitting];;;['overfitting_rf']
The [1] for the presented tree is [2] than [3]%.;[accuracy,recall,precision,specificity];[higher,lower];[60,75,90];;['decision_tree']
The chart reporting the recall for different trees shows that the model enters in overfitting for models with depth higher than [1].;[3,4,5,6,7,8,9,10];;;;['overfitting_decision_tree']
The decision tree is in overfitting for depths above [1].;[3,4,5,6,7,8,9,10];;;;['overfitting_decision_tree']
The difference between recall and accuracy becomes smaller with the depth due to the overfitting phenomenon.;;;;;['overfitting_dt_acc_rec']
The number of [1] reported in the same tree is [2].;[True Positives,False Positives,True Negatives,False Negatives];[10,30,50];;;['decision_tree']
The number of [1] is [2] than the number of [3] for the presented tree.;[True Positives,False Positives,True Negatives,False Negatives];[higher,lower];[True Positives,False Positives,True Negatives,False Negatives];;['decision_tree']
The [1] for the presented tree is [2] than its [3].;[accuracy,recall,precision,specificity];[higher,lower];[accuracy,recall,precision,specificity];;['decision_tree']
The random forests results shown can be explained by the lack of diversity resulting from the number of features considered.;;;;;['overfitting_rf']
We are able to identify the existence of overfitting for decision tree models with more than [1] nodes of depth.;[2,3,4,5,6];;;;['overfitting_decision_tree']
We are able to identify the existence of overfitting for KNN models with less than [1] neighbors.;[2,3,4,5,6];;;;['overfitting_knn']
We are able to identify the existence of overfitting for random forest models with more than [1] estimators.;[502,1002,1502];;;;['overfitting_rf']
We are able to identify the existence of overfitting for gradient boosting models with more than [1] estimators.;[502,1002,1502];;;;['overfitting_gb']
We are able to identify the existence of overfitting for MLP models trained longer than [1] episodes.;[300,700,500];;;;['overfitting_mlp']
Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], the Decision Tree presented classifies [1] as [2].;[(A,B)/(not A, B)/(A, not B)/(not A, not B)];[<target-values>];;;['decision_tree']
Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], it is possible to state that KNN algorithm classifies [1] as [2] for any k ≤ [3].;[(A,B)/(not A, B)/(A, not B)/(not A, not B)];[<target-values>];[<neighbors>];;['decision_tree']
Considering that A=True<=>[ConditionA] and B=True<=>[ConditionB], it is possible to state that Naive Bayes algorithm classifies [1], as [2].;[(A,B)/(not A, B)/(A, not B)/(not A, not B)];[<target-values>];;;['decision_tree']
